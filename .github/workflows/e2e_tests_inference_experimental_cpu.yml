name: E2E tests - inference_models (CPU)
permissions:
  contents: read
on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  e2e-tests-inference-models-gpu:
    runs-on:
      labels: depot-ubuntu-22.04-4
      group: public-depot
    timeout-minutes: 30
    steps:
      - name: ğŸ›ï¸ Checkout
        uses: actions/checkout@v4
      - name: ğŸ“¦ Cache Python packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/uv
          key: ${{ runner.os }}-uv-3.12-${{ hashFiles('inference_models/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-3.12-
      - name: ğŸ“¦ Cache apt packages
        uses: actions/cache@v3
        with:
          path: ~/apt-cache
          key: ${{ runner.os }}-apt-cuda-12-4
          restore-keys: |
            ${{ runner.os }}-apt-cuda-
      - name: ğŸ Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: 3.12
          check-latest: true
      - name: ğŸ“¦ Install dependencies
        working-directory: inference_models
        run: |
          python -m pip install --upgrade pip
          python -m pip install uv
          python -m uv venv
          python -m uv pip install -e .[test,onnx-cpu,torch-cpu,mediapipe] --python .venv
      - name: ğŸ§ª Run E2E tests
        working-directory: inference_models
        timeout-minutes: 20
        run: |
          source .venv/bin/activate
          ROBOFLOW_API_KEY=${{ secrets.LOAD_TEST_PRODUCTION_API_KEY }} python -m pytest -m "e2e_model_inference and not gpu_only" tests/e2e_platform_tests
