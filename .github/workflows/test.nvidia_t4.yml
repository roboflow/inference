name: Code Quality & Regression Tests - NVIDIA T4

on:
  workflow_run:
    workflows: ["Build and Push GPU Container"] # The name of the first workflow
    types:
      - completed # Trigger this workflow when the first workflow completes

jobs:
  build:
    if: ${{ !github.event.act }}
    runs-on: Roboflow-GPU-VM-Runner
    timeout-minutes: 120

    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: ðŸ›Žï¸ Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
      - name: ðŸ“¦ Cache Python packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('requirements/**') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
      - name: Read version from file
        run: echo "VERSION=$(DISABLE_VERSION_CHECK=true python ./inference/core/version.py)" >> $GITHUB_ENV
      - name: ðŸ¦¾ Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements/requirements.test.integration.txt
      - name: Set up Depot CLI
        uses: depot/setup-action@v1
      - name: Retireve build ID
        uses: actions/download-artifact@v3
        with:
          name: build_id
          path: .
      - name: Read build ID
        run: echo "BUILD_ID=$(cat build_id/build_id.txt)" >> $GITHUB_ENV
      - name: Set Repo
        run: echo "INFERENCE_SERVER_REPO=roboflow-inference-server-gpu" >> $GITHUB_ENV
      - name: Pull GPU Image
        run: depot pull $BUILD_ID -t roboflow/${INFERENCE_SERVER_REPO}:${VERSION}
      - name: ðŸ”‹ Start Test Docker - GPU
        run: |
          docker tag roboflow/${INFERENCE_SERVER_REPO}:${VERSION} roboflow/${INFERENCE_SERVER_REPO}:test
          PORT=9101 make start_test_docker_gpu
      - name: ðŸ§ª Regression Tests - GPU
        id: regression_tests
        run: |
          MINIMUM_FPS=25 FUNCTIONAL=true PORT=9101 SKIP_LMM_TEST=True API_KEY=${{ secrets.API_KEY }} asl_instance_segmentation_API_KEY=${{ secrets.ASL_INSTANCE_SEGMENTATION_API_KEY }} asl_poly_instance_seg_API_KEY=${{ secrets.ASL_POLY_INSTANCE_SEG_API_KEY }} bccd_favz3_API_KEY=${{ secrets.BCCD_FAVZ3_API_KEY }} bccd_i4nym_API_KEY=${{ secrets.BCCD_I4NYM_API_KEY }} cats_and_dogs_smnpl_API_KEY=${{ secrets.CATS_AND_DOGS_SMNPL_API_KEY }} coins_xaz9i_API_KEY=${{ secrets.COINS_XAZ9I_API_KEY }} melee_API_KEY=${{ secrets.MELEE_API_KEY }} yolonas_test_API_KEY=${{ secrets.YOLONAS_TEST_API_KEY }} python3 -m pytest tests/inference/integration_tests/
      - name: ðŸš¨ Show server logs on error
        run: docker logs inference-test
        if: ${{ steps.regression_tests.outcome != 'success' }}
      - name: ðŸ§¹ Cleanup Test Docker - GPU
        run: make stop_test_docker
        if: success() || failure()
      - name: ðŸ”‹ Start Test Docker - GPU
        run: |
          PORT=9101 INFERENCE_SERVER_REPO=roboflow-inference-server-gpu make start_test_docker_gpu
      - name: ðŸ§ª Regression CogVLM - GPU
        id: cog_vlm_tests
        run: |
          PORT=9101 API_KEY=${{ secrets.API_KEY }} python3 -m pytest tests/inference/integration_tests/test_cogvlm.py
      - name: ðŸš¨ Show server logs on error
        run: docker logs inference-test
        if: ${{ steps.cog_vlm_tests.outcome != 'success' }}
      - name: ðŸ§¹ Cleanup Test Docker - GPU
        run: make stop_test_docker
        if: success() || failure()
      - name: ðŸ”‹ Start Test Docker - GPU
        run: |
          PORT=9101 INFERENCE_SERVER_REPO=roboflow-inference-server-gpu make start_test_docker_gpu
      - name: ðŸ§ª Regression Paligemma - GPU
        id: paligemma_tests
        run: |
          PORT=9101 melee_API_KEY=${{ secrets.MELEE_API_KEY }} python3 -m pytest tests/inference/integration_tests/test_paligemma.py
      - name: ðŸš¨ Show server logs on error
        run: docker logs inference-test
        if: ${{ steps.paligemma_tests.outcome != 'success' }}
      - name: ðŸ§¹ Cleanup Test Docker - GPU
        run: make stop_test_docker
        if: success() || failure()
      - name: ðŸ”‹ Start Test Docker - GPU
        run: |
          PORT=9101 INFERENCE_SERVER_REPO=roboflow-inference-server-gpu make start_test_docker_gpu
      - name: ðŸ§ª Regression Florence - GPU
        id: florence_tests
        run: |
          PORT=9101 melee_API_KEY=${{ secrets.MELEE_API_KEY }} python3 -m pytest tests/inference/integration_tests/test_florence.py
      - name: ðŸš¨ Show server logs on error
        run: docker logs inference-test
        if: ${{ steps.florence_tests.outcome != 'success' }}
      - name: ðŸ§¹ Cleanup Test Docker - GPU
        run: make stop_test_docker
        if: success() || failure()
      - name: ï¿½ Start Test Docker - SAM2
        run: |
          PORT=9101 INFERENCE_SERVER_REPO=roboflow-inference-server-gpu make start_test_docker_gpu
      - name: ðŸ§ª Regression Tests - SAM2
        id: sam2_tests
        run: |
          PORT=9101 API_KEY=${{ secrets.API_KEY }} SKIP_SAM2_TESTS=False python3 -m pytest tests/inference/integration_tests/test_sam2.py
      - name: ðŸš¨ Show server logs on error
        run: docker logs inference-test
        if: ${{ steps.sam2_tests.outcome != 'success' }}
      - name: ðŸ§¹ Cleanup Test Docker - SAM2
        run: make stop_test_docker
        if: success() || failure()
