# @package _global_

cluster_path: /checkpoint/3dfy/
trellis_ckpts_dir: /checkpoint/3dfy/shared/weights/tdfy/TRELLIS-image-large/ckpts
config_dir: ${cluster_path}/haotang/dev/lidra/etc/lidra/
lidra_ckpt_dir: ${cluster_path}/weiyaowang/lidra 

cluster:
  path:
    weights: /checkpoint/3dfy/shared/weights/

inference_pipeline:
  _target_: lidra.pipeline.inference_pipeline.InferencePipeline
  ss_generator_config_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit/v1-tricks-moge-bf16-ft-Feb-Aprial/config_converted.yaml
  ss_generator_ckpt_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit/v1-tricks-moge-bf16-ft-Feb-Aprial/lightning/version_0/checkpoints/last_converted.ckpt

  slat_generator_config_path: ${lidra_ckpt_dir}/logs/tagged/_submit/fm-trellisDiT-features-ObjV1-ABO-ObjGithub-nvoxels15k-dinov2_vitl14_reg-518-16x8/config.yaml
  slat_generator_ckpt_path: ${trellis_ckpts_dir}/slat_flow_img_dit_L_64l8p2_fp16.safetensors
  ss_decoder_config_path: ${config_dir}/model/backbone/trellis_vae/ss_decoder.yaml
  ss_decoder_ckpt_path: ${trellis_ckpts_dir}/ss_dec_conv3d_16l8_fp16.safetensors
  slat_decoder_gs_config_path: ${config_dir}/model/backbone/trellis_vae/slat_decoder_gs.yaml
  slat_decoder_gs_ckpt_path: ${trellis_ckpts_dir}/slat_dec_gs_swin8_B_64l8gs32_fp16.safetensors
  slat_decoder_mesh_config_path: ${config_dir}/model/backbone/trellis_vae/slat_decoder_mesh.yaml
  slat_decoder_mesh_ckpt_path: ${trellis_ckpts_dir}/slat_dec_mesh_swin8_B_64l8m256c_fp16.safetensors

  layout_model_config_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit//mm-r3v0_2-May7-eitr100-SSI-actuallymoge-8x8/config_converted.yaml
  layout_model_ckpt_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit//mm-r3v0_2-May7-eitr100-SSI-actuallymoge-8x8/lightning/version_0/checkpoints/last_converted.ckpt

  pad_size: 1.0
  dtype: float16
  version: 3dfy_v1

  ss_condition_input_mapping: []  
  layout_condition_input_mapping: ["image", "mask", "rgb_image", "rgb_image_mask"]
  ss_preprocessor:
    _target_: lidra.data.dataset.tdfy.trellis.dataset.PreProcessor
    img_transform:
      _target_: torchvision.transforms.Compose
      transforms:
        - _target_: lidra.data.dataset.tdfy.img_processing.pad_to_square_centered
          _partial_: true
        - _target_: torchvision.transforms.Resize
          size: 518
    mask_transform:
      _target_: torchvision.transforms.Compose
      transforms:
        - _target_: lidra.data.dataset.tdfy.img_processing.pad_to_square_centered
          _partial_: true
        - _target_: torchvision.transforms.Resize
          size: 518
          interpolation: 0 # nearest
    img_mask_joint_transform:
      - _target_: lidra.data.dataset.tdfy.img_and_mask_transforms.crop_around_mask_with_padding
        _partial_: true
        box_size_factor: 1.2
        padding_factor: 0.0 

  pose_decoder_name: ScaleShiftInvariant