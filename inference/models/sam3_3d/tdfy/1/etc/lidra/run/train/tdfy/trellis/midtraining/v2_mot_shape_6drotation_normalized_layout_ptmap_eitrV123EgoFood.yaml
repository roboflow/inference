# @package _global_
defaults:
  - ../fm-trellis-trellis500k-tricks-moge-mot-shape-6drotation-normalized

  # - /data/dataset/tdfy/trellis500k/subset_train@loop.dataloaders.training.dataset.datasets.elephant_v1: elephant_2M
  # - /data/dataset/tdfy/trellis500k/subset_train@loop.dataloaders.training.dataset.datasets.elephant_v2: elephant_sketchfaball
  # - /data/dataset/tdfy/trellis500k/subset_train@loop.dataloaders.training.dataset.datasets.elephant_v3: elephant_shutterstock
  - /data/dataset/tdfy/trellis500k/subset_train@loop.dataloaders.training.dataset.datasets.elephant_ego: elephant_ego
  - /data/dataset/tdfy/trellis500k/subset_train@loop.dataloaders.training.dataset.datasets.elephant_food: elephant_food

  - /data/dataset/tdfy/trellis500k/preprocessor@tdfy.preprocessor: train_keep_bg_w_ptmp_normalized
  - /data/dataset/tdfy/trellis500k/preprocessor@tdfy.val_preprocessor: val_keep_bg_w_ptmp_normalized
  - /model/backbone@tdfy.reverse_fn_pretrained: trellis_dit/mot_image_large_shape_6drotation_normalized_layout_pretrained
  
  - override /data/dataset/tdfy/trellis500k@loop.dataloaders.training.dataset: training_empty
  - override /model/backbone/dit/embedder@tdfy.reverse_fn_pretrained.model.condition_embedder: image_dino_mask_pointmap_embedder_fuser

module:
  pose_target_convention: ScaleShiftInvariant
  keep_preds: ["shape", "6drotation_normalized", "translation", "scale", "translation_scale"]
  # cropped image, cropped mask, full image, mask
  batch_conditions_mapping: 
    image: image
    mask: mask
    pointmap: pointmap
    rgb_image: rgb_image
    rgb_image_mask: rgb_image_mask
    rgb_pointmap: rgb_pointmap
  generator:
    backbone:
      reverse_fn:
        unconditional_handling: "add_flag"
      loss_weights:
        # help optree broadcase, it does not broadcast DictConfig
        _target_: lidra.config.utils.make_dict
        # layout midtraining we only train pose
        shape: 1.0
        6drotation_normalized: ${tdfy.pose_weight}
        translation: ${tdfy.pose_weight}
        scale: ${tdfy.pose_weight}
        translation_scale: ${tdfy.pose_weight}

lr: 1e-4  # Smaller learning rate; tune yourself

loop:
  max_epochs: 200
  dataloaders:
    training:
      dataset:
        datasets:
          # elephant_v1:
          #   preprocessor: ${tdfy.preprocessor}
          #   pointmap_loader:
          #     pointcloud_directory: ${tdfy.pointcloud_base_dir}/moge_corrected_dense
          # elephant_v2:
          #   preprocessor: ${tdfy.preprocessor}
          #   pointmap_loader:
          #     pointcloud_directory: ${tdfy.pointcloud_base_dir}/moge_corrected_dense
          # elephant_v3:
          #   preprocessor: ${tdfy.preprocessor}
          #   pointmap_loader:
          #     pointcloud_directory: ${tdfy.pointcloud_base_dir}/moge_corrected_dense_shutterstock
          elephant_ego:
            preprocessor: ${tdfy.preprocessor}
            pointmap_loader:
              pointcloud_directory: ${tdfy.pointcloud_base_dir}/moge_corrected_dense_ego
          elephant_food:
            preprocessor: ${tdfy.preprocessor}
            pointmap_loader:
              pointcloud_directory: ${tdfy.pointcloud_base_dir}/moge_corrected_dense_food
    validation: null

tdfy:
  pointcloud_base_dir: "/fsx-3dfy-v2/shared/datasets/eitr_data/pointmap/"
  pose_weight: 0.1
  reverse_fn_pretrained:
    checkpoint_path: /fsx-3dfy-v2/ssax/3dfy/lidra/logs/tagged/_submit/v2-mot-shape-6drotation-normalized-midtrain-no-moge/lightning/version_0/checkpoints/last.ckpt
    # checkpoint_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit/v2-mot-shape-6drotation-normalized-midtrain-no-moge/lightning/version_0/checkpoints/last.ckpt
    strict: false
    device: "cpu"
    remove_name: null
    model:
      use_fp16: false
      force_zeros_cond: true
  reverse_fn: ${tdfy.reverse_fn_pretrained}
  train_preprocessor: ${tdfy.preprocessor}

batch_size: 6