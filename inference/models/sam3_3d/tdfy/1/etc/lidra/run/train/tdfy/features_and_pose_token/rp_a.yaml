# @package _global_
defaults:
  - base

  # generator
  - /model/backbone@tdfy.reverse_fn: trellis_dit/image_large_pretrained

  - override /model/backbone/dit/embedder@tdfy.reverse_fn.model.condition_embedder: image_mask_dino_dino

  # dataset
  - override /data/module@loop.dataloaders: tdfy/trellisRP_a

  - override /model/backbone/generator@module.generator.backbone: flow_matching_cfg

  - _self_

batch_size: 6
lr: 1e-5 # smaller learning rate; tune yourself

module:
  batch_conditions_mapping: ["image", "mask"]
  generator:
    backbone:
      reverse_fn:
        backbone: ${tdfy.reverse_fn}

    # scheduler:
    #   _target_: lidra.model.module.tdfy.scheduler.MCCScheduler
    #   _partial_: true
    #   warmup_epochs: 10
    #   max_epochs: ${loop.max_epochs}
    #   min_factor: 1e-3
    #   max_factor: 1.0

    # # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
    # scheduler_kwargs:
    #   interval: epoch
    #   frequency: 1
    #   name: "train/learning_rate"
loop:
  max_epochs: 200
  check_val_every_n_epoch: 25
  deterministic: false
  precision: bf16-mixed
  # dataloaders:
  #   validation: null
  callbacks:
    model_checkpoint2:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      save_on_train_epoch_end: true
      every_n_epochs: 25
      save_top_k: -1

tdfy:
  collator:
    _target_: lidra.data.collator.auto_collate
  reverse_fn:
    checkpoint_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit/cropped-cropped-mask-image-mask-midtraining/lightning/version_0/checkpoints/epoch=24-step=129875-v1.ckpt
    remove_name: ["_base_models.generator.reverse_fn.backbone.pos_emb"]
    strict: false
    model:
      resolution: 16
      patch_size: 1
      include_pose: true
      pose_weight: 0.01
