# @package _global_
defaults:
  - base

  - /optimizer@module.model.optimizer: adam
  - /scheduler@module.model.scheduler: base
  - /model/backbone@module.model: trainable
  - /model/backbone@module.model.backbone: vae/compressed
  - /model/module/vae@module: occupancy

  - override /loop: normal
  - override /hydra: slurm
  - override /strategy: auto
  - override /compute: 8x8
  - override /data/module@loop.dataloaders: tdfy/objaverse

  - _self_

batch_size: 16
lr: 1e-4

loop:
  max_epochs: 100
  # log_every_n_steps: 50
  ## enable in wandb logger
  ## TODO(Pierre) : formalize the project / experiment names globally ?
  # logger:
  #   name: vae-occ
  #   project: lidra-vae-occ

  callbacks:
    model_checkpoint:
      save_top_k: 10
      monitor: "val/loss"

module:
  model:
    optimizer:
      lr: ${lr}
      eps: 1e-6 # for stability

    scheduler:
      path: lidra.model.module.tdfy.scheduler.MCCScheduler
      warmup_epochs: 10
      max_epochs: ${loop.max_epochs}
      min_factor: 1e-3
      max_factor: 1.0

    # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
    scheduler_kwargs:
      interval: epoch
      frequency: 1
      name: "train/learning_rate"

tdfy:
  collator: ${rv.locate:lidra.data.dataset.tdfy.collator.collate_fn}
