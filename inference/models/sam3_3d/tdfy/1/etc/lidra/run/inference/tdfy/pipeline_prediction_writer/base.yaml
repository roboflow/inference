# @package _global_
# Base configuration for pipeline-based inference
# This provides the core structure for running InferencePipeline with various datasets
defaults:
  - /run@_here_: base
  - /hydra: local
  - /cluster: aws-h100
  - /compute: 1x1
  - /strategy: auto
  - /loop: normal
  - /loop/callbacks@loop.callbacks.prediction_writer: cached_prediction_dataset_writer
  - _self_

data: ???

demo_config_path: ???

inference_pipeline: ${rv.merge:${rv.config_node_resolved:${demo_config_path},inference_pipeline},${overwrites.inference_pipeline}}
  # Pipeline configuration - loaded from demo config using config_node_resolved resolver

module:
  _target_: lidra.model.module.tdfy.inference.InferenceWrapper
  pipeline: ${inference_pipeline}
  max_trials: 1 
  
  # Optional: Configure preprocessing and postprocessing
  input_preprocessor:
    _target_: lidra.model.module.tdfy.inference.InferenceWrapper._default_preprocessor
    _partial_: true
    include_pointmap: true
  output_postprocessor: null  # No postprocessing
  
  # Additional kwargs passed to pipeline.run()
  inference_kwargs:
    stage1_only: false
    with_mesh_postprocess: false

loop:
  enable_progress_bar: true
  enable_model_summary: true
  module: ${module}
  checkpoint: null
  dataloaders: ${data}
  callbacks:
    _target_: lidra.config.utils.make_list_from_kwargs

# Predict arguments for loop.predict()
predict:
  args:
    dataloader: ${data.validation}  # Use validation dataloader for prediction

tag:
  name: pipeline_inference
  overwrite: true

overwrites:
  inference_pipeline:
    decode_formats: ["gaussian"]
    device: ${rv.auto_device:}

    # If you face an error in the depth model remove this line.
    depth_model:
      device: ${rv.auto_device:}


# Allow running on dev partition in H200
cluster:
  slurm:
    timeout: 1440 # 1440 minutes = 24 hours
