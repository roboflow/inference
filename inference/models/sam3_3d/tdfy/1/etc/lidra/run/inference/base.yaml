# @package _global_
defaults:
  - /run@_here_: base
  - /hydra: local
  - /compute: 1x1
  - /cluster: fair
  - /strategy: auto
  - /loop: normal

logs:
  path: ???

  # load entire config from trained model
  config: ${rv.config:${logs.path}/config.yaml}
  checkpoint:
    _target_: lidra.model.io.get_last_checkpoint
    path: ${logs.path}/lightning/version_0/checkpoints

overwrites:
  module:
    checkpoint: ${logs.checkpoint}

  callbacks:
    # Merge EMA callback from training config with any overrides
    # If no EMA in checkpoint, use ema_overrides if provided
    ema: ${rv.select_merge:${oc.select:logs.config.loop.callbacks.ema,null},${overwrites.ema_overrides}}
  
  # Allow EMA overrides to be specified (partial overrides supported)
  ema_overrides: null
    # Example: {default_decay_for_eval: medium}

loop:
  enable_progress_bar: true
  module: ${module}
  callbacks: ${rv.merge:${callbacks},${overwrites.callbacks}}

# merge module with checkpoint path
module: ${rv.merge:${logs.config.module},${overwrites.module}}
callbacks:
  _target_: lidra.config.utils.make_list_from_kwargs

predict:
  args: ???

output:
  path: results.json

hydra:
  job:
    name: ${oc.select:tag.name,lidra_inferencet}