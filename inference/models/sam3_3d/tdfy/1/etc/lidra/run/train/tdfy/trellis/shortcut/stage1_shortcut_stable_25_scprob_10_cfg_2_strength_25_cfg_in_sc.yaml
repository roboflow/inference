# @package _global_
defaults:
  - ../finetune/v2_mot_6drotation_normalized_no_layout_inhouse_VAE_Artists_3D.yaml
  - /data/dataset/tdfy/trellis500k/preprocessor@tdfy.val_preprocessor: val_keep_bg

  # generator
  - override /model/backbone/generator@module.generator.backbone: shortcut_cfg

  - _self_


batch_size: 4
lr: 1e-5

module:
  generator:
    backbone:
      batch_mode: true
      self_consistency_cfg_strength: 2.0
      ratio_cfg_samples_in_self_consistency_target: 0.25
      loss_weights:
        # help optree broadcase, it does not broadcast DictConfig
        _target_: lidra.config.utils.make_dict
        shape: 1.0
      reverse_fn:
        _target_: lidra.model.backbone.generator.classifier_free_guidance.ClassifierFreeGuidanceWithExternalUnconditionalProbability
        unconditional_handling: "add_flag"
        backbone: ${tdfy.reverse_fn}
        interval:
          - 0
          - 500
        strength: 2.0
        p_unconditional: 0.1
      training_time_sampler_fn:
        _target_: lidra.model.backbone.generator.flow_matching.model.lognorm_sampler
        _partial_: true
        mean: -1.0
        std: 1.0 
      inference_steps: 2
      rescale_t: 1
      shortcut_loss_weight: 1.0
      self_consistency_prob: 0.25
  validation_dump_type: null # don't save anything to disk during validation


tdfy:
  reverse_fn_pretrained:
    checkpoint_path: /fsx-3dfy-v2/haotang/3dfy/lidra/logs/tagged/_submit/v2_mot_6drotation_normalized_no_layout_inhouse_VAE_Artists_3D_0822/lightning/version_0/checkpoints/last.ckpt
    strict: false
    device: "cpu"
    model:
      use_fp16: false
      force_zeros_cond: true
      is_shortcut_model: true

loop:
  precision: bf16
  max_epochs: 50
  check_val_every_n_epoch: 2
  limit_val_batches: 1.0  # float means 100% of the validation set
  num_sanity_val_steps: -1 # validate on all batches at the beginning
  callbacks:
    model_checkpoint2:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      save_on_train_epoch_end: true
      every_n_epochs: ${loop.check_val_every_n_epoch}
      save_top_k: -1