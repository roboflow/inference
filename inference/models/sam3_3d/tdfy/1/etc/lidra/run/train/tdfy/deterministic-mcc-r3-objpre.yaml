# @package _global_
defaults:
  - base

  - /optimizer@module.model.optimizer: adamw
  - /scheduler@module.model.scheduler: base
  - /model/backbone@module.model: trainable
  - /model/backbone@module.model.backbone: mcc/pretrained

  - override /loop: normal
  - override /hydra: slurm
  - override /strategy: auto
  - override /compute: 8x8

  - override /model/module/tdfy@module: deterministic
  - override /data/module@loop.dataloaders: tdfy/r3

  - _self_

batch_size: 8
lr: 1e-4

loop:
  max_epochs: 100

module:
  model:
    optimizer:
      betas: [0.9, 0.95]
      eps: 1e-6
      lr: ${lr}

    scheduler:
      path: lidra.model.module.tdfy.scheduler.MCCScheduler
      warmup_epochs: 5
      max_epochs: ${loop.max_epochs}
      min_factor: 1e-3
      max_factor: 1.0

    # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
    scheduler_kwargs:
      interval: epoch
      frequency: 1
      name: "train/learning_rate"

    backbone:
      _target_: lidra.model.io.load_model_from_checkpoint
      checkpoint_path: /checkpoint/gleize/lidra/logs/tagged/_submit/pretrained/no-scheduler/lightning/version_0/checkpoints/epoch=99-step=68300.ckpt
      strict: true
      state_dict_key: "state_dict"
      state_dict_fn: 
        _target_: lidra.model.io.remove_prefix_state_dict_fn
        prefix: "_base_models.main." 

dataloaders:
  validation:
    batch_size: 2