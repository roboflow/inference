# @package _global_
defaults:
  - ../finetune/v2_mot_shape_6drotation_normalized_inhouse_vae_ptmap_dropoutCFG_Artist-TrObjScScene.yaml

  # generator
  - override /model/backbone/generator@module.generator.backbone: shortcut_cfg

  - _self_


batch_size: 4
lr: 1e-5

module:
  generator:
    backbone:
      batch_mode: true
      self_consistency_cfg_strength: 2.0
      ratio_cfg_samples_in_self_consistency_target: 0.25
      loss_weights:
        shape: 1.0
        translation: ${tdfy.pose_weight}
      reverse_fn:
        _target_: lidra.model.backbone.generator.classifier_free_guidance.ClassifierFreeGuidanceWithExternalUnconditionalProbability
        unconditional_handling: "add_flag"
        backbone: ${tdfy.reverse_fn}
        interval:
          - 0
          - 500
        strength: 2.0
        p_unconditional: 0.1
      training_time_sampler_fn:
        _target_: lidra.model.backbone.generator.flow_matching.model.lognorm_sampler
        _partial_: true
        mean: -1.0
        std: 1.0 
      inference_steps: 2
      rescale_t: 1
      shortcut_loss_weight: 1.0
      self_consistency_prob: 0.25
      cfg_modalities: ["shape"]
  validation_dump_type: null # don't save anything to disk during validation


tdfy:
  pose_weight: 0.0
  reverse_fn_pretrained:
    # checkpoint_path: /checkpoint/3dfy/haotang/lidra/logs/tagged/_submit/xiang-v2-release1017-artistdpo-planA/lightning/version_0/checkpoints/epoch=4-step=200.ckpt
    checkpoint_path: /checkpoint/3dfy/shared/weights/ssax/v2-release1017-PlanA-layoutOnlyFTNoArtist-warmdown/lightning/version_0/checkpoints/epoch=9-step=6270.ckpt
    strict: false
    device: "cpu"
    model:
      use_fp16: false
      force_zeros_cond: true
      is_shortcut_model: true
      # force_zero_d: true
      freeze_shared_parameters: True
      condition_embedder:
        freeze: True

loop:
  precision: bf16
  max_epochs: 100
  check_val_every_n_epoch: 25
  limit_val_batches: 1.0  # float means 100% of the validation set
  num_sanity_val_steps: -1 # validate on all batches at the beginning
  callbacks:
    ema: null
    model_checkpoint2:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      save_on_train_epoch_end: true
      every_n_epochs: ${loop.check_val_every_n_epoch}
      save_top_k: -1