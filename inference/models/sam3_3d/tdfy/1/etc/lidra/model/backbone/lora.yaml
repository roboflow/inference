_target_: lidra.model.module.base.LoraBackbone
backbone: ???
optimizer: ???
lora_config:
  adapter_name: default
  mixed: false
  peft_config:
    # https://huggingface.co/docs/peft/v0.10.0/en/package_reference/lora#peft.LoraConfig
    _target_: peft.LoraConfig
    peft_type: null
    auto_mapping: null
    base_model_name_or_path: null
    revision: null
    # task_type: null
    inference_mode: false
    # https://github.com/fairinternal/eht_unified_dev/blob/8732b908aa5cbe97506c8934de405123dd2352f1/lib/models.py#L202
    r: 64 # rank
    lora_alpha: 16 
    lora_dropout: 0.1
    target_modules: "all-linear"
    bias: "none"
    task_type: "CAUSAL_LM"
    use_rslora: false
