batch_size: 3
cluster:
  name: aws-a100
  path:
    base: /fsx-3dfy-v2/ryanxiangli/lidra
    cache: ${cluster.path.base}/cache
    cwd: ${hydra:runtime.cwd}
    datasets: /fsx-3dfy-v2/shared/datasets/
    logs: ${cluster.path.base}/logs
    models: ${cluster.path.base}/models
    weights: /fsx-3dfy-v2/shared/weights/
  slurm:
    account: 3dfy
    constraint: null
    mem_per_gpu: 128G
    partition: learn
    qos: 3dfy_high
    timeout: 10000
compute:
  n_cpus_per_gpu: 8
  n_gpus_per_node: 8
  n_nodes: 8
loop:
  _target_: lidra.loop.Loop
  accelerator: auto
  accumulate_grad_batches: 1
  benchmark: false
  callbacks:
    _target_: lidra.config.utils.make_list_from_kwargs
    ema:
      _target_: lidra.callback.ema.EMA
      decays:
        fast: 0.99
        medium: 0.999
        slow: 0.9999
      default_decay_for_eval: slow
      device: null
      force_fp32: true
      recover_existing: true
    garbage_collection:
      _target_: lidra.callback.garbage_collection.GarbageCollection
      collect_cuda: true
      collect_on_epoch_end: true
      frequency: 1000
    gradient_clipping:
      _target_: lidra.callback.tdfy.adaptive_gradient_clipping.AdaptiveGradientClipCallback
      buffer_size: 1000
      clip_percentile: 95.0
      max_clip_val: 1.0
    gradient_scaler:
      _target_: lidra.callback.tdfy.gradual_grad_scale.GradualGradScaleCallback
      initial_scale: 30.0
      scale_growth: 0.001
    learning_rate_monitor:
      _target_: lightning.pytorch.callbacks.LearningRateMonitor
      log_momentum: false
      log_weight_decay: false
      logging_interval: null
    memory_manager:
      _target_: lidra.callback.tdfy.memory_manager.MemoryManagerCallback
      get_size_func:
        _partial_: true
        _target_: lidra.callback.tdfy.memory_manager.get_input_size_slat_flow
      mem_based_func:
        _partial_: true
        _target_: lidra.callback.tdfy.memory_manager.set_grad_ckpt_slat
        slat_path: _base_models.generator.reverse_fn.backbone
      update_every: 500
    model_checkpoint:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      every_n_epochs: 1
      save_last: link
      save_on_train_epoch_end: true
      save_top_k: -1
    model_checkpoint2:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      every_n_epochs: null
      every_n_train_steps: 50
      save_on_train_epoch_end: true
      save_top_k: -1
  dataloaders:
    training:
      _target_: torch.utils.data.DataLoader
      batch_size: ${batch_size}
      collate_fn: ${tdfy.collator}
      dataset:
        _target_: torch.utils.data.dataset.ConcatDataset
        datasets:
          _target_: lidra.config.utils.make_list_from_kwargs
          preference:
            _target_: lidra.data.dataset.tdfy.preference.dataset.PreferenceJobDataset
            latent_loader_dataset:
              _target_: lidra.data.dataset.tdfy.trellis.dataset.PerSubsetDataset
              latent_dir: latents/inhouseDepthVAE_gsplat_epoch=329-step=814400-v1_area_ambient_8k
              latent_loader:
                _partial_: true
                _target_: lidra.data.dataset.tdfy.trellis.latent_loader.load_sparse_feature_latents
                version: depth-vae_500k_ambient4k
              mesh_loader:
                _partial_: true
                _target_: lidra.data.dataset.tdfy.trellis.mesh_loader.load_trellis_mesh
              metadata_fname: metadata_dummy.csv
              path: /fsx-3dfy-v3/shared/datasets/trellis500k/r3_texture_0930_dpo
              pose_loader: null
              preprocessor: ${tdfy.preprocessor}
              split: train
            lose_sampling_strategy: random
            metadata_fname: metadata_pref_filtered_ab_clean.csv
            path: /fsx-3dfy-v3/shared/datasets/trellis500k/r3_texture_0930_dpo
            return_job_metadata: true
            return_mesh: false
            split: train
          preference_1010:
            _target_: lidra.data.dataset.tdfy.preference.dataset.PreferenceJobDataset
            latent_loader_dataset:
              _target_: lidra.data.dataset.tdfy.trellis.dataset.PerSubsetDataset
              latent_dir: latents/inhouseDepthVAE_gsplat_epoch=329-step=814400-v1_area_ambient_8k
              latent_loader:
                _partial_: true
                _target_: lidra.data.dataset.tdfy.trellis.latent_loader.load_sparse_feature_latents
                version: depth-vae_500k_ambient4k
              metadata_fname: metadata_dummy.csv
              path: /fsx-3dfy-v3/shared/datasets/trellis500k/r3_texture_1010_dpo
              pose_loader: null
              preprocessor: ${tdfy.preprocessor}
              split: train
            lose_sampling_strategy: random
            metadata_fname: metadata_pref_filtered_clean_no_unitex.csv
            path: /fsx-3dfy-v3/shared/datasets/trellis500k/r3_texture_1010_dpo
            return_job_metadata: true
            return_mesh: false
            split: train
          preference_aesthetic:
            _target_: lidra.data.dataset.tdfy.preference.dataset.PreferenceJobDataset
            latent_loader_dataset:
              _target_: lidra.data.dataset.tdfy.trellis.dataset.PerSubsetDataset
              latent_dir: latents/inhouseDepthVAE_gsplat_epoch=329-step=814400-v1_area_ambient_8k
              latent_loader:
                _partial_: true
                _target_: lidra.data.dataset.tdfy.trellis.latent_loader.load_sparse_feature_latents
                version: depth-vae_500k_ambient4k
              metadata_fname: metadata_dummy.csv
              path: /fsx-3dfy-v3/shared/datasets/trellis500k/r3_texture_aesthetic_dpo
              pose_loader: null
              preprocessor: ${tdfy.preprocessor}
              split: train
            lose_sampling_strategy: random
            metadata_fname: metadata_pref_filtered_clean_no_unitex.csv
            path: /fsx-3dfy-v3/shared/datasets/trellis500k/r3_texture_aesthetic_dpo
            return_job_metadata: true
            return_mesh: false
            split: train
      drop_last: true
      num_workers: ${compute.n_cpus_per_gpu}
      persistent_workers: false
      pin_memory: true
      shuffle: true
    validation: null
  detect_anomaly: false
  deterministic: false
  devices: auto
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: true
  fast_dev_run: false
  gradient_clip_algorithm: norm
  gradient_clip_val: null
  limit_predict_batches: null
  limit_test_batches: null
  limit_train_batches: null
  limit_val_batches: null
  log_every_n_steps: 1
  logger:
  - _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    log_graph: false
    name: lightning
    save_dir: .
    version: 0
  - _target_: lightning.pytorch.loggers.csv_logs.CSVLogger
    name: csv/
    prefix: ''
    save_dir: .
  - _target_: lightning.pytorch.loggers.wandb.WandbLogger
    anonymous: null
    entity: 3dpref
    group: ''
    id: null
    job_type: ''
    log_model: false
    name: ${tag.name}-${oc.env:SLURM_JOB_ID}-${now:%Y-%m-%d}/${now:%H-%M-%S}
    offline: false
    prefix: ''
    project: preference
    save_dir: .
    tags:
    - ${now:%Y-%m-%d}/${now:%H-%M-%S}
    - ${cluster.name}_slurm_${oc.env:SLURM_JOB_ID}
  max_epochs: 10
  max_steps: -1
  min_epochs: null
  min_steps: null
  module: ${module}
  num_nodes: ${compute.n_nodes}
  plugins: null
  precision: null
  profiler: null
  strategy: ${strategy}
  sync_batchnorm: false
  val_check_interval: null
lr: 1.0e-06
module:
  _target_: lidra.model.module.tdfy.generative_dpo.TrellisDINOFeaturesOnlyDPO
  batch_conditions_mapping:
    coords: coords
    image: image
    mask: mask
    rgb_image: rgb_image
    rgb_image_mask: rgb_image_mask
  batch_preprocessing_fn:
    _partial_: true
    _target_: lidra.model.module.tdfy.batch_transform_utils.trellis.TrellisDINOFeaturesOnly.batch_preprocessing
    deterministic_sampling: true
  generator:
    _target_: lidra.model.module.base.TrainableBackbone
    backbone:
      _target_: lidra.model.backbone.generator.flow_matching.model.FlowMatching
      inference_steps: 12
      reverse_fn:
        _target_: lidra.model.backbone.generator.classifier_free_guidance.ClassifierFreeGuidance
        backbone:
          _target_: lidra.model.backbone.trellis.models.structured_latent_flow.SLatFlowModelTdfyWrapper
          cond_channels: 1024
          condition_embedder: null
          force_zeros_cond: true
          in_channels: 8
          io_block_channels:
          - 128
          mlp_ratio: 4
          model_channels: 1024
          num_blocks: 24
          num_heads: 16
          num_io_res_blocks: 2
          out_channels: 8
          patch_size: 2
          pe_mode: ape
          qk_rms_norm: true
          resolution: 64
          use_fp16: true
        p_unconditional: 0.0
        strength: 0.0
        unconditional_handling: add_flag
      sigma_min: 0.0
      time_scale: 1000.0
      training_time_sampler_fn:
        _partial_: true
        _target_: lidra.model.backbone.generator.flow_matching.model.lognorm_sampler
        mean: -1.0
        std: 1.0
    optimizer:
      _target_: lidra.config.utils.Partial
      betas:
      - 0.9
      - 0.999
      eps: 1.0e-08
      lr: ${lr}
      path: torch.optim.AdamW
      weight_decay: 0.0
  condition_embedder:
    _target_: lidra.model.module.base.TrainableBackbone
    optimizer: null
    backbone:
      _target_: lidra.model.backbone.dit.embedder.embedder_fuser.EmbedderFuser
      embedder_list:
      - - _target_: lidra.model.backbone.dit.embedder.dino.Dino
          dino_model: dinov2_vitl14_reg
          input_size: 518
          normalize_images: true
          prenorm_features: true
        - - - image
            - cropped
          - - rgb_image
            - full
      - - _target_: lidra.model.backbone.dit.embedder.dino.Dino
          dino_model: dinov2_vitl14_reg
          input_size: 518
          normalize_images: true
          prenorm_features: true
        - - - mask
            - cropped
          - - rgb_image_mask
            - full
      projection_net_hidden_dim_multiplier: 4.0
      use_pos_embedding: learned
seed: 42
strategy:
  _target_: lidra.config.utils.make_string
  value: auto
tag:
  name: xiang-v2-texdpo-newckpt-no-unitex
  overwrite: true
tdfy:
  collator:
    _target_: lidra.data.collator.auto_collate
  preprocessor:
    _target_: lidra.data.dataset.tdfy.trellis.dataset.PreProcessor
    img_mask_joint_transform:
    - _partial_: true
      _target_: lidra.data.dataset.tdfy.img_and_mask_transforms.crop_around_mask_with_padding
      box_size_factor: 1.0
      padding_factor: 0.1
    img_transform:
      _target_: torchvision.transforms.Compose
      transforms:
      - _partial_: true
        _target_: lidra.data.dataset.tdfy.img_processing.pad_to_square_centered
      - _target_: torchvision.transforms.Resize
        interpolation: 3
        size: 518
    mask_transform:
      _target_: torchvision.transforms.Compose
      transforms:
      - _partial_: true
        _target_: lidra.data.dataset.tdfy.img_processing.pad_to_square_centered
      - _target_: torchvision.transforms.Resize
        interpolation: 0
        size: 518
  reverse_fn: ${tdfy.reverse_fn_pretrained}
  reverse_fn_pretrained:
    _target_: lidra.model.io.load_model_from_checkpoint
    checkpoint_path: /fsx-3dfy-v3/michelleguo/3dfy/lidra/logs/tagged/_submit/feature-s2-recipe_fov1-rl-amb_rp-sep__sft_r3-1010_aes/lightning/version_0/checkpoints/epoch=28-step=3500.ckpt
    device: cpu
    model:
      _target_: lidra.model.backbone.trellis.models.structured_latent_flow.SLatFlowModelTdfyWrapper
      cond_channels: 1024
      condition_embedder:
        _target_: lidra.model.backbone.dit.embedder.embedder_fuser.EmbedderFuser
        embedder_list:
        - - _target_: lidra.model.backbone.dit.embedder.dino.Dino
            dino_model: dinov2_vitl14_reg
            input_size: 518
            normalize_images: true
            prenorm_features: true
          - - - image
              - cropped
            - - rgb_image
              - full
        - - _target_: lidra.model.backbone.dit.embedder.dino.Dino
            dino_model: dinov2_vitl14_reg
            input_size: 518
            normalize_images: true
            prenorm_features: true
          - - - mask
              - cropped
            - - rgb_image_mask
              - full
        projection_net_hidden_dim_multiplier: 4.0
        use_pos_embedding: learned
      force_zeros_cond: true
      in_channels: 8
      io_block_channels:
      - 128
      mlp_ratio: 4
      model_channels: 1024
      num_blocks: 24
      num_heads: 16
      num_io_res_blocks: 2
      out_channels: 8
      patch_size: 2
      pe_mode: ape
      qk_rms_norm: true
      resolution: 64
      use_fp16: true
    state_dict_fn:
      _target_: lidra.model.io.remove_prefix_state_dict_fn
      prefix: _base_models.generator.reverse_fn.backbone.
    strict: false
validation_batch_size: ${batch_size}
