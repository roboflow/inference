"""Profile GPU and CPU memory usage as CUDA graphs are cached.

Loads yolov8n-640 as a TRT model with dynamic batch size, runs forward passes
with batch sizes 1-16 in a deterministic random order, and after each capture
records both GPU VRAM (driver-level) and process CPU RSS. Produces a two-panel
plot: cumulative memory over capture order, and per-graph delta sorted by batch
size.

Example invocation:
    python profile_cudagraph_vram.py --device cuda:0

    python profile_cudagraph_vram.py --device cuda:0 --shuffle --max-batch-size 32 --output mem.png
"""

import argparse
import gc
import os
import random
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import torch

from inference_models import AutoModel
from inference_models.models.common.trt import TRTCudaGraphLRUCache

MODEL_ID = "yolov8n-640"
MB = 1024 ** 2


def gpu_used_bytes(device: torch.device) -> int:
    free, total = torch.cuda.mem_get_info(device)
    return total - free


def cpu_rss_bytes() -> int:
    with open(f"/proc/{os.getpid()}/statm") as f:
        pages = int(f.read().split()[1])
    return pages * os.sysconf("SC_PAGE_SIZE")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Profile GPU + CPU memory vs. number of cached CUDA graphs.",
    )
    parser.add_argument("--device", type=str, default="cuda:0")
    parser.add_argument("--max-batch-size", type=int, default=16)
    parser.add_argument("--shuffle", action="store_true", help="Randomize batch size order (deterministic seed).")
    parser.add_argument("--output", type=str, default=None)
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    device = torch.device(args.device)

    model = AutoModel.from_pretrained(
        model_id_or_path=MODEL_ID,
        device=device,
        backend="trt",
        batch_size=(1, args.max_batch_size),
        cuda_graph_cache_capacity=args.max_batch_size + 10,
    )

    image = (np.random.rand(640, 640, 3) * 255).astype(np.uint8)
    single_preprocessed, _ = model.pre_process(image)

    model.forward(single_preprocessed, use_cuda_graph=False)
    gc.collect()
    torch.cuda.synchronize(device)
    torch.cuda.empty_cache()

    baseline_gpu = gpu_used_bytes(device)
    baseline_cpu = cpu_rss_bytes()

    model._trt_cuda_graph_cache = TRTCudaGraphLRUCache(
        capacity=args.max_batch_size + 10,
    )

    batch_size_order = list(range(1, args.max_batch_size + 1))
    if args.shuffle:
        random.Random(42).shuffle(batch_size_order)

    batch_sizes = []
    cumulative_gpu_mb = []
    cumulative_cpu_mb = []
    delta_gpu_mb = []
    delta_cpu_mb = []

    prev_gpu = baseline_gpu
    prev_cpu = baseline_cpu

    for i, bs in enumerate(batch_size_order):
        batched = single_preprocessed.expand(bs, -1, -1, -1).contiguous()
        output = model.forward(batched, use_cuda_graph=True)
        del output
        gc.collect()
        torch.cuda.synchronize(device)

        gpu = gpu_used_bytes(device)
        cpu = cpu_rss_bytes()

        batch_sizes.append(bs)
        cumulative_gpu_mb.append((gpu - baseline_gpu) / MB)
        cumulative_cpu_mb.append((cpu - baseline_cpu) / MB)
        delta_gpu_mb.append((gpu - prev_gpu) / MB)
        delta_cpu_mb.append((cpu - prev_cpu) / MB)

        print(
            f"[{i + 1}/{args.max_batch_size}] bs={bs:>2d} | "
            f"GPU: {cumulative_gpu_mb[-1]:>7.1f} MB (+{delta_gpu_mb[-1]:>6.1f}) | "
            f"CPU: {cumulative_cpu_mb[-1]:>7.1f} MB (+{delta_cpu_mb[-1]:>6.1f})"
        )
        prev_gpu = gpu
        prev_cpu = cpu

    autogenerated_name = f"vram_{MODEL_ID}_{'shuffle' if args.shuffle else 'sequential'}.png"
    output_path = Path(args.output) if args.output else Path(autogenerated_name)

    fig, (ax_cum, ax_delta) = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle(
        f"Memory vs. CUDA Graph Count (varying batch size) â€” {MODEL_ID}",
        fontsize=14,
    )

    capture_order = list(range(1, len(batch_sizes) + 1))
    x = np.arange(len(capture_order))
    w = 0.35

    ax_cum.bar(x - w / 2, cumulative_gpu_mb, w, color="steelblue", label="GPU VRAM")
    ax_cum.bar(x + w / 2, cumulative_cpu_mb, w, color="seagreen", label="CPU RSS")
    ax_cum.set_ylabel("Memory above baseline (MB)")
    ax_cum.set_xlabel("Capture order")
    ax_cum.set_xticks(x)
    ax_cum.set_xticklabels(
        [f"{n}\n(bs={bs})" for n, bs in zip(capture_order, batch_sizes)],
        fontsize=7,
    )
    ax_cum.legend()

    sorted_idx = sorted(range(len(batch_sizes)), key=lambda k: batch_sizes[k])
    s_bs = [batch_sizes[k] for k in sorted_idx]
    s_gpu = [delta_gpu_mb[k] for k in sorted_idx]
    s_cpu = [delta_cpu_mb[k] for k in sorted_idx]

    x2 = np.arange(len(s_bs))
    ax_delta.bar(x2 - w / 2, s_gpu, w, color="steelblue", label="GPU VRAM")
    ax_delta.bar(x2 + w / 2, s_cpu, w, color="seagreen", label="CPU RSS")
    ax_delta.set_ylabel("Per-graph memory delta (MB)")
    ax_delta.set_xlabel("Batch size")
    ax_delta.set_xticks(x2)
    ax_delta.set_xticklabels([str(bs) for bs in s_bs])
    ax_delta.legend()

    plt.tight_layout()
    fig.savefig(output_path, dpi=150)
    print(f"\nPlot saved to {output_path}")

    final_gpu = (prev_gpu - baseline_gpu) / MB
    final_cpu = (prev_cpu - baseline_cpu) / MB
    n = len(batch_sizes)
    print(f"\nAfter {n} graphs:")
    print(f"  GPU VRAM: +{final_gpu:.1f} MB total ({final_gpu / n:.1f} MB/graph avg)")
    print(f"  CPU RSS:  +{final_cpu:.1f} MB total ({final_cpu / n:.1f} MB/graph avg)")


if __name__ == "__main__":
    main()
