FROM nvcr.io/nvidia/cuda:13.0.1-cudnn-devel-ubuntu24.04 as builder

WORKDIR /app

RUN rm -rf /var/lib/apt/lists/* && apt-get clean && apt-get update -y && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    libxext6 \
    libopencv-dev \
    uvicorn \
    python3-pip \
    git \
    libgdal-dev \
    libvips-dev \
    wget \
    rustc \
    cargo \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv using standalone installer (installs to /root/.local/bin)
RUN curl -LsSf https://astral.sh/uv/install.sh | env INSTALLER_NO_MODIFY_PATH=1 sh && \
    ln -s /root/.local/bin/uv /usr/local/bin/uv

COPY requirements/requirements.sam.txt \
    requirements/requirements.clip.txt \
    requirements/requirements.http.txt \
    requirements/requirements.gpu.txt \
    requirements/requirements.gaze.txt \
    requirements/requirements.doctr.txt \
    requirements/requirements.groundingdino.txt \
    requirements/requirements.yolo_world.txt \
    requirements/_requirements.txt \
    requirements/requirements.transformers.txt \
    requirements/requirements.pali.flash_attn.txt \
    ./

# Use uv for much faster package installation (WITHOUT onnxruntime-gpu, we'll build from source)
RUN uv pip install --system --break-system-packages \
    -r _requirements.txt \
    -r requirements.sam.txt \
    -r requirements.clip.txt \
    -r requirements.http.txt \
    -r requirements.gaze.txt \
    -r requirements.groundingdino.txt \
    -r requirements.doctr.txt \
    -r requirements.yolo_world.txt \
    -r requirements.transformers.txt \
    jupyterlab \
    "setuptools<=75.5.0" \
    packaging \
    numpy \
    && rm -rf ~/.cache/uv

# Install build tools for ONNX Runtime
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    cmake \
    ninja-build \
    && rm -rf /var/lib/apt/lists/*

# Build ONNX Runtime from source for CUDA 13.0 (using main branch for latest CUDA 13 fixes)
WORKDIR /tmp
RUN git clone --recursive --branch main https://github.com/microsoft/onnxruntime.git /tmp/onnxruntime
WORKDIR /tmp/onnxruntime

# Build ONNX Runtime with CUDA 13 - using exact working config from GitHub PR
RUN ./build.sh \
    --config Release \
    --build_dir build/cuda13 \
    --parallel 16 \
    --use_cuda \
    --cuda_version 13.0 \
    --cuda_home /usr/local/cuda \
    --cudnn_home /usr/local/cuda \
    --build_wheel \
    --build_shared_lib \
    --skip_tests \
    --cmake_generator Ninja \
    --enable_cuda_nhwc_ops \
    --use_binskim_compliant_compile_flags \
    --allow_running_as_root \
    --cmake_extra_defines CMAKE_CUDA_FLAGS="-I/usr/local/cuda/include/cccl" \
    --cmake_extra_defines CMAKE_CXX_FLAGS="-I/usr/local/cuda/include/cccl" \
    --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES="120-real;121-real;121-virtual" \
    --cmake_extra_defines onnxruntime_BUILD_UNIT_TESTS=OFF

# Install the built ONNX Runtime wheel
RUN uv pip install --system --break-system-packages /tmp/onnxruntime/build/cuda13/Release/dist/onnxruntime_gpu-*.whl

# Install GPU-enabled PyTorch 2.9.0 with CUDA 13.0 support
RUN uv pip uninstall torch torchvision torchaudio || true && \
    uv pip install --system --break-system-packages torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# Remove any existing install and clone fresh
RUN uv pip uninstall xformers || true && \
    rm -rf /tmp/xformers && \
    git clone --recursive https://github.com/facebookresearch/xformers.git /tmp/xformers

WORKDIR /tmp/xformers
RUN MAX_JOBS=8 CMAKE_BUILD_PARALLEL_LEVEL=8 uv pip install --system --break-system-packages . --no-build-isolation -v

ENV CMAKE_BUILD_PARALLEL_LEVEL=4
ENV MAX_JOBS=4
ENV SETUPTOOLS_BUILD_PARALLEL=1
# Clone and build FlashAttention from source
WORKDIR /tmp
RUN git clone https://github.com/Dao-AILab/flash-attention.git
WORKDIR /tmp/flash-attention
RUN MAX_JOBS=4 CMAKE_BUILD_PARALLEL_LEVEL=4 uv pip install --system --break-system-packages . --no-build-isolation -v

RUN ls -R /usr/local/lib/python3.12

# Start runtime stage
FROM nvcr.io/nvidia/cuda:13.0.1-cudnn-runtime-ubuntu24.04 as runtime

WORKDIR /app

# Copy Python and installed packages from builder
COPY --from=builder /usr/local/lib/python3.12 /usr/local/lib/python3.12
COPY --from=builder /usr/local/bin /usr/local/bin

# Install runtime dependencies
RUN rm -rf /var/lib/apt/lists/* && apt-get clean && apt-get update -y && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    libxext6 \
    libopencv-dev \
    uvicorn \
    python3-pip \
    git \
    libgdal-dev \
    libvips-dev \
    wget \
    rustc \
    cargo \
    curl \
    && rm -rf /var/lib/apt/lists/*

# uv was already copied from builder stage, no need to reinstall

WORKDIR /build
COPY . .
RUN ln -s /usr/bin/python3 /usr/bin/python
RUN ls -R /usr/local/lib/python3.12
RUN find /usr/local/lib/python3.12 -name "onnxruntime*"
RUN /bin/make create_wheels_for_gpu_notebook

# First install the GPU wheel with no dependencies to avoid re-installing onnxruntime-gpu
RUN uv pip install --system --break-system-packages --no-deps dist/inference_gpu*.whl

# Then install the rest with dependency resolution enabled
RUN uv pip install --system --break-system-packages \
    dist/inference_cli*.whl \
    dist/inference_core*.whl \
    dist/inference_sdk*.whl \
    "setuptools<=75.5.0"


WORKDIR /notebooks
COPY examples/notebooks .

WORKDIR /app/
COPY inference inference
COPY docker/config/gpu_http.py gpu_http.py

ENV VERSION_CHECK_MODE=continuous
ENV PROJECT=roboflow-platform
ENV NUM_WORKERS=1
ENV HOST=0.0.0.0
ENV PORT=9001
ENV WORKFLOWS_STEP_EXECUTION_MODE=local
ENV WORKFLOWS_MAX_CONCURRENT_STEPS=4
ENV API_LOGGING_ENABLED=True
ENV LMM_ENABLED=True
ENV CORE_MODEL_SAM2_ENABLED=True
ENV CORE_MODEL_OWLV2_ENABLED=True
ENV ENABLE_STREAM_API=True
ENV ENABLE_PROMETHEUS=True
ENV STREAM_API_PRELOADED_PROCESSES=2

ENTRYPOINT uvicorn gpu_http:app --workers $NUM_WORKERS --host $HOST --port $PORT
