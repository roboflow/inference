FROM nvcr.io/nvidia/l4t-ml:r32.7.1-py3
#has python 3.8 we manually install python 3.9  in here

ARG DEBIAN_FRONTEND=noninteractive
ENV LANG en_US.UTF-8 

RUN apt-get update -y && apt-get upgrade -y && apt-get install -y \
    build-essential \
    zlib1g-dev \
    libncurses5-dev \
    libgdbm-dev \
    libnss3-dev \
    libssl-dev \ 
    libreadline-dev \
    libffi-dev \
    libsqlite3-dev \
    curl \
    libbz2-dev \
    liblzma-dev \
    software-properties-common \
    && rm -rf /var/lib/apt/lists/*

RUN curl https://sh.rustup.rs -sSfy | sh

RUN wget https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tar.xz && tar -xf Python-3.9.0.tar.xz && rm Python-3.9.0.tar.xz
WORKDIR ./Python-3.9.0
RUN ./configure --enable-shared && make altinstall

RUN apt-get update -y && apt-get upgrade -y && apt-get install -y \
    lshw \
    git \
    python3-pip \
    python3-matplotlib \
    gfortran \
    libatlas-base-dev \
    libsm6 \
    libxext6 \
    wget \
    python3-shapely \
    gdal-bin \
    libgdal-dev \
    python3-setuptools \
    && rm -rf /var/lib/apt/lists/*

COPY requirements/requirements.clip.txt \
    requirements/requirements.http.txt \
    requirements/requirements.doctr.txt \
    requirements/requirements.groundingdino.txt \
    requirements/requirements.sdk.http.txt \
    requirements/requirements.yolo_world.txt \
    requirements/_requirements.txt \
    ./

RUN python3.9 -m pip install --ignore-installed PyYAML && rm -rf ~/.cache/pip

RUN python3.9 -m pip install --upgrade pip "h5py<=3.10.0" && python3.9 -m pip install \
    -r _requirements.txt \
    -r requirements.clip.txt \
    -r requirements.http.txt \
    -r requirements.doctr.txt \
    -r requirements.groundingdino.txt \
    -r requirements.sdk.http.txt \
    -r requirements.yolo_world.txt \
    jupyterlab \
    "setuptools<=75.5.0" \
    "pydantic<=2.10.6" \
    --upgrade \
    && rm -rf ~/.cache/pip

WORKDIR /app/
COPY inference inference
COPY inference_cli inference_cli
COPY inference_sdk inference_sdk
COPY docker/config/gpu_http.py gpu_http.py
COPY .release .release
COPY requirements requirements
COPY Makefile Makefile

RUN make create_inference_cli_whl PYTHON=python3.9
RUN python3.9 -m pip install dist/inference_cli*.whl

# BE CAREFUL, WE ENFORCE numpy 1.x for the sake of compatibility with onnxruntime
RUN python3.9 -m pip uninstall --yes onnxruntime numpy
RUN wget https://nvidia.box.com/shared/static/jmomlpcctmjojz14zbwa12lxmeh2h6o5.whl -O onnxruntime_gpu-1.11.0-cp39-cp39-linux_aarch64.whl
RUN python3.9 -m pip install "numpy<=1.26.4" onnxruntime_gpu-1.11.0-cp39-cp39-linux_aarch64.whl \
    && rm -rf ~/.cache/pip \
    && rm onnxruntime_gpu-1.11.0-cp39-cp39-linux_aarch64.whl

ENV VERSION_CHECK_MODE=continuous \
    PROJECT=roboflow-platform \
    ORT_TENSORRT_FP16_ENABLE=1 \
    ORT_TENSORRT_ENGINE_CACHE_ENABLE=1 \
    PROJECT=roboflow-platform \
    NUM_WORKERS=1 \
    HOST=0.0.0.0 \
    PORT=9001 \
    OPENBLAS_CORETYPE=ARMV8 \
    WORKFLOWS_STEP_EXECUTION_MODE=local \
    WORKFLOWS_MAX_CONCURRENT_STEPS=2 \
    API_LOGGING_ENABLED=True \
    RUNS_ON_JETSON=True \
    ENABLE_PROMETHEUS=True \
    ENABLE_STREAM_API=True \
    STREAM_API_PRELOADED_PROCESSES=2 \
    CORE_MODEL_GAZE_ENABLED=False \
    CORE_MODEL_OWLV2_ENABLED=False \
    CORE_MODEL_PE_ENABLED=False \
    CORE_MODEL_SAM_ENABLED=False \
    CORE_MODEL_SAM2_ENABLED=False \
    CORE_MODEL_TROCR_ENABLED=False \
    DEPTH_ESTIMATION_ENABLED=False \
    FLORENCE2_ENABLED=False \
    MOONDREAM2_ENABLED=False \
    PALIGEMMA_ENABLED=False \
    QWEN_2_5_ENABLED=False \
    QWEN_3_ENABLED=False \
    SMOLVLM2_ENABLED=False \
    PYTHONPATH=/app:$PYTHONPATH

ENTRYPOINT uvicorn gpu_http:app --workers $NUM_WORKERS --host $HOST --port $PORT
