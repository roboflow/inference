FROM nvcr.io/nvidia/cuda:12.9.0-cudnn-devel-ubuntu24.04 as builder

WORKDIR /app

RUN rm -rf /var/lib/apt/lists/* && apt-get clean && apt-get update -y && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    libxext6 \
    libopencv-dev \
    uvicorn \
    python3-pip \
    git \
    libgdal-dev \
    libvips-dev \
    wget \
    rustc \
    cargo \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv using standalone installer (installs to /root/.local/bin)
RUN curl -LsSf https://astral.sh/uv/install.sh | env INSTALLER_NO_MODIFY_PATH=1 sh && \
    ln -s /root/.local/bin/uv /usr/local/bin/uv

COPY requirements/requirements.sam.txt \
    requirements/requirements.clip.txt \
    requirements/requirements.http.txt \
    requirements/requirements.gpu.txt \
    requirements/requirements.gaze.txt \
    requirements/requirements.doctr.txt \
    requirements/requirements.groundingdino.txt \
    requirements/requirements.yolo_world.txt \
    requirements/_requirements.txt \
    requirements/requirements.transformers.txt \
    requirements/requirements.pali.flash_attn.txt \
    ./

# Use uv for much faster package installation (includes onnxruntime-gpu from PyPI)
RUN uv pip install --system --break-system-packages \
    -r _requirements.txt \
    -r requirements.sam.txt \
    -r requirements.clip.txt \
    -r requirements.http.txt \
    -r requirements.gaze.txt \
    -r requirements.groundingdino.txt \
    -r requirements.doctr.txt \
    -r requirements.yolo_world.txt \
    -r requirements.transformers.txt \
    jupyterlab \
    "setuptools<=75.5.0" \
    && rm -rf ~/.cache/uv

# Install build tools
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    ninja-build \
    && rm -rf /var/lib/apt/lists/*

# Install GPU-enabled PyTorch stack (CUDA 12.8)
RUN uv pip uninstall torch torchvision torchaudio || true && \
    uv pip install --system --break-system-packages torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Remove any existing install and clone fresh
RUN uv pip uninstall xformers || true && \
    rm -rf /tmp/xformers && \
    git clone --recursive https://github.com/facebookresearch/xformers.git /tmp/xformers

WORKDIR /tmp/xformers
RUN MAX_JOBS=8 CMAKE_BUILD_PARALLEL_LEVEL=8 uv pip install --system --break-system-packages . --no-build-isolation -v

ENV CMAKE_BUILD_PARALLEL_LEVEL=4
ENV MAX_JOBS=4
ENV SETUPTOOLS_BUILD_PARALLEL=1
# Clone and build FlashAttention from source
WORKDIR /tmp
RUN git clone https://github.com/Dao-AILab/flash-attention.git
WORKDIR /tmp/flash-attention
RUN MAX_JOBS=4 CMAKE_BUILD_PARALLEL_LEVEL=4 uv pip install --system --break-system-packages . --no-build-isolation -v

RUN ls -R /usr/local/lib/python3.12

# Start runtime stage
FROM nvcr.io/nvidia/cuda:12.9.0-cudnn-runtime-ubuntu24.04 as runtime

WORKDIR /app

# Copy Python and installed packages from builder
COPY --from=builder /usr/local/lib/python3.12 /usr/local/lib/python3.12
COPY --from=builder /usr/local/bin /usr/local/bin

# Install runtime dependencies
RUN rm -rf /var/lib/apt/lists/* && apt-get clean && apt-get update -y && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    libxext6 \
    libopencv-dev \
    uvicorn \
    python3-pip \
    git \
    libgdal-dev \
    libvips-dev \
    wget \
    rustc \
    cargo \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install uv in runtime stage for wheel installation
RUN curl -LsSf https://astral.sh/uv/install.sh | env INSTALLER_NO_MODIFY_PATH=1 sh && \
    ln -s /root/.local/bin/uv /usr/local/bin/uv

WORKDIR /build
COPY . .
RUN ln -s /usr/bin/python3 /usr/bin/python
RUN ls -R /usr/local/lib/python3.12
RUN find /usr/local/lib/python3.12 -name "onnxruntime*"
RUN /bin/make create_wheels_for_gpu_notebook

# First install the GPU wheel with no dependencies to avoid re-installing onnxruntime-gpu
RUN uv pip install --system --break-system-packages --no-deps dist/inference_gpu*.whl

# Then install the rest with dependency resolution enabled
RUN uv pip install --system --break-system-packages \
    dist/inference_cli*.whl \
    dist/inference_core*.whl \
    dist/inference_sdk*.whl \
    "setuptools<=75.5.0"


WORKDIR /notebooks
COPY examples/notebooks .

WORKDIR /app/
COPY inference inference
COPY docker/config/gpu_http.py gpu_http.py

ENV VERSION_CHECK_MODE=continuous
ENV PROJECT=roboflow-platform
ENV NUM_WORKERS=1
ENV HOST=0.0.0.0
ENV PORT=9001
ENV WORKFLOWS_STEP_EXECUTION_MODE=local
ENV WORKFLOWS_MAX_CONCURRENT_STEPS=4
ENV API_LOGGING_ENABLED=True
ENV LMM_ENABLED=True
ENV CORE_MODEL_SAM2_ENABLED=True
ENV CORE_MODEL_OWLV2_ENABLED=True
ENV ENABLE_STREAM_API=True
ENV ENABLE_PROMETHEUS=True
ENV STREAM_API_PRELOADED_PROCESSES=2

ENTRYPOINT uvicorn gpu_http:app --workers $NUM_WORKERS --host $HOST --port $PORT
