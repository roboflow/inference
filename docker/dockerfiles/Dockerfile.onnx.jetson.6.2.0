# Build from existing working Jetson 6.2.0 image
# This avoids layer depth issues with the large l4t-ml base image (186 layers)
FROM roboflow/roboflow-inference-server-jetson:jetpack-6.2.0

# Update with latest inference code
COPY inference/ /app/inference/
COPY inference_cli/ /app/inference_cli/
COPY inference_sdk/ /app/inference_sdk/
COPY .release /app/.release
COPY requirements /app/requirements
COPY Makefile /app/Makefile

WORKDIR /app

# Rebuild and reinstall inference packages
RUN make create_inference_cli_whl PYTHON=python3 && \
    pip3 install --index-url https://pypi.jetson-ai-lab.io/jp6/cu126 --force-reinstall dist/inference_cli*.whl

# Environment variables are already set in base image
# Just ensure VERSION_CHECK_MODE is set correctly
ENV VERSION_CHECK_MODE=continuous

EXPOSE 9001
ENTRYPOINT ["sh", "-c", "uvicorn gpu_http:app --workers $NUM_WORKERS --host $HOST --port $PORT"]
