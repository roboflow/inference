# Jetson 6.2.0 with PyTorch/OpenCV compiled from source for numpy 2.x support
# Stage 1: Builder - Compile everything from source
FROM nvcr.io/nvidia/l4t-jetpack:r36.4.0 AS builder

ARG DEBIAN_FRONTEND=noninteractive
ARG CMAKE_VERSION=3.31.10
ARG PYTORCH_VERSION=2.8.0
ARG TORCHVISION_VERSION=0.23.0
ARG OPENCV_VERSION=4.10.0
ARG ONNXRUNTIME_VERSION=1.20.0
ENV LANG=en_US.UTF-8

WORKDIR /build

# Install comprehensive build dependencies
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    ninja-build \
    git \
    wget \
    curl \
    ca-certificates \
    python3-dev \
    python3-pip \
    libopenblas-dev \
    libproj-dev \
    libsqlite3-dev \
    libtiff-dev \
    libcurl4-openssl-dev \
    libssl-dev \
    zlib1g-dev \
    libxext6 \
    libvips-dev \
    pkg-config \
    libjpeg-dev \
    libpng-dev \
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    libv4l-dev \
    libatlas-base-dev \
    liblapack-dev \
    gfortran \
    libhdf5-dev \
    libgflags-dev \
    libgoogle-glog-dev \
    liblmdb-dev \
    libleveldb-dev \
    libsnappy-dev \
    libprotobuf-dev \
    protobuf-compiler \
    && rm -rf /var/lib/apt/lists/*

# Install CMake
RUN wget -q https://github.com/Kitware/CMake/releases/download/v${CMAKE_VERSION}/cmake-${CMAKE_VERSION}-linux-aarch64.sh && \
    chmod +x cmake-${CMAKE_VERSION}-linux-aarch64.sh && \
    ./cmake-${CMAKE_VERSION}-linux-aarch64.sh --prefix=/usr/local --skip-license && \
    rm cmake-${CMAKE_VERSION}-linux-aarch64.sh

# Install pip and numpy 2.x FIRST
RUN python3 -m pip install --upgrade pip setuptools wheel && \
    python3 -m pip install "numpy>=2.0.0,<2.4.0"

# Install uv
RUN curl -LsSf https://astral.sh/uv/install.sh | env INSTALLER_NO_MODIFY_PATH=1 sh && \
    ln -s /root/.local/bin/uv /usr/local/bin/uv

# Compile GDAL from source
WORKDIR /build/gdal
RUN wget https://github.com/OSGeo/gdal/releases/download/v3.11.5/gdal-3.11.5.tar.gz && \
    tar -xzf gdal-3.11.5.tar.gz && \
    cd gdal-3.11.5 && mkdir build && cd build && \
    cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_PYTHON_BINDINGS=OFF && \
    ninja && ninja install && ldconfig && \
    cd ../.. && rm -rf gdal-3.11.5*

# Compile PyTorch from source with Jetson optimizations
WORKDIR /build/pytorch
RUN git clone --recursive --branch v${PYTORCH_VERSION} https://github.com/pytorch/pytorch && \
    cd pytorch && \
    python3 -m pip install -r requirements.txt && \
    export USE_CUDA=1 USE_CUDNN=1 CUDA_HOME=/usr/local/cuda && \
    export CUDNN_LIB_DIR=/usr/lib/aarch64-linux-gnu CUDNN_INCLUDE_DIR=/usr/include && \
    export TORCH_CUDA_ARCH_LIST="8.7" && \
    export USE_MKLDNN=0 USE_OPENMP=0 && \
    export USE_DISTRIBUTED=0 USE_GLOO=0 USE_MPI=0 USE_TENSORPIPE=0 USE_NCCL=0 && \
    export BUILD_TEST=0 && \
    export USE_QNNPACK=0 USE_PYTORCH_QNNPACK=0 USE_XNNPACK=0 USE_NNPACK=0 && \
    export USE_FBGEMM=0 USE_KINETO=0 USE_CUPTI_SO=0 && \
    export USE_FLASH_ATTENTION=1 USE_MEM_EFF_ATTENTION=1 && \
    export USE_MPS=0 USE_ROCM=0 && \
    export PYTORCH_BUILD_VERSION=${PYTORCH_VERSION} PYTORCH_BUILD_NUMBER=1 && \
    export CMAKE_BUILD_TYPE=Release BUILD_SHARED_LIBS=ON USE_PRIORITIZED_TEXT_FOR_LD=1 && \
    export MAX_JOBS=12 && \
    python3 setup.py bdist_wheel && \
    python3 -m pip install dist/torch-*.whl

# Compile torchvision from source (with retry for transient GitHub issues)
WORKDIR /build/torchvision
RUN for i in 1 2 3; do \
        git clone --branch v${TORCHVISION_VERSION} https://github.com/pytorch/vision && break || sleep 60; \
    done && \
    cd vision && \
    export BUILD_VERSION=${TORCHVISION_VERSION} TORCH_CUDA_ARCH_LIST="8.7" && \
    export FORCE_CUDA=1 CMAKE_BUILD_TYPE=Release USE_PRIORITIZED_TEXT_FOR_LD=1 && \
    python3 setup.py bdist_wheel && \
    python3 -m pip install dist/torchvision-*.whl

# Set CUDA environment variables globally (needed for onnxruntime build)
ENV CUDA_HOME=/usr/local/cuda \
    PATH=/usr/local/cuda/bin:$PATH \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Build onnxruntime-gpu from source with TensorRT support
WORKDIR /build/onnxruntime
RUN git clone --recursive --branch v${ONNXRUNTIME_VERSION} https://github.com/microsoft/onnxruntime.git && \
    cd onnxruntime && \
    sed -i 's/be8be39fdbc6e60e94fa7870b280707069b5b81a/32b145f525a8308d7ab1c09388b2e288312d8eba/g' cmake/deps.txt && \
    sed -i '/PATCH_COMMAND.*nsync_1.26.0.patch/s|$| \&\& sed -i "s/cmake_minimum_required.*/cmake_minimum_required(VERSION 3.5)/" CMakeLists.txt|' cmake/external/onnxruntime_external_deps.cmake && \
    ./build.sh \
        --config Release \
        --build_dir build/cuda12 \
        --parallel 12 \
        --use_cuda \
        --cuda_version 12.6 \
        --cuda_home /usr/local/cuda \
        --cudnn_home /usr/lib/aarch64-linux-gnu \
        --use_tensorrt \
        --tensorrt_home /usr/lib/aarch64-linux-gnu \
        --build_wheel \
        --build_shared_lib \
        --skip_tests \
        --cmake_generator Ninja \
        --compile_no_warning_as_error \
        --allow_running_as_root \
        --cmake_extra_defines CMAKE_CUDA_ARCHITECTURES="87" \
        --cmake_extra_defines onnxruntime_BUILD_UNIT_TESTS=OFF \
        --cmake_extra_defines onnxruntime_USE_FLASH_ATTENTION=OFF \
        --cmake_extra_defines onnxruntime_USE_MEMORY_EFFICIENT_ATTENTION=OFF \
        --cmake_extra_defines CMAKE_POLICY_VERSION_MINIMUM=3.5 && \
    python3 -m pip install --break-system-packages build/cuda12/Release/dist/onnxruntime_gpu-*.whl

# Install flash-attn package (requires CUDA-enabled PyTorch already installed)
# Limit MAX_JOBS to prevent timeout during CUDA kernel compilation
# Using 2.8.3 - removed legacy features (rotary, xentropy, fused_softmax, ft_attention) for faster builds
RUN export MAX_JOBS=4 && \
    python3 -m pip install --break-system-packages --no-build-isolation flash-attn==2.8.3

# Copy requirements files (done after compilation to improve cache efficiency)
WORKDIR /build/reqs
COPY requirements/_requirements.txt \
     requirements/requirements.http.txt \
     requirements/requirements.clip.txt \
     requirements/requirements.transformers.txt \
     requirements/requirements.sam.txt \
     requirements/requirements.gaze.txt \
     requirements/requirements.groundingdino.txt \
     requirements/requirements.yolo_world.txt \
     requirements/requirements.doctr.txt \
     requirements/requirements.sdk.http.txt \
     requirements/requirements.easyocr.txt \
     requirements/requirements.jetson.txt \
     ./

# Install Python dependencies (torch/torchvision/numpy/onnxruntime already installed from source)
RUN uv pip install --system --break-system-packages --index-strategy unsafe-best-match \
    -r _requirements.txt \
    -r requirements.http.txt \
    -r requirements.clip.txt \
    -r requirements.transformers.txt \
    -r requirements.sam.txt \
    -r requirements.gaze.txt \
    -r requirements.groundingdino.txt \
    -r requirements.yolo_world.txt \
    -r requirements.doctr.txt \
    -r requirements.sdk.http.txt \
    -r requirements.easyocr.txt \
    -r requirements.jetson.txt \
    "pycuda>=2025.0.0,<2026.0.0" \
    "setuptools<=75.5.0" \
    packaging \
    && rm -rf ~/.cache/uv

# Build inference packages
WORKDIR /build/inference
COPY . .
RUN ln -sf /usr/bin/python3 /usr/bin/python && \
    python -m pip install wheel twine requests && \
    rm -f dist/* && \
    python .release/pypi/inference.core.setup.py bdist_wheel && \
    python .release/pypi/inference.gpu.setup.py bdist_wheel && \
    python .release/pypi/inference.cli.setup.py bdist_wheel && \
    python .release/pypi/inference.sdk.setup.py bdist_wheel && \
    python -m pip install --no-deps dist/inference_gpu*.whl && \
    python -m pip install \
        dist/inference_core*.whl \
        dist/inference_cli*.whl \
        dist/inference_sdk*.whl \
        "setuptools<=75.5.0"

# Remove test directories, development tools, and unnecessary packages to reduce image size
# This happens AFTER all package installations to ensure everything gets cleaned
# Note: Keep numpy/torch test dirs (public APIs depend on them), keep .pyi files (lazy_loader needs them)
RUN cd /usr/local/lib/python3.10/dist-packages && \
    find . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true && \
    rm -rf debugpy* jupyterlab* jupyter_* notebook* ipython* ipykernel* || true && \
    rm -rf onnx/backend/test onnx/test || true && \
    rm -rf scipy/*/tests pandas/tests || true && \
    rm -rf */examples */benchmarks */docs || true && \
    rm -rf skimage/data || true

# Stage 2: Runtime
FROM nvcr.io/nvidia/l4t-cuda:12.6.11-runtime

ARG DEBIAN_FRONTEND=noninteractive
ENV LANG=en_US.UTF-8

WORKDIR /app

RUN ln -sf /usr/bin/python3 /usr/bin/python

# Install runtime dependencies
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends \
    file \
    libopenblas0 \
    libproj22 \
    libsqlite3-0 \
    libtiff5 \
    libcurl4 \
    libssl3 \
    zlib1g \
    libgomp1 \
    python3 \
    python3-pip \
    libxext6 \
    libvips42 \
    libglib2.0-0 \
    libsm6 \
    libjpeg-turbo8 \
    libpng16-16 \
    libexpat1 \
    ca-certificates \
    curl \
    libv4l-0 \
    libavcodec58 \
    libavformat58 \
    libswscale5 \
    libatlas3-base \
    && rm -rf /var/lib/apt/lists/*

# Copy GDAL (skip headers - not needed in runtime)
COPY --from=builder /usr/local/bin/gdal* /usr/local/bin/
COPY --from=builder /usr/local/bin/ogr* /usr/local/bin/
COPY --from=builder /usr/local/bin/gnm* /usr/local/bin/
COPY --from=builder /usr/local/lib/libgdal* /usr/local/lib/
COPY --from=builder /usr/local/share/gdal /usr/local/share/gdal

ENV GDAL_DATA=/usr/local/share/gdal
ENV LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

# Copy CUDA libraries (only versioned files, then create symlinks to save ~2GB)
COPY --from=builder /usr/lib/aarch64-linux-gnu/libcudnn*.so.*.*.* /usr/local/cuda/lib64/
COPY --from=builder /usr/local/cuda/targets/aarch64-linux/lib/libcupti*.so.*.*.* /usr/local/cuda/lib64/
COPY --from=builder /usr/local/cuda/targets/aarch64-linux/lib/libnvToolsExt*.so.*.*.* /usr/local/cuda/lib64/

# TensorRT libraries (only versioned files, then create symlinks)
COPY --from=builder /usr/lib/aarch64-linux-gnu/libnvinfer*.so.*.*.* /usr/local/cuda/lib64/
COPY --from=builder /usr/lib/aarch64-linux-gnu/libnvonnxparser*.so.*.*.* /usr/local/cuda/lib64/
COPY --from=builder /usr/lib/aarch64-linux-gnu/libnvparsers*.so.*.*.* /usr/local/cuda/lib64/

# Create symlinks for cuDNN and TensorRT (saves ~2GB by not duplicating files)
RUN cd /usr/local/cuda/lib64 && \
    for lib in *.so.*.*.*; do \
        base=$(echo $lib | sed 's/\.so\..*/\.so/') && \
        maj=$(echo $lib | sed 's/\(.*\.so\.[0-9]*\)\..*/\1/') && \
        ln -sf $lib $maj && \
        ln -sf $lib $base; \
    done

ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
RUN ldconfig

# Copy Python packages
COPY --from=builder /usr/local/lib/python3.10/dist-packages /usr/local/lib/python3.10/dist-packages
COPY --from=builder /usr/local/bin/inference /usr/local/bin/inference

ENV PYTHONPATH=/usr/local/lib/python3.10/dist-packages:$PYTHONPATH

# Copy application code
COPY inference inference
COPY inference_cli inference_cli
COPY inference_sdk inference_sdk
COPY docker/config/gpu_http.py gpu_http.py

# Environment variables
ENV VERSION_CHECK_MODE=once \
    CORE_MODEL_SAM2_ENABLED=True \
    NUM_WORKERS=1 \
    HOST=0.0.0.0 \
    PORT=9001 \
    ORT_TENSORRT_FP16_ENABLE=1 \
    ORT_TENSORRT_ENGINE_CACHE_ENABLE=1 \
    ORT_TENSORRT_ENGINE_CACHE_PATH=/tmp/ort_cache \
    ORT_TENSORRT_MAX_WORKSPACE_SIZE=4294967296 \
    ORT_TENSORRT_BUILDER_OPTIMIZATION_LEVEL=5 \
    ONNXRUNTIME_EXECUTION_PROVIDERS=[TensorrtExecutionProvider] \
    REQUIRED_ONNX_PROVIDERS=TensorrtExecutionProvider \
    OPENBLAS_CORETYPE=ARMV8 \
    LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1 \
    WORKFLOWS_STEP_EXECUTION_MODE=local \
    WORKFLOWS_MAX_CONCURRENT_STEPS=4 \
    API_LOGGING_ENABLED=True \
    DISABLE_WORKFLOW_ENDPOINTS=false

LABEL org.opencontainers.image.description="Inference Server - Jetson 6.2.0 (PyTorch from source, numpy 2.x)"

EXPOSE 9001

ENTRYPOINT ["/bin/sh", "-c", "python3 -m uvicorn gpu_http:app --workers $NUM_WORKERS --host $HOST --port $PORT"]
