[project]
name = "inference-exp"
version = "0.12.0"
description = "Experimental vresion of inference package which is supposed to evolve into inference 1.0"
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
  "numpy",
  "torch>=2.0.0,<3.0.0",
  "torchvision",
  "opencv-python>=4.8.1.78",
  "requests>=2.32.0,<3.0.0",
  "supervision>=0.26.0",
  "backoff~=2.2.0",
  "transformers>=4.50.0,<5.0.0",
  "timm>=1.0.0,<2.0.0",
  "accelerate>=1.0.0,<2.0.0",
  "einops>=0.7.0,<1.0.0",
  "peft>=0.11.1,<0.16.0",
  "num2words~=0.5.14",
  "bitsandbytes>=0.42.0,<0.47.0; sys_platform != 'darwin'",
  "pyvips>=2.2.3,<3.0.0",
  "rf-clip==1.1",
  "python-doctr[torch]>=0.10.0,<=0.11.0",
  "packaging>=24.0.0",
  "rich>=13.0.0,<15.0.0",
  "pydantic>=2.0.0,<3.0.0",
  "filelock>=3.12.0,<4.0.0"
]

[project.optional-dependencies]
torch-cpu = [
  "torch>=2.0.0,<3.0.0",
  "torchvision"
]
torch-cu118 = [
  "torch>=2.0.0,<3.0.0",
  "torchvision",
  "pycuda>=2025.0.0,<2026.0.0",
]
torch-cu124 = [
  "torch>=2.0.0,<3.0.0",
  "torchvision",
  "pycuda>=2025.0.0,<2026.0.0",
]
torch-cu126 = [
  "torch>=2.0.0,<3.0.0",
  "torchvision",
  "pycuda>=2025.0.0,<2026.0.0",
]
torch-cu128 = [
  "torch>=2.0.0,<3.0.0",
  "torchvision",
  "pycuda>=2025.0.0,<2026.0.0",
]
torch-jp6-cu126 = [
  "numpy<2.0.0",
  "torch>=2.0.0,<3.0.0",
  "torchvision",
  "flash-attn==2.7.4.post1",
  "pycuda>=2025.0.0,<2026.0.0",
]
onnx-cpu = [
  "onnxruntime>=1.15.1,<1.23.0"
]
onnx-cu118 = [
  "onnxruntime-gpu>=1.15.1,<1.23.0; platform_system != 'darwin'",
  "pycuda>=2025.0.0,<2026.0.0; platform_system != 'darwin'",
]
onnx-cu12 = [
  "onnxruntime-gpu>=1.17.0,<1.23.0; platform_system != 'darwin'",
  "pycuda>=2025.0.0,<2026.0.0; platform_system != 'darwin'",
]
onnx-jp6-cu126 = [
  "numpy<2.0.0",
  "onnxruntime-gpu>=1.17.0,<1.24.0; platform_system != 'darwin'",
  "pycuda>=2025.0.0,<2026.0.0; platform_system != 'darwin'",
]
mediapipe = [
  "mediapipe>=0.9,<0.11"
]
grounding-dino = [
  "rf_groundingdino==0.2.0"
]
flash-attn = [
  "flash-attn==2.7.4.post1"
]
trt10 = [
  "tensorrt>=10.0.0,<11.0.0; platform_system == 'Linux' or platform_system == 'Windows'",
  "tensorrt-cu12>=10.0.0,<11.0.0; platform_system == 'Linux' or platform_system == 'Windows'",
  "tensorrt-lean>=10.0.0,<11.0.0; platform_system == 'Linux' or platform_system == 'Windows'",
  "tensorrt-lean-cu12>=10.0.0,<11.0.0; platform_system == 'Linux' or platform_system == 'Windows'",
   "pycuda>=2025.0.0,<2026.0.0",
]
test = [
  "pytest>=8.0.0",
  "pytest-timeout==2.4.0",
  "pytest-xdist>=3.0.0",
  "pytest-timeout",
  "requests-mock>=1.12.1"
]

[tool.uv.sources]
torch = [
  { index = "torch-cpu", extra = "torch-cpu" },
  { index = "torch-cu118", extra = "torch-cu118" },
  { index = "torch-cu124", extra = "torch-cu124" },
  { index = "torch-cu128", extra = "torch-cu128" },
  { index = "jp6-cu126", extra = "torch-jp6-cu126" },
]
torchvision = [
  { index = "torch-cpu", extra = "torch-cpu" },
  { index = "torch-cu118", extra = "torch-cu118" },
  { index = "torch-cu124", extra = "torch-cu124" },
  { index = "torch-cu128", extra = "torch-cu128" },
  { index = "jp6-cu126", extra = "torch-jp6-cu126" },
]

onnxruntime-gpu = [
  { index = "onnx-cu118", extra = "onnx-cu118"},
  { index = "jp6-cu126", extra = "onnx-jp6-cu126" },
]

flash-attn = [
   { index = "jp6-cu126", extra = "torch-jp6-cu126" },
]

[[tool.uv.index]]
name = "torch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "torch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[[tool.uv.index]]
name = "torch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "torch-cu118"
url = "https://download.pytorch.org/whl/cu118"
explicit = true

[[tool.uv.index]]
name = "jp6-cu126"
url = "https://pypi.jetson-ai-lab.io/jp6/cu126/+simple"
explicit = true

[[tool.uv.index]]
name = "onnx-cu118"
url = "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-11/pypi/simple/"
explicit = true

[tool.setuptools.packages.find]
include = ["inference_exp*"]
exclude = ["development*", "dockerfiles*", "tests*"]

[[tool.uv.dependency-metadata]]
name = "tensorrt-cu12"
requires-dist = ["tensorrt-cu12-bindings", "tensorrt-cu12-libs"]

[[tool.uv.dependency-metadata]]
name = "tensorrt-lean-cu12"
requires-dist = ["tensorrt-lean-cu12-bindings", "tensorrt-lean-cu12-libs"]

[[tool.uv.dependency-metadata]]
name = "tensorrt-cu12-libs"
requires-dist = ["nvidia-cuda-runtime-cu12"]

[[tool.uv.dependency-metadata]]
name = "tensorrt-lean-cu12-libs"
requires-dist = ["nvidia-cuda-runtime-cu12"]

[[tool.uv.dependency-metadata]]
name = "nvidia-cuda-runtime-cu12"

[[tool.uv.dependency-metadata]]
name = "tensorrt-cu12-bindings"

[[tool.uv.dependency-metadata]]
name = "tensorrt-lean-cu12-bindings"


[tool.uv]
no-build-isolation-package = ["flash-attn"]
conflicts = [
  [
    { extra = "torch-cpu" },
    { extra = "torch-cu118" },
    { extra = "torch-cu124" },
    { extra = "torch-cu126" },
    { extra = "torch-cu128" },
    { extra = "torch-jp6-cu126" },
  ],
  [
    { extra = "onnx-cpu" },
    { extra = "onnx-cu118" },
    { extra = "onnx-cu12" },
    { extra = "onnx-jp6-cu126" },
  ],
]

[tool.setuptools.package-data]
inference_exp = [
  "models/rfdetr/dinov2_configs/*.json"
]
