{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>template: home.html title: Roboflow Inference hide:     - toc     - navigation</p>"},{"location":"api/","title":"OpenAPI Spec","text":"<p>When the Inference Server is running, it provides OpenAPI documentation at the <code>/docs</code> endpoint for use in development.</p> <p>Below is the OpenAPI specification for the Inference Server for the current release version.</p> <p></p>"},{"location":"contributing/","title":"Contributing to the Roboflow Inference Server \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to Inference.</p> <p>We welcome any contributions to help us improve the quality of Inference.</p> <p>Note</p> <p>Interested in seeing a new model in Inference? File a Feature Request on GitHub.</p>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add support for running inference on a new model.</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new task or feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features","text":"<p>The Inference Server provides a standard interface through which you can work with computer vision models. With Inference Server, you can use state-of-the-art models with your own weights without having to spend time installing dependencies, configuring environments, and writing inference code.</p> <p>We welcome contributions that add support for new models to the project. Before you begin, please make sure that another contributor has not already begun work on the model you want to add. You can check the project README for our roadmap on adding more models.</p> <p>You will need to add documentation for your model and link to it from the <code>inference-server</code> README. You can add a new page to the <code>docs/models</code> directory that describes your model and how to use it. You can use the existing model documentation as a guide for how to structure your documentation.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Create a new branch that describes your changes (i.e. <code>line-counter-docs</code>). Push your changes to the branch on your fork and then submit a pull request to this repository.</p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> </ol> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#code-quality","title":"\ud83e\uddf9 Code quality","text":"<p>We provide two handy commands inside the <code>Makefile</code>, namely:</p> <ul> <li><code>make style</code> to format the code</li> <li><code>make check_code_quality</code> to check code quality (PEP8 basically)</li> </ul>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"cookbooks/","title":"Cookbooks","text":"Inference Cookbooks <p>Read our getting started guides that show how to solve specific problems using Inference.</p> <p></p> <p></p> <p></p>"},{"location":"download/","title":"Downloading Roboflow InferenceThanks for Downloading Roboflow Inference!","text":"<p>Getting Started with Roboflow Inference</p> <p>Your download should start automatically. If it doesn't, click here to download manually.</p>"},{"location":"models/","title":"Models","text":"<p>Roboflow Inference enables you to deploy computer vision models faster than ever.</p> <p>With a <code>pip install inference</code> and <code>inference server start</code>, you can start a server to run a fine-tuned model on images, videos, and streams.</p> <p>Inference supports running object detection, classification, instance segmentation, and foundation models (i.e. SAM, CLIP).</p> <p>You can train and deploy your own custom model or use one of the 50,000+ fine-tuned models shared by the Roboflow Universe community.</p> <p>You can run Inference on an edge device like an NVIDIA Jetson, or on cloud computing platforms like AWS, GCP, and Azure.</p> <p>Get started with our \"Run your first model\" guide</p> <p>Here is an example of a model running on a video using Inference:</p>"},{"location":"models/#features","title":"\ud83d\udcbb Features","text":"<p>Inference provides a scalable method through which you can use computer vision models.</p> <p>Inference is backed by:</p> <ul> <li> <p>A server, so you don\u2019t have to reinvent the wheel when it comes to serving your model to disperate parts of your application.</p> </li> <li> <p>Standard APIs for computer vision tasks, so switching out the model weights and architecture can be done independently of your application code.</p> </li> <li> <p>Model architecture implementations, which implement the tensor parsing glue between images and predictions for supervised models that you've fine-tuned to perform custom tasks.</p> </li> <li> <p>A model registry, so your code can be independent from your model weights &amp; you don't have to re-build and re-deploy every time you want to iterate on your model weights.</p> </li> <li> <p>Data management integrations, so you can collect more images of edge cases to improve your dataset &amp; model the more it sees in the wild.</p> </li> </ul> <p>And more!</p>"},{"location":"models/#install-pip-vs-docker","title":"\ud83d\udccc Install pip vs Docker:","text":"<ul> <li>pip: Installs <code>inference</code> into your Python environment. Lightweight, good for Python-centric projects.</li> <li>Docker: Packages <code>inference</code> with its environment. Ensures consistency across setups; ideal for scalable deployments.</li> </ul>"},{"location":"models/#install","title":"\ud83d\udcbb install","text":""},{"location":"models/#with-onnx-cpu-runtime","title":"With ONNX CPU Runtime:","text":"<p>For CPU powered inference:</p> <pre><code>pip install inference\n</code></pre> <p>or</p> <pre><code>pip install inference-cpu\n</code></pre>"},{"location":"models/#with-onnx-gpu-runtime","title":"With ONNX GPU Runtime:","text":"<p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"models/#without-onnx-runtime","title":"Without ONNX Runtime:","text":"<p>Roboflow Inference uses Onnxruntime as its core inference engine. Onnxruntime provides an array of different execution providers that can optimize inference on differnt target devices. If you decide to install onnxruntime on your own, install inference with:</p> <pre><code>pip install inference-core\n</code></pre> <p>Alternatively, you can take advantage of some advanced execution providers using one of our published docker images.</p>"},{"location":"models/#extras","title":"Extras:","text":"<p>Some functionality requires extra dependencies. These can be installed by specifying the desired extras during installation of Roboflow Inference. e.x. <code>pip install inference[extra]</code></p> extra description <code>clip</code> Ability to use the core <code>CLIP</code> model (by OpenAI) <code>gaze</code> Ability to use the core <code>Gaze</code> model <code>http</code> Ability to run the http interface <code>sam</code> Ability to run the core <code>Segment Anything</code> model (by Meta AI) <code>doctr</code> Ability to use the core <code>doctr</code> model (by Mindee) <code>easy-ocr</code> Ability to use the core <code>easy-ocr</code> model (by JaidedAI) <code>transformers</code> Ability to use transformers based multi-modal models such as <code>Florence2</code> and <code>PaliGemma</code>. To use Florence2 you will need to manually install flash_attn <p>Note: Both CLIP and Segment Anything require PyTorch to run. These are included in their respective dependencies however PyTorch installs can be highly environment dependent. See the official PyTorch install page for instructions specific to your enviornment.</p> <p>Example install with CLIP dependencies:</p> <pre><code>pip install inference[clip]\n</code></pre>"},{"location":"models/#docker","title":"\ud83d\udc0b docker","text":"<p>You can learn more about Roboflow Inference Docker Image build, pull and run in our documentation.</p> <ul> <li>Run on x86 CPU:</li> </ul> <pre><code>docker run -it --net=host roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <ul> <li>Run on NVIDIA GPU:</li> </ul> <pre><code>docker run -it --network=host --gpus=all roboflow/roboflow-inference-server-gpu:latest\n</code></pre> \ud83d\udc49 more docker run options  - Run on arm64 CPU:  <pre><code>docker run -p 9001:9001 roboflow/roboflow-inference-server-arm-cpu:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `4.x` (Deprecated):  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `5.x`:  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `6.x`:  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson-6.0.0:latest\n</code></pre> <p></p>"},{"location":"models/#cli","title":"\ud83d\udcdf CLI","text":"<p>To use the CLI you will need python 3.7 or higher. To ensure you have the correct version of python, run <code>python --version</code> in your terminal. To install python, follow the instructions here.</p> <p>After you have python installed, install the pypi package <code>inference-cli</code> or <code>inference</code>:</p> <pre><code>pip install inference-cli\n</code></pre> <p>From there you can run the inference server. See Docker quickstart via CLI for more information.</p> <pre><code>inference server start\n</code></pre> <p>CLI supports also stopping the server via: <pre><code>inference server stop\n</code></pre></p> <p>To use the CLI to make inferences, first find your project ID and model version number in Roboflow.</p> <p>See more detailed documentation on HTTP Inference quickstart.</p> <pre><code>inference infer {image_path} \\\n    --project-id {project_id} \\\n    --model-version {model_version} \\\n    --api-key {api_key}\n</code></pre>"},{"location":"models/#enterprise-license","title":"Enterprise License","text":"<p>With a Roboflow Inference Enterprise License, you can access additional Inference features, including:</p> <ul> <li>Server cluster deployment</li> <li>Active learning</li> <li>YOLOv5 and YOLOv8 model sub-license</li> </ul> <p>To learn more, contact the Roboflow team.</p>"},{"location":"models/#more-roboflow-open-source-projects","title":"More Roboflow Open Source Projects","text":"Project Description supervision General-purpose utilities for use in computer vision projects, from predictions filtering and display to object tracking to model evaluation. Autodistill Automatically label images for use in training computer vision models. Inference (this project) An easy-to-use, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. Notebooks Tutorials for computer vision tasks, from training state-of-the-art models to tracking objects to counting objects in a zone. Collect Automated, intelligent data collection powered by CLIP."},{"location":"resources/","title":"More Resources","text":"<ul> <li> <p>GitHub Repo</p> <p>The Inference repository contains all of the source code behind the project. It's the place to go to file issues, make feature requests, or contribute.</p> <p> Go to Repo</p> </li> <li> <p>Roboflow Docs</p> <p>Roboflow's Platform docs cover using all of the complementary cloud-connected features to the open source Inference project.</p> <p> Go to Docs</p> </li> <li> <p>Roboflow Forum</p> <p>Get help from and connect with a community of computer vision enthusiasts</p> <p> Go to Forum</p> </li> <li> <p>Roboflow YouTube</p> <p>Regular videos with computer Vision news, tutorials, product updates, and more</p> <p> Go to Channel</p> </li> </ul>"},{"location":"video-tutorials/","title":"Video tutorials","text":"Tutorial: Build an AI-Powered Self-Serve Checkout Created: 2 Feb 2025 <p>         Make a computer vision app that identifies different pieces of hardware, calculates       the total cost, and records the results to a database.       </p> Tutorial: Intro to Workflows Created: 6 Jan 2025 <p>         Learn how to build and deploy Workflows for common use-cases like detecting          vehicles, filtering detections, visualizing results, and calculating dwell          time on a live video stream.       </p> Tutorial: Build a Smart Parking System Created: 27 Nov 2024 <p>         Build a smart parking lot management system using Roboflow Workflows!         This tutorial covers license plate detection with YOLOv8, object tracking         with ByteTrack, and real-time notifications with a Telegram bot.       </p>"},{"location":"enterprise/enterprise/","title":"Enterprise Features","text":"<p>Roboflow Enterprise customers have access to advanced Inference features. These include:</p> <ol> <li>Active learning: Actively collect data from your production line for use in training new, more accurate models over time.</li> <li>Parallel processing server: Run requests in parallel to achieve increased throughput and lower latency when running inference on models.</li> <li>A license to run Inference on more than one device.</li> </ol> <p>To learn more about Roboflow's enterprise offerings, contact the sales team.</p>"},{"location":"enterprise/manage_devices/","title":"Manage devices","text":"<p>The Roboflow team is working on support for remote device management with Inference. </p> <p>To learn more about this feature, contact the Roboflow sales team.</p>"},{"location":"enterprise/parallel_processing/","title":"Parallel Inference","text":"<p>Note</p> <p>This feature is only available for Roboflow Enterprise users. Contact our sales team to learn more about Roboflow Enterprise.</p> <p>You can run multiple models in parallel with Inference with parallel processing, a version of Roboflow Inference that processes inference requests asynchronously.</p> <p>Inference Parallel supports all the same features as Roboflow Inference, with the exception that it does not support Core models (i.e. CLIP and SAM).</p> <p>With Inference Parallel, preprocessing, auto batching, inference, and post processing all run in separate threads to increase server FPS throughput.</p> <p>Separate requests to the same model will be batched on the fly as allowed by <code>$MAX_BATCH_SIZE</code>, and then response handling will occurr independently. Images are passed via Python's SharedMemory module to maximize throughput.</p> <p>These changes result in as much as a 76% speedup on one measured workload.</p>"},{"location":"enterprise/parallel_processing/#how-to-use-inference-with-parallel-processing","title":"How To Use Inference with Parallel Processing","text":"<p>You can run Inference with Parallel Processing in two ways: via the CLI or via Docker.</p> BashDocker <p>First, build the parallel server</p> <pre><code>./inference/enterprise/parallel/build.sh\n</code></pre> <p>Then, run the server:</p> <pre><code>./inference/enterprise/parallel/run.sh\n</code></pre> <p>A message will appear in the terminal indicating that the server is running and ready for use.</p> <p>We provide a container at Docker Hub that you can pull using <code>docker pull roboflow/roboflow-inference-server-gpu-parallel:latest</code>. If you are pulling a pinned tag, be sure to change the <code>$TAG</code> variable in <code>run.sh</code>.</p>"},{"location":"enterprise/parallel_processing/#benchmarking","title":"Benchmarking","text":"<p>We evaluated the performance of Inference Parallel on a variety of models from Roboflow Universe. We compared the performance of Inference Parallel to the latest version of Inference Server (0.9.5.rc) on the same hardware.</p> <p>We ran our tests on a computer with eight cores and one GPU. Instance segmentation metrics are calculated using <code>\"mask_decode_mode\": \"fast\"</code> in the request body. Requests are posted concurrently with a parallelism of 1000.</p> <p>Here are the results of our tests:</p> Workspace Model Model Type split 0.9.5.rc FPS 0.9.5.parallel FPS senior-design-project-j9gpp nbafootage/3 object-detection train 30.2 fps 44.03 fps niklas-bommersbach-jyjff dart-scorer/8 object-detection train 26.6 fps 47.0 fps geonu water-08xpr/1 instance-segmentation valid 4.7 fps 6.1 fps university-of-bradford detecting-drusen_1/2 instance-segmentation train 6.2 fps 7.2 fps fy-project-y9ecd cataract-detection-viwsu/2 classification train 48.5 fps 65.4 fps hesunyu playing-cards-ir0wr/1 classification train 44.6 fps 57.7 fps <p>Inference with parallel processing enabled achieved higher FPS on every test. On eome models, the FPS increase by using Inference with parallel processing was greater than 10 FPS.</p>"},{"location":"enterprise/stream_management_api/","title":"Stream Management","text":"<p>[!IMPORTANT]  We require a Roboflow Enterprise License to use this in production. See inference/enterpise/LICENSE.txt for details.</p>"},{"location":"enterprise/stream_management_api/#overview","title":"Overview","text":"<p>This feature is designed to cater to users requiring the execution of inference to generate predictions using Roboflow object-detection models, particularly when dealing with online video streams. It enhances the functionalities of the familiar <code>inference.Stream()</code> and <code>InferencePipeline()</code> interfaces, as found in the open-source version of the library, by introducing a sophisticated management layer. The inclusion of additional capabilities empowers users to remotely manage the state of inference pipelines through the HTTP management interface integrated into this package.</p> <p>This functionality proves beneficial in various scenarios, including but not limited to:</p> <ul> <li>Performing inference across multiple online video streams simultaneously.</li> <li>Executing inference on multiple devices that necessitate coordination.</li> <li>Establishing a monitoring layer to oversee video processing based on the <code>inference</code> package.</li> </ul>"},{"location":"enterprise/stream_management_api/#design","title":"Design","text":""},{"location":"enterprise/stream_management_api/#example-use-case","title":"Example use-case","text":"<p>Joe aims to monitor objects within the footage captured by a fleet of IP cameras installed in his factory. After successfully training an object-detection model on the Roboflow platform, he is now prepared for deployment. With four cameras in his factory, Joe opts for a model that is sufficiently compact, allowing for over 30 inferences per second on his Jetson devices. Considering this computational budget per device, Joe determines that he requires two Jetson devices to efficiently process footage from all cameras, anticipating an inference throughput of approximately 15 frames per second for each video source.</p> <p>To streamline the deployment, Joe chooses to deploy Stream Management containers to all available Jetson devices within his local network. This setup enables him to communicate with each Jetson device via HTTP, facilitating the orchestration of processing tasks. Joe develops a web app through which he can send commands to the devices and retrieve metrics regarding the statuses of the video streams.</p> <p>Finally, Joe implements a UDP server capable of receiving predictions, leveraging the <code>supervision</code> package to effectively track objects in the footage. This comprehensive approach allows Joe to manage and monitor the object-detection process seamlessly across his fleet of Jetson devices.</p>"},{"location":"enterprise/stream_management_api/#how-to-run","title":"How to run?","text":""},{"location":"enterprise/stream_management_api/#in-docker-using-docker-compose","title":"In docker - using <code>docker compose</code>","text":"<p>The most prevalent use-cases are conveniently encapsulated with Docker Compose configurations, ensuring readiness for immediate use. Nevertheless, in specific instances where custom configuration adjustments are required within Docker containers, such as passing camera devices, alternative options may prove more suitable.</p>"},{"location":"enterprise/stream_management_api/#cpu-based-devices","title":"CPU-based devices","text":"<pre><code>repository_root$ docker compose -f ./docker/dockerfiles/stream-management-api.compose-cpu.yaml up\n</code></pre>"},{"location":"enterprise/stream_management_api/#gpu-based-devices","title":"GPU-based devices","text":"<pre><code>repository_root$ docker compose -f ./docker/dockerfiles/stream-management-api.compose-gpu.yaml up\n</code></pre>"},{"location":"enterprise/stream_management_api/#jetson-devices-jetpack-511","title":"Jetson devices (<code>JetPack 5.1.1</code>)","text":"<pre><code>repository_root$ docker-compose -f ./docker/dockerfiles/stream-management-api.compose-jetson.5.1.1.yaml up\n</code></pre> <p>Disclaimer: At Jetson devices, some operations (like container bootstrap or initialisation of model) takes more time than for other ones. In particular - docker compose definition in current form do not define active awaiting TCP socket port to be opened by Stream Manager - which means that initial requests to HTTP API may be responded with HTTP 503.</p>"},{"location":"enterprise/stream_management_api/#in-docker-running-api-and-stream-manager-containers-separately","title":"In docker - running API and stream manager containers separately","text":""},{"location":"enterprise/stream_management_api/#run","title":"Run","text":""},{"location":"enterprise/stream_management_api/#cpu-based-devices_1","title":"CPU-based devices","text":"<pre><code>docker run -d --name stream_manager --network host roboflow/roboflow-inference-stream-manager-cpu:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#gpu-based-devices_1","title":"GPU-based devices","text":"<pre><code>docker run -d --name stream_manager --network host --runtime nvidia roboflow/roboflow-inference-stream-manager-gpu:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#jetson-devices-jetpack-511_1","title":"Jetson devices (<code>JetPack 5.1.1</code>)","text":"<pre><code>docker run -d --name stream_manager --network host --runtime nvidia roboflow/roboflow-inference-stream-manager-jetson-5.1.1:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#configuration-parameters","title":"Configuration parameters","text":""},{"location":"enterprise/stream_management_api/#stream-management-api","title":"Stream Management API","text":"<ul> <li><code>STREAM_MANAGER_HOST</code> - hostname for stream manager container (alter with container name if <code>--network host</code> not used   or used against remote machine)</li> <li><code>STREAM_MANAGER_PORT</code> - port to communicate with stream manager (must match with stream manager container)</li> </ul>"},{"location":"enterprise/stream_management_api/#stream-manager","title":"Stream Manager","text":"<ul> <li><code>PORT</code> - port at which server will be running</li> <li>one can mount volume under container's <code>/tmp/cache</code> to enable permanent storage of models - for faster inference   pipelines initialisation</li> <li>at the level of this container the connectivity to camera must be enabled - so if device passing to docker must   happen - it should happen at this stage</li> </ul>"},{"location":"enterprise/stream_management_api/#build-optional","title":"Build (Optional)","text":""},{"location":"enterprise/stream_management_api/#stream-management-api_1","title":"Stream Management API","text":"<pre><code>docker build -t roboflow/roboflow-inference-stream-management-api:dev -f docker/dockerfiles/Dockerfile.stream_management_api .\n</code></pre>"},{"location":"enterprise/stream_management_api/#stream-manager_1","title":"Stream Manager","text":"<pre><code>docker build -t roboflow/roboflow-inference-stream-manager-{device}:dev -f docker/dockerfiles/Dockerfile.onnx.{device}.stream_manager .\n</code></pre>"},{"location":"enterprise/stream_management_api/#bare-metal-deployment","title":"Bare-metal deployment","text":"<p>In some cases, it would be required to deploy the application at host level. This is possible, although client must resolve the environment in a way that is presented in Stream Manager and Stream Management API dockerfiles appropriate for specific platform. Once this is done the following command should be run:</p> <pre><code>repository_root$ python -m inference.enterprise.stream_management.manager.app  # runs manager\n</code></pre> <pre><code>repository_root$ python -m inference.enterprise.stream_management.api.app  # runs management API\n</code></pre>"},{"location":"enterprise/stream_management_api/#how-to-integrate","title":"How to integrate?","text":"<p>After running <code>roboflow-inference-stream-management-api</code> container, HTTP API will be available under <code>http://127.0.0.1:8080</code> (given that default configuration is used).</p> <p>One can call <code>wget http://127.0.0.1:8080/openapi.json</code> to get OpenApi specification of API that can be rendered here</p> <p>Example Python client is provided below:</p> <pre><code>import requests\nfrom typing import Optional\n\nURL = \"http://127.0.0.1:8080\"\n\ndef list_pipelines() -&gt; dict:\n    response = requests.get(f\"{URL}/list_pipelines\")\n    return response.json()\n\n\ndef get_pipeline_status(pipeline_id: str) -&gt; dict:\n    response = requests.get(f\"{URL}/status/{pipeline_id}\")\n    return response.json()\n\n\ndef pause_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/pause/{pipeline_id}\")\n    return response.json()\n\n\ndef resume_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/resume/{pipeline_id}\")\n    return response.json()\n\ndef terminate_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/terminate/{pipeline_id}\")\n    return response.json()\n\ndef initialise_pipeline(\n    video_reference: str,\n    model_id: str,\n    api_key: str,\n    sink_host: str,\n    sink_port: int,\n    max_fps: Optional[int] = None,\n) -&gt; dict:\n    response = requests.post(\n        f\"{URL}/initialise\",\n        json={\n            \"type\": \"init\",\n            \"sink_configuration\": {\n                \"type\": \"udp_sink\",\n                \"host\": sink_host,\n                \"port\": sink_port,\n            },\n            \"video_reference\": video_reference,\n            \"model_id\": model_id,\n            \"api_key\": api_key,\n            \"max_fps\": max_fps,\n\n        },\n    )\n    return response.json()\n</code></pre>"},{"location":"enterprise/stream_management_api/#important-notes","title":"Important notes","text":"<ul> <li>Please remember that <code>initialise_pipeline()</code> must be filled with <code>video_reference</code> and <code>sink_configuration</code>   in such a way, that any resource (video file / camera device) or URI (stream reference, sink reference) must be   reachable from Stream Manager environment! For instance - in some cases inside docker containers <code>localhost</code> will   be bound into container localhost not the localhost of the machine hosting container.</li> </ul>"},{"location":"enterprise/stream_management_api/#developer-notes","title":"Developer notes","text":"<p>The pivotal element of the implementation is the Stream Manager component, operating as an application in single-threaded, TCP-server mode. It systematically processes requests received from a TCP socket, taking on the responsibility of spawning and overseeing processes that run the <code>InferencePipelineManager</code>. Communication between the <code>InferencePipelineManager</code> processes and the main process of the Stream Manager occurs through multiprocessing queues. These queues facilitate the exchange of input commands and the retrieval of results.</p> <p>Requests directed to the Stream Manager are sequentially handled in blocking mode, ensuring that each request must conclude before the initiation of the next one.</p>"},{"location":"enterprise/stream_management_api/#communication-protocol-requests","title":"Communication protocol - requests","text":"<p>Stream Manager accepts the following binary protocol in communication. Each communication payload contains:</p> <pre><code>[HEADER: 4B, big-endian, not signed - int value with message size][MESSAGE: utf-8 serialised json of size dictated by header]\n</code></pre> <p>Message must be a valid JSON after decoding and represent valid command.</p>"},{"location":"enterprise/stream_management_api/#list_pipelines-command","title":"<code>list_pipelines</code> command","text":"<pre><code>{\n  \"type\": \"list_pipelines\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#init-command","title":"<code>init</code> command","text":"<pre><code>{\n  \"type\": \"init\",\n  \"model_id\": \"some/1\",\n  \"video_reference\": \"rtsp://192.168.0.1:554\",\n  \"sink_configuration\": {\n    \"type\": \"udp_sink\",\n    \"host\": \"192.168.0.3\",\n    \"port\": 9999\n  },\n  \"api_key\": \"YOUR-API-KEY\",\n  \"max_fps\": 16,\n  \"model_configuration\": {\n    \"type\": \"object-detection\",\n    \"class_agnostic_nms\": true,\n    \"confidence\": 0.5,\n    \"iou_threshold\": 0.4,\n    \"max_candidates\": 300,\n    \"max_detections\": 3000\n  },\n  \"video_source_properties\": {\n    \"frame_width\": 1920,\n    \"frame_height\": 1080,\n    \"fps\": 30\n  }\n}\n</code></pre> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"enterprise/stream_management_api/#terminate-command","title":"<code>terminate</code> command","text":"<pre><code>{\n  \"type\": \"terminate\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#pause-command","title":"<code>pause</code> command","text":"<pre><code>{\n  \"type\": \"mute\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#resume-command","title":"<code>resume</code> command","text":"<pre><code>{\n  \"type\": \"resume\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#status-command","title":"<code>status</code> command","text":"<pre><code>{\n  \"type\": \"status\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#communication-protocol-responses","title":"Communication protocol - responses","text":"<p>Stream Manager, for each request that can be processed (without timeout or source disconnection), will return the result in a format:</p> <pre><code>[HEADER: 4B, big-endian, not signed - int value with result size][RESULT: utf-8 serialised json of size dictated by header]\n</code></pre> <p>Structure of result:</p> <ul> <li><code>request_id</code> - field with random string representing request id assigned by Stream Manager - to ease debugging</li> <li><code>pipeline_id</code> - if command from request can be associated to specific pipeline - its ID will be denoted in response</li> <li><code>response</code> - payload of operation response</li> </ul> <p>Each <code>response</code> has the <code>status</code> key with two values possible: <code>success</code> or <code>failure</code> to denote operation status. Each failed response contain <code>error_type</code> key to dispatch error handling and optional fields <code>error_class</code> and <code>error_message</code> representing inner details of error.</p> <p>Content of successful responses depends on type of operation.</p>"},{"location":"enterprise/stream_management_api/#future-work","title":"Future work","text":"<ul> <li>securing API connection layer (to enable safe remote control)</li> <li>securing TCP socket of Stream Manager</li> </ul>"},{"location":"fine-tuned/rfdetr/","title":"RF-DETR","text":"<p>RF-DETR is a real-time object detection transformer-based architecture designed to transfer well to both a wide variety of domains and to datasets big and small.</p> <p>RF-DETR is the first real-time model to exceed 60 AP on the Microsoft COCO benchmark alongside competitive performance at base sizes. It also achieves state-of-the-art performance on RF100-VL, an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is comparable speed to current real-time objection models.</p> <p></p> <p>The model comes in two variants:</p> <ul> <li>RF-DETR Base, which has 29M parameters, and;</li> <li>RF-DETR Large, which has 129M parameters.</li> </ul> <p>The RF-DETR source code and COCO checkpoint weights is available under an Apache 2.0 license.</p>"},{"location":"fine-tuned/rfdetr/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following RF-DETR model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/rfdetr/#model-overview","title":"Model Overview","text":"<ul> <li>RF-DETR background and architecture overview</li> <li>Train an RF-DETR model on a custom dataset</li> <li>Train and deploy an RF-DETR model on Roboflow</li> </ul>"},{"location":"fine-tuned/rfdetr/#usage-example","title":"Usage Example","text":"<p>You can use RF-DETR with the following code:</p> <pre><code>import os\nimport supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nresponse = requests.get(\"https://media.roboflow.com/dog.jpeg\")\n\nif response.status_code == 200:\n    image_data = BytesIO(response.content)\n\n    image = Image.open(image_data)\n\nmodel = get_model(\"rfdetr-base\")\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n\nannotated_image.save(\"annotated_image_base.jpg\")\n\nmodel = get_model(\"rfdetr-large\")\n\npredictions = model.infer(image, confidence=0.5)[0]\n\ndetections = sv.Detections.from_inference(predictions)\n\nlabels = [prediction.class_name for prediction in predictions.predictions]\n\nannotated_image = image.copy()\nannotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)\nannotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)\n\nsv.plot_image(annotated_image)\n\nannotated_image.save(\"annotated_image_large.jpg\")\n</code></pre> <p>If you have fine-tuned an RF-DETR model on Roboflow, you can deploy it by replacing the <code>rfdetr-base</code> model ID in the <code>get_model()</code> function call above with the ID of your trained model on Roboflow. This model ID will look like <code>model-name/1</code>, where <code>model-name</code> is the name of the model and <code>1</code> is your model version. Learn how to find your model ID</p>"},{"location":"fine-tuned/rfdetr/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of RF-DETR is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolonas/","title":"YOLO-NAS","text":"<p>YOLO-NAS is a computer vision model architecture developed by Deci AI.</p>"},{"location":"fine-tuned/yolonas/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLO-NAS model types with Inference:</p> <ul> <li>Object Detection</li> <li>Keypoint Detection</li> </ul>"},{"location":"fine-tuned/yolonas/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLO-NAS model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolonas/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLO-NAS Object Detection Model</li> <li>Configure a YOLO-NAS Classification Model</li> <li>Configure a YOLO-NAS Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolonas/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLO-NAS is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov10/","title":"YOLOv10","text":"<p>YOLOv10, released on May 23, 2024, is a real-time object detection model developed by researchers from Tsinghua University. YOLOv10 follows in the long-running series of YOLO models, created by authors from a wide variety of researchers and organizations.</p>"},{"location":"fine-tuned/yolov10/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv10 model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/yolov10/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv10 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov10/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv10 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov10/#see-also","title":"See Also","text":"<ul> <li>How to Train a YOLOv10 Model</li> <li>Deploy a YOLOv10 Model with Roboflow</li> </ul>"},{"location":"fine-tuned/yolov11/","title":"YOLOv11","text":"<p>YOLOv11, released in September 2024, is a real-time object detection model developed by Ultralytics.</p>"},{"location":"fine-tuned/yolov11/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv11 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Image Segmentation</li> <li>Keypoint Detection</li> </ul>"},{"location":"fine-tuned/yolov11/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv10 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov11/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv11 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov11/#see-also","title":"See Also","text":"<ul> <li>What is YOLOv11? An Introduction</li> <li>How to Train a YOLOv11 Model</li> </ul>"},{"location":"fine-tuned/yolov5/","title":"YOLOv5","text":"<p>YOLOv5 is a computer vision model architecture implemented in the <code>ultralytics</code> Python package.</p>"},{"location":"fine-tuned/yolov5/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv5 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Classification</li> <li>Image Segmentation</li> </ul>"},{"location":"fine-tuned/yolov5/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv5 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov5/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv5 Object Detection Model</li> <li>Configure a YOLOv5 Classification Model</li> <li>Configure a YOLOv5 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov5/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv5 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov7/","title":"YOLOv7","text":"<p>YOLOv7 is a computer vision model architecture introduced in the paper \"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\".</p>"},{"location":"fine-tuned/yolov7/#supported-model-types","title":"Supported Model Types","text":"<ul> <li>Classification</li> </ul>"},{"location":"fine-tuned/yolov7/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov7/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Deploy a YOLOv7 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov7/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv7 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov8/","title":"YOLOv8","text":"<p>YOLOv8 is a computer vision model architecture implemented in the <code>ultralytics</code> Python package.</p>"},{"location":"fine-tuned/yolov8/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv8 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Classification</li> <li>Image Segmentation</li> <li>Keypoint Detection</li> </ul>"},{"location":"fine-tuned/yolov8/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov8/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv8 Object Detection Model</li> <li>Configure a YOLOv8 Classification Model</li> <li>Configure a YOLOv8 Segmentation Model</li> <li>Configure a YOLOv8 Keypoint Detection Model</li> </ul>"},{"location":"fine-tuned/yolov8/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv8 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov9/","title":"YOLOv9","text":"<p>YOLOv9 is a computer vision model architecture introduced in the paper \"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information\".</p>"},{"location":"fine-tuned/yolov9/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv9 model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/yolov9/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv9 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov9/#available-pretrained-models","title":"Available Pretrained Models","text":"<p>You may use YOLOv9 object detection models available on the Universe.</p>"},{"location":"fine-tuned/yolov9/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv9 Object Detection Model</li> </ul>"},{"location":"fine-tuned/yolov9/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv9 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov9/#see-also","title":"See Also","text":"<ul> <li>YOLOv9 on Roboflow Blog</li> </ul>"},{"location":"foundation/about/","title":"About","text":"<p>Foundation models are machine learning models that have been trained on vast amounts of data to accomplish a specific task.</p> <p>For example, OpenAI trained CLIP, a foundation model. CLIP enables you to classify images. You can also compare the similarity of images and text with CLIP.</p> <p>The CLIP training process, which was run using over 400 million pairs of images and text, allowed the model to build an extensive range of knowledge, which can be applied to a range of domains.</p> <p>Foundation models are being built for a range of vision tasks, from image segmentation to classification to zero-shot object detection.</p> <p>Inference supports the following foundation models:</p> <ul> <li>Gaze (LC2S-Net): Detect the direction in which someone is looking.</li> <li>CLIP: Classify images and compare the similarity of images and text.</li> <li>DocTR: Read characters in images.</li> <li>Grounding DINO: Detect objects in images using text prompts.</li> <li>Segment Anything (SAM): Segment objects in images.</li> </ul> <p>All of these models can be used over a HTTP request with Inference. This means you don't need to spend time setting up and configuring each model.</p>"},{"location":"foundation/about/#how-are-foundation-models-used","title":"How Are Foundation Models Used?","text":"<p>Use cases vary depending on the foundation model with which you are working. For example, CLIP has been used extensively in the field of computer vision for tasks such as:</p> <ol> <li>Clustering images to identify groups of similar images and outliers;</li> <li>Classifying images;</li> <li>Moderating image content;</li> <li>Identifying if two images are too similar or too different, ideal for dataset management and cleaning;</li> <li>Building dataset search experiences,</li> <li>And more.</li> </ol> <p>Grounding DINO, on the other hand, can be used out-of-the-box to detect a range of objects. Or you can use Grounding DINO to automatically label data for use in training a smaller, faster object detection model that is fine-tuned to your use case.</p>"},{"location":"foundation/about/#how-to-use-foundation-models","title":"How to Use Foundation Models","text":"<p>The guides in this section walk through how to use each of the foundation models listed above with Inference. No machine learning experience is required to use each model. Our code snippets and accompanying reference material provide the knowledge you need to get started working with foundation models.</p>"},{"location":"foundation/clip/","title":"CLIP (Classification, Embeddings)","text":"<p>CLIP is a computer vision model that can measure the similarity between text and images.</p> <p>CLIP can be used for, among other things:</p> <ul> <li>Image classification</li> <li>Automated labeling for classification models</li> <li>Image clustering</li> <li>Gathering images for model training that are sufficiently dissimilar from existing samples</li> <li>Content moderation</li> </ul> <p>With Inference, you can calculate CLIP embeddings for images and text in real-time.</p> <p>In this guide, we will show:</p> <ol> <li>How to classify video frames with CLIP in real time, and;</li> <li>How to calculate CLIP image and text embeddings for use in clustering and comparison.</li> </ol>"},{"location":"foundation/clip/#how-can-i-use-clip-model-in-inference","title":"How can I use CLIP model in <code>inference</code>?","text":"<ul> <li>directly from <code>inference[clip]</code> package, integrating the model directly into your code</li> <li>using <code>inference</code> HTTP API (hosted locally, or on the Roboflow platform), integrating via HTTP protocol</li> <li>using <code>inference-sdk</code> package (<code>pip install inference-sdk</code>) and <code>InferenceHTTPClient</code></li> <li>creating custom code to make HTTP requests (see API Reference)</li> </ul>"},{"location":"foundation/clip/#supported-clip-versions","title":"Supported CLIP versions","text":"<ul> <li><code>clip/RN101</code></li> <li><code>clip/RN50</code></li> <li><code>clip/RN50x16</code></li> <li><code>clip/RN50x4</code></li> <li><code>clip/RN50x64</code></li> <li><code>clip/ViT-B-16</code></li> <li><code>clip/ViT-B-32</code></li> <li><code>clip/ViT-L-14-336px</code></li> <li><code>clip/ViT-L-14</code></li> </ul>"},{"location":"foundation/clip/#classify-video-frames","title":"Classify Video Frames","text":"<p>With CLIP, you can classify images and video frames without training a model. This is because CLIP has been pre-trained to recognize many different objects.</p> <p>To use CLIP to classify video frames, you need a prompt. In the example below, we will use the prompt \"cell phone\".</p> <p>We can compare the similarity of \"cell phone\" to each video frame and use that to classify the video frame.</p> <p>Below is a demo of CLIP classifying video frames in real time. The code for the example is below the video.</p> <p>First, install the Inference CLIP extension:</p> <pre><code>pip install \"inference[clip]\"\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import cv2\nimport inference\nfrom inference.core.utils.postprocess import cosine_similarity\n\nfrom inference.models import Clip\nclip = Clip(model_id=\"clip/ViT-B-16\")  # `model_id` has default, but here is how to test other versions\n\nprompt = \"an ace of spades playing card\"\ntext_embedding = clip.embed_text(prompt)\n\ndef render(result, image):\n    # get the cosine similarity between the prompt &amp; the image\n    similarity = cosine_similarity(result[\"embeddings\"][0], text_embedding[0])\n\n    # scale the result to 0-100 based on heuristic (~the best &amp; worst values I've observed)\n    range = (0.15, 0.40)\n    similarity = (similarity-range[0])/(range[1]-range[0])\n    similarity = max(min(similarity, 1), 0)*100\n\n    # print the similarity\n    text = f\"{similarity:.1f}%\"\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (255, 255, 255), 30)\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (206, 6, 103), 16)\n\n    # print the prompt\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 10)\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (206, 6, 103), 5)\n\n    # display the image\n    cv2.imshow(\"CLIP\", image)\n    cv2.waitKey(1)\n\n# start the stream\ninference.Stream(\n    source=\"webcam\",\n    model=clip,\n\n    output_channel_order=\"BGR\",\n    use_main_thread=True,\n\n    on_prediction=render\n)\n</code></pre> <p>Run the code to use CLIP on your webcam.</p> <p>Note: The model will take a minute or two to load. You will not see output while the model is loading.</p>"},{"location":"foundation/clip/#calculate-a-clip-embedding","title":"Calculate a CLIP Embedding","text":"<p>CLIP enables you to calculate embeddings. Embeddings are numeric, semantic representations of images and text. They are useful for clustering and comparison.</p> <p>You can use CLIP embeddings to compare the similarity of text and images.</p> <p>There are two types of CLIP embeddings: image and text.</p> <p>Below we show how to calculate, then compare, both types of embeddings.</p>"},{"location":"foundation/clip/#image-embedding","title":"Image Embedding","text":"<p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <p>In the code below, we calculate an image embedding.</p> <p>Create a new Python file and add this code:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\nembeddings = CLIENT.get_clip_image_embeddings(inference_input=\"https://media.roboflow.com/inference/people-walking.jpg\")\nprint(embeddings)\n\n# since release `0.9.17`, you may pass extra argument `clip_version` to get_clip_image_embeddings(...) to select\n# model version\n</code></pre>"},{"location":"foundation/clip/#text-embedding","title":"Text Embedding","text":"<p>In the code below, we calculate a text embedding.</p> <p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nembeddings = CLIENT.get_clip_text_embeddings(text=\"the quick brown fox jumped over the lazy dog\")\nprint(embeddings)\n\n# since release `0.9.17`, you may pass extra argument `clip_version` to get_clip_text_embeddings(...) to select\n# model version\n</code></pre>"},{"location":"foundation/clip/#compare-embeddings","title":"Compare Embeddings","text":"<p>To compare embeddings for similarity, you can use cosine similarity.</p> <p>The code you need to compare image and text embeddings is the same.</p> <p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nresult = CLIENT.clip_compare(\n  subject=\"./image.jpg\",\n  prompt=[\"dog\", \"cat\"]\n)\nprint(result)\n# since release `0.9.17`, you may pass extra argument `clip_version` to clip_compare(...) to select\n# model version\n</code></pre> <p>The resulting number will be between 0 and 1. The higher the number, the more similar the image and text are.</p>"},{"location":"foundation/clip/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of CLIP.</p> <ul> <li>CLIP Embed Images: 0.5 seconds per inference (59.55 seconds for 100 inferences).</li> <li>CLIP Embed Text: 0.5 seconds per inference (51.52 seconds for 100 inferences).</li> <li>CLIP Compare Image and Text: 0.58 seconds per inference (58.03 seconds for 100 inferences).</li> </ul>"},{"location":"foundation/clip/#see-also","title":"See Also","text":"<ul> <li>What is CLIP?</li> <li>Build an Image Search Engine with CLIP and Faiss</li> <li>Build a Photo Memories App with CLIP</li> <li>Analyze and Classify Video with CLIP</li> </ul>"},{"location":"foundation/cogvlm/","title":"Cogvlm","text":"<p>CogVLM is a Large Multimodal Model (LMM). CogVLM is available for use in Inference.</p> <p>You can ask CogVLM questions about the contents of an image and retrieve a text response.</p> <p>CogVLM reached End Of Life</p> <p>Due to dependencies conflicts with newer models and security vulnerabilities discovered in <code>transformers</code> library patched in the versions of library incompatible with the model we announced End Of Life for CogVLM support in <code>inference</code>, effective since release <code>0.38.0</code>.</p> <p>We are leaving this page only for future reference, explicitly marking the last version of <code>inference</code>  supporting the feature (which is <code>0.37.1</code>). This tutorial should be treated as demonstration of  capabilities of Visual Language Models and should not be reproduced in any production enviromnets  (due to security issues).</p> <p>We encourage Roboflow clients to try another Visual Language Models supported by <code>inference</code>, including  Qwen2.5-VL which is now available.</p>"},{"location":"foundation/cogvlm/#model-quantization","title":"Model Quantization","text":"<p>You can run CogVLM through Roboflow Inference with three degrees of quantization. Quantization allows you to make a model smaller, but there is an accuracy trade-off. The three degrees of quantization are:</p> <ul> <li>No quantization: Run the full model. For this, you will need 80 GB of RAM. You could run the model on an 80 GB NVIDIA A100.</li> <li>8-bit quantization: Run the model with less accuracy than no quantization. You will need 32 GB of RAM.You could run this model on an A100 with sufficient virtual RAM.</li> <li>4-bit quantization: Run the model with less accuracy than 8-bit quantization. You will need 16 GB of RAM. You could run this model on an NVIDIA T4.</li> </ul>"},{"location":"foundation/cogvlm/#use-cogvlm-with-inference","title":"Use CogVLM with Inference","text":"<p>To use CogVLM with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. </p> <p>Then, retrieve your API key from the Roboflow dashboard. Learn how to retrieve your API key.</p> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>We recommend using CogVLM paired with inference HTTP API adjusted to run in GPU environment. It's easy to set up  with our <code>inference-cli</code> tool. Run the following command to set up environment and run the API under  <code>http://localhost:9001</code></p> <p>Warning</p> <p>Make sure that you are running this at machine with an NVidia GPU! Otherwise CogVLM will not be available.</p> <pre><code>pip install \"inference==0.37.1\"\ninference server start\n</code></pre> <p>Let's ask a question about the following image:</p> <p></p> <p>Use <code>inference-sdk</code> to prompt the model:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = CLIENT.prompt_cogvlm(\n    visual_prompt=\"./forklift.jpg\",\n    text_prompt=\"Is there a forklift close to a conveyor belt?\",\n)\nprint(result)\n</code></pre> <p>Above, replace <code>forklift.jpeg</code> with the path to the image in which you want to detect objects.</p> <p>Let's use the prompt \"Is there a forklift close to a conveyor belt?\u201d\"</p> <p>The results of CogVLM will appear in your terminal:</p> <pre><code>{\n    'response': 'yes, there is a forklift close to a conveyor belt, and it appears to be transporting a stack of items onto it.',\n    'time': 12.89864671198302\n}\n</code></pre> <p>CogVLM successfully answered our question, noting there is a forklift close to the conveyor belt in the image.</p>"},{"location":"foundation/cogvlm/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of CogVLM.</p> <p>CogVLM ran 100 inferences in 365.22 seconds (11.69 seconds per inference, on average).</p>"},{"location":"foundation/cogvlm/#see-also","title":"See Also","text":"<ul> <li>How to deploy CogVLM</li> </ul>"},{"location":"foundation/depth_estimation/","title":"Depth Estimation (Depth Anything V2 Small)","text":"<p>Depth-Anything-V2-Small is a depth estimation model developed by Hugging Face.</p> <p>You can use Depth-Anything-V2-Small to estimate the depth of objects in images, creating a depth map where: - Each pixel's value represents its relative distance from the camera - Lower values (darker colors) indicate closer objects - Higher values (lighter colors) indicate further objects</p> <p>You can deploy Depth-Anything-V2-Small with Inference.</p>"},{"location":"foundation/depth_estimation/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run Depth-Anything-V2-Small, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/depth_estimation/#how-to-use-depth-anything-v2-small","title":"How to Use Depth-Anything-V2-Small","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom inference.models.depth_estimation.depthestimation import DepthEstimator\n\n# Initialize the model\nmodel = DepthEstimator()\n\n# Load an image\nimage = Image.open(\"your_image.jpg\")\n\n# Run inference\nresults = model.predict(image)\n\n# Get the depth map and visualization\ndepth_map = results[0]['normalized_depth']\nvisualization = results[0]['image']\n\n# Convert visualization to numpy array for display\nvisualization_array = visualization.numpy()\n\n# Display the results\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(visualization_array)\nplt.title('Depth Map')\nplt.axis('off')\n\nplt.show()\n</code></pre> <p>In this code, we: 1. Load the Depth-Anything-V2-Small model 2. Load an image for depth estimation 3. Run inference to get the depth map 4. Display both the original image and the depth map visualization</p> <p>The depth map visualization uses a viridis colormap where: - Darker colors (purple/blue) represent objects closer to the camera - Lighter colors (yellow/green) represent objects further from the camera</p> <p>To use Depth-Anything-V2-Small with Inference, you will need a Hugging Face token. If you don't already have a Hugging Face account, sign up for a free Hugging Face account.</p> <p>Then, set your Hugging Face token as an environment variable:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=your_token_here\n</code></pre> <p>Or you can log in using the Hugging Face CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The script will display both the original image and the depth map visualization.</p>"},{"location":"foundation/doctr/","title":"DocTR (OCR)","text":"<p>DocTR is an Optical Character Recognition (OCR) model.</p> <p>You can use DocTR to read the text in an image.</p> <p>To use DocTR with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. </p> <p>Then, retrieve your API key from the Roboflow dashboard. Learn how to retrieve your API key.</p> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Let's retrieve the text in the following image:</p> <p></p> <p>Create a new Python file and add the following code:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = CLIENT.ocr_image(inference_input=\"./container.jpg\")  # single image request\nprint(result)\n</code></pre> <p>Above, replace <code>container.jpeg</code> with the path to the image in which you want to detect objects.</p> <p>The results of DocTR will appear in your terminal:</p> <pre><code>{'result': '', 'time': 3.98263641900121, 'result': 'MSKU 0439215', 'time': 3.870879542999319}\n</code></pre>"},{"location":"foundation/doctr/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of DocTR.</p> <p>DocTR ran 100 inferences in 365.22 seconds (3.65 seconds per inference, on average).</p>"},{"location":"foundation/doctr/#see-also","title":"See Also","text":"<ul> <li>How to detect text in images with OCR</li> </ul>"},{"location":"foundation/florence2/","title":"Florence-2","text":"<p>Florence-2 is a multimodal model developed by Microsoft Research.</p> <p>You can use Florence-2 for:</p> <ol> <li>Object detection: Identify the location of all objects in an image. (<code>&lt;OD&gt;</code>)</li> <li>Dense region captioning: Generate dense captions for all identified regions in an image. (<code>&lt;DENSE_REGION_CAPTION&gt;</code>)</li> <li>Image captioning: Generate a caption for a whole image. (<code>&lt;CAPTION&gt;</code> for a short caption, <code>&lt;DETAILED_CAPTION&gt;</code> for a more detailed caption, and <code>&lt;MORE_DETAILED_CAPTION&gt;</code> for an even more detailed caption)</li> <li>Region proposal: Identify regions where there are likely to be objects in an image. (<code>&lt;REGION_PROPOSAL&gt;</code>)</li> <li>Phrase grounding: Identify the location of objects that match a text description. (<code>&lt;CAPTION_TO_PHRASE_GROUNDING&gt;</code>)</li> <li>Referring expression segmentation: Identify a segmentation mask that corresponds with a text input. (<code>&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;</code>)</li> <li>Region to segmentation: Calculate a segmentation mask for an object from a bounding box region. (<code>&lt;REGION_TO_SEGMENTATION&gt;</code>)</li> <li>Open vocabulary detection: Identify the location of objects that match a text prompt. (<code>&lt;OPEN_VOCABULARY_DETECTION&gt;</code>)</li> <li>Region to description: Generate a description for a region in an image. (<code>&lt;REGION_TO_DESCRIPTION&gt;</code>)</li> <li>Optical Character Recognition (OCR): Read the text in an image. (<code>&lt;OCR&gt;</code>)</li> <li>OCR with region: Read the text in a specific region in an image. (<code>&lt;OCR_WITH_REGION&gt;</code>)</li> </ol> <p>You can use Inference for all the Florence-2 tasks above.</p> <p>The text in the parentheses are the task prompts you will need to use each task.</p>"},{"location":"foundation/florence2/#how-to-use-florence-2","title":"How to Use Florence-2","text":"Install <code>inference</code> <p>To install <code>inference</code> with Florence 2 support use the following command on CPU machine:</p> <pre><code>pip install inference[transformers]\n</code></pre> <p>or the following one for GPU machine:</p> <pre><code>pip install inference-gpu[transformers]\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from inference import get_model\n\nmodel = get_model(\"florence-2-base\", api_key=\"API_KEY\")\n\nresult = model.infer(\n    \"https://media.roboflow.com/inference/seawithdock.jpeg\", \n    prompt=\"&lt;CAPTION&gt;\",\n)\n\nprint(result[0].response)\n</code></pre> <p>Above, replace <code>&lt;CAPTION&gt;</code> with the name of the task you want to use.</p> <p>Replace <code>API_KEY</code> with your Roboflow API key. Learn how to retrieve your Roboflow API key</p> <p>To use PaliGemma with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be printed to the console.</p>"},{"location":"foundation/gaze/","title":"L2CS-Net (Gaze Detection)","text":"<p>L2CS-Net is a gaze estimation model.</p> <p>You can detect the direction in which someone is looking using the L2CS-Net model.</p>"},{"location":"foundation/gaze/#how-to-use-l2cs-net","title":"How to Use L2CS-Net","text":"<p>To use L2CS-Net with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>L2CS-Net accepts an image and returns pitch and yaw values that you can use to:</p> <ol> <li>Figure out the direction in which someone is looking, and;</li> <li>Estimate, roughly, where someone is looking.</li> </ol> <p>We recommend using L2CS-Net paired with inference HTTP API. It's easy to set up with our <code>inference-cli</code> tool. Run the  following command to set up environment and run the API under <code>http://localhost:9001</code></p> <pre><code>pip install inference inference-cli inference-sdk\ninference server start  # this starts server under http://localhost:9001\n</code></pre> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nCLIENT.detect_gazes(inference_input=\"./image.jpg\")  # single image request\n</code></pre> <p>Above, replace <code>image.jpg</code> with the image in which you want to detect gazes.</p> <p>The code above makes two assumptions:</p> <ol> <li>Faces are roughly one meter away from the camera.</li> <li>Faces are roughly 250mm tall.</li> </ol> <p>These assumptions are a good starting point if you are using a computer webcam with L2CS-Net, where people in the frame are likely to be sitting at a desk.</p> <p>On the first run, the model will be downloaded. On subsequent runs, the model will be cached locally and loaded from the cache. It will take a few moments for the model to download.</p> <p>The results of L2CS-Net will appear in your terminal:</p> <pre><code>[{'face': {'x': 1107.0, 'y': 1695.5, 'width': 1056.0, 'height': 1055.0, 'confidence': 0.9355756640434265, 'class': 'face', 'class_confidence': None, 'class_id': 0, 'tracker_id': None, 'landmarks': [{'x': 902.0, 'y': 1441.0}, {'x': 1350.0, 'y': 1449.0}, {'x': 1137.0, 'y': 1692.0}, {'x': 1124.0, 'y': 1915.0}, {'x': 625.0, 'y': 1551.0}, {'x': 1565.0, 'y': 1571.0}]}, 'yaw': -0.04104889929294586, 'pitch': 0.029525401070713997}]\n</code></pre> <p>We have created a full gaze detection example that shows how to:</p> <ol> <li>Use L2CS-Net with a webcam;</li> <li>Calculate the direction in which and point in space at which someone is looking;</li> <li>Calculate what quadrant of the screen someone is looking at, and;</li> <li>Annotate the image with the direction someone is looking.</li> </ol> <p>This example will let you run L2CS-Net and see the results of the model in real time. Here is an recording of the example working:</p> <p>Learn how to set up the example.</p>"},{"location":"foundation/gaze/#l2cs-net-inference-response","title":"L2CS-Net Inference Response","text":"<p>Here is the structure of the data returned by a gaze request:</p> <pre><code>[{'face': {'class': 'face',\n           'class_confidence': None,\n           'class_id': 0,\n           'confidence': 0.9355756640434265,\n           'height': 1055.0,\n           'landmarks': [{'x': 902.0, 'y': 1441.0},\n                         {'x': 1350.0, 'y': 1449.0},\n                         {'x': 1137.0, 'y': 1692.0},\n                         {'x': 1124.0, 'y': 1915.0},\n                         {'x': 625.0, 'y': 1551.0},\n                         {'x': 1565.0, 'y': 1571.0}],\n           'tracker_id': None,\n           'width': 1056.0,\n           'x': 1107.0,\n           'y': 1695.5},\n  'pitch': 0.029525401070713997,\n  'yaw': -0.04104889929294586}]\n</code></pre>"},{"location":"foundation/gaze/#see-also","title":"See Also","text":"<ul> <li>Gaze Detection and Eye Tracking: A How-To Guide</li> </ul>"},{"location":"foundation/grounding_dino/","title":"Grounding DINO (Object Detection)","text":"<p>Grounding DINO is a zero-shot object detection model.</p> <p>You can use Grounding DINO to identify objects in images and videos using arbitrary text prompts.</p> <p>To use Grounding DINO effectively, we recommend experimenting with the model to understand which text prompts help achieve the desired results.</p> <p>Note</p> <p>Grounding DINO is most effective at identifying common objects (i.e. cars, people, dogs, etc.). It is less effective at identifying uncommon objects (i.e. a specific type of car, a specific person, a specific dog, etc.).</p>"},{"location":"foundation/grounding_dino/#how-to-use-grounding-dino","title":"How to Use Grounding DINO","text":"<p>First, install the Inference Grounding DINO extension:</p> <pre><code>pip install \"inference[grounding-dino]\"\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from inference.models.grounding_dino import GroundingDINO\n\nmodel = GroundingDINO(api_key=\"\")\n\nresults = model.infer(\n    {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/fruit.png\",\n        },\n        \"text\": [\"apple\"],\n\n        # Optional params\n        \"box_threshold\": 0.5\n        \"text_threshold\": 0.5\n    }\n)\n\nprint(results.json())\n</code></pre> <p>In this code, we load Grounding DINO, run Grounding DINO on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>apple</code> with the object you want to detect.</li> <li><code>fruit.png</code> with the path to the image in which you want to detect objects.</li> </ol> <p>Additionally, you can tweak the optional <code>box_threshold</code> and <code>class_threshold</code> params for your specific use case. Both values default to 0.5 if not set. See the Grounding DINO README for an explanation of the model's thresholds.</p> <p>To use Grounding DINO with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The predictions from your model will be printed to the console.</p>"},{"location":"foundation/moondream2/","title":"Moondream2","text":"<p>Moondream2 is a multimodal model that supports image captioning, zero-shot object detection, point-prompt detection, and visual question answering.</p> <p>You can deploy Moondream2 with Inference.</p>"},{"location":"foundation/moondream2/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run Moondream2, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/moondream2/#how-to-use-moondream2","title":"How to Use Moondream2","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from PIL import Image\n\nfrom inference.models.moondream2.moondream2 import Moondream2\n\npg = Moondream2(api_key=\"API_KEY\")\n\nimage = Image.open(\"dog.jpeg\")\n\nprompt = \"How many dogs are in this image?\"\n\nresult = pg.query(image, prompt)\n\nprint(result)\n</code></pre> <p>In this code, we load Moondream2 run Moondream2 on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>prompt</code> with the prompt for the model.</li> <li><code>image.jpeg</code> with the path to the image that you want to run inference on.</li> </ol> <p>To use Moondream2 with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be printed to the console.</p>"},{"location":"foundation/owlv2/","title":"OwlV2 (Object Detection)","text":"<p>OWLv2 is an open set object detectio model trained by Google. OWLv2 was primarily trained to detect objects from text. The implementation in <code>Inference</code> currently only supports detecting objects from visual examples of that object.</p>"},{"location":"foundation/owlv2/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run OWLv2, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/owlv2/#how-to-use-owlv2","title":"How to Use OWLv2","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import inference\nfrom inference.models.owlv2.owlv2 import OwlV2\nfrom inference.core.entities.requests.owlv2 import OwlV2InferenceRequest\nfrom PIL import Image\nimport io\nimport base64\n\nmodel = OwlV2()\n\n\nim_url = \"https://media.roboflow.com/inference/seawithdock.jpeg\"\nimage = {\n    \"type\": \"url\",\n    \"value\": im_url\n}\nrequest = OwlV2InferenceRequest(\n    image=image,\n    training_data=[\n        {\n            \"image\": image,\n            \"boxes\": [{\"x\": 223, \"y\": 306, \"w\": 40, \"h\": 226, \"cls\": \"post\"}],\n        }\n    ],\n    visualize_predictions=True,\n    confidence=0.9999,\n)\n\nresponse = OwlV2().infer_from_request(request)\n\ndef load_image_from_base64(base64_str):\n    image = Image.open(io.BytesIO(base64_str))\n    return image\n\nvisualization = load_image_from_base64(response.visualization)\nvisualization.save(\"owlv2_visualization.jpg\")\n</code></pre> <p>In this code, we run OWLv2 on an image, using example objects from that image. Above, replace:</p> <ol> <li><code>training_data</code> with the locations of the objects you want to detect.</li> <li><code>im_url</code> with the image you would like to perform inference on.</li> </ol> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be save to disk at <code>owlv2_visualization.jpg</code></p> <p>Note the blue bounding boxes surrounding each pole of the dock.</p> <p></p>"},{"location":"foundation/paligemma/","title":"PaliGemma","text":"<p>PaliGemma is a large multimodal model developed by Google Research.</p> <p>You can use PaliGemma to:</p> <ol> <li>Ask questions about images (Visual Question Answering)</li> <li>Identify the location of objects in an image (object detection)</li> <li>Identify the precise location of objects in an imageh (image segmentation)</li> </ol> <p>You can deploy PaliGemma object detection models with Inference, and use PaliGemma for object detection.</p>"},{"location":"foundation/paligemma/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run PaliGemma, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/paligemma/#how-to-use-paligemma-vqa","title":"How to Use PaliGemma (VQA)","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import inference\n\nfrom inference.models.paligemma.paligemma import PaliGemma\n\npg = PaliGemma(\"paligemma-3b-mix-224\", api_key=\"YOUR ROBOFLOW API KEY\")\n\nfrom PIL import Image\n\nimage = Image.open(\"image.jpeg\") # Change to your image\n\nprompt = \"How many dogs are in this image?\"\n\nresult = pg.predict(image,prompt)\n</code></pre> <p>In this code, we load PaliGemma run PaliGemma on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>prompt</code> with the prompt for the model.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>To use PaliGemma with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be printed to the console.</p>"},{"location":"foundation/paligemma/#how-to-use-paligemma-object-detection","title":"How to Use PaliGemma (Object Detection)","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import os\nimport transformers\nimport re\nimport numpy as np\nimport supervision as sv\nfrom typing import Tuple, List, Optional\nfrom PIL import Image\n\nimage = Image.open(\"/content/data/dog.jpeg\")\n\ndef from_pali_gemma(response: str, resolution_wh: Tuple[int, int], class_list: Optional[List[str]] = None) -&gt; sv.Detections:\n    _SEGMENT_DETECT_RE = re.compile(\n        r'(.*?)' +\n        r'&lt;loc(\\d{4})&gt;' * 4 + r'\\s*' +\n        '(?:%s)?' % (r'&lt;seg(\\d{3})&gt;' * 16) +\n        r'\\s*([^;&lt;&gt;]+)? ?(?:; )?',\n    )\n\n    width, height = resolution_wh\n    xyxy_list = []\n    class_name_list = []\n\n    while response:\n        m = _SEGMENT_DETECT_RE.match(response)\n        if not m:\n            break\n\n        gs = list(m.groups())\n        before = gs.pop(0)\n        name = gs.pop()\n        y1, x1, y2, x2 = [int(x) / 1024 for x in gs[:4]]\n        y1, x1, y2, x2 = map(round, (y1*height, x1*width, y2*height, x2*width))\n\n        content = m.group()\n        if before:\n            response = response[len(before):]\n            content = content[len(before):]\n\n        xyxy_list.append([x1, y1, x2, y2])\n        class_name_list.append(name.strip())\n        response = response[len(content):]\n\n    xyxy = np.array(xyxy_list)\n    class_name = np.array(class_name_list)\n\n    if class_list is None:\n        class_id = None\n    else:\n        class_id = np.array([class_list.index(name) for name in class_name])\n\n    return sv.Detections(\n        xyxy=xyxy,\n        class_id=class_id,\n        data={'class_name': class_name}\n    )\n\nprompt = \"detect person; car; backpack\"\nresponse = pali_gemma.predict(image, prompt)[0]\nprint(response)\n\ndetections = from_pali_gemma(response=response, resolution_wh=image.size, class_list=['person', 'car', 'backpack'])\n\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotatrd_image = bounding_box_annotator.annotate(image, detections)\nannotatrd_image = label_annotator.annotate(annotatrd_image, detections)\nsv.plot_image(annotatrd_image)\n</code></pre> <p>In this code, we load PaliGemma run PaliGemma on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>prompt</code> with the prompt for the model.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>To use PaliGemma with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from the model will be displayed:</p> <p></p>"},{"location":"foundation/perception_encoder/","title":"Perception Encoder","text":"<p>Perception Encoder is a computer vision model that can measure the similarity between text and images, as well as compute useful text and image embeddings.</p> <p>Perception Encoder embeddings can be used for, among other things:</p> <ul> <li>Image classification</li> <li>Image clustering</li> <li>Gathering images for model training that are sufficiently dissimilar from existing samples</li> <li>Content moderation</li> </ul> <p>With Inference, you can calculate PE embeddings for images and text in real-time.</p> <p>In this guide, we will show:</p> <ol> <li>How to classify video frames with PE in real time, and;</li> <li>How to calculate PE image and text embeddings for use in clustering and comparison.</li> </ol>"},{"location":"foundation/perception_encoder/#how-can-i-use-pe-in-inference","title":"How can I use PE in <code>inference</code>?","text":"<ul> <li>directly from <code>inference[transformers]</code> package, integrating the model directly into your code</li> <li>using <code>inference</code> HTTP API (hosted locally, or on the Roboflow platform), integrating via HTTP protocol</li> <li>using <code>inference-sdk</code> package (<code>pip install inference-sdk</code>) and <code>InferenceHTTPClient</code></li> <li>creating custom code to make HTTP requests (see API Reference)</li> </ul>"},{"location":"foundation/perception_encoder/#supported-pe-versions","title":"Supported PE versions","text":"<ul> <li><code>perception_encoder/PE-Core-B16-224</code></li> <li><code>perception_encoder/PE-Core-L14-336</code></li> <li><code>perception_encoder/PE-Core-G14-448</code></li> </ul> <p>We currently only support the CLIP interface for PE models. We don't support the language or spatial aligned models yet.</p>"},{"location":"foundation/perception_encoder/#classify-video-frames","title":"Classify Video Frames","text":"<p>With PE, you can classify images and video frames without training a model. This is because PE has been pre-trained to recognize many different objects.</p> <p>To use PE to classify video frames, you need a prompt. In the example below, we will use the prompt \"an image of a guy with a beard holding a can of sparkling water\".</p> <p>We can compare the similarity of the prompt to each video frame and use that to classify the video frame.</p> <p>Below is a demo of PE classifying video frames in real time. The code for the example is below the image.</p> <p></p> <p>First, install the Inference transformers extension:</p> <pre><code>pip install \"inference[transformers]\"\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import cv2\nimport inference\nfrom inference.core.utils.postprocess import cosine_similarity\n\nfrom inference.models import PerceptionEncoder\npe = PerceptionEncoder(model_id=\"perception_encoder/PE-Core-B16-224\", device=\"mps\")  # `model_id` has default, but here is how to test other versions\n\nprompt = \"an image of a guy with a beard holding a can of sparkling water\"\ntext_embedding = pe.embed_text(prompt)\n\ndef render(result, image):\n    # get the cosine similarity between the prompt &amp; the image\n    similarity = cosine_similarity(result[\"embeddings\"][0], text_embedding[0])\n\n    # scale the result to 0-100 based on heuristic (~the best &amp; worst values I've observed)\n    range = (0.15, 0.40)\n    similarity = (similarity-range[0])/(range[1]-range[0])\n    similarity = max(min(similarity, 1), 0)*100\n\n    # print the similarity\n    text = f\"{similarity:.1f}%\"\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (255, 255, 255), 30)\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (206, 6, 103), 16)\n\n    # print the prompt\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 10)\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (206, 6, 103), 5)\n\n    # display the image\n    cv2.imshow(\"PE\", image)\n    cv2.waitKey(1)\n\n# start the stream\ninference.Stream(\n    source=\"webcam\",\n    model=pe,\n    output_channel_order=\"BGR\",\n    use_main_thread=True,\n    on_prediction=render\n)\n</code></pre> <p>Run the code to use Perception Encoder on your webcam.</p> <p>Note: The model will take a minute or two to load. You will not see output while the model is loading.</p>"},{"location":"foundation/perception_encoder/#using-pe-in-workflows","title":"Using PE in Workflows","text":"<p>Perception Encoder can be used in Roboflow Workflows via the Perception Encoder Embedding Model block. This block lets you generate embeddings for images or text without writing code.</p>"},{"location":"foundation/perception_encoder/#api-compatibility","title":"API Compatibility","text":"<p>The Perception Encoder model uses the same API as CLIP. This means you can use all the same methods and request/response formats as you would with CLIP, including <code>embed_text</code>, <code>embed_image</code>, and <code>compare</code>.</p> <p>For more details and advanced usage, see the CLIP documentation.</p>"},{"location":"foundation/qwen2.5-vl/","title":"Qwen2.5 VL","text":"<p>Qwen2.5 VL is a large multimodal model developed by the Qwen Team.</p> <p>You can use Qwen2.5 VL to:</p> <ol> <li>Ask questions about images (Visual Question Answering)</li> <li>Recognize objects and landmarks worldwide with high accuracy</li> <li>Precisely locate objects using bounding boxes or points (Object Grounding)</li> <li>Extract and understand text from images with enhanced multi-language OCR</li> <li>Parse and analyze documents using the unique QwenVL HTML format</li> <li>Understand videos (including hour-long videos) and locate specific events</li> <li>Generate structured JSON outputs for coordinates and attributes</li> <li>Act as a visual agent for computer and phone interfaces</li> </ol>"},{"location":"foundation/qwen2.5-vl/#installation","title":"Installation","text":"<p>To use Qwen2.5 VL with the Inference SDK, install:</p> <pre><code>pip install inference-sdk\n</code></pre>"},{"location":"foundation/qwen2.5-vl/#how-to-use-qwen25-vl-visual-question-answering","title":"How to Use Qwen2.5 VL (Visual Question Answering)","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\ndef run_qwen25_inference():\n    # Create a client pointing to your inference server\n    client = InferenceHTTPClient(\n        api_url=\"http://localhost:9001\",  # You can also use a remote server if needed\n        api_key=\"YOUR_API_KEY\"            # Optional if your model requires an API key\n    )\n\n    # Invoke the model with an image and a prompt\n    result = client.run_workflow(\n        workspace_name=\"YOUR_WORKSPACE_NAME\",  # Replace with your workspace name\n        workflow_id=\"image-text/93\",           # The model or workflow id\n        images={\n            \"image\": \"https://media.roboflow.com/dog.jpeg\"  # Can be a URL or local path\n        },\n        parameters={\n            \"prompt\": \"Tell me something about this dog!\"\n        }\n    )\n\n    print(result)\n\nif __name__ == \"__main__\":\n    run_qwen25_inference()\n</code></pre> <p>In this code, we: 1. Create an Inference HTTP client that connects to your inference server 2. Specify an image (either by URL or local path) 3. Define a prompt to ask about the image 4. Run the model and print the results</p> <p>To use Qwen2.5 VL with Inference, you will need an API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p>"},{"location":"foundation/qwen2.5-vl/#how-to-use-qwen25-vl-object-detection","title":"How to Use Qwen2.5 VL (Object Detection)","text":"<p>Create a new Python file called <code>object_detection.py</code> and add the following code:</p> <pre><code>from inference_sdk import InferenceHTTPClient\nimport json\nimport supervision as sv\nimport numpy as np\nfrom PIL import Image\n\ndef run_qwen25_object_detection():\n    # Create a client pointing to your inference server\n    client = InferenceHTTPClient(\n        api_url=\"http://localhost:9001\",\n        api_key=\"YOUR_API_KEY\"\n    )\n\n    # Path to your local image\n    image_path = \"path/to/your/image.jpg\"\n\n    # Invoke the model with an image and a detection prompt\n    result = client.run_workflow(\n        workspace_name=\"YOUR_WORKSPACE_NAME\",\n        workflow_id=\"image-text/93\",\n        images={\n            \"image\": image_path\n        },\n        parameters={\n            \"prompt\": \"Detect all objects in this image and return their locations as JSON.\"\n        }\n    )\n\n    # Parse the JSON result\n    detections_data = json.loads(result[0])\n\n    # Load the image for visualization\n    image = Image.open(image_path)\n\n    # Create a Detections object\n    xyxy_list = []\n    class_name_list = []\n\n    for detection in detections_data:\n        if \"bbox_2d\" in detection:\n            xyxy_list.append(detection[\"bbox_2d\"])\n            class_name_list.append(detection[\"label\"])\n\n    xyxy = np.array(xyxy_list)\n    class_name = np.array(class_name_list)\n\n    detections = sv.Detections(\n        xyxy=xyxy,\n        class_id=None,\n        data={'class_name': class_name}\n    )\n\n    # Visualize\n    bounding_box_annotator = sv.BoxAnnotator()\n    label_annotator = sv.LabelAnnotator()\n\n    annotated_image = bounding_box_annotator.annotate(image, detections)\n    annotated_image = label_annotator.annotate(annotated_image, detections)\n    sv.plot_image(annotated_image)\n\nif __name__ == \"__main__\":\n    run_qwen25_object_detection()\n</code></pre> <p>This code will: 1. Connect to your inference server 2. Ask Qwen2.5 VL to detect objects in an image 3. Parse the results (which come in JSON format) 4. Visualize the detections with bounding boxes</p>"},{"location":"foundation/qwen2.5-vl/#advanced-usage-with-system-prompts","title":"Advanced Usage with System Prompts","text":"<p>You can customize the model's behavior by providing a system prompt:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\n# Create a client pointing to your inference server\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"YOUR_API_KEY\"\n)\n\n# Invoke the model with a system prompt\nresult = client.run_workflow(\n    workspace_name=\"YOUR_WORKSPACE_NAME\",\n    workflow_id=\"image-text/93\",\n    images={\n        \"image\": \"path/to/image.jpg\"\n    },\n    parameters={\n        \"prompt\": \"Identify all landmarks in this image&lt;system_prompt&gt;You are an expert in world landmarks recognition\"\n    }\n)\n\nprint(result)\n</code></pre> <p>The system prompt is appended to the user prompt with the <code>&lt;system_prompt&gt;</code> delimiter.</p>"},{"location":"foundation/qwen2.5-vl/#model-variants","title":"Model Variants","text":"<p>Qwen2.5 VL is currently only available as Qwen2.5-VL-7B.</p> <p>The workflow ID may vary depending on which model variant you're using. Contact your administrator or refer to your deployment documentation for the correct workflow ID.</p>"},{"location":"foundation/qwen2.5-vl/#learn-more","title":"Learn More","text":"<p>For more details about Qwen2.5 VL's capabilities, including video understanding and visual agent abilities, visit the official Qwen2.5 VL page.</p>"},{"location":"foundation/sam/","title":"Segment Anything (Segmentation)","text":"<p>Segment Anything is an open source image segmentation model.</p> <p>You can use Segment Anything to identify the precise location of objects in an image.</p> <p>To use Segment Anything, you need to:</p> <ol> <li>Create an embedding for an image, and;</li> <li>Specify the coordinates of the object you want to segment.</li> </ol>"},{"location":"foundation/sam/#how-to-use-segment-anything","title":"How to Use Segment Anything","text":"<p>To use Segment Anything with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre>"},{"location":"foundation/sam/#embed-an-image","title":"Embed an Image","text":"<p>An embedding is a numeric representation of an image. SAM uses embeddings as input to calcualte the location of objects in an image.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"image_id\": \"example_image_id\",\n}\n\nbase_url = \"http://localhost:9001\"\n\n# Define your Roboflow API Key\napi_key = \"YOUR ROBOFLOW API KEY\"\n\nres = requests.post(\n    f\"{base_url}/sam/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n\nembeddings = res.json()['embeddings']\n</code></pre> <p>This code makes a request to Inference to embed an image using SAM.</p> <p>The <code>example_image_id</code> is used to cache the embeddings for later use so you don't have to send them back in future segmentation requests.</p>"},{"location":"foundation/sam/#segment-an-object","title":"Segment an Object","text":"<p>To segment an object, you need to know at least one point in the image that represents the object that you want to use.</p> <p>For testing with a single image, you can upload an image to the Polygon Zone web interface and hover over a point in the image to see the coordinates of that point.</p> <p>You may also opt to use an object detection model to identify an object, then use the center point of the bounding box as a prompt for segmentation.</p> <p>Create a new Python file and add the following code:</p> <pre><code>#Define request payload\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"point_coords\": [[380, 350]],\n    \"point_labels\": [1],\n    \"image_id\": \"example_image_id\",\n}\n\nres = requests.post(\n    f\"{base_url}/sam/embed_image?api_key={api_key}\",\n    json=infer_clip_payload,\n)\n\nmasks = request.json()['masks']\n</code></pre> <p>This request returns segmentation masks that represent the object of interest.</p>"},{"location":"foundation/sam/#see-also","title":"See Also","text":"<ul> <li>What is Segment Anything Model (SAM)?</li> </ul>"},{"location":"foundation/sam2/","title":"Segment Anything 2 (Segmentation)","text":"<p>Segment Anything 2 is an open source image segmentation model.</p> <p>You can use Segment Anything 2 to identify the precise location of objects in an image. This process can generate masks for objects in an image iteratively, by specifying points to be included or discluded from the segmentation mask.</p>"},{"location":"foundation/sam2/#how-to-use-segment-anything","title":"How to Use Segment Anything","text":"<p>To use Segment Anything 2 with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard.</p>"},{"location":"foundation/sam2/#how-to-use-sam2-locally-with-inference","title":"How To Use SAM2 Locally With Inference","text":"<p>We will follow along with the example located at <code>examples/sam2/sam2_example.py</code>.</p> <p>We start with the following image,</p> <p></p> <p>compute the most prominent mask,</p> <p></p> <p>and negative prompt the wrist to obtain only the fist.</p> <p></p>"},{"location":"foundation/sam2/#running-within-docker","title":"Running within docker","text":"<p>Build the dockerfile (make sure your cwd is at the root of inference) with <pre><code>docker build -f docker/dockerfiles/Dockerfile.sam2 -t sam2 .\n</code></pre></p> <p>Start up an interactive terminal with <pre><code>docker run -it --rm --entrypoint bash -v $(pwd)/scratch/:/app/scratch/ -v /tmp/cache/:/tmp/cache/ -v $(pwd)/inference/:/app/inference/ --gpus=all --net=host sam2\n</code></pre> You can save files to <code>/app/scratch/</code> to use them on the host device.</p> <p>Or, start a sam2 server with <pre><code>docker run -it --rm -v /tmp/cache/:/tmp/cache/ -v $(pwd)/inference/:/app/inference/ --gpus=all --net=host sam2\n</code></pre></p> <p>and interact over http.</p>"},{"location":"foundation/sam2/#imports","title":"Imports","text":"<p>Set up your api key, and install Segment Anything 2</p> <p>Note</p> <p>There's currently a problem with sam2 + flash attention on certain gpus, like the L4 or A100. Use the fix in the posted thread, or use the docker image we provide for sam2. </p> <pre><code>import os\n\nos.environ[\"API_KEY\"] = \"&lt;YOUR-API-KEY&gt;\"\nfrom inference.models.sam2 import SegmentAnything2\nfrom inference.core.utils.postprocess import masks2poly\nfrom inference.core.entities.requests.sam2 import Sam2PromptSet\nimport supervision as sv\nfrom PIL import Image\nimport numpy as np\n\nimage_path = \"./examples/sam2/hand.png\"\n</code></pre>"},{"location":"foundation/sam2/#model-loading","title":"Model Loading","text":"<p>Load the model with  <pre><code>m = SegmentAnything2(model_id=\"sam2/hiera_large\")\n</code></pre></p> <p>Other values for <code>model_id</code> are <code>\"hiera_small\", \"hiera_large\", \"hiera_tiny\", \"hiera_b_plus\"</code>.</p>"},{"location":"foundation/sam2/#compute-the-most-prominent-mask","title":"Compute the Most Prominent Mask","text":"<p><pre><code># call embed_image before segment_image to precompute embeddings\nembedding, img_shape, id_ = m.embed_image(image_path)\n\n# segments image using cached embedding if it exists, else computes it on the fly\nraw_masks, raw_low_res_masks = m.segment_image(image_path)\n\n# convert binary masks to polygons\nraw_masks = raw_masks &gt;= m.predictor.mask_threshold\npoly_masks = masks2poly(raw_masks)\n</code></pre> Note that you can embed the image as soon as you know you want to process it, and the embeddings are cached automatically for faster downstream processing.</p> <p>The resulting mask will look like this:</p> <p></p>"},{"location":"foundation/sam2/#negative-prompt-the-model","title":"Negative Prompt the Model","text":"<p><pre><code>point = [250, 800]\nlabel = False\n# give a negative point (point_label 0) or a positive example (point_label 1)\nprompt = Sam2PromptSet(\n    prompts=[{\"points\": [{\"x\": point[0], \"y\": point[1], \"positive\": label}]}]\n)\n\n# uses cached masks from prior call\n\nraw_masks2, raw_low_res_masks2 = m.segment_image(\n    image_path,\n    prompts=prompt,\n)\n\nraw_masks2 = raw_masks2 &gt;= m.predictor.mask_threshold\nraw_masks2 = raw_masks2[0]\n</code></pre> Here we tell the model that the cached mask should not include the wrist.</p> <p>The resulting mask will look like this:</p> <p></p>"},{"location":"foundation/sam2/#annotate","title":"Annotate","text":"<p>Use Supervision to draw the results of the model.</p> <pre><code>image = np.array(Image.open(image_path).convert(\"RGB\"))\n\nmask_annotator = sv.MaskAnnotator()\ndot_annotator = sv.DotAnnotator()\n\ndetections = sv.Detections(\n    xyxy=np.array([[0, 0, 100, 100]]), mask=np.array([raw_masks])\n)\ndetections.class_id = [i for i in range(len(detections))]\nannotated_image = mask_annotator.annotate(image.copy(), detections)\nim = Image.fromarray(annotated_image)\nim.save(\"sam.png\")\n\ndetections = sv.Detections(\n    xyxy=np.array([[0, 0, 100, 100]]), mask=np.array([raw_masks2])\n)\ndetections.class_id = [i for i in range(len(detections))]\nannotated_image = mask_annotator.annotate(image.copy(), detections)\n\ndot_detections = sv.Detections(\n    xyxy=np.array([[point[0] - 1, point[1] - 1, point[0] + 1, point[1] + 1]]),\n    class_id=np.array([1]),\n)\nannotated_image = dot_annotator.annotate(annotated_image, dot_detections)\nim = Image.fromarray(annotated_image)\nim.save(\"sam_negative_prompted.png\")\n</code></pre>"},{"location":"foundation/sam2/#how-to-use-sam2-with-a-local-docker-container-http-server","title":"How To Use SAM2 With a Local Docker Container HTTP Server","text":""},{"location":"foundation/sam2/#build-and-start-the-server","title":"Build and Start The Server","text":"<p>Build the dockerfile (make sure your cwd is at the root of inference) with <pre><code>docker build -f docker/dockerfiles/Dockerfile.sam2 -t sam2 .\n</code></pre> and start a sam2 server with <pre><code>docker run -it --rm -v /tmp/cache/:/tmp/cache/ -v $(pwd)/inference/:/app/inference/ --gpus=all --net=host sam2\n</code></pre></p>"},{"location":"foundation/sam2/#embed-an-image","title":"Embed an Image","text":"<p>An embedding is a numeric representation of an image. SAM uses embeddings as input to calcualte the location of objects in an image.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"image_id\": \"example_image_id\",\n}\n\nbase_url = \"http://localhost:9001\"\n\n# Define your Roboflow API Key\napi_key = \"YOUR ROBOFLOW API KEY\"\n\nres = requests.post(\n    f\"{base_url}/sam2/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n</code></pre> <p>This code makes a request to Inference to embed an image using SAM.</p> <p>The <code>example_image_id</code> is used to cache the embeddings for later use so you don't have to send them back in future segmentation requests.</p>"},{"location":"foundation/sam2/#segment-an-object","title":"Segment an Object","text":"<p>To segment an object, you need to know at least one point in the image that represents the object that you want to use.</p> <p>For testing with a single image, you can upload an image to the Polygon Zone web interface and hover over a point in the image to see the coordinates of that point.</p> <p>You may also opt to use an object detection model to identify an object, then use the center point of the bounding box as a prompt for segmentation.</p> <p>Create a new Python file and add the following code:</p> <pre><code>#Define request payload\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"point_coords\": [[380, 350]],\n    \"point_labels\": [1],\n    \"image_id\": \"example_image_id\",\n}\n\nres = requests.post(\n    f\"{base_url}/sam2/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n\nmasks = request.json()['masks']\n</code></pre> <p>This request returns segmentation masks that represent the object of interest.</p>"},{"location":"foundation/smolvlm/","title":"SmolVLM2","text":"<p>SmolVLM2 is a multimodal model developed by Hugging Face.</p> <p>You can use SmolVLM2 for a range of multimodal tasks, including VQA, document OCR, document VQA, and object counting.</p> <p>You can deploy SmolVLM2 with Inference.</p>"},{"location":"foundation/smolvlm/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run SmolVLM2, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/smolvlm/#how-to-use-smolvlm2","title":"How to Use SmolVLM2","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from PIL import Image\n\nfrom inference.models.smolvlm.smolvlm import SmolVLM\n\npg = SmolVLM(api_key=\"API_KEY\")\n\nimage = Image.open(\"dog.jpeg\")\n\nprompt = \"How many dogs are in this image?\"\n\nresult = pg.predict(image,prompt)\n\nprint(result)\n</code></pre> <p>In this code, we load SmolVLM2 run SmolVLM2 on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>prompt</code> with the prompt for the model.</li> <li><code>image.jpeg</code> with the path to the image that you want to run inference on.</li> </ol> <p>To use SmolVLM2 with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be printed to the console.</p>"},{"location":"foundation/trocr/","title":"TrOCR (OCR)","text":"<p>TrOCR is a transformer-based model for text recognition, otherwise known as Optical Character Recognition (OCR).</p> <p>TrOCR works best on focused, single-line printed text.</p> <p>Be sure to use with cropped images since unlike some other OCR models, TrOCR will not perform well on uncropped or multi-line text.</p> <p>Let's try running TrOCR on this image:</p> <p></p> <p>Note</p> <p>TROCR model is only supported in <code>inference</code> Python package and <code>inference</code> server deployed locally (excluding Roboflow Hosted Platform).</p> <p>To run the example, start <code>inference</code> server locally:</p> <pre><code>inference server start\n</code></pre> <p>Make sure you have <code>inference-cli</code> installed - if that's not the case run:</p> <pre><code>pip install inference-cli\n</code></pre> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(api_url=\"http://127.0.0.1:9001\")\n\nresult = CLIENT.ocr_image(inference_input=\"./serial_number.png\", model=\"trocr\")  # single image request\nprint(result)\n</code></pre>"},{"location":"foundation/yolo_world/","title":"YOLO-World (Object Detection)","text":"<p>YOLO-World is a zero-shot object detection model.</p> <p>You can use YOLO-World to identify objects in images and videos using arbitrary text prompts.</p> <p>To use YOLO-World effectively, we recommend experimenting with the model to understand which text prompts help achieve the desired results.</p> <p>YOLO World is faster than many other zero-shot object detection models like YOLO-World. On powerful hardware like a V100 GPU, YOLO World can run in real-time.</p> <p>Note</p> <p>YOLO-World, like most state-of-the-art zero-shot detection models, is most effective at identifying common objects (i.e. cars, people, dogs, etc.). It is less effective at identifying uncommon objects (i.e. a specific type of car, a specific person, a specific dog, etc.).</p> <p>Note</p> <p>In <code>inference</code> package YOLO-World models are identified by <code>yolo_world/&lt;version&gt;</code>, where <code>&lt;version&gt;</code> can be one of the following: <code>s</code>, <code>m</code>, <code>l</code>, <code>x</code>, <code>v2-s</code>, <code>v2-m</code>, <code>v2-l</code>, <code>v2-x</code>. Versions <code>v2-...</code> denote newer models, with improved evaluation metrics.</p>"},{"location":"foundation/yolo_world/#how-to-use-yolo-world","title":"How to Use YOLO-World","text":"Inference Python LibraryInference Server HTTP APIInference Pipeline (Video) <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import cv2\nimport supervision as sv\n\nfrom inference.models.yolo_world.yolo_world import YOLOWorld\n\nimage = cv2.imread(\"image.jpeg\")\n\nmodel = YOLOWorld(model_id=\"yolo_world/l\")\nclasses = [\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]\nresults = model.infer(\"image.jpeg\", text=classes, confidence=0.03)[0]\n\ndetections = sv.Detections.from_inference(results)\n\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [classes[class_id] for class_id in detections.class_id]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections\n)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, you will need to set up an Inference server to use the YOLO World HTTP API.</p> <p>To do this, run:</p> <pre><code>pip install inference inference-sdk\ninference server start\n</code></pre> <p>Then, create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import os\nimport cv2\nimport supervision as sv\n\nfrom inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresults = client.infer_from_yolo_world(\n    inference_input=[\"https://media.roboflow.com/dog.jpeg\"],\n    class_names=[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"],\n    model_version=\"l\",\n    confidence=0.1,\n)[0]\n\ndetections = sv.Detections.from_inference(results)\n\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [classes[class_id] for class_id in detections.class_id]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections\n)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Info</p> <pre><code>**Breaking change!** There were versions: `0.9.14` and `0.9.15` where Yolo World was exposed\nbehind `InferencePipeline.init(...)` initializer that you needed to run with specific combination \nof parameters to alter default behavior of pipeline such that it runs against YoloWorld model. \nWe decided to provide an explicit way of running this foundation model in `InferencePipeline` providing\na dedicated init function starting from version `0.9.16`\n</code></pre> <p>You can easily run predictions against <code>YoloWorld</code> model using <code>InferencePipeline</code>. There is a custom init method to ease handling that use-case:</p> <pre><code># import the InferencePipeline interface\nfrom inference import InferencePipeline\n# import a built-in sink called render_boxes (sinks are the logic that happens after inference)\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init_with_yolo_world(\n    video_reference=\"./your_video.mp4\",\n    classes=[\"person\", \"dog\", \"car\", \"truck\"],\n    model_size=\"s\",\n    on_prediction=render_boxes,\n)\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>In this code, we load YOLO-World, run YOLO-World on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]</code> with the objects you want to detect.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from YOLO-World will be displayed in a new window.</p> <p></p>"},{"location":"foundation/yolo_world/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of YOLO-World.</p> <p>YOLO-World ran 100 inferences in 9.18 seconds (0.09 seconds per inference, on average).</p>"},{"location":"guides/compare-models/","title":"Compare Two Models","text":"<p>In this tutorial, we will use Workflows to run two different Instance Segmentation models on the same image and qualitatively compare their predictions.</p> <p>Note: We will use YOLO models pre-trained on the 80 classes in the COCO dataset in this guide, but Workflows is even more powerful when used with your fine-tuned models custom-trained on domain-specific objects of interest.</p> <p>Difficulty: Easy Time to Complete: 5 minutes</p>"},{"location":"guides/compare-models/#prerequisites","title":"Prerequisites","text":"Cloud ConnectedDetached <p>This tutorial only requires a free Roboflow account and can run on the Serverless Hosted API with no setup required. This is the easiest way to get started and you can migrate to self-hosting your Workflows later.</p> <p>You can also connect from the cloud platform to an Inference Server running locally by clicking the \"Running on\" selector at the top-left of the platform UI and pointing it to <code>localhost</code> or your server's IP.</p> <p>Once you have an account, create a new (empty) Workflow then continue below.</p> <p>In Detached mode, you run both the Inference Server and Workflow Builder UI locally without a Roboflow account or API Key. In Detached mode, you forego cloud connected functionality like remote deployment, monitoring, integration with the cloud model hub and dataset management platform, and are responsible for implementing your own access control.</p> <p>To run on your own machine without a Roboflow account, follow the installation instructions and start your Inference Server in development mode (using <code>inference server start --dev</code>).</p> <p>Then, navigate to the local Workflows builder at localhost:9001/build and create an empty Workflow using the purple \"Create a Workflow\" button. If prompted, choose \"Build My Own\".</p> <p>You should now have an empty Workflow and be ready to start building.</p> <p></p>"},{"location":"guides/compare-models/#rename-workflow","title":"Rename Workflow","text":"<p>It's best practice to give your Workflows a descriptive name so that you can keep them straight. To do so, click the \"Edit\" icon on the top left of the Editor UI and update the name and endpoint URL. This identifier will be how you reference this Workflow via the API later.</p> <p></p>"},{"location":"guides/compare-models/#add-first-model","title":"Add First Model","text":"<p>We will be adding two Instance Segmentation models to this Workflow. Segmentation models find objects of interest in an image and predict their contour. They are best for use-cases that need to measure objects' size or cut them out from their background.</p> <p>Click the \"Add Block\" button and select Models, then choose Instance Segmentation as the task type.</p> <p></p> <p>We will use the pre-trained YOLOv8 Segmentation model. If you have already trained a model on Roboflow, you can alternatively choose one of your fine-tuned models or a model ID another user has shared on Roboflow Universe that predicts your desired objects of interest instead of the 80 classes of common objects that these models identify.</p> <p></p>"},{"location":"guides/compare-models/#add-second-model","title":"Add Second Model","text":"<p>Next, we will fork the execution flow by adding a second model alongside the first. Workflows will automatically parallelize execution for faster processing where possible. To do this, click the \"Fork\" button to the side of the first model block.</p> <p></p> <p>Then choose to add a second Instance Segmentation Model block.</p> <p></p> <p>And configure it to run the YOLOv11 Segmentation model.</p> <p></p>"},{"location":"guides/compare-models/#model-comparison","title":"Model Comparison","text":"<p>Next, we will add the Model Comparison Visualization Block that takes the predictions from two different models and visualizes the differences between them.</p> <p>Click the \"+\" button on the output of one of the two model blocks you added.</p> <p></p> <p>Then choose the Model Comparison Visualization Block.</p> <p></p> <p>Now we need to wire its second input up to the predictions outputted by our second model. We do this in the block configuration sidebar. This is also where we can change additional settings like colors and opacity.</p> <p></p>"},{"location":"guides/compare-models/#customize-outputs","title":"Customize Outputs","text":"<p>Instance Segmentation models return a lot of data to describe the complex shapes of detected objects. It can be useful for downstream tasks like measurement but for this tutorial, we only care about our visualization so we can remove the additional outputs to simplify the response.</p> <p>To delete the unneeded outputs, click the \"Outputs\" block and use the trash can icon for the predictions coming from our two model blocks.</p> <p></p> <p>This leaves us with only one output, the image coming from our Model Comparison Visualization block.</p> <p></p>"},{"location":"guides/compare-models/#test-the-results","title":"Test the Results","text":"<p>Now we're ready to test the Workflow. Click \"Test Workflow\" on the top right of the Builder UI and then add an image and click \"Run\".</p> <p></p> <p>The resulting visualization shows the comparison of the two models' predictions. Dark regions are predicted by neither model, bright regions are predicted by both model, green regions are predicted only by Model A (YOLOv8), and red regions are predicted only by Model B (YOLOv11).</p> <p>From this, we can see that YOLOv11 performs better than YOLOv8 on this image. It identifies two people that YOLOv8 missed, better captures peoples' feet and legs, and correctly does not predict parts of the background in the players' contours.</p> <p></p>"},{"location":"guides/compare-models/#next-steps","title":"Next Steps","text":"<p>In this tutorial we have learned how to configure a Workflow, run multiple models in parallel, and configure blocks with customized inputs. We could use this Workflow to evaluate other pre-trained models or to iterate on our own models' performanc and visualize the results over time as compared to an initial baseline.</p> <p>Next, let's learn how we can improve models' performance on small objects without changing the models themselves.</p>"},{"location":"guides/detect-small-objects/","title":"Detect Small Objects","text":"<p>The best way to improve the performance of a machine learning system is often peripheral to improving the model. Collecting more data, labeling datasets more accurately, adding redundancy to detections, or breaking down the problem into smaller pieces are usually higher leverage endeavors than iterating on model architecture.</p> <p>One common shortcoming of model performance occurs when the objects of interest are very small relative to the size of the image. One approach is tiling the image into smaller chunks and feeding them through the model in parallel. This method  is called SAHI (Slicing Aided Hyper Inference). A large image is divided into overlapping slices, each slice is run through a model, and a single prediction is reconstructed as the output.</p> <p>Using SAHI in Workflows is easy using the Image Slicer and Detections Stitch blocks.</p> <p>Difficulty: Easy Time to Complete: 10 minutes</p>"},{"location":"guides/detect-small-objects/#starting-point","title":"Starting Point","text":"<p>We will start with an output similar to the Hello World tutorial. Follow that guide to create a simple Workflow that visualizes predictions from an object detection model:</p> <p></p> <p>But when the objects are small, this model fails to find them.</p> <p></p> <p>Even though cars are one of the most widely represented classes in the MS COCO dataset that this model was trained on, it still only finds one of them because when the image is scaled down to the model's input resolution the cars become only a few pixels wide.</p> <p></p>"},{"location":"guides/detect-small-objects/#adding-sahi","title":"Adding SAHI","text":"<p>We will fix this by slicing the image into smaller tiles so that the model gets more pixels of information to work from.</p> <p></p> <p>The first step is to add the Image Slicer block to the Workflow. We will do this in a parallel execution path so that we can compare the outputs to the original non-SAHI predictions at the end.</p> <p></p> <p>Then we will add an Object Detection model that will run on each frame. Everything between an Image Slicer and an Image Stitch block operates element-wise on the set of slices. Workflows will automatically parallelize work (such as batching predictions through the GPU) where it can.</p> <p></p> <p>Next, we'll add the Detections Stitch block to aggregate the predictions back into the dimensionality of the original image.</p> <p></p> <p>Now, we can visualize these aggregated predictions in the same way as we visualized outputs of a regular model.</p> <p></p> <p>We will add a Background Color Visualization to dim regions of the image that were not predicted by the model to make the predicted areas stand out.</p> <p></p> <p>And a bounding box around predicted objects of interest.</p> <p></p>"},{"location":"guides/detect-small-objects/#testing-sahi","title":"Testing SAHI","text":"<p>Now we're ready to see how our revised approach is working. We'll simplify the output to predict only the two visualizations we're interested in.</p> <p></p> <p>When we run the Workflow we see that now all of the cars are detected!</p> <p></p> <p></p>"},{"location":"guides/detect-small-objects/#filter-unwanted-predictions","title":"Filter Unwanted Predictions","text":"<p>But the SAHI approach worked a little bit too well. The model is now picking up some objects from the background that we weren't interested in. Let's fix this by modifying the predictions from the model.</p> <p></p> <p>Add a Detections Filter block. This block lets us exclude predictions that don't match a set of criteria that we define.</p> <p></p> <p>To set our criteria, click the Operations configuration in the block's sidebar.</p> <p></p> <p>Then choose \"Filter By Class &amp; Confidence\".</p> <p></p> <p>And include only the <code>car</code> class.</p> <p></p> <p>Now, when we test our model again we see boxes around the cars and no extraneous detections in the background.</p> <p></p> <p></p>"},{"location":"guides/detect-small-objects/#next-steps","title":"Next Steps","text":"<p>In this tutorial we learned how to use SAHI to detect small objects, how to perform operations on a batch of images, and how to transform detections.</p> <p>Next, we will run a Workflow on a live video stream.</p>"},{"location":"guides/hello-world/","title":"Hello World","text":"<p>In this tutorial, we will build and run a simple Workflow to validate that our setup is installed and working correctly. We will run inference on a computer vision model and visualize its output via the UI debugger.</p> <p>Difficulty: Easy Time to Complete: 5 minutes</p>"},{"location":"guides/hello-world/#prerequisites","title":"Prerequisites","text":"Cloud ConnectedDetached <p>This tutorial only requires a free Roboflow account and can run on the Serverless Hosted API with no setup required. This is the easiest way to get started and you can migrate to self-hosting your Workflows later.</p> <p>You can also connect from the cloud platform to an Inference Server running locally by clicking the \"Running on\" selector at the top-left of the platform UI and pointing it to <code>localhost</code> or your server's IP.</p> <p>Once you have an account, create a new (empty) Workflow then continue below.</p> <p>In Detached mode, you run both the Inference Server and Workflow Builder UI locally without a Roboflow account or API Key. In Detached mode, you forego cloud connected functionality like remote deployment, monitoring, integration with the cloud model hub and dataset management platform, and are responsible for implementing your own access control.</p> <p>To run on your own machine without a Roboflow account, follow the installation instructions and start your Inference Server in development mode (using <code>inference server start --dev</code>).</p> <p>Then, navigate to the local Workflows builder at localhost:9001/build and create an empty Workflow using the purple \"Create a Workflow\" button. If prompted, choose \"Build My Own\".</p> <p>You should now have an empty Workflow and be ready to start building.</p> <p></p>"},{"location":"guides/hello-world/#add-a-model","title":"Add a Model","text":"<p>The first step is adding a Model Block. Click \"Add a Block\" to open the block selection sidebar.</p> <p></p> <p>For this guide, choose the object detection model block.</p> <p></p> <p>Then select a model. You can use a pre-trained model (trained on the 80 classes of common objects present in the Microsoft COCO dataset). Or, if you have linked your Roboflow account, any of your fine-tuned models or from the 100,000+ community-trained models shared on Roboflow Universe.</p> <p></p>"},{"location":"guides/hello-world/#test-your-workflow","title":"Test Your Workflow","text":"<p>Once you've added a model, you can test it on an image or video. Click \"Test Workflow\" on the top right, then add an image and click \"Run\". By default, your output will contain a JSON representation of the model's predictions.</p> <p></p>"},{"location":"guides/hello-world/#add-a-visualization","title":"Add a Visualization","text":"<p>To get a better view of what your model is predicting, add a visualization block by clicking the \"+\" button on the bottom of the Object Detection Model block. The \"Bounding Box Visualization\" and \"Label Visualization\" work well together.</p> <p></p> <p>Next, we will swap the order of the Outputs to show the visualization first (above the JSON) for convenience. Click the \"Outputs\" block then click the \"Move Up\" button for the visualization you selected.</p> <p></p> <p>Now, when we test our Workflow we see a rendered image in addition to the JSON. This can be useful both for debugging and as part of an app's UI.</p> <p></p> <p>We can also click on the thumbnail (or the \"Visual\" output toggle) to see a larger version of the image.</p> <p></p>"},{"location":"guides/hello-world/#deploy","title":"Deploy","text":"<p>Finally, we can use this Workflow as part of a larger application using the client code snippet accessible via the \"Deploy\" button at the top of the screen.</p> <p></p>"},{"location":"guides/hello-world/#next-steps","title":"Next Steps","text":"<p>Now that we've built a simple Workflow and validated that we can connect and run models on our Inference Server we can start building more complex and powerful Workflows like comparing the output of two models.</p>"},{"location":"include/install/","title":"Install","text":"<p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"include/model_id/","title":"Model id","text":"<p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"inference_helpers/inference_cli/","title":"About CLI","text":""},{"location":"inference_helpers/inference_cli/#roboflow-inference-cli","title":"Roboflow Inference CLI","text":"<p>Roboflow Inference CLI is command-line interface for <code>inference</code> ecosystem, providing an easy way to:</p> <ul> <li> <p>run and manage <code>inference</code> server locally</p> </li> <li> <p>process data with Workflows</p> </li> <li> <p>benchmark <code>inference</code> performance </p> </li> <li> <p>make predictions from your models</p> </li> <li> <p>deploy <code>inference</code> server in cloud</p> </li> </ul>"},{"location":"inference_helpers/inference_cli/#installation","title":"Installation","text":"<pre><code>pip install roboflow-cli\n</code></pre> <p><code>inference-cli</code> is part of <code>inference</code></p> <p>If you have installed <code>inference</code> Python package, the CLI extensions is already included.</p>"},{"location":"inference_helpers/inference_cli/#supported-devices","title":"Supported Devices","text":"<p>Roboflow Inference CLI currently supports the following device targets:</p> <ul> <li>x86 CPU</li> <li>ARM64 CPU</li> <li>NVIDIA GPU</li> </ul> <p>For Jetson specific inference server images, check out the Roboflow Inference package, or pull the images directly following instructions in the official Roboflow Inference documentation.</p>"},{"location":"inference_helpers/inference_landing_page/","title":"Inference landing page","text":"<p>The Roboflow Inference server hosts a landing page. This page contains links to helpful resources including documentation and examples.</p>"},{"location":"inference_helpers/inference_landing_page/#visit-the-inference-landing-page","title":"Visit the Inference Landing Page","text":"<p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>The easiest way to start an inference server is with the inference CLI. Install it via pip:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Now run the <code>inference sever start</code> command.</p> <pre><code>inference server start\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. This page contains links to resources and examples related to <code>inference</code>.</p>"},{"location":"inference_helpers/inference_landing_page/#inference-notebook","title":"Inference Notebook","text":"<p>Roboflow Inference Servers come equipped with a built in Jupyterlab environment. This environment is the fastest way to get up and running with inference for development and testing. To use it, first start an inference server. Be sure to specify the <code>--dev</code> flag so that the notebook environment is enabled (it is disabled by default).</p> <pre><code>inference server start --dev\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. From the landing page, select the button labeled \"Jump Into an Inference Enabled Notebook\" to open a new tab for the Jupyterlab environment.</p> <p>This Jupyterlab environment comes preloaded with several example notebooks and all of the dependencies needed to run <code>inference</code>.</p>"},{"location":"inference_helpers/inference_sdk/","title":"Inference Client","text":"<p>The <code>InferenceHTTPClient</code> enables you to interact with Inference over HTTP.</p> <p>You can use this client to run models hosted:</p> <ol> <li>On the Roboflow platform (use client version <code>v0</code>), and;</li> <li>On device with Inference.</li> </ol> <p>For models trained on the Roboflow platform, client accepts the following inputs:</p> <ul> <li>A single image (Given as a local path, URL, <code>np.ndarray</code> or <code>PIL.Image</code>);</li> <li>Multiple images;</li> <li>A directory of images, or;</li> <li>A video file.</li> <li>Single image encoded as <code>base64</code></li> </ul> <p>For core model - client exposes dedicated methods to be used, but standard image loader used accepts file paths, URLs, <code>np.ndarray</code> and <code>PIL.Image</code> formats. Apart from client version (<code>v0</code> or <code>v1</code>) - options provided via configuration are used against models trained at the platform, not the core models.</p> <p>The client returns a dictionary of predictions for each image or frame.</p> <p>Starting from <code>0.9.10</code> - <code>InferenceHTTPClient</code> provides async equivalents for the majority of methods and support for requests parallelism and batching implemented (yet in limited scope, not for all methods).  Further details to be found in specific sections of this document. </p> <p>Tip</p> <p>Read our Run Model on an Image guide to learn how to run a model with the Inference Client.</p>"},{"location":"inference_helpers/inference_sdk/#quickstart","title":"Quickstart","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\nresult = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#asyncio-client","title":"AsyncIO client","text":"<pre><code>import asyncio\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\nloop = asyncio.get_event_loop()\nresult = loop.run_until_complete(\n  CLIENT.infer_async(image_url, model_id=\"soccer-players-5fuqs/1\")\n)\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#configuration-options-used-for-models-trained-on-the-roboflow-platform","title":"Configuration options (used for models trained on the Roboflow platform)","text":""},{"location":"inference_helpers/inference_sdk/#configuring-with-context-managers","title":"configuring with context managers","text":"<p>Methods <code>use_configuration(...)</code>, <code>use_api_v0(...)</code>, <code>use_api_v1(...)</code>, <code>use_model(...)</code> are designed to work in context managers. Once context manager is left - old config values are restored.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.8)\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nwith CLIENT.use_api_v0():\n    _ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\nwith CLIENT.use_configuration(custom_configuration):\n    _ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\nwith CLIENT.use_model(\"soccer-players-5fuqs/1\"):\n    _ = CLIENT.infer(image_url)\n\n# after leaving context manager - changes are reverted and `model_id` is still required\n_ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n</code></pre> <p>As you can see - <code>model_id</code> is required to be given for prediction method only when default model is not configured.</p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"inference_helpers/inference_sdk/#setting-the-configuration-once-and-using-till-next-change","title":"Setting the configuration once and using till next change","text":"<p>Methods <code>configure(...)</code>, <code>select_api_v0(...)</code>, <code>select_api_v1(...)</code>, <code>select_model(...)</code> are designed alter the client state and will be preserved until next change.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.8)\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.select_api_v0()\n_ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\n# API v0 still holds\nCLIENT.configure(custom_configuration)\nCLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\n# API v0 and custom configuration still holds\nCLIENT.select_model(model_id=\"soccer-players-5fuqs/1\")\n_ = CLIENT.infer(image_url)\n\n# API v0, custom configuration and selected model - still holds\n_ = CLIENT.infer(image_url)\n</code></pre> <p>One may also initialise in <code>chain</code> mode:</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(api_url=\"http://localhost:9001\", api_key=\"ROBOFLOW_API_KEY\") \\\n    .select_api_v0() \\\n    .select_model(\"soccer-players-5fuqs/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#overriding-model_id-for-specific-call","title":"Overriding <code>model_id</code> for specific call","text":"<p><code>model_id</code> can be overriden for specific call</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(api_url=\"http://localhost:9001\", api_key=\"ROBOFLOW_API_KEY\") \\\n    .select_model(\"soccer-players-5fuqs/1\")\n\n_ = CLIENT.infer(image_url, model_id=\"another-model/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#parallel-batch-inference","title":"Parallel / Batch inference","text":"<p>You may want to predict against multiple images at single call. There are two parameters of <code>InferenceConfiguration</code> that specifies batching and parallelism options: - <code>max_concurrent_requests</code> - max number of concurrent requests that can be started  - <code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only  support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code> to prevent errors)</p> <p>Thanks to that the following improvements can be achieved: - if you run inference container with API on prem on powerful GPU machine - setting <code>max_batch_size</code> properly may bring performance / throughput benefits - if you run inference against hosted Roboflow API - setting <code>max_concurrent_requests</code> will cause multiple images being served at once bringing performance / throughput benefits - combination of both options can be beneficial for clients running inference container with API on cluster of machines, then the load of single node can be optimised and parallel requests to different nodes can be made at a time  ``</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\npredictions = CLIENT.infer([image_url] * 5, model_id=\"soccer-players-5fuqs/1\")\n\nprint(predictions)\n</code></pre> <p>Methods that support batching / parallelism: -<code>infer(...)</code> and <code>infer_async(...)</code> - <code>infer_from_api_v0(...)</code> and <code>infer_from_api_v0_async(...)</code> (enforcing <code>max_batch_size=1</code>) - <code>ocr_image(...)</code> and <code>ocr_image_async(...)</code> (enforcing <code>max_batch_size=1</code>) - <code>detect_gazes(...)</code> and <code>detect_gazes_async(...)</code> - <code>get_clip_image_embeddings(...)</code> and <code>get_clip_image_embeddings_async(...)</code></p>"},{"location":"inference_helpers/inference_sdk/#client-for-core-models","title":"Client for core models","text":"<p><code>InferenceHTTPClient</code> now supports core models hosted via <code>inference</code>. Part of the models can be used on the Roboflow  hosted inference platform (use <code>https://infer.roboflow.com</code> as url), other are possible to be deployed locally (usually local server will be available under <code>http://localhost:9001</code>).</p> <p>Tip</p> <p>Install <code>inference-cli</code> package to easily run <code>inference</code> API locally <pre><code>pip install inference-cli\ninference server start\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#clip","title":"Clip","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.get_clip_image_embeddings(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.get_clip_image_embeddings(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\nCLIENT.get_clip_text_embeddings(text=\"some\")  # single text request\nCLIENT.get_clip_text_embeddings(text=[\"some\", \"other\"])  # other text request\nCLIENT.clip_compare(\n    subject=\"./my_image.jpg\",\n    prompt=[\"fox\", \"dog\"],\n)\n</code></pre> <p><code>CLIENT.clip_compare(...)</code> method allows to compare different combination of <code>subject_type</code> and <code>prompt_type</code>:</p> <ul> <li><code>(image, image)</code></li> <li><code>(image, text)</code></li> <li><code>(text, image)</code></li> <li><code>(text, text)</code>   Default mode is <code>(image, text)</code>.</li> </ul> <p>Tip</p> <p>Check out async methods for Clip model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.get_clip_image_embeddings_async(inference_input=\"./my_image.jpg\")  # single image request\n  await CLIENT.get_clip_image_embeddings_async(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n  await CLIENT.get_clip_text_embeddings_async(text=\"some\")  # single text request\n  await CLIENT.get_clip_text_embeddings_async(text=[\"some\", \"other\"])  # other text request\n  await CLIENT.clip_compare_async(\n      subject=\"./my_image.jpg\",\n      prompt=[\"fox\", \"dog\"],\n  )\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#cogvlm-deprecated-in-inference-0380","title":"CogVLM - DEPRECATED in <code>inference 0.38.0</code>","text":"<p>Method deprecated</p> <p>CogVLM was deprecated in <code>inference 0.38.0</code> due to changes we need to apply to mitigate  CVE-2024-11393.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.prompt_cogvlm(\n    visual_prompt=\"./my_image.jpg\",\n    text_prompt=\"So - what is your final judgement about the content of the picture?\",\n    chat_history=[(\"I think the image shows XXX\", \"You are wrong - the image shows YYY\")], # optional parameter\n)\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#doctr","title":"DocTR","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.ocr_image(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.ocr_image(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n</code></pre> <p>Tip</p> <p>Check out async methods for DocTR model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.ocr_image(inference_input=\"./my_image.jpg\")  # single image request\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#gaze","title":"Gaze","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.detect_gazes(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.detect_gazes(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n</code></pre> <p>Tip</p> <p>Check out async methods for Gaze model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.detect_gazes(inference_input=\"./my_image.jpg\")  # single image request\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#inference-against-stream","title":"Inference against stream","text":"<p>One may want to infer against video or directory of images - and that modes are supported in <code>inference-client</code></p> <pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nfor frame_id, frame, prediction in CLIENT.infer_on_stream(\"video.mp4\", model_id=\"soccer-players-5fuqs/1\"):\n    # frame_id is the number of frame\n    # frame - np.ndarray with video frame\n    # prediction - prediction from the model\n    pass\n\nfor file_path, image, prediction in CLIENT.infer_on_stream(\"local/dir/\", model_id=\"soccer-players-5fuqs/1\"):\n    # file_path - path to the image\n    # frame - np.ndarray with video frame\n    # prediction - prediction from the model\n    pass\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#what-is-actually-returned-as-prediction","title":"What is actually returned as prediction?","text":"<p><code>inference_client</code> returns plain Python dictionaries that are responses from model serving API. Modification is done only in context of <code>visualization</code> key that keep server-generated prediction visualisation (it can be transcoded to the format of choice) and in terms of client-side re-scaling.</p>"},{"location":"inference_helpers/inference_sdk/#methods-to-control-inference-server-in-v1-mode-only","title":"Methods to control <code>inference</code> server (in <code>v1</code> mode only)","text":""},{"location":"inference_helpers/inference_sdk/#getting-server-info","title":"Getting server info","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.get_server_info()\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#listing-loaded-models","title":"Listing loaded models","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.list_loaded_models()\n</code></pre> <p>Tip</p> <p>This method has async equivaluent: <code>list_loaded_models_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#getting-specific-model-description","title":"Getting specific model description","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.get_model_description(model_id=\"some/1\", allow_loading=True)\n</code></pre> <p>If <code>allow_loading</code> is set to <code>True</code>: model will be loaded as side-effect if it is not already loaded. Default: <code>True</code>.</p> <p>Tip</p> <p>This method has async equivaluent: <code>get_model_description_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#loading-model","title":"Loading model","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.load_model(model_id=\"some/1\", set_as_default=True)\n</code></pre> <p>The pointed model will be loaded. If <code>set_as_default</code> is set to <code>True</code>: after successful load, model will be used as default model for the client. Default value: <code>False</code>.</p> <p>Tip</p> <p>This method has async equivaluent: <code>load_model_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#unloading-model","title":"Unloading model","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.unload_model(model_id=\"some/1\")\n</code></pre> <p>Sometimes (to avoid OOM at server side) - unloading model will be required.</p> <p>Tip</p> <p>This method has async equivaluent: <code>unload_model_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#unloading-all-models","title":"Unloading all models","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.unload_all_models()\n</code></pre> <p>Tip</p> <p>This method has async equivaluent: <code>unload_all_models_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#inference-workflows","title":"Inference <code>workflows</code>","text":"<p>Tip</p> <p>This feature is in <code>alpha</code> preview. We encourage you to experiment and reach out to us with issues spotted. Check out documentation of deployment specs, create one and run</p> <p>Tip</p> <p>This feature only works with locally hosted inference container and hosted platform (access may be limited).  Use inefernce-cli to run local container with HTTP API: <pre><code>inference server start\n</code></pre></p> <p>Warning</p> <p>Method <code>infer_from_workflow(...)</code> is deprecated starting from <code>v0.9.21</code> and  will be removed end of Q2 2024. Please migrate - the signature is the same,  what changes is underlying inference server endpoint used to run workflow.</p> <p>New method is called <code>run_workflow(...)</code> and is compatible with Roboflow hosted API and inverence servers in versions <code>0.9.21+</code> </p> <pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\n# for older versions of server than v0.9.21 use: CLIENT.infer_from_workflow(...) \nCLIENT.run_workflow(\n    specification={\n        \"version\": \"1.0\",\n        \"inputs\": [\n            {\"type\": \"InferenceImage\", \"name\": \"image\"},\n            {\"type\": \"InferenceParameter\", \"name\": \"my_param\"},\n        ],\n        # ...\n    },\n    # OR\n    # workspace_name=\"my_workspace_name\",\n    # workflow_id=\"my_workflow_id\",\n\n    images={\n        \"image\": \"url or your np.array\",\n    },\n    parameters={\n        \"my_param\": 37,\n    },\n)\n</code></pre> <p>Please note that either <code>specification</code> is provided with specification of workflow as described here or  both <code>workspace_name</code> and <code>workflow_id</code> are given to use workflow predefined in Roboflow app. <code>workspace_name</code> can be found in Roboflow APP URL once browser shows the main panel of workspace.</p> <p>Server-side caching of Workflow definitions</p> <p>In <code>inference v0.22.0</code> we've added server-side caching of Workflows reginsted on Roboflow platform which is enabled by default. When you use <code>run_workflow(...)</code> method with <code>workspace_name</code> and <code>workflow_id</code> server will cache the definition for 15 minutes. If you change the definition in Workflows UI and re-run the method, you may not see the change. To force processing without cache, pass <code>use_cache=False</code> as a parameter of  <code>run_workflow(...)</code> method. </p> <p>Workflows profiling</p> <p>Since <code>inference v0.22.0</code>, you may request profiler trace of your Workflow execution from server passing  <code>enable_profiling=True</code> parameter to <code>run_workflow(...)</code> method. If server configuration enables traces exposure, you will be able to find a JSON file with trace in a directory specified by <code>profiling_directory</code> parameter of  <code>InferenceConfiguration</code> - by default it is <code>inference_profiling</code> directory in your current working directory. The traces can be directly loaded and rendered in Google Chrome - navigate into <code>chrome://tracing</code> in your  borwser and hit \"load\" button. </p>"},{"location":"inference_helpers/inference_sdk/#details-about-client-configuration","title":"Details about client configuration","text":"<p><code>inference-client</code> provides <code>InferenceConfiguration</code> dataclass to hold whole configuration.</p> <pre><code>from inference_sdk import InferenceConfiguration\n</code></pre> <p>Overriding fields in this config changes the behaviour of client (and API serving model). Specific fields are used in specific contexts. In particular:</p>"},{"location":"inference_helpers/inference_sdk/#inference-in-v0-mode","title":"Inference in <code>v0</code> mode","text":"<p>The following fields are passed to API</p> <ul> <li><code>confidence_threshold</code> (as <code>confidence</code>) - to alter model thresholding</li> <li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints   based on model confidence</li> <li><code>format</code>: to visualise on server side - use <code>image</code> (just the image) or <code>image_and_json</code> (prediction details and image base64)</li> <li><code>visualize_labels</code> (as <code>labels</code>) - used in visualisation to show / hide labels for classes</li> <li><code>mask_decode_mode</code></li> <li><code>tradeoff_factor</code></li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>iou_threshold</code> (as <code>overlap</code>) - to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>count_inference</code> as <code>countinference</code></li> <li><code>service_secret</code></li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#classification-model-in-v1-mode","title":"Classification model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li> <p><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></p> </li> <li> <p><code>visualize_predictions</code>: flag to enable / disable visualisation</p> </li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li> <p><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful, for instance, while testing the model)</p> </li> <li> <p><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</p> </li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> </ul>"},{"location":"inference_helpers/inference_sdk/#object-detection-model-in-v1-mode","title":"Object detection model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>class_filter</code> to filter out list of classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#keypoints-detection-model-in-v1-mode","title":"Keypoints detection model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints   based on model confidence</li> <li><code>class_filter</code> to filter out list of object classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#instance-segmentation-model-in-v1-mode","title":"Instance segmentation model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>class_filter</code> to filter out list of classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>mask_decode_mode</code></li> <li><code>tradeoff_factor</code></li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#configuration-of-client","title":"Configuration of client","text":"<ul> <li><code>output_visualisation_format</code>: one of (<code>VisualisationResponseFormat.BASE64</code>, <code>VisualisationResponseFormat.NUMPY</code>,   <code>VisualisationResponseFormat.PILLOW</code>) - given that server-side visualisation is enabled - one may choose what   format should be used in output</li> <li><code>image_extensions_for_directory_scan</code>: while using <code>CLIENT.infer_on_stream(...)</code> with local directory   this parameter controls type of files (extensions) allowed to be processed -   default: <code>[\"jpg\", \"jpeg\", \"JPG\", \"JPEG\", \"png\", \"PNG\"]</code></li> <li><code>client_downsizing_disabled</code>: set to <code>False</code> if you want to perform client-side downsizing - default <code>True</code> (   changed in version <code>0.16.0</code> - previously was <code>False</code>).   Client-side scaling is only supposed to down-scale (keeping aspect-ratio) the input for inference -   to utilise internet connection more efficiently (but for the price of images manipulation / transcoding).   If model registry endpoint is available (mode <code>v1</code>) - model input size information will be used, if not:   <code>default_max_input_size</code> will be in use.</li> <li><code>max_concurrent_requests</code> - max number of concurrent requests that can be started </li> <li><code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only  support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code> to prevent errors)</li> <li><code>workflow_run_retries_enabled</code> - flag that decides if transient errors in Workflows executions should be retried.  Defaults to <code>true</code> and the default can be altered with environment variable called <code>WORKFLOW_RUN_RETRIES_ENABLED</code></li> </ul> <p>Warning</p> <p>The default value for flag <code>client_downsizing_disabled</code> was changed from <code>False</code> to <code>True</code> in release <code>0.16.0</code>! For clients using models with input size above <code>1024x1024</code> running models on hosted  platform it should improve predictions quality (as previous default behaviour was causing that input was downsized  and then artificially upsized on the server side with worse image quality).  There may be some clients that would like to remain previous settings to potentially improve speed ( when internet connection is a bottleneck and large images are submitted despite small  model input size). </p>"},{"location":"inference_helpers/inference_sdk/#configuration-of-workflows-execution","title":"Configuration of Workflows execution","text":"<ul> <li><code>profiling_directory</code>: parameter specify the location where Workflows profiler traces are saved. By default, it is <code>./inference_profiling</code> directory.</li> </ul>"},{"location":"inference_helpers/inference_sdk/#faqs","title":"FAQs","text":""},{"location":"inference_helpers/inference_sdk/#why-does-the-inference-client-have-two-modes-v0-and-v1","title":"Why does the Inference client have two modes (<code>v0</code> and <code>v1</code>)?","text":"<p>We are constantly improving our <code>infrence</code> package - initial version (<code>v0</code>) is compatible with models deployed on the Roboflow platform (task types: <code>classification</code>, <code>object-detection</code>, <code>instance-segmentation</code> and <code>keypoints-detection</code>) are supported. Version <code>v1</code> is available in locally hosted Docker images with HTTP API.</p> <p>Locally hosted <code>inference</code> server exposes endpoints for model manipulations, but those endpoints are not available at the moment for models deployed on the Roboflow platform.</p> <p><code>api_url</code> parameter passed to <code>InferenceHTTPClient</code> will decide on default client mode - URLs with <code>*.roboflow.com</code> will be defaulted to version <code>v0</code>.</p> <p>Usage of model registry control methods with <code>v0</code> clients will raise <code>WrongClientModeError</code>.</p>"},{"location":"inference_helpers/cli_commands/benchmark/","title":"Benchmarking <code>inference</code>","text":"<p><code>inference benchmark</code> offers you an easy way to check the performance of <code>inference</code> in your setup. The command  is capable of benchmarking both <code>inference</code> server and <code>inference</code> Python package.</p> <p>Discovering command capabilities</p> <p>To check detail of the command, run:</p> <pre><code>inference benchmark --help\n</code></pre> <p>Additionally, help guide is also available for each sub-command:</p> <pre><code>inference benchmark api-speed --help\n</code></pre>"},{"location":"inference_helpers/cli_commands/benchmark/#benchmarking-inference-python-package","title":"Benchmarking <code>inference</code> Python package","text":"<p><code>inference</code> needs to be installed</p> <p>Running this command, make sure <code>inference</code> package is installed.</p> <pre><code>pip install inference\n</code></pre> <p>Basic benchmark can be run using the following command: </p> <p><pre><code>inference benchmark python-package-speed \\\n  -m {your_model_id} \\\n  -d {pre-configured dataset name or path to directory with images} \\\n  -o {output_directory}  \n</code></pre> Command runs specified number of inferences using pointed model and saves statistics (including benchmark  parameter, throughput, latency, errors and platform details) in pointed directory.</p>"},{"location":"inference_helpers/cli_commands/benchmark/#benchmarking-inference-server","title":"Benchmarking <code>inference</code> server","text":"<p>Note</p> <p>Before running API benchmark of your local <code>inference</code> server - make sure the server is up and running:</p> <pre><code>inference server start\n</code></pre> <p>Basic benchmark can be run using the following command: </p> <p><pre><code>inference benchmark api-speed \\\n  -m {your_model_id} \\\n  -d {pre-configured dataset name or path to directory with images} \\\n  -o {output_directory}  \n</code></pre> Command runs specified number of inferences using pointed model and saves statistics (including benchmark  parameter, throughput, latency, errors and platform details) in pointed directory.</p> <p>This benchmark has more configuration options to support different ways HTTP API profiling. In default mode, single client will be spawned, and it will send one request after another sequentially. This may be suboptimal in specific cases, so one may specify number of concurrent clients using <code>-c {number_of_clients}</code> option. Each client will send next request once previous is handled. This option will also not cover all scenarios of tests. For instance one may want to send <code>x</code> requests each second (which is closer to the scenario of production environment where multiple clients are sending requests concurrently). In this scenario, <code>--rps {value}</code>  option can be used (and <code>-c</code> will be ignored). Value provided in <code>--rps</code> option specifies how many requests  are to be spawned each second without waiting for previous requests to be handled. In I/O intensive benchmark  scenarios - we suggest running command from multiple separate processes and possibly multiple hosts.</p>"},{"location":"inference_helpers/cli_commands/cloud/","title":"Deploying <code>inference</code> to Cloud","text":"<p>You can deploy Roboflow Inference containers to virtual machines in the cloud. These VMs are configured to run CPU or  GPU-based Inference servers under the hood, so you don't have to deal with OS/GPU drivers/docker installations, etc!  The Inference cli currently supports deploying the Roboflow Inference container images into a virtual machine running  on Google (GCP) or Amazon cloud (AWS).</p> <p>The Roboflow Inference CLI assumes the corresponding cloud CLI is configured for the project you want to deploy the  virtual machine into. Read instructions for setting up Google/GCP - gcloud cli or the Amazon/AWS aws cli.</p> <p>Roboflow Inference cloud deploy is powered by the popular Skypilot project.</p> <p>Make sure <code>cloud-deploy</code> extras is installed</p> <p>To run commands presented below, you need to have <code>cloud-deploy</code> extras installed:</p> <pre><code>pip install \"inference-cli[cloud-deploy]\"\n</code></pre> <p>Discovering command capabilities</p> <p>To check detail of the command, run:</p> <pre><code>inference cloud --help\n</code></pre> <p>Additionally, help guide is also available for each sub-command:</p> <pre><code>inference cloud deploy --help\n</code></pre>"},{"location":"inference_helpers/cli_commands/cloud/#inference-cloud-deploy","title":"<code>inference cloud deploy</code>","text":"<p>We illustrate Inference cloud deploy with some examples, below.</p> <p>Deploy GPU or CPU inference to AWS or GCP</p> <pre><code># Deploy the roboflow Inference GPU container into a GPU-enabled VM in AWS\n\ninference cloud deploy --provider aws --compute-type gpu\n</code></pre> <pre><code># Deploy the roboflow Inference CPU container into a CPU-only VM in GCP\n\ninference cloud deploy --provider gcp --compute-type cpu\n</code></pre> <p>Note the \"cluster name\" printed after the deployment completes. This handle is used in many subsequent commands. The deploy command also prints helpful debug and cost information about your VM.</p> <p>Deploying Inference into a cloud VM will also print out an endpoint of the form \"http://1.2.3.4:9001\"; you can now run inferences against this endpoint.</p> <p>Note that the port 9001 is automatically opened - check with your security admin if this is acceptable for your cloud/project.</p>"},{"location":"inference_helpers/cli_commands/cloud/#inference-cloud-status","title":"<code>inference cloud status</code>","text":"<p>To check the status of your deployment, run:</p> <pre><code>inference cloud status\n</code></pre>"},{"location":"inference_helpers/cli_commands/cloud/#stop-and-start-deployments","title":"Stop and start deployments","text":"<p>You can start and stop your deployment using:</p> <pre><code>inference cloud start &lt;deployment_handle&gt;\n</code></pre> <p>and</p> <pre><code># Stop the VM, you only pay for disk storage while the VM is stopped\ninference cloud stop &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/cli_commands/cloud/#inference-cloud-undeploy","title":"<code>inference cloud undeploy</code>","text":"<p>To delete (undeploy) your deployment, run:</p> <pre><code>inference cloud undeploy &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/cli_commands/cloud/#ssh-into-the-cloud-deployment","title":"SSH into the cloud deployment","text":"<p>You can SSH into your cloud deployment with the following command: <pre><code>ssh &lt;deployment_handle&gt;\n</code></pre></p> <p>The required SSH key is automatically added to your <code>~/.ssh/config</code>, you don't need to configure this manually.</p>"},{"location":"inference_helpers/cli_commands/cloud/#cloud-deploy-customization","title":"Cloud Deploy Customization","text":"<p>Roboflow Inference cloud deploy will create VMs based on internally tested templates.</p> <p>For advanced usecases and to customize the template, you can use your sky yaml template on the command-line, like so:</p> <pre><code>inference cloud deploy --custom /path/to/sky-template.yaml\n</code></pre> <p>If you want you can download the standard template stored in the roboflow cli and the modify it for your needs, this command will do that.</p> <pre><code># This command will print out the standard gcp/cpu sky template.\ninference cloud deploy --dry-run --provider gcp --compute-type cpu\n</code></pre> <p>Then you can deploy a custom template based off your changes.</p> <p>As an aside, you can also use the sky cli to control your deployment(s) and access some more advanced functionality.</p> <p>Roboflow Inference deploy currently supports AWS and GCP, please open an issue on the Inference GitHub repository if you would like to see other cloud providers supported.</p>"},{"location":"inference_helpers/cli_commands/infer/","title":"Making predictions from your models","text":"<p><code>inference infer</code> command offers an easy way to make predictions from your model based on your input images or video files sending requests to <code>inference</code> server, depending on command configuration.</p> <p>Discovering command capabilities</p> <p>To check detail of the command, run:</p> <pre><code>inference infer --help\n</code></pre>"},{"location":"inference_helpers/cli_commands/infer/#command-details","title":"Command details","text":"<p><code>inference infer</code> takes input path / url and model version to produce predictions (and optionally make visualisation  using <code>supervision</code>). You can also specify a host to run inference on our hosted inference server.</p> <p>Note</p> <p>If you decided to use hosted inference server - make sure command <code>inference server start</code> was used first</p> <p>Tip</p> <p>Roboflow API key can be provided via <code>ROBOFLOW_API_KEY</code> environment variable</p>"},{"location":"inference_helpers/cli_commands/infer/#examples","title":"Examples","text":"<p>Below, you have usage examples illustrated.</p>"},{"location":"inference_helpers/cli_commands/infer/#predict-on-local-image","title":"Predict On Local Image","text":"<p>This command is going to make a prediction from local image using selected model and print the prediction on  the console.</p> <pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY}\n</code></pre> <p>To display visualised prediction use <code>-D</code> option. To save prediction and visualisation in a local directory, use <code>-o {path_to_your_directory}</code> option. Those options work also in other modes.</p> <pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY} -D -o {path_to_your_output_directory}\n</code></pre>"},{"location":"inference_helpers/cli_commands/infer/#predict-on-image-url","title":"Predict On Image URL","text":"<pre><code>inference infer -i https://[YOUR_HOSTED_IMAGE_URL] -m {your_project}/{version} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/cli_commands/infer/#using-hosted-api","title":"Using Hosted API","text":"<pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY} -h https://detect.roboflow.com\n</code></pre>"},{"location":"inference_helpers/cli_commands/infer/#predict-from-local-directory","title":"Predict From Local Directory","text":"<pre><code>inference infer -i {your_directory_with_images} -m {your_project}/{version} -o {path_to_your_output_directory} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/cli_commands/infer/#predict-on-video-file","title":"Predict On Video File","text":"<pre><code>inference infer -i {path_to_your_video_file} -m {your_project}/{version} -o {path_to_your_output_directory} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/cli_commands/infer/#configure-the-visualization","title":"Configure The Visualization","text":"<p>Option <code>-c</code> can be provided with a path to <code>*.yml</code> file configuring <code>supervision</code> visualisation. There are few pre-defined configs: - <code>bounding_boxes</code> - with <code>BoxAnnotator</code> and <code>LabelAnnotator</code> annotators - <code>bounding_boxes_tracing</code> - with <code>ByteTracker</code> and annotators (<code>BoxAnnotator</code>, <code>LabelAnnotator</code>) - <code>masks</code> - with <code>MaskAnnotator</code> and <code>LabelAnnotator</code> annotators - <code>polygons</code> - with <code>PolygonAnnotator</code> and <code>LabelAnnotator</code> annotators</p> <p>Custom configuration can be created following the schema: <pre><code>annotators:\n  - type: \"bounding_box\"\n    params:\n      thickness: 2\n  - type: \"label\"\n    params:\n      text_scale: 0.5\n      text_thickness: 2\n      text_padding: 5\n  - type: \"trace\"\n    params:\n      trace_length: 60\n      thickness: 2\ntracking:\n  track_activation_threshold: 0.25\n  lost_track_buffer: 30\n  minimum_matching_threshold: 0.8\n  frame_rate: 30\n</code></pre> <code>annotators</code> field is a list of dictionaries with two keys: <code>type</code> and <code>param</code>. <code>type</code> points to  name of annotator class: <pre><code>from supervision import *\nANNOTATOR_TYPE2CLASS = {\n    \"bounding_box\": BoxAnnotator,\n    \"box\": BoxAnnotator,\n    \"mask\": MaskAnnotator,\n    \"polygon\": PolygonAnnotator,\n    \"color\": ColorAnnotator,\n    \"halo\": HaloAnnotator,\n    \"ellipse\": EllipseAnnotator,\n    \"box_corner\": BoxCornerAnnotator,\n    \"circle\": CircleAnnotator,\n    \"dot\": DotAnnotator,\n    \"label\": LabelAnnotator,\n    \"blur\": BlurAnnotator,\n    \"trace\": TraceAnnotator,\n    \"heat_map\": HeatMapAnnotator,\n    \"pixelate\": PixelateAnnotator,\n    \"triangle\": TriangleAnnotator,\n}\n</code></pre> <code>param</code> is a dictionary of annotator constructor parameters (check them in  <code>supervision</code> docs - you would only be able to use primitive values, classes and enums that are defined in constructors may not be possible to resolve from yaml config).</p> <p><code>tracking</code> is an optional key that holds a dictionary with constructor parameters for <code>ByteTrack</code>.</p>"},{"location":"inference_helpers/cli_commands/infer/#provide-inference-hyperparameters","title":"Provide Inference Hyperparameters","text":"<p><code>-mc</code> parameter can be provided with path to <code>*.yml</code> file that specifies  model configuration (like confidence threshold or IoU threshold). If given, configuration will be used to initialise <code>InferenceConfiguration</code> object from <code>inference_sdk</code> library. See sdk docs to discover which options can be configured via <code>*.yml</code> file - configuration keys must match with names of fields in <code>InferenceConfiguration</code> object.</p>"},{"location":"inference_helpers/cli_commands/reference/","title":"<code>inference</code>","text":"<p>Usage:</p> <pre><code>$ inference [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--version</code></li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>infer</code></li> <li><code>server</code>: Commands for running the inference server...</li> <li><code>cloud</code>: Commands for running the inference in...</li> <li><code>benchmark</code>: Commands for running inference benchmarks.</li> <li><code>workflows</code>: Commands for interacting with Roboflow...</li> <li><code>rf-cloud</code>: Commands for interacting with Roboflow Cloud</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-infer","title":"<code>inference infer</code>","text":"<p>Usage:</p> <pre><code>$ inference infer [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-i, --input TEXT</code>: URL or local path of image / directory with images or video to run inference on.  [required]</li> <li><code>-m, --model_id TEXT</code>: Model ID in format project/version.  [required]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-h, --host TEXT</code>: Host to run inference on.  [default: http://localhost:9001]</li> <li><code>-o, --output_location TEXT</code>: Location where to save the result (path to directory)</li> <li><code>-D, --display / -d, --no-display</code>: Boolean flag to decide if visualisations should be displayed on the screen  [default: no-display]</li> <li><code>-V, --visualise / -v, --no-visualise</code>: Boolean flag to decide if visualisations should be preserved  [default: visualise]</li> <li><code>-c, --visualisation_config TEXT</code>: Location of yaml file with visualisation config</li> <li><code>-mc, --model_config TEXT</code>: Location of yaml file with model config</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-server","title":"<code>inference server</code>","text":"<p>Commands for running the inference server locally. </p> <p>Supported devices targets are x86 CPU, ARM64 CPU, and NVIDIA GPU.</p> <p>Usage:</p> <pre><code>$ inference server [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>start</code></li> <li><code>status</code></li> <li><code>stop</code></li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-server-start","title":"<code>inference server start</code>","text":"<p>Usage:</p> <pre><code>$ inference server start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-p, --port INTEGER</code>: Port to run the inference server on (default is 9001).  [default: 9001]</li> <li><code>-rfe, --rf-env TEXT</code>: Roboflow environment to run the inference server with (default is roboflow-platform).  [default: roboflow-platform]</li> <li><code>-e, --env-file TEXT</code>: Path to file with env variables (in each line KEY=VALUE). Optional. If given - values will be overriden by any explicit parameter of this command.</li> <li><code>-d, --dev</code>: Run inference server in development mode (default is False).</li> <li><code>-k, --roboflow-api-key TEXT</code>: Roboflow API key (default is None).</li> <li><code>--tunnel</code>: Start a tunnel to expose inference to external requests on a TLS-enabled https://&lt;subdomain&gt;.roboflow.run endpoint</li> <li><code>--image TEXT</code>: Point specific docker image you would like to run with command (useful for development of custom builds of inference server)</li> <li><code>--use-local-images / --not-use-local-images</code>: Flag to allow using local images (if set False image is always attempted to be pulled)  [default: not-use-local-images]</li> <li><code>--metrics-enabled / --metrics-disabled</code>: Flag controlling if metrics are enabled (default is True)  [default: metrics-enabled]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-server-status","title":"<code>inference server status</code>","text":"<p>Usage:</p> <pre><code>$ inference server status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-server-stop","title":"<code>inference server stop</code>","text":"<p>Usage:</p> <pre><code>$ inference server stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-cloud","title":"<code>inference cloud</code>","text":"<p>Commands for running the inference in cloud with skypilot. </p> <p>Supported devices targets are x86 CPU and NVIDIA GPU VMs.</p> <p>Usage:</p> <pre><code>$ inference cloud [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>status</code></li> <li><code>deploy</code></li> <li><code>undeploy</code></li> <li><code>stop</code></li> <li><code>start</code></li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-cloud-status","title":"<code>inference cloud status</code>","text":"<p>Usage:</p> <pre><code>$ inference cloud status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-cloud-deploy","title":"<code>inference cloud deploy</code>","text":"<p>Usage:</p> <pre><code>$ inference cloud deploy [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-p, --provider TEXT</code>: Cloud provider to deploy to. Currently aws or gcp.  [required]</li> <li><code>-t, --compute-type TEXT</code>: Execution environment to deploy to: cpu or gpu.  [required]</li> <li><code>-d, --dry-run</code>: Print out deployment plan without executing.</li> <li><code>-c, --custom TEXT</code>: Path to config file to override default config.</li> <li><code>-r, --roboflow-api-key TEXT</code>: Roboflow API key for your workspace.</li> <li><code>-h, --help</code>: Print out help text.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-cloud-undeploy","title":"<code>inference cloud undeploy</code>","text":"<p>Usage:</p> <pre><code>$ inference cloud undeploy [OPTIONS] CLUSTER_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>CLUSTER_NAME</code>: Name of cluster to undeploy.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-cloud-stop","title":"<code>inference cloud stop</code>","text":"<p>Usage:</p> <pre><code>$ inference cloud stop [OPTIONS] CLUSTER_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>CLUSTER_NAME</code>: Name of cluster to stop.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-cloud-start","title":"<code>inference cloud start</code>","text":"<p>Usage:</p> <pre><code>$ inference cloud start [OPTIONS] CLUSTER_NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>CLUSTER_NAME</code>: Name of cluster to start.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-benchmark","title":"<code>inference benchmark</code>","text":"<p>Commands for running inference benchmarks.</p> <p>Usage:</p> <pre><code>$ inference benchmark [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>api-speed</code></li> <li><code>python-package-speed</code></li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-benchmark-api-speed","title":"<code>inference benchmark api-speed</code>","text":"<p>Usage:</p> <pre><code>$ inference benchmark api-speed [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-m, --model_id TEXT</code>: Model ID in format project/version.</li> <li><code>-wid, --workflow-id TEXT</code>: Workflow ID.</li> <li><code>-wn, --workspace-name TEXT</code>: Workspace Name.</li> <li><code>-ws, --workflow-specification TEXT</code>: Workflow specification.</li> <li><code>-wp, --workflow-parameters TEXT</code>: Model ID in format project/version.</li> <li><code>-d, --dataset_reference TEXT</code>: Name of predefined dataset (one of ['coco']) or path to directory with images  [default: coco]</li> <li><code>-h, --host TEXT</code>: Host to run inference on.  [default: http://localhost:9001]</li> <li><code>-wr, --warm_up_requests INTEGER</code>: Number of warm-up requests  [default: 10]</li> <li><code>-br, --benchmark_requests INTEGER</code>: Number of benchmark requests  [default: 1000]</li> <li><code>-bs, --batch_size INTEGER</code>: Batch size of single request  [default: 1]</li> <li><code>-c, --clients INTEGER</code>: Meaningful if <code>rps</code> not specified - number of concurrent threads that will send requests one by one  [default: 1]</li> <li><code>-rps, --rps INTEGER</code>: Number of requests per second to emit. If not specified - requests will be sent one-by-one by requested number of client threads</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-mc, --model_config TEXT</code>: Location of yaml file with model config</li> <li><code>-o, --output_location TEXT</code>: Location where to save the result (path to file or directory)</li> <li><code>-L, --legacy-endpoints / -l, --no-legacy-endpoints</code>: Boolean flag to decide if legacy endpoints should be used (applicable for self-hosted API benchmark)  [default: no-legacy-endpoints]</li> <li><code>-y, --yes / -n, --no</code>: Boolean flag to decide on auto <code>yes</code> answer given on user input required.  [default: no]</li> <li><code>--max_error_rate FLOAT</code>: Max error rate for API speed benchmark - if given and the error rate is higher - command will return non-success error code. Expected percentage values in range 0.0-100.0</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-benchmark-python-package-speed","title":"<code>inference benchmark python-package-speed</code>","text":"<p>Usage:</p> <pre><code>$ inference benchmark python-package-speed [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-m, --model_id TEXT</code>: Model ID in format project/version.  [required]</li> <li><code>-d, --dataset_reference TEXT</code>: Name of predefined dataset (one of ['coco']) or path to directory with images  [default: coco]</li> <li><code>-wi, --warm_up_inferences INTEGER</code>: Number of warm-up requests  [default: 10]</li> <li><code>-bi, --benchmark_requests INTEGER</code>: Number of benchmark requests  [default: 1000]</li> <li><code>-bs, --batch_size INTEGER</code>: Batch size of single request  [default: 1]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-mc, --model_config TEXT</code>: Location of yaml file with model config</li> <li><code>-o, --output_location TEXT</code>: Location where to save the result (path to file or directory)</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-workflows","title":"<code>inference workflows</code>","text":"<p>Commands for interacting with Roboflow Workflows</p> <p>Usage:</p> <pre><code>$ inference workflows [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>process-video</code>: Process video file with your Workflow...</li> <li><code>process-image</code>: Process single image with Workflows...</li> <li><code>process-images-directory</code>: Process whole images directory with...</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-workflows-process-video","title":"<code>inference workflows process-video</code>","text":"<p>Process video file with your Workflow locally (inference Python package required)</p> <p>Usage:</p> <pre><code>$ inference workflows process-video [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-v, --video_path TEXT</code>: Path to video to be processed  [required]</li> <li><code>-o, --output_dir TEXT</code>: Path to output directory  [required]</li> <li><code>-ft, --output_file_type [jsonl|csv]</code>: Type of the output file  [default: csv]</li> <li><code>-ws, --workflow_spec TEXT</code>: Path to JSON file with Workflow definition (mutually exclusive with <code>workspace_name</code> and <code>workflow_id</code>)</li> <li><code>-wn, --workspace_name TEXT</code>: Name of Roboflow workspace the that Workflow belongs to (mutually exclusive with <code>workflow_specification_path</code>)</li> <li><code>-wid, --workflow_id TEXT</code>: Identifier of a Workflow on Roboflow platform (mutually exclusive with <code>workflow_specification_path</code>)</li> <li><code>--workflow_params TEXT</code>: Path to JSON document with Workflow parameters - helpful when Workflow is parametrized and passing the parameters in CLI is not handy / impossible due to typing conversion issues.</li> <li><code>--image_input_name TEXT</code>: Name of the Workflow input that defines placeholder for image to be processed  [default: image]</li> <li><code>--max_fps FLOAT</code>: Use the parameter to limit video FPS (additional frames will be skipped in processing).</li> <li><code>--save_out_video / --no_save_out_video</code>: Flag deciding if image outputs of the workflow should be saved as video file  [default: save_out_video]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--allow_override / --no_override</code>: Flag to decide if content of output directory can be overridden.  [default: no_override]</li> <li><code>--debug_mode / --no_debug_mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no_debug_mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-workflows-process-image","title":"<code>inference workflows process-image</code>","text":"<p>Process single image with Workflows (inference Package may be needed dependent on mode)</p> <p>Usage:</p> <pre><code>$ inference workflows process-image [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-i, --image_path TEXT</code>: Path to image to be processed  [required]</li> <li><code>-o, --output_dir TEXT</code>: Path to output directory  [required]</li> <li><code>-pt, --processing_target [api|inference_package]</code>: Defines where the actual processing will be done, either in inference Python package running locally, or behind the API (which ensures greater throughput).  [default: api]</li> <li><code>-ws, --workflow_spec TEXT</code>: Path to JSON file with Workflow definition (mutually exclusive with <code>workspace_name</code> and <code>workflow_id</code>)</li> <li><code>-wn, --workspace_name TEXT</code>: Name of Roboflow workspace the that Workflow belongs to (mutually exclusive with <code>workflow_specification_path</code>)</li> <li><code>-wid, --workflow_id TEXT</code>: Identifier of a Workflow on Roboflow platform (mutually exclusive with <code>workflow_specification_path</code>)</li> <li><code>--workflow_params TEXT</code>: Path to JSON document with Workflow parameters - helpful when Workflow is parametrized and passing the parameters in CLI is not handy / impossible due to typing conversion issues.</li> <li><code>--image_input_name TEXT</code>: Name of the Workflow input that defines placeholder for image to be processed  [default: image]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--api_url TEXT</code>: URL of the API that will be used for processing, when API processing target pointed.  [default: https://detect.roboflow.com]</li> <li><code>--allow_override / --no_override</code>: Flag to decide if content of output directory can be overridden.  [default: no_override]</li> <li><code>--save_image_outputs / --no_save_image_outputs</code>: Flag controlling persistence of Workflow outputs that are images  [default: save_image_outputs]</li> <li><code>--force_reprocessing / --no_reprocessing</code>: Flag to enforce re-processing of specific images. Images are identified by file name.  [default: no_reprocessing]</li> <li><code>--debug_mode / --no_debug_mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no_debug_mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-workflows-process-images-directory","title":"<code>inference workflows process-images-directory</code>","text":"<p>Process whole images directory with Workflows (inference Package may be needed dependent on mode)</p> <p>Usage:</p> <pre><code>$ inference workflows process-images-directory [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-i, --input_directory TEXT</code>: Path to directory with images  [required]</li> <li><code>-o, --output_dir TEXT</code>: Path to output directory  [required]</li> <li><code>-pt, --processing_target [api|inference_package]</code>: Defines where the actual processing will be done, either in inference Python package running locally, or behind the API (which ensures greater throughput).  [default: api]</li> <li><code>-ws, --workflow_spec TEXT</code>: Path to JSON file with Workflow definition (mutually exclusive with <code>workspace_name</code> and <code>workflow_id</code>)</li> <li><code>-wn, --workspace_name TEXT</code>: Name of Roboflow workspace the that Workflow belongs to (mutually exclusive with <code>workflow_specification_path</code>)</li> <li><code>-wid, --workflow_id TEXT</code>: Identifier of a Workflow on Roboflow platform (mutually exclusive with <code>workflow_specification_path</code>)</li> <li><code>--workflow_params TEXT</code>: Path to JSON document with Workflow parameters - helpful when Workflow is parametrized and passing the parameters in CLI is not handy / impossible due to typing conversion issues.</li> <li><code>--image_input_name TEXT</code>: Name of the Workflow input that defines placeholder for image to be processed  [default: image]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--api_url TEXT</code>: URL of the API that will be used for processing, when API processing target pointed.  [default: https://detect.roboflow.com]</li> <li><code>--allow_override / --no_override</code>: Flag to decide if content of output directory can be overridden.  [default: no_override]</li> <li><code>--save_image_outputs / --no_save_image_outputs</code>: Flag controlling persistence of Workflow outputs that are images  [default: save_image_outputs]</li> <li><code>--force_reprocessing / --no_reprocessing</code>: Flag to enforce re-processing of specific images. Images are identified by file name.  [default: no_reprocessing]</li> <li><code>--aggregate / --no_aggregate</code>: Flag to decide if processing results for a directory should be aggregated to a single file at the end of processing.  [default: aggregate]</li> <li><code>-af, --aggregation_format [jsonl|csv]</code>: Defines the format of aggregated results - either CSV of JSONL  [default: csv]</li> <li><code>--threads INTEGER</code>: Defines number of threads that will be used to send requests when processing target is API. Default for Roboflow Hosted API is 32, and for on-prem deployments: 1.</li> <li><code>--debug_mode / --no_debug_mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no_debug_mode]</li> <li><code>--max-failures INTEGER</code>: Maximum number of Workflow executions for directory images which will be tolerated before give up. If not set - unlimited.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud","title":"<code>inference rf-cloud</code>","text":"<p>Commands for interacting with Roboflow Cloud</p> <p>Usage:</p> <pre><code>$ inference rf-cloud [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>data-staging</code>: Commands for interacting with Roboflow...</li> <li><code>batch-processing</code>: Commands for interacting with Roboflow...</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging","title":"<code>inference rf-cloud data-staging</code>","text":"<p>Commands for interacting with Roboflow Data Staging. THIS IS ALPHA PREVIEW OF THE FEATURE.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list-batches</code>: List staging batches in your workspace.</li> <li><code>list-batch-content</code>: List content of individual batch.</li> <li><code>create-batch-of-images</code>: Create new batch with your images.</li> <li><code>create-batch-of-videos</code>: Create new batch with your videos.</li> <li><code>show-batch-details</code>: Show batch details</li> <li><code>export-batch</code>: Export batch</li> <li><code>list-ingest-details</code>: List details of your data ingest.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-list-batches","title":"<code>inference rf-cloud data-staging list-batches</code>","text":"<p>List staging batches in your workspace.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging list-batches [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-p, --pages INTEGER</code>: Number of pages to pull  [default: 1]</li> <li><code>--page-size INTEGER</code>: Size of pagination page</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-list-batch-content","title":"<code>inference rf-cloud data-staging list-batch-content</code>","text":"<p>List content of individual batch.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging list-batch-content [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of new batch (must be lower-cased letters with '-' and '_' allowed  [required]</li> <li><code>-pn, --part-name TEXT</code>: Name of the part to be listed (if not given - all parts are presented). Invalid if batch is not multipart</li> <li><code>-l, --limit INTEGER</code>: Number of entries to display / dump into th output files. If not given - whole content will be presented.</li> <li><code>-o, --output-file TEXT</code>: Path to the output file - if not provided, command will dump result to the console. File type is JSONL.</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-create-batch-of-images","title":"<code>inference rf-cloud data-staging create-batch-of-images</code>","text":"<p>Create new batch with your images.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging create-batch-of-images [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of new batch (must be lower-cased letters with '-' and '_' allowed  [required]</li> <li><code>-ds, --data-source [local-directory|references-file]</code>: Source of the data - either local directory or JSON file with references.  [default: local-directory]</li> <li><code>-i, --images-dir TEXT</code>: Path to your images directory to upload (required if data source is 'local-directory')</li> <li><code>-r, --references TEXT</code>: Path to JSON file with URLs of files to be ingested (required if data source is 'references-file')</li> <li><code>-i, --ingest-id TEXT</code>: Identifier assigned for references ingest (if value not provided - system will auto-assign) - only relevant when <code>notifications-url</code> specified</li> <li><code>--notifications-url TEXT</code>: Webhook URL where system should send notifications about ingest status</li> <li><code>--notification-category TEXT</code>: Selecting specific notification categories (ingest-status / files-status) in combination with <code>--notifications-url</code> you may filter which notifications are going to be sent to your system. Please note that filtering of notifications do only work with ingests of files references via signed URLs and will not be applicable for ingests from local storage.</li> <li><code>-bn, --batch-name TEXT</code>: Display name of your batch</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-create-batch-of-videos","title":"<code>inference rf-cloud data-staging create-batch-of-videos</code>","text":"<p>Create new batch with your videos.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging create-batch-of-videos [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of new batch (must be lower-cased letters with '-' and '_' allowed  [required]</li> <li><code>-ds, --data-source [local-directory|references-file]</code>: Source of the data - either local directory or JSON file with references.  [default: local-directory]</li> <li><code>-v, --videos-dir TEXT</code>: Path to your videos directory to upload</li> <li><code>-r, --references TEXT</code>: Path to JSON file with URLs of files to be ingested (required if data source is 'references-file')</li> <li><code>-i, --ingest-id TEXT</code>: Identifier assigned for references ingest (if value not provided - system will auto-assign) - only relevant if data source is 'references-file'</li> <li><code>--notifications-url TEXT</code>: Webhook URL where system should send notifications about ingest status - only relevant if data source is 'references-file'</li> <li><code>--notification-category TEXT</code>: Selecting specific notification categories (ingest-status / files-status) in combination with <code>--notifications-url</code> you may filter which notifications are going to be sent to your system. Please note that filtering of notifications do only work with ingests of files references via signed URLs and will not be applicable for ingests from local storage.</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-bn, --batch-name TEXT</code>: Display name of your batch</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-show-batch-details","title":"<code>inference rf-cloud data-staging show-batch-details</code>","text":"<p>Show batch details</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging show-batch-details [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of new batch (must be lower-cased letters with '-' and '_' allowed  [required]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-export-batch","title":"<code>inference rf-cloud data-staging export-batch</code>","text":"<p>Export batch</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging export-batch [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of new batch (must be lower-cased letters with '-' and '_' allowed  [required]</li> <li><code>-t, --target-dir TEXT</code>: Path to export directory  [required]</li> <li><code>-pn, --part-name TEXT</code>: Name of the part to be exported (if not given - all parts are presented). Invalid if batch is not multipart</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--override-existing / --no-override-existing</code>: Flag to enforce export even if partial content is already exported  [default: no-override-existing]</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-data-staging-list-ingest-details","title":"<code>inference rf-cloud data-staging list-ingest-details</code>","text":"<p>List details of your data ingest.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud data-staging list-ingest-details [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of new batch (must be lower-cased letters with '-' and '_' allowed  [required]</li> <li><code>-o, --output-file TEXT</code>: Path to the output file - if not provided, command will dump result to the console. File type is JSONL.</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--page-size INTEGER</code>: Size of pagination page</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing","title":"<code>inference rf-cloud batch-processing</code>","text":"<p>Commands for interacting with Roboflow Batch Processing. THIS IS ALPHA PREVIEW OF THE FEATURE.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list-jobs</code>: List batch jobs in your workspace.</li> <li><code>show-job-details</code>: Get job details.</li> <li><code>process-images-with-workflow</code>: Trigger batch job to process images with...</li> <li><code>process-videos-with-workflow</code>: Trigger batch job to process videos with...</li> <li><code>abort-job</code>: Terminate running job</li> <li><code>restart-job</code>: Restart failed job</li> <li><code>fetch-logs</code>: Fetches job logs</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-list-jobs","title":"<code>inference rf-cloud batch-processing list-jobs</code>","text":"<p>List batch jobs in your workspace.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing list-jobs [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-p, --max-pages INTEGER</code>: Number of pagination pages with batch jobs to display  [default: 1]</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-show-job-details","title":"<code>inference rf-cloud batch-processing show-job-details</code>","text":"<p>Get job details.</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing show-job-details [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-j, --job-id TEXT</code>: Identifier of job  [required]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-process-images-with-workflow","title":"<code>inference rf-cloud batch-processing process-images-with-workflow</code>","text":"<p>Trigger batch job to process images with Workflow</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing process-images-with-workflow [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of batch to be processed  [required]</li> <li><code>-w, --workflow-id TEXT</code>: Identifier of the workflow  [required]</li> <li><code>--workflow-params TEXT</code>: Path to JSON document with Workflow parameters - helpful when Workflow is parametrized and passing the parameters in CLI is not handy / impossible due to typing conversion issues.</li> <li><code>--image-input-name TEXT</code>: Name of the Workflow input that defines placeholder for image to be processed</li> <li><code>--save-image-outputs / --no-save-image-outputs</code>: Flag controlling persistence of Workflow outputs that are images  [default: no-save-image-outputs]</li> <li><code>--image-outputs-to-save TEXT</code>: Use this option to filter out workflow image outputs you want to save</li> <li><code>-p, --part-name TEXT</code>: Name of the batch part (relevant for multipart batches</li> <li><code>-mt, --machine-type [cpu|gpu]</code>: Type of machine</li> <li><code>--workers-per-machine INTEGER</code>: Number of workers to run on a single machine - more workers equals better resources utilisation, but may cause out of memory errors for bulky Workflows.</li> <li><code>-ms, --machine-size [xs|s|m|l|xl]</code>: (Deprecated - use --workers-per-machine) Size of machine</li> <li><code>--max-runtime-seconds INTEGER</code>: Max processing duration</li> <li><code>--max-parallel-tasks INTEGER</code>: Max number of concurrent processing tasks</li> <li><code>--aggregation-format [csv|jsonl]</code>: Format of results aggregation</li> <li><code>-j, --job-id TEXT</code>: Identifier of job (if not given - will be generated)</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--notifications-url TEXT</code>: URL of the Webhook to be used for job state notifications.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-process-videos-with-workflow","title":"<code>inference rf-cloud batch-processing process-videos-with-workflow</code>","text":"<p>Trigger batch job to process videos with Workflow</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing process-videos-with-workflow [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-b, --batch-id TEXT</code>: Identifier of batch to be processed  [required]</li> <li><code>-w, --workflow-id TEXT</code>: Identifier of the workflow  [required]</li> <li><code>--workflow-params TEXT</code>: Path to JSON document with Workflow parameters - helpful when Workflow is parametrized and passing the parameters in CLI is not handy / impossible due to typing conversion issues.</li> <li><code>--image-input-name TEXT</code>: Name of the Workflow input that defines placeholder for image to be processed</li> <li><code>--save-image-outputs / --no-save-image-outputs</code>: Flag controlling persistence of Workflow outputs that are images  [default: no-save-image-outputs]</li> <li><code>--image-outputs-to-save TEXT</code>: Use this option to filter out workflow image outputs you want to save</li> <li><code>-p, --part-name TEXT</code>: Name of the batch part (relevant for multipart batches</li> <li><code>-mt, --machine-type [cpu|gpu]</code>: Type of machine</li> <li><code>--workers-per-machine INTEGER</code>: Number of workers to run on a single machine - more workers equals better resources utilisation, but may cause out of memory errors for bulky Workflows.</li> <li><code>-ms, --machine-size [xs|s|m|l|xl]</code>: (Deprecated - use --workers-per-machine) Size of machine</li> <li><code>--max-runtime-seconds INTEGER</code>: Max processing duration</li> <li><code>--max-parallel-tasks INTEGER</code>: Max number of concurrent processing tasks</li> <li><code>--aggregation-format [csv|jsonl]</code>: Format of results aggregation</li> <li><code>--max-video-fps INTEGER</code>: Limit for FPS to process for video (subsampling predictions rate) - smaller FPS means faster processing and less accurate video analysis.</li> <li><code>-j, --job-id TEXT</code>: Identifier of job (if not given - will be generated)</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--notifications-url TEXT</code>: URL of the Webhook to be used for job state notifications.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-abort-job","title":"<code>inference rf-cloud batch-processing abort-job</code>","text":"<p>Terminate running job</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing abort-job [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-j, --job-id TEXT</code>: Identifier of job  [required]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-restart-job","title":"<code>inference rf-cloud batch-processing restart-job</code>","text":"<p>Restart failed job</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing restart-job [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-j, --job-id TEXT</code>: Identifier of job  [required]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>-mt, --machine-type [cpu|gpu]</code>: Type of machine</li> <li><code>--workers-per-machine INTEGER</code>: Number of workers to run on a single machine - more workers equals better resources utilisation, but may cause out of memory errors for bulky Workflows.</li> <li><code>--max-runtime-seconds INTEGER</code>: Max processing duration</li> <li><code>--max-parallel-tasks INTEGER</code>: Max number of concurrent processing tasks</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/reference/#inference-rf-cloud-batch-processing-fetch-logs","title":"<code>inference rf-cloud batch-processing fetch-logs</code>","text":"<p>Fetches job logs</p> <p>Usage:</p> <pre><code>$ inference rf-cloud batch-processing fetch-logs [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-j, --job-id TEXT</code>: Identifier of job  [required]</li> <li><code>-a, --api-key TEXT</code>: Roboflow API key for your workspace. If not given - env variable <code>ROBOFLOW_API_KEY</code> will be used</li> <li><code>--log-severity [info|error|warning]</code>: Severity of logs to pick</li> <li><code>-o, --output-file TEXT</code>: Path to the output file - if not provided, command will dump result to the console. File type is JSONL.</li> <li><code>--debug-mode / --no-debug-mode</code>: Flag enabling errors stack traces to be displayed (helpful for debugging)  [default: no-debug-mode]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"inference_helpers/cli_commands/server/","title":"Controlling <code>inference</code> server","text":"<p><code>inference server</code> command provides a control layer around HTTP server exposing <code>inference</code>.</p> <p>Discovering command capabilities</p> <p>To check detail of the command, run:</p> <pre><code>inference server --help\n</code></pre> <p>Additionally, help guide is also available for each sub-command:</p> <pre><code>inference server start --help\n</code></pre>"},{"location":"inference_helpers/cli_commands/server/#inference-server-start","title":"<code>inference server start</code>","text":"<p>Starts a local Inference server. It optionally takes a port number (default is 9001) and will only start the docker container if there is not already a container running on that port.</p> <p>If you would rather run your server on a virtual machine in Google cloud or Amazon cloud, skip to the section titled \"Deploy Inference on Cloud\" below.</p> <p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If you haven't installed Docker yet, you can get it from Docker's official website.</p> <p>The CLI will automatically detect the device you are running on and pull the appropriate Docker image.</p> <pre><code>inference server start --port 9001 [-e {optional_path_to_file_with_env_variables}]\n</code></pre> <p>Parameter <code>--env-file</code> (or <code>-e</code>) is the optional path for .env file that will be loaded into your Inference server in case that values of internal parameters needs to be adjusted. Any value passed explicitly as command parameter is considered as more important and will shadow the value defined in <code>.env</code> file under the same target variable name.</p>"},{"location":"inference_helpers/cli_commands/server/#development-mode","title":"Development Mode","text":"<p>Use the <code>--dev</code> flag to start the Inference Server in development mode. Development mode enables the Inference Server's built in notebook environment for easy testing and development.</p>"},{"location":"inference_helpers/cli_commands/server/#tunnel","title":"Tunnel","text":"<p>Use the <code>--tunnel</code> flag to start the Inference Server with a tunnel to expose inference to external requests on a TLS-enabled endpoint.</p> <p>The random generated address will be on server start output:</p> <pre><code>Tunnel to local inference running on https://somethingrandom-ip-192-168-0-1.roboflow.run\n</code></pre>"},{"location":"inference_helpers/cli_commands/server/#inference-server-status","title":"inference server status","text":"<p>Checks the status of the local inference server.</p> <pre><code>inference server status\n</code></pre>"},{"location":"inference_helpers/cli_commands/server/#inference-server-stop","title":"inference server stop","text":"<p>Stops the inference server.</p> <pre><code>inference server stop\n</code></pre>"},{"location":"inference_helpers/cli_commands/workflows/","title":"Processing data with Workflows","text":"<p><code>inference workflows</code> command provides a way to process images and videos with Workflow. It is possible to  process:</p> <ul> <li> <p>individual images</p> </li> <li> <p>directories of images</p> </li> <li> <p>video files</p> </li> </ul> <p>Discovering command capabilities</p> <p>To check detail of the command, run:</p> <pre><code>inference workflows --help\n</code></pre> <p>Additionally, help guide is also available for each sub-command:</p> <pre><code>inference workflows process-image --help\n</code></pre>"},{"location":"inference_helpers/cli_commands/workflows/#process-individual-image","title":"Process individual image","text":"<p>Basic usage of the command is illustrated below:</p> <pre><code>inference workflows process-image \\\n    --image_path {your-input-image} \\\n    --output_dir {your-output-dir} \\\n    --workspace_name {your-roboflow-workspace-url} \\\n    --workflow_id {your-workflow-id} \\\n    --api-key {your_roboflow_api_key}\n</code></pre> <p>which would take your input image, run it against your Workflow and save results in output directory. By default,  Workflow will be processed using Roboflow Hosted API. You can tweak behaviour of the command:</p> <ul> <li> <p>if you want to process the image locally, using <code>inference</code> Python package - use  <code>--processing_target inference_package</code> option (requires <code>inference</code> to be installed)</p> </li> <li> <p>to see all options, use <code>inference workflows process-image --help</code> command</p> </li> <li> <p>any option that starts from <code>--</code> which is not enlisted in <code>--help</code> command will be treated as input parameter  to the workflow execution - with automatic type conversion applied. Additionally <code>--workflow_params</code> option may  specify path to <code>*.json</code> file providing workflow parameters (explicit parameters will override parameters defined  in file)</p> </li> <li> <p>if your Workflow defines image parameter placeholder under a name different from <code>image</code>, you can point the  proper image input by <code>--image_input_name</code></p> </li> <li> <p><code>--allow_override</code> flag must be used if output directory is not empty</p> </li> </ul>"},{"location":"inference_helpers/cli_commands/workflows/#process-directory-of-images","title":"Process directory of images","text":"<p>Basic usage of the command is illustrated below:</p> <pre><code>inference workflows process-images-directory \\\n    -i {your_input_directory} \\\n    -o {your_output_directory} \\\n    --workspace_name {your-roboflow-workspace-url} \\\n    --workflow_id {your-workflow-id} \\\n    --api-key {your_roboflow_api_key}\n</code></pre> <p>You can tweak behaviour of the command:</p> <ul> <li> <p>if you want to process the image locally, using <code>inference</code> Python package - use  <code>--processing_target inference_package</code> option (requires <code>inference</code> to be installed)</p> </li> <li> <p>to see all options, use <code>inference workflows process-image --help</code> command</p> </li> <li> <p>any option that starts from <code>--</code> which is not enlisted in <code>--help</code> command will be treated as input parameter  to the workflow execution - with automatic type conversion applied. Additionally <code>--workflow_params</code> option may  specify path to <code>*.json</code> file providing workflow parameters (explicit parameters will override parameters defined  in file)</p> </li> <li> <p>if your Workflow defines image parameter placeholder under a name different from <code>image</code>, you can point the  proper image input by <code>--image_input_name</code></p> </li> <li> <p><code>--allow_override</code> flag must be used if output directory is not empty</p> </li> <li> <p><code>--threads</code> option can specify number of threads used to run the requests when processing target is API</p> </li> </ul>"},{"location":"inference_helpers/cli_commands/workflows/#process-video-file","title":"Process video file","text":"<p><code>inference</code> required</p> <p>This command requires <code>inference</code> to be installed.</p> <p>Basic usage of the command is illustrated below:</p> <pre><code>inference workflows process-video \\\n    --video_path {video_to_be_processed} \\\n    --output_dir {empty_directory} \\\n    --workspace_name {your-roboflow-workspace-url} \\\n    --workflow_id {your-workflow-id} \\\n    --api-key {your_roboflow_api_key}\n</code></pre> <p>You can tweak behaviour of the command:</p> <ul> <li> <p><code>--max_fps</code> option can be used to subsample video frames while processing</p> </li> <li> <p>to see all options, use <code>inference workflows process-image --help</code> command</p> </li> <li> <p>any option that starts from <code>--</code> which is not enlisted in <code>--help</code> command will be treated as input parameter  to the workflow execution - with automatic type conversion applied. Additionally <code>--workflow_params</code> option may  specify path to <code>*.json</code> file providing workflow parameters (explicit parameters will override parameters defined  in file)</p> </li> <li> <p>if your Workflow defines image parameter placeholder under a name different from <code>image</code>, you can point the  proper image input by <code>--image_input_name</code></p> </li> <li> <p><code>--allow_override</code> flag must be used if output directory is not empty</p> </li> </ul>"},{"location":"install/","title":"\ud83d\ude80 Native Desktop Apps","text":"<p>You can now run Roboflow Inference Server on your Windows or macOS machine with our native desktop applications! This is the quickest and most effortless way to get up and running.</p>"},{"location":"install/#download-for-latest-version","title":"Download for Latest Version","text":"Download for Windows           Download for Mac          <p> I need a previous release </p>"},{"location":"install/#installation-instructions","title":"Installation Instructions","text":""},{"location":"install/#windows-x86","title":"Windows (x86)","text":"<ul> <li>Download the latest installer and run it to install Roboflow Inference</li> <li>When the install is finished it will offer to launch the Inference server after the setup completes</li> <li>To stop the inference server simply close the terminal window it opens</li> <li>To start it again later, you can find Roboflow Inference in your Start Menu</li> </ul>"},{"location":"install/#macos-apple-silicon","title":"MacOS (Apple Silicon)","text":"<ul> <li>Download the Roboflow Inference DMG </li> <li>Mount the DMG by double clicking it</li> <li>Drag the Roboflow Inference App to the Application Folder</li> <li>Go to your Application Folder and double click the Roboflow Inference App to start the server</li> </ul>"},{"location":"install/#local-installation-using-docker","title":"Local Installation using Docker","text":"<p>Inference is built to be run at the edge. It loads and executes model weights and does computation locally. It can run fully offline (once model weights are downloaded) but it's often useful to maintain a network connection for interfacing with outside systems (like PLCs on the local network, or remote systems for storing data and sending notifications).</p>"},{"location":"install/#run-via-docker","title":"Run via Docker","text":"<p>The preferred way to use Inference is via Docker (see Why Docker).</p> <p>Install Docker (and NVIDIA Container Toolkit for GPU acceleration if you have a CUDA-enabled GPU). Then run:</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre> <p>The <code>inference server start</code> command attempts to automatically choose and configure the optimal container to optimize performance on your machine. See Using Your New Server for next steps.</p> <p>Tip</p> <p>Special installation notes and performance tips by device are also available. Browse the navigation on the left for detailed install guides.</p>"},{"location":"install/#dev-mode","title":"Dev Mode","text":"<p>The <code>--dev</code> parameter to <code>inference server start</code> starts in development mode. This spins up a companion Jupyter notebook server with a quickstart guide on <code>localhost:9002</code>. Dive in there for a whirlwind tour of your new Inference Server's functionality!</p> <pre><code>inference server start --dev\n</code></pre>"},{"location":"install/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/enterprise-considerations/","title":"Enterprise considerations","text":""},{"location":"install/enterprise-considerations/#enterprise-considerations","title":"Enterprise Considerations","text":"<p>A Helm Chart is available for enterprise cloud deployments. Enterprise networking solutions to support deployment in OT networks are also available upon request.</p> <p>Roboflow also offers customized support and installation packages and a pre-configured Jetson-based edge device suitable for rapid prototyping. Contact our sales team if you're part of a large organization and interested in learning more.</p>"},{"location":"install/jetson/","title":"Install on NVIDIA Jetson","text":""},{"location":"install/jetson/#overview","title":"Overview","text":"<p>Jetson is NVIDIA\u2019s line of compact, power-efficient modules designed to run AI and deep learning workloads at the edge. They combine a GPU, CPU, and neural accelerators on a single board, making them ideal for robotics, drones, smart cameras, and other embedded applications where you need real-time computer vision or inference without a cloud connection. For more details, see NVIDIA\u2019s official Jetson overview: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/</p>"},{"location":"install/jetson/#prerequisites","title":"Prerequisites","text":"<p>Disk Space: Allocate at least 10 GB free for the Roboflow Jetson image (8.14 GB). </p> <p>JetPack Version: Must be running a supported JetPack (4.5, 4.6, 5.x, or 6.x).</p> <p>Recommended Hardware: For best performance while running Inference, we recommend an NVIDIA Orin NX 16 GB or above. </p> <p>Docker &amp; NVIDIA Container Toolkit   - Requires Docker + NVIDIA runtime so containers can access the GPU.   - Instead of detailing installation here, follow these instructions:     - Docker install:       https://docs.docker.com/engine/install/ubuntu/     - NVIDIA Container Toolkit:       https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html</p> <p>We have specialized containers built with support for hardware acceleration on JetPack L4T. To automatically detect your JetPack version and use the right container with good default settings run:</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre>"},{"location":"install/jetson/#manually-starting-the-container","title":"Manually Starting the Container","text":"<p>If you want more control of the container settings you can also start it manually. Jetson devices with NVIDIA Jetpack are pre-configured with NVIDIA Container Runtime and will be hardware accelerated out of the box:</p> Jetpack 6Jetpack 5Jetpack 4.6Jetpack 4.5 <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    roboflow/roboflow-inference-server-jetson-6.0.0:latest\n</code></pre> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    roboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre> <p>Warning</p> <p>Jetpack 4 is deprecated and will not receive future updates. Please migrate to Jetpack 6.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-jetson-4.6.1:latest\n</code></pre> <p>Warning</p> <p>Jetpack 4 is deprecated and will not receive future updates. Please migrate to Jetpack 6.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-jetson-4.5.0:latest\n</code></pre>"},{"location":"install/jetson/#tensorrt","title":"TensorRT","text":"<p>You can optionally enable TensorRT, NVIDIA's model optimization runtime that will greatly increase your models' speed at the expense of a heavy compilation and optimization step (sometimes 15+ minutes) the first time you load each model.</p> <p>Enable TensorRT by adding <code>TensorrtExecutionProvider</code> to the <code>ONNXRUNTIME_EXECUTION_PROVIDERS</code> environment variable.</p> Jetpack 6Jetpack 5Jetpack 4.6Jetpack 4.5 <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    -e ONNXRUNTIME_EXECUTION_PROVIDERS=\"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-jetson-6.0.0:latest\n</code></pre> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    -e ONNXRUNTIME_EXECUTION_PROVIDERS=\"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre> <p>Warning</p> <p>Jetpack 4 is deprecated and will not receive future updates. Please migrate to Jetpack 6.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    -e ONNXRUNTIME_EXECUTION_PROVIDERS=\"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-jetson-4.6.1:latest\n</code></pre> <p>Warning</p> <p>Jetpack 4 is deprecated and will not receive future updates. Please migrate to Jetpack 6.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --runtime nvidia \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    -e ONNXRUNTIME_EXECUTION_PROVIDERS=\"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-jetson-4.5.0:latest\n</code></pre>"},{"location":"install/jetson/#docker-compose","title":"Docker Compose","text":"<p>If you are using Docker Compose for your application, the equivalent yaml is:</p> Jetpack 6Jetpack 5Jetpack 4.6Jetpack 4.5 <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-jetson-6.0.0:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    runtime: nvidia\n\n    # Optionally: uncomment the following lines to enable TensorRT:\n    # environment:\n    #   ONNXRUNTIME_EXECUTION_PROVIDERS: \"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-jetson-5.1.1:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    runtime: nvidia\n\n    # Optionally: uncomment the following lines to enable TensorRT:\n    # environment:\n    #   ONNXRUNTIME_EXECUTION_PROVIDERS: \"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <p>Warning</p> <p>Jetpack 4 is deprecated and will not receive future updates. Please migrate to Jetpack 6.</p> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-jetson-4.6.1:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    runtime: nvidia\n\n    # Optionally: uncomment the following lines to enable TensorRT:\n    # environment:\n    #   ONNXRUNTIME_EXECUTION_PROVIDERS: \"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <p>Warning</p> <p>Jetpack 4 is deprecated and will not receive future updates. Please migrate to Jetpack 6.</p> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-jetson-4.5.0:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    runtime: nvidia\n\n    # Optionally: uncomment the following lines to enable TensorRT:\n    # environment:\n    #   ONNXRUNTIME_EXECUTION_PROVIDERS: \"[TensorrtExecutionProvider,CUDAExecutionProvider,CPUExecutionProvider]\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre>"},{"location":"install/jetson/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/jetson/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/jetson/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/linux/","title":"Install on Linux","text":"<p>The easiest way to start the correct container optimized for your machine and with good default settings (like a cache volume and a secure, non-privileged execution mode) is to use the CLI to choose and start the container using the <code>inference server start</code> command. (Note: you will need to install docker first):</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre>"},{"location":"install/linux/#manually-starting-the-container","title":"Manually Starting the Container","text":"<p>If you want more control of the container settings you can also start it manually.</p> CPUGPUTensorRT <p>The core CPU Docker image includes support for OpenVINO acceleration on x64 CPUs via onnxruntime. Heavy models like SAM2 may run too slowly (dozens of seconds per image) to be practical (and you should look into getting a CUDA-capable GPU if you want to use them).</p> <p>The primary use-cases for CPU inference are processing still images (eg for NSFW classification of uploads or document verification) or infrequent sampling of frames on a video (eg for occupancy tracking of a parking lot).</p> <p>To get started with CPU inference, use the <code>roboflow/roboflow-inference-server-cpu:latest</code> container.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <p>The GPU container adds support for hardware acceleration on cards that support CUDA via NVIDIA-Docker. First follow the NVIDIA Container Toolkit isntallation guide then add <code>--gpus all</code> to the <code>docker run</code> command:</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --gpus all \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    roboflow/roboflow-inference-server-gpu:latest\n</code></pre> <p>With the GPU container you can optionally enable TensorRT, NVIDIA's model optimization runtime that will greatly increase your models' speed at the expense of a heavy compilation and optimization step (sometimes 15+ minutes) the first time you load each model.</p> <p>You can enable TensorRT by adding <code>TensorrtExecutionProvider</code> to the <code>ONNXRUNTIME_EXECUTION_PROVIDERS</code> environment variable.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --gpus all \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    -e ONNXRUNTIME_EXECUTION_PROVIDERS=\"[TensorrtExecutionProvider,CUDAExecutionProvider,OpenVINOExecutionProvider,CPUExecutionProvider]\" \\\n    roboflow/roboflow-inference-server-gpu:latest\n</code></pre>"},{"location":"install/linux/#docker-compose","title":"Docker Compose","text":"<p>If you are using Docker Compose for your application, the equivalent yaml is:</p> CPUGPUTensorRT <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-cpu:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-gpu:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-gpu:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n    environment:\n      ONNXRUNTIME_EXECUTION_PROVIDERS: \"[TensorrtExecutionProvider,CUDAExecutionProvider,OpenVINOExecutionProvider,CPUExecutionProvider]\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre>"},{"location":"install/linux/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/linux/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/linux/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/mac/","title":"Install on MacOS","text":""},{"location":"install/mac/#osx-native-app-apple-silicon","title":"OSX Native App (Apple Silicon)","text":"<p>You can now run Roboflow Inference Server on your Apple Silicon Mac using our native desktop app!</p> <p>Simply download the latest DMS disk image from the latest release on Github. \u27a1\ufe0f View Latest Release and Download Installers on Github</p>"},{"location":"install/mac/#osx-installation-steps","title":"OSX Installation Steps","text":"<ul> <li>Download the Roboflow Inference DMG disk image</li> <li>Mount hte disk image by double clicking it</li> <li>Drag the Roboflow Inference App to the Application Folder</li> <li>Go to your Application Folder and double click the Roboflow Inference App to start the server</li> </ul>"},{"location":"install/mac/#using-docker","title":"Using Docker","text":"CPUGPU <p>First, you'll need to install Docker Desktop. Then, use the CLI to start the container.</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre> <p>Apple does not yet support passing the Metal Performance Shader (MPS) device to Docker so hardware acceleration is not possible inside a container on Mac.</p> <p>Tip</p> <p>It's easiest to get started with the CPU Docker and switch to running outside of Docker with MPS acceleration later if you need more speed.</p> <p>We recommend using <code>pyenv</code> and <code>pyenv-virtualenv</code> to manage your Python environments on Mac (especially because, in 2025, homebrew is defaulting to Python 3.13 which is not yet compatible with several of the machine learning dependencies that Inference uses).</p> <p>Once you have installed and setup <code>pyenv</code> and <code>pyenv-virtualenv</code> (be sure to follow the full instructions for setting up your shell), create and activate an <code>inference</code> virtual environment with Python 3.12:</p> <pre><code>pyenv install 3.12\npyenv virtualenv 3.12 inference\npyenv activate inference\n</code></pre> <p>To install and run the server outside of Docker, clone the repo, install the dependencies, copy <code>cpu_http.py</code> into the top level of the repo, and start the server with <code>uvicorn</code>:</p> <pre><code>git clone https://github.com/roboflow/inference.git\ncd inference\npip install .\ncp docker/config/cpu_http.py .\nuvicorn cpu_http:app --port 9001 --host 0.0.0.0\n</code></pre> <p>Your server is now running at <code>localhost:9001</code> with MPS acceleration.</p>"},{"location":"install/mac/#manually-starting-the-container","title":"Manually Starting the Container","text":"<p>If you want more control of the container settings you can also start it manually:</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    roboflow/roboflow-inference-server-cpu:latest\n</code></pre>"},{"location":"install/mac/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/mac/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/mac/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/minimum-requirements/","title":"Minimum Requirements","text":"<p>Inference adapts to your machine's resources and will run faster on more powerful machines but it cannot run on all devices.</p> <p>In order to run Inference, we recommend the following minimum requirements:</p> <ul> <li>64-bit Processor</li> <li>4GB of RAM</li> <li>20GB of free disk space</li> </ul> Additional Requirements for Windows <p>To run on Windows, you need Windows 10 or Windows 11 with Windows Subsystem for Linux (WSL 2) activated.</p>"},{"location":"install/minimum-requirements/#gpu-recommended","title":"GPU Recommended","text":"<p>Inference is capable of using hardware acceleration on NVIDIA GPUs. While not required, to run bigger models and live streaming video we recommend a CUDA-capable GPU.</p>"},{"location":"install/minimum-requirements/#suggested-edge-devices","title":"Suggested Edge Devices","text":"<p>NVIDIA Jetson Orin devices with JetPack 5 or JetPack 6 are powerful, well-rounded machines. Our test-suite regularly runs against these devices.</p> <p>The Jetson Orin Nano Super Developer Kit is a good device to start building with.</p>"},{"location":"install/other/","title":"Using Other Devices","text":"<p>Inference is tested and supported x64 and ARM processors (optionally with a NVIDIA/CUDA GPU). Running on other devices may be possible but is not officially tested or supported.</p>"},{"location":"install/other/#other-gpus","title":"Other GPUs","text":"<p>We do not currently support hardware acceleration on non-NVIDIA, non-Apple GPUs but ONNX Runtime has additional execution providers for AMD/ROCm, Arm NN, Rockchip, and others.</p> <p>If you install one of these runtimes, you can enable it via the <code>ONNXRUNTIME_EXECUTION_PROVIDERS</code> environment variable. For example: <pre><code>export ONNXRUNTIME_EXECUTION_PROVIDERS=\"[ROCMExecutionProvider,OpenVINOExecutionProvider,CPUExecutionProvider]\"\n</code></pre></p> <p>This is untested and performance improvements are not guaranteed. Acceleration of non-CUDA GPUs is unlikely to work inside Docker. See the Mac GPU installation guide for an example of how to run the server outside of a container.</p>"},{"location":"install/other/#other-edge-devices","title":"Other Edge Devices","text":"<p>Roboflow has SDKs for running object detection natively on other deployment targets like Tensorflow.js in a web browser, Native Swift on iOS via CoreML, and Snap Lens Studio.</p> <p>For additional functionality, like running Workflows and other types of models on another device, connect to an Inference Server over HTTP via its API.</p>"},{"location":"install/raspberry-pi/","title":"Install on Raspberry Pi","text":"<p>Inference works on Raspberry Pi 4 Model B and Raspberry Pi 5 so long as you are using the 64-bit version of the operating system (if your SD Card is big enough, we recommend the 64-bit \"Raspberry Pi OS with desktop and recommended software\" version).</p> <p>Once you've installed the 64-bit OS, install Docker then use the Inference CLI to automatically select, configure, and start the correct Inference Docker container:</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre>"},{"location":"install/raspberry-pi/#hardware-acceleration","title":"Hardware Acceleration","text":"<p>Inference does not yet support any hardware acceleration on the Raspberry Pi. Expect about 1fps on Pi 4 and 4fps on Pi 5 for a \"Roboflow 3.0 Fast\" object detection model (equivalent to a \"nano\" sized YOLO model).</p> <p>Larger models like Segment Anything and VLMs like Florence 2 will struggle to run with high performance on the Pi's compute. If you need more power for higher framerates or bigger models consider an NVIDIA Jetson.</p>"},{"location":"install/raspberry-pi/#manually-starting-the-container","title":"Manually Starting the Container","text":"<p>If you want more control of the container settings you can also start it manually.</p> <pre><code>sudo docker run -d \\\n    --name inference-server \\\n    --read-only \\\n    -p 9001:9001 \\\n    --volume ~/.inference/cache:/tmp:rw \\\n    --security-opt=\"no-new-privileges\" \\\n    --cap-drop=\"ALL\" \\\n    --cap-add=\"NET_BIND_SERVICE\" \\\n    roboflow/roboflow-inference-server-cpu:latest\n</code></pre>"},{"location":"install/raspberry-pi/#docker-compose","title":"Docker Compose","text":"<p>If you are using Docker Compose for your application, the equivalent yaml is:</p> <pre><code>version: \"3.9\"\n\nservices:\n    inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-cpu:latest\n\n    read_only: true\n    ports:\n        - \"9001:9001\"\n\n    volumes:\n        - \"${HOME}/.inference/cache:/tmp:rw\"\n\n    security_opt:\n        - no-new-privileges\n    cap_drop:\n        - ALL\n    cap_add:\n        - NET_BIND_SERVICE\n</code></pre>"},{"location":"install/raspberry-pi/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/raspberry-pi/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/raspberry-pi/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/using-your-new-server/","title":"Using your new server","text":""},{"location":"install/using-your-new-server/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/using-your-new-server/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/using-your-new-server/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/windows/","title":"Install on Windows","text":""},{"location":"install/windows/#windows-installer-x86","title":"Windows Installer (x86)","text":"<p>You can now run Roboflow Inference Server on your Windows machine using our native desktop app! </p> <p>Simply download the latest windows installer from the latest release on Github. \u27a1\ufe0f View Latest Release and Download Installers on Github</p>"},{"location":"install/windows/#windows-installation-steps","title":"Windows Installation Steps","text":"<ul> <li>Download the latest installer and run it to install Roboflow Inference</li> <li>When the install is finished it will offer to launch the Inference server after the setup completes</li> <li>To stop the inference server simply close the terminal window it opens</li> <li>To start it again later, you can find Roboflow Inference in your Start Menu</li> </ul>"},{"location":"install/windows/#using-docker","title":"Using Docker","text":"<p>First, you'll need to install Docker Desktop. Then, use the CLI to start the container.</p> CPUGPU <pre><code>pip install inference-cli\ninference server start\n</code></pre> <p>To access the GPU, you'll need to ensure you've installed the up to date NVIDIA drivers and the latest version of WSL2 and that the WSL 2 backend is configured in Docker. Follow the setup instructions from Docker.</p> <p>Then, use the CLI to start the container:</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre> <p>Note</p> <p>If the <code>pip install</code> command fails, you may need to install Python first. Once you have Python version 3.12, 3.11, 3.10, or 3.9 on your machine, retry the command.</p>"},{"location":"install/windows/#manually-starting-the-container","title":"Manually Starting the Container","text":"<p>If you want more control of the container settings you can also start it manually.</p> CPUGPUTensorRT <p>The core CPU Docker image includes support for OpenVINO acceleration on x64 CPUs via onnxruntime. Heavy models like SAM2 and CogVLM may run too slowly (dozens of seconds per image) to be practical (and you should look into getting a CUDA-capable GPU if you want to use them).</p> <p>The primary use-cases for CPU inference are processing still images (eg for NSFW classification of uploads or document verification) or infrequent sampling of frames on a video (eg for occupancy tracking of a parking lot).</p> <p>To get started with CPU inference, use the <code>roboflow/roboflow-inference-server-cpu:latest</code> container.</p> <pre><code>docker run -d ^\n    --name inference-server ^\n    --read-only ^\n    -p 9001:9001 ^\n    --volume \"%USERPROFILE%\\.inference\\cache:/tmp:rw\" ^\n    --security-opt=\"no-new-privileges\" ^\n    --cap-drop=\"ALL\" ^\n    --cap-add=\"NET_BIND_SERVICE\" ^\n    roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <p>The GPU container adds support for hardware acceleration on cards that support CUDA via NVIDIA-Docker. Ensure you have setup Docker to access the GPU then add <code>--gpus all</code> to the <code>docker run</code> command:</p> <pre><code>docker run -d ^\n    --name inference-server ^\n    --gpus all ^\n    --read-only ^\n    -p 9001:9001 ^\n    --volume \"%USERPROFILE%\\.inference\\cache:/tmp:rw\" ^\n    --security-opt=\"no-new-privileges\" ^\n    --cap-drop=\"ALL\" ^\n    --cap-add=\"NET_BIND_SERVICE\" ^\n    roboflow/roboflow-inference-server-gpu:latest\n</code></pre> <p>With the GPU container you can optionally enable TensorRT, NVIDIA's model optimization runtime that will greatly increase your models' speed at the expense of a heavy compilation and optimization step (sometimes 15+ minutes) the first time you load each model.</p> <p>You can enable TensorRT by adding <code>TensorrtExecutionProvider</code> to the <code>ONNXRUNTIME_EXECUTION_PROVIDERS</code> environment variable.</p> <pre><code>docker run -d ^\n    --name inference-server ^\n    --gpus all ^\n    --read-only ^\n    -p 9001:9001 ^\n    --volume \"%USERPROFILE%\\.inference\\cache:/tmp:rw\" ^\n    --security-opt=\"no-new-privileges\" ^\n    --cap-drop=\"ALL\" ^\n    --cap-add=\"NET_BIND_SERVICE\" ^\n    -e ONNXRUNTIME_EXECUTION_PROVIDERS=\"[TensorrtExecutionProvider,CUDAExecutionProvider,OpenVINOExecutionProvider,CPUExecutionProvider]\" ^\n    roboflow/roboflow-inference-server-gpu:latest\n</code></pre>"},{"location":"install/windows/#docker-compose","title":"Docker Compose","text":"<p>If you are using Docker Compose for your application, the equivalent yaml is:</p> CPUGPUTensorRT <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-cpu:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${USERPROFILE}/.inference/cache:/tmp:rw\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-gpu:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${USERPROFILE}/.inference/cache:/tmp:rw\"\n\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre> <pre><code>version: \"3.9\"\n\nservices:\n  inference-server:\n    container_name: inference-server\n    image: roboflow/roboflow-inference-server-gpu:latest\n\n    read_only: true\n    ports:\n      - \"9001:9001\"\n\n    volumes:\n      - \"${USERPROFILE}/.inference/cache:/tmp:rw\"\n\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n\n    environment:\n      ONNXRUNTIME_EXECUTION_PROVIDERS: \"[TensorrtExecutionProvider,CUDAExecutionProvider,OpenVINOExecutionProvider,CPUExecutionProvider]\"\n\n    security_opt:\n      - no-new-privileges\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n</code></pre>"},{"location":"install/windows/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p>"},{"location":"install/windows/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"install/windows/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"install/cloud/","title":"Deploy in Your Own Cloud","text":"<p>You can run Roboflow Inference on major cloud platforms like AWS, Azure, or GCP.</p> <p>Deploying in your own cloud is ideal if you want the scalability and flexibility of the cloud but have technical or organizational constraints around where you can send your data.</p> <p>Roboflow Inference has an integration with SkyPilot that makes deploying a cloud instance to run Inference as quick as running one command after you have authenticated with your cloud provider.</p> <p>Read our deployment guides for more information:</p> <ul> <li>Set up Inference on AWS</li> <li>Set up Inference on Azure</li> <li>Set up Inference on GCP</li> </ul>"},{"location":"install/cloud/#manual-setup","title":"Manual Setup","text":"<p>You can also use the Linux setup guide to manually configure a Docker container to run on your cloud VM.</p> <p>A Helm Chart is available for enterprise cloud deployments via Kubernetes.</p>"},{"location":"install/cloud/aws/","title":"Deploy on AWS","text":"<p>You can run Roboflow Inference on machines hosted on Amazon Web Services.</p> <p>This is ideal if you want to benefit from all of the features Inference has to offer but also want to manage your own cloud infrastructure.</p>"},{"location":"install/cloud/aws/#set-up-an-aws-ec2-instance","title":"Set up an AWS EC2 instance","text":"<p>To get started, you will need an EC2 instance running on AWS.</p> <p>For deploying instances, we recommend SkyPilot, a tool designed to help you set up cloud instances for AI projects.</p> <p>To get started, run the following command on your own machine:</p> <pre><code>pip install inference \"skypilot[gcp,aws]\"\n</code></pre> <p>Follow the SkyPilot AWS documentation to authenticate with AWS</p> <p>Then, run:</p> <pre><code>inference cloud deploy --provider aws --compute-type gpu\n</code></pre> <p>This will provision a GPU-capable instance in AWS.</p> <p>The latest version of Roboflow Inference will be automatically installed on the machine.</p> <p>When the command has run, you should see a message like:</p> <pre><code>Deployed Roboflow Inference to aws on gpu, deployment name is ...\nTo get a list of your deployments: inference status\nTo delete your deployment: inference undeploy ...\nTo ssh into the deployed server: ssh ...\nThe Roboflow Inference Server is running at http://34.66.116.66:9001\n</code></pre> <p>You can then use the API endpoint for your server for use in running models.</p> <p>You can run any model that Inference supports, including object detection, segmentation, classification, and keypoint models that you have available on Roboflow, and foundation models like CLIP, PaliGemma, SAM-2, and more.</p>"},{"location":"install/cloud/aws/#next-steps","title":"Next Steps","text":"<p>Once you've decided on a deployment method and have a server running, interfacing with it is easy. </p> <p>Fill in your <code>api_url</code> with the IP address of your VM and the port (<code>9001</code> by default) your server is running on.</p>"},{"location":"install/cloud/azure/","title":"Deploy on Azure","text":"<p>You can run Roboflow Inference on machines hosted on Azure.</p> <p>This is ideal if you want to benefit from all of the features Inference has to offer but also want to manage your own cloud infrastructure.</p>"},{"location":"install/cloud/azure/#set-up-an-azure-cloud-compute-vm","title":"Set up an Azure Cloud Compute VM","text":"<p>To get started, you will need a cloud compute instance running on Azure.</p> <p>For deploying instances, we recommend SkyPilot, a tool designed to help you set up cloud instances for AI projects.</p> <p>To get started, run the following command on your own machine:</p> <pre><code>pip install inference \"skypilot[azure]\"\n</code></pre> <p>Follow the SkyPilot Azure documentation to authenticate with AWS</p> <p>Then, run:</p> <pre><code>inference cloud deploy --provider azure --compute-type gpu\n</code></pre> <p>This will provision a GPU-capable instance in Azure.</p> <p>The latest version of Roboflow Inference will be automatically installed on the machine.</p> <p>When the command has run, you should see a message like:</p> <pre><code>Deployed Roboflow Inference to azure on gpu, deployment name is ...\nTo get a list of your deployments: inference status\nTo delete your deployment: inference undeploy ...\nTo ssh into the deployed server: ssh ...\nThe Roboflow Inference Server is running at http://34.66.116.66:9001\n</code></pre> <p>You can then use the API endpoint for your server for use in running models.</p> <p>You can run any model that Inference supports, including object detection, segmentation, classification, and keypoint models that you have available on Roboflow, and foundation models like CLIP, PaliGemma, SAM-2, and more.</p>"},{"location":"install/cloud/azure/#next-steps","title":"Next Steps","text":"<p>Once you've decided on a deployment method and have a server running, interfacing with it is easy. </p> <p>Fill in your <code>api_url</code> with the IP address of your VM and the port (<code>9001</code> by default) your server is running on.</p>"},{"location":"install/cloud/gcp/","title":"Deploy on Google Cloud Platform","text":"<p>You can run Roboflow Inference on machines hosted on Google Cloud Platform (GCP).</p> <p>This is ideal if you want to benefit from all of the features Inference has to offer but also want to manage your own cloud infrastructure.</p>"},{"location":"install/cloud/gcp/#set-up-a-google-cloud-compute-vm","title":"Set up a Google Cloud Compute VM","text":"<p>To get started, you will need a cloud compute instance running on GCP.</p> <p>For deploying instances, we recommend SkyPilot, a tool designed to help you set up cloud instances for AI projects.</p> <p>To get started, run the following command on your own machine:</p> <pre><code>pip install inference \"skypilot[gcp]\"\n</code></pre> <p>Follow the SkyPilot GCP documentation to authenticate with AWS</p> <p>Then, run:</p> <pre><code>inference cloud deploy --provider gcp --compute-type gpu\n</code></pre> <p>This will provision a GPU-capable instance in GCP.</p> <p>The latest version of Roboflow Inference will be automatically installed on the machine.</p> <p>When the command has run, you should see a message like:</p> <pre><code>Deployed Roboflow Inference to gcp on gpu, deployment name is ...\nTo get a list of your deployments: inference status\nTo delete your deployment: inference undeploy ...\nTo ssh into the deployed server: ssh ...\nThe Roboflow Inference Server is running at http://34.66.116.66:9001\n</code></pre> <p>You can then use the API endpoint for your server for use in running models.</p> <p>You can run any model that Inference supports, including object detection, segmentation, classification, and keypoint models that you have available on Roboflow, and foundation models like CLIP, PaliGemma, SAM-2, and more.</p>"},{"location":"install/cloud/gcp/#next-steps","title":"Next Steps","text":"<p>Once you've decided on a deployment method and have a server running, interfacing with it is easy. </p> <p>Fill in your <code>api_url</code> with the IP address of your VM and the port (<code>9001</code> by default) your server is running on.</p>"},{"location":"managed/","title":"Managed Compute","text":"<p>By far the easiest way to get started is with Roboflow's managed services. You can jump straight to building without having to setup any infrastructure. It's often the front-door to using Inference even for those who know they will eventually want to self host.</p> <p>There are two cloud hosted offerings with different targeted use-cases, capabilities, and pricing models.</p>"},{"location":"managed/#serverless-hosted-api","title":"Serverless Hosted API","text":"<p>The Serverless Hosted API supports running Workflows on pre-trained &amp; fine-tuned models, chaining models, basic logic, visualizations, and external integrations.</p> <p>It supports cloud-hosted VLMs like ChatGPT and Anthropic Claude, but does not support running heavy models like Florence-2 or SAM 2. It also does not support streaming video.</p> <p>The Serverless API scales down to zero when you're not using it (and up to infinity under load) with quick (a couple of seconds) cold-start time. You pay per model inference with no minimums. Roboflow's free tier credits may be used.</p>"},{"location":"managed/#dedicated-deployments","title":"Dedicated Deployments","text":"<p>Dedicated Deployments are single-tenant virtual machines that are allocated for your exclusive use. They can optionally be configured with a GPU and used in development mode (where you may be evicted if capacity is needed for a higher priority task &amp; are limited to 3-hour sessions) or production mode (guaranteed capacity and no session time limit).</p> <p>On a Dedicated Deployment, you can stream video, run custom Python code, access heavy foundation models like SAM 2, Florence-2, and Paligemma (including your fine-tunes of those models), and install additional dependencies. They are much higher performance machines than the instances backing the Serverless Hosted API.</p> <p>Scale-up time is on the order of a minute or two.</p> <p>Dedicated Deployments Availability</p> <p>Dedicated Deployments are only available to Roboflow Workspaces with an active subscription (and are not available on the free trial). They are billed hourly.</p>"},{"location":"managed/#bring-your-own-cloud","title":"Bring Your Own Cloud","text":"<p>Sometimes enterprise compliance policies regarding sensitive data requires running workloads on-premises. This is supported via self-hosting on your own cloud. Billing is the same as for self-hosting on an edge device.</p> <p></p>"},{"location":"managed/#next-steps","title":"Next Steps","text":"<p>Once you've decided on a deployment method and have a server running, interfacing with it is easy.</p>"},{"location":"managed/dedicated/","title":"Dedicated Deployments","text":"<p>Dedicated Deployments are private cloud servers managed by Roboflow, specifically designed to run your computer vision models.</p> <p>Dedicated Deployments run Roboflow Inference.</p> <p>You can set up two types of Dedicated Deployments:</p> <ol> <li>Development, which spins up a machine that you can access for three hours for use in testing your model and logic.</li> <li>Production, which spins up a machine that you can use for production applications. This machine will run until you turn it off.</li> </ol> <p>Dedicated Deployments can both models trained or hosted on Roboflow, foundation models, and Roboflow Workflows.</p> <p>Here are a few model types available:</p> <ul> <li>Object detection</li> <li>Image segmentation</li> <li>Classification</li> <li>Keypoint detection</li> <li>CLIP</li> <li>SAM-2</li> <li>Florence-2</li> <li>PaliGemma</li> <li>Roboflow Workflows</li> </ul> <p>You can create Dedicated Deployments in the Roboflow web application or using the Roboflow Command Line Interface.</p>"},{"location":"managed/dedicated/#why-use-dedicated-deployments","title":"Why Use Dedicated Deployments?","text":"<ol> <li>Focus on your machine vision business problem, leave the infrastructure to us: Spin up inference serving infrastructure with a few clicks and without having to signup with cloud providers, installing and securing servers, managing TLS certificates or worrying about server management, patching, updates etc.</li> <li>Dedicated Resources: Get cloud servers allocated specifically for your use, ensuring consistent performance for your models. </li> <li>Secure Access: Dedicated Deployments are accessible with your workspace's unique API key and utilize HTTPS for secure communication.</li> <li>Easy Integration: Each deployment receives a subdomain within roboflow.cloud, simplifying integration with your applications.</li> </ol> <p>Note</p> <p>All dedicated deployments are currently hosted in US-based data centers; users from other Geographies may see higher latencies. Please contact us for a customized solution if you are outside of US, we can help you to reduce the network latency.</p>"},{"location":"managed/dedicated/#create-a-dedicated-deployment-from-the-web-app","title":"Create a Dedicated Deployment from the Web App","text":"<p>Go to your Roboflow Dashboard and click \"Deployments\" in the right sidebar.</p> <p>This will take you to your deployments dashboard.</p> <p></p> <p>Click \"New Deployment\" to create a new Dedicated Deployment.</p> <p></p> <p>A modal will appear in which you can configure your server.</p> <p>You can configure the following values:</p> Property Description Name Choose a unique name (5\u201315 characters) to identify your Dedicated Deployment. This name will also become the subdomain for your deployment endpoint (e.g., <code>[invalid URL removed]</code>).Easy to Remember: Pick a name that clearly reflects your deployment's purpose (e.g., \"prod-inference\", \"dev-testing\").Unique within Workspace: If your chosen name is already taken, a short random code will be added to create a unique subdomain.Tips:- Use lowercase letters, numbers, and hyphens (-) for your name.- Avoid special characters or spaces. Machine Type Whether a CPU-only or a GPU dedicated deployment is needed. Deployment Type This is the deployment environment\u2014Development (dev) or Production (prod). Duration This is the time for which the dedicated deployment remains online.- For development environments this can range from 1\u20136 hours. Fractional hours are permitted, and they will be rounded to the nearest minute for billing purposes.- For production there is no expiration time; the deployment will run until the user deletes it explicitly."},{"location":"managed/dedicated/#create-a-dedicated-deployment-from-workflows","title":"Create a Dedicated Deployment from Workflows","text":"<p>Dedicated deployments can also be created from within Roboflow Workflows. Roboflow Workflows is a low-code, web-based application builder for creating computer vision applications.</p> <p>There is no difference between provisioning a deployment from Workflows or the web app: both methods work the same.</p> <p>To create a Dedicated Deployment, first create a Roboflow Workflow. To do so, click on Workflows on the left tab in the Roboflow dashboard, then create a Workflow.</p> <p>Then, click on the Running on Hosted API link in the top left corner:</p> <p></p> <p>Click Dedicated Deployments to create and see your Dedicated Deployments, the dialog presented here is identical to the one described above:</p> <p></p> <p>When your Deployment is ready, the status will be updated to Ready. You can then click Connect to use your deployment with your Workflow in the Workflows editor:</p> <p></p> <p>Connecting to your Dedicated Deployment will allow you to use the deployment in your Workflow. This means you can run models like SAM-2 and Florence-2 that are only supported when running Workflows on-device or on a Dedicated Deployment.</p>"},{"location":"managed/dedicated/#use-a-dedicated-deployment","title":"Use a Dedicated Deployment","text":"<p>With a Dedicated Deployment ready, you can use it to run inference images, video frames, and with Workflows.</p>"},{"location":"managed/dedicated/#roboflow-models","title":"Roboflow Models","text":"<p>To use your Dedicated Deployment with models hosted on Roboflow:</p> <ol> <li>Navigate to the Deployment tab of any model in your Workspace.</li> <li>Click on \"Hosted Image Inference\".</li> <li>Copy the code snippet and replace the API (i.e. <code>https://detect.roboflow.com</code>) with the URL of your Dedicated Deployment.</li> </ol> <p>Here is an example:</p> <pre><code># import the inference-sdk\nfrom inference_sdk import InferenceHTTPClient\n\n# initialize the client\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://yourdeployment.roboflow.cloud\"\n    api_key=\"YOUR_API_KEY\"\n)\n\n# infer on a local image\nresult = CLIENT.infer(\"YOUR_IMAGE.jpg\", model_id=\"counting-screws/3\")\n</code></pre>"},{"location":"managed/dedicated/#workflows","title":"Workflows","text":"<p>To deploy with a Dedicated Deployment in Workflows, go to any Workflow in your Roboflow workspace. Then, click \"Deploy Workflow\".</p> <p>A window will appear with several deployment options. Select the \"Run on an Image (Local)\" or \"Run on a Video (Local)\" option, depending on how you want to deploy.</p> <p></p> <p>Copy the code snippet relevant to you, and replace the API URL with the URL of your Dedicated Deployment.</p>"},{"location":"managed/dedicated/#provision-and-manage-dedicated-deployments-roboflow-cli","title":"Provision and Manage Dedicated Deployments (Roboflow CLI)","text":"<p>The roboflow deployment command provides a set of subcommands to manage your Roboflow Dedicated Deployments. These deployments allow you to run inference on your computer vision models on dedicated servers.</p> <p>Subcommands</p> <ul> <li> <p><code>machine_type</code>: List available machine types for your Dedicated Deployments. Please be noted that the output is a combination of Deployment Type and Machine Type mentioned above, i.e., dev-cpu, dev-gpu, prod-cpu, prod-gpu.</p> </li> <li> <p><code>add</code>: Create a new Dedicated Deployment.</p> </li> <li> <p><code>get</code>: Get detailed information about a specific Dedicated Deployment.</p> </li> <li> <p><code>list</code>: List all Dedicated Deployments in your workspace.</p> </li> <li> <p><code>usage_workspace</code>: Get usage statistics for all Dedicated Deployments in your workspace.</p> </li> <li> <p><code>usage_deployment</code>: Get usage statistics for a specific Dedicated Deployment.</p> </li> <li> <p><code>delete</code>: Delete a Dedicated Deployment.</p> </li> <li> <p><code>log</code>: View logs for a specific Dedicated Deployment.</p> </li> </ul>"},{"location":"managed/dedicated/#subcommand-examples","title":"Subcommand Examples","text":""},{"location":"managed/dedicated/#create-a-new-deployment","title":"Create a new deployment","text":"<pre><code>roboflow deployment add my-deployment -m prod-gpu\n</code></pre>"},{"location":"managed/dedicated/#get-deployment-information","title":"Get deployment information","text":"<pre><code>roboflow deployment get my-deployment\n</code></pre>"},{"location":"managed/dedicated/#list-all-deployments","title":"List all deployments","text":"<pre><code>roboflow deployment list\n</code></pre>"},{"location":"managed/dedicated/#get-workspace-usage","title":"Get workspace usage","text":"<pre><code>roboflow deployment usage_workspace\n</code></pre>"},{"location":"managed/dedicated/#get-deployment-usage","title":"Get deployment usage","text":"<pre><code>roboflow deployment usage_deployment my-deployment\n</code></pre>"},{"location":"managed/dedicated/#delete-a-deployment","title":"Delete a deployment","text":"<pre><code>roboflow deployment delete my-deployment\n</code></pre>"},{"location":"managed/dedicated/#view-deployment-logs","title":"View deployment logs","text":"<pre><code>roboflow deployment log my-deployment -t 60 -n 20\n</code></pre>"},{"location":"managed/serverless/","title":"Serverless Hosted API","text":"<p>You can run fine-tuned computer vision models hosted on Roboflow and Workflows built in Roboflow using the Roboflow Serverless API.</p> <p>The Serverless API requires no server management and scales as you go. Whether you are running one hundred or a million inferences a month, the Serverless API is ready to use.</p> <p>You may want to use the Serverless API if any of the following are true:</p> <ul> <li>You are running inference on single images.</li> <li>You need infrastructure that automatically scales.</li> <li>You only need the results from models and plan to write code to process the results.</li> </ul> <p>If you want to build multi-stage vision applications that run a model without writing code for each step in your application, check out Workflows.</p> <p>APIs are automatically set up for the following model types:</p> <ul> <li>Object detection</li> <li>Segmentation</li> <li>Classification</li> <li>Keypoint detection</li> </ul>"},{"location":"managed/serverless/#find-your-api-endpoint","title":"Find Your API Endpoint","text":"<p>To find your API endpoint, go to your project in Roboflow and click Deployments in the sidebar.</p> <p>Then, click on the \"Hosted Image Inference\" deployment option:</p> <p></p> <p>A window will appear with code snippets in various programming languages that you can use to run your model.</p> <p>The Python code snippet runs the Inference SDK. This SDK lets you make requests to the Serverless API in a few lines of code.</p> <p>Here is an example of what the code snippet will look like:</p> <pre><code># import the inference-sdk\nfrom inference_sdk import InferenceHTTPClient\n\n# initialize the client\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=\"YOUR_API_KEY\"\n)\n\n# infer on a local image\nresult = CLIENT.infer(\"YOUR_IMAGE.jpg\", model_id=\"counting-screws/3\")\n</code></pre> <p>This code will call the Roboflow Serverless API and return results from the inference.</p> <p>We can then plot predictions with the Roboflow supervision package:</p> <pre><code>detections = sv.Detections.from_inference(results)\n\n# create supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre>"},{"location":"models/from_local_weights/","title":"From Local Weights","text":"<p>You can upload supported weights to Roboflow and deploy them to your device.</p> <p>This is ideal if you have already trained a model outside of Roboflow that you want to deploy with Inference.</p> <p>To upload weights to Roboflow, you will need:</p> <ol> <li>A Roboflow account</li> <li>A project with your dataset (that does not have a trained model)</li> </ol> <p>To learn how to create a project and a dataset, refer to these guides:</p> <ul> <li>Create a project</li> <li>Create a dataset</li> </ul> <p>Once you have a project with a dataset, you can upload your weights.</p> <p>Install the Roboflow Python package:</p> <pre><code>pip install roboflow\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.project(\"your-project-id\")\nversion = project.version(1)\nversion.deploy(\"model-type\", \"path/to/training/results/\")\n</code></pre> <p>The following model types are supported:</p> Model Architecture Task Model Type ID RF-DETR Object Detection rfdetr-base RF-DETR Object Detection rfdetr-large YOLOv5 Object Detection yolov5 YOLOv5 Segmentation yolov5-seg YOLOv7 Object Detection yolov7-seg YOLOv8 Object Detection yolov8 YOLOv8 Segmentation yolov8-seg YOLOv8 Classification yolov8-cls YOLOv8 Pose Estimation yolov8-pose YOLOv9 Object Detection yolov9 YOLOv9 Segmentation yolov9 YOLO-NAS Object Detection yolonas YOLOv10 Object Detection yolov10 PaliGemma Multimodal paligemma-3b-pt-224 PaliGemma Multimodal paligemma-3b-pt-448 PaliGemma Multimodal paligemma-3b-pt-896 Florence-2 Multimodal florence-2-large Florence-2 Multimodal florence-2-base <p>In the code above, replace:</p> <ol> <li><code>your-project-id</code> with the ID of your project. Learn how to retrieve your Roboflow project ID.</li> <li><code>1</code> with the version number of your project.</li> <li><code>model-type</code> with the model type you want to deploy.</li> <li><code>path/to/training/results/</code> with the path to the weights you want to upload. This path will vary depending on what model architecture you are using.</li> </ol> <p>Your model weights will be uploaded to Roboflow. It may take a few minutes for your weights to be processed. Once your weights have been processed, your dataset version page will be updated to say that a model is available with your weights.</p> <p>You can then use the model with Inference following our Run a Private, Fine-Tuned Model model.</p>"},{"location":"notebooks/clip_classification/","title":"CLIP Classify Content of Video","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install supervision opencv-python\n</pre> !pip install supervision opencv-python In\u00a0[25]: Copied! <pre>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport supervision as sv\nfrom tqdm import tqdm\nfrom supervision import get_video_frames_generator\nimport time\n\nINFERENCE_ENDPOINT = \"https://infer.roboflow.com\"\nAPI_KEY = \"YOUR_API_KEY\"\nVIDEO = \"VIDEO_PATH\"\n</pre> import requests import base64 from PIL import Image from io import BytesIO import os import supervision as sv from tqdm import tqdm from supervision import get_video_frames_generator import time  INFERENCE_ENDPOINT = \"https://infer.roboflow.com\" API_KEY = \"YOUR_API_KEY\" VIDEO = \"VIDEO_PATH\"   In\u00a0[\u00a0]: Copied! <pre>#Prompt list to evaluate similarity between each image and each prompt. If something else is selected, then we ignore the caption\n#change this to your desired prompt list\nprompt_list = [['action video game shooting xbox','Drake rapper music','soccer game ball',\n                'marvel combic book','beyonce','Church pope praying',\n                'Mcdonalds French Fries',\"something else\"]]\n</pre> #Prompt list to evaluate similarity between each image and each prompt. If something else is selected, then we ignore the caption #change this to your desired prompt list prompt_list = [['action video game shooting xbox','Drake rapper music','soccer game ball',                 'marvel combic book','beyonce','Church pope praying',                 'Mcdonalds French Fries',\"something else\"]] In\u00a0[26]: Copied! <pre>def classify_image(image: str, prompt: str) -&gt; dict:\n    \n    image_data = Image.fromarray(image)\n\n    buffer = BytesIO()\n    image_data.save(buffer, format=\"JPEG\")\n    image_data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n    payload = {\n        \"api_key\": API_KEY,\n        \"subject\": {\n            \"type\": \"base64\",\n            \"value\": image_data\n        },\n        \"prompt\": prompt,\n    }\n\n    data = requests.post(INFERENCE_ENDPOINT + \"/clip/compare?api_key=\" + API_KEY, json=payload)\n\n    response = data.json()\n    #print(response[\"similarity\"])\n    sim = response[\"similarity\"]\n\n    highest_prediction = 0\n    highest_prediction_index = 0\n\n    for i, prediction in enumerate(response[\"similarity\"]):\n        if prediction &gt; highest_prediction:\n            highest_prediction = prediction\n            highest_prediction_index = i\n\n    return prompt[highest_prediction_index], sim[highest_prediction_index]\n</pre> def classify_image(image: str, prompt: str) -&gt; dict:          image_data = Image.fromarray(image)      buffer = BytesIO()     image_data.save(buffer, format=\"JPEG\")     image_data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")      payload = {         \"api_key\": API_KEY,         \"subject\": {             \"type\": \"base64\",             \"value\": image_data         },         \"prompt\": prompt,     }      data = requests.post(INFERENCE_ENDPOINT + \"/clip/compare?api_key=\" + API_KEY, json=payload)      response = data.json()     #print(response[\"similarity\"])     sim = response[\"similarity\"]      highest_prediction = 0     highest_prediction_index = 0      for i, prediction in enumerate(response[\"similarity\"]):         if prediction &gt; highest_prediction:             highest_prediction = prediction             highest_prediction_index = i      return prompt[highest_prediction_index], sim[highest_prediction_index] In\u00a0[\u00a0]: Copied! <pre>def process_video_frames(video_path, prompt_list, total_frames=160, total_seconds=80, stride_length=30,max_retries):\n    if not os.path.exists(video_path):\n        print(f\"The specified video file '{video_path}' does not exist.\")\n        return\n\n    frames_per_second = total_frames / total_seconds\n    frame_dict = {}\n\n    for frame_index, frame in enumerate(sv.get_video_frames_generator(source_path=video_path, stride=stride_length, start=0)):\n        frame_second = frame_index * (1 / frames_per_second)\n        frame_key = f\"Frame {frame_index}: {frame_second:.2f} seconds\"\n        frame_dict[frame_key] = []\n\n        print(frame_key)\n        retries = 0\n\n        for prompt in prompt_list:\n            try: \n                label, similarity = classify_image(frame)\n                if label != \"something else\":\n                    print('label found')\n                    frame_dict[frame_key].append({label: similarity})\n                    print('\\n')\n\n            except Exception as e:\n                retries += 1\n                print(f\"Error: {e}\")\n                print(f\"Retrying... (Attempt {retries}/{max_retries})\")\n\n                if retries &gt;= max_retries:\n                    print(\"Max retries exceeded. Skipping frame.\")\n                    break\n\n    return frame_dict\n\n# Example usage:\nmax_retries = 4\nprompt_list = prompt_list\nclip_results = process_video_frames(VIDEO, prompt_list,max_retries)\n</pre>  def process_video_frames(video_path, prompt_list, total_frames=160, total_seconds=80, stride_length=30,max_retries):     if not os.path.exists(video_path):         print(f\"The specified video file '{video_path}' does not exist.\")         return      frames_per_second = total_frames / total_seconds     frame_dict = {}      for frame_index, frame in enumerate(sv.get_video_frames_generator(source_path=video_path, stride=stride_length, start=0)):         frame_second = frame_index * (1 / frames_per_second)         frame_key = f\"Frame {frame_index}: {frame_second:.2f} seconds\"         frame_dict[frame_key] = []          print(frame_key)         retries = 0          for prompt in prompt_list:             try:                  label, similarity = classify_image(frame)                 if label != \"something else\":                     print('label found')                     frame_dict[frame_key].append({label: similarity})                     print('\\n')              except Exception as e:                 retries += 1                 print(f\"Error: {e}\")                 print(f\"Retrying... (Attempt {retries}/{max_retries})\")                  if retries &gt;= max_retries:                     print(\"Max retries exceeded. Skipping frame.\")                     break      return frame_dict  # Example usage: max_retries = 4 prompt_list = prompt_list clip_results = process_video_frames(VIDEO, prompt_list,max_retries)  In\u00a0[\u00a0]: Copied! <pre># Flatten the nested dictionary\ndata = clip_results\n# Define the threshold based on the similarity score returned for the most similar prompt\nthreshold = 0.22\n\n# Filter out key-value pairs below the threshold for each frame\nfiltered_data = [\n    {\n        frame: [\n            {key: value}\n            for item in items\n            for key, value in item.items()\n            if value &gt; threshold\n        ]\n    }\n    for frame, items in data.items()\n]\nprint(filtered_data)\n</pre> # Flatten the nested dictionary data = clip_results # Define the threshold based on the similarity score returned for the most similar prompt threshold = 0.22  # Filter out key-value pairs below the threshold for each frame filtered_data = [     {         frame: [             {key: value}             for item in items             for key, value in item.items()             if value &gt; threshold         ]     }     for frame, items in data.items() ] print(filtered_data) In\u00a0[44]: Copied! <pre># Specify the filename for the JSON file\nimport json\nfilename = f\"{str(threshold)}.json\"\n\n# Write the dictionary to the JSON file\nwith open(filename, 'w') as json_file:\n    json.dump(filtered_data, json_file, indent=4)  # The indent parameter is optional for pretty-printing\n\n#print(f'Data has been written to {filename})\n</pre> # Specify the filename for the JSON file import json filename = f\"{str(threshold)}.json\"  # Write the dictionary to the JSON file with open(filename, 'w') as json_file:     json.dump(filtered_data, json_file, indent=4)  # The indent parameter is optional for pretty-printing  #print(f'Data has been written to {filename})"},{"location":"notebooks/clip_classification/#clip-classify-content-of-video","title":"CLIP Classify Content of Video\u00b6","text":"<p>CLIP is a powerful foundation model for zero-shot classification. In this scenario, we are using CLIP to classify the topics in a Youtube video. Plug in your own video and set of prompts!</p> <p>Click the Open in Colab button to run the cookbook on Google Colab.</p> <p>Let's begin!</p>"},{"location":"notebooks/clip_classification/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>opencv</code> and <code>supervision</code></p>"},{"location":"notebooks/clip_classification/#imports-configure-roboflow-inference-server","title":"Imports &amp; Configure Roboflow Inference Server\u00b6","text":""},{"location":"notebooks/clip_classification/#prompt-list-for-clip-similarity-function","title":"Prompt List for CLIP similarity function\u00b6","text":""},{"location":"notebooks/clip_classification/#clip-endpoint-compare-frame-prompt-list-similarity","title":"CLIP Endpoint Compare Frame &amp; Prompt List Similarity\u00b6","text":""},{"location":"notebooks/clip_classification/#process-video-return-most-similar-prompt-to-frame","title":"Process Video &amp; Return Most Similar Prompt to Frame\u00b6","text":""},{"location":"notebooks/clip_classification/#create-json-file-and-filter-out-low-similarity-classes","title":"Create JSON file and filter out low similarity classes\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/","title":"InferencePipeline on RTSP Stream","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install inference supervision==0.18.0\n</pre> !pip install inference supervision==0.18.0 In\u00a0[\u00a0]: Copied! <pre>from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\nimport supervision as sv\nimport pandas as pd\nfrom collections import defaultdict\nimport cv2\nimport numpy as np\nimport time\n</pre> from inference.core.interfaces.stream.inference_pipeline import InferencePipeline from inference.core.interfaces.stream.sinks import render_boxes import supervision as sv import pandas as pd from collections import defaultdict import cv2 import numpy as np import time  In\u00a0[\u00a0]: Copied! <pre># Create an instance of FPSMonitor\nfps_monitor = sv.FPSMonitor()\n\nREGISTERED_ALIASES = {\n    \"yolov8n-640\": \"coco/3\",\n    \"yolov8n-1280\": \"coco/9\",\n    \"yolov8m-640\": \"coco/8\"\n}\n\nAPI_KEY = \"API_KEY\"\nRTSP_STREAM = \"RTSP_URL\"\n\n# Example alias\nalias = \"yolov8n-640\"\n\n# Function to resolve an alias to the actual model ID\ndef resolve_roboflow_model_alias(model_id: str) -&gt; str:\n    return REGISTERED_ALIASES.get(model_id, model_id)\n\n# Resolve the alias to get the actual model ID\nmodel_name = resolve_roboflow_model_alias(alias)\n\n# Modify the render_boxes function to enable displaying statistics\ndef on_prediction(predictions, video_frame):\n    render_boxes(\n        predictions=predictions,\n        video_frame=video_frame,\n        fps_monitor=fps_monitor,  # Pass the FPS monitor object\n        display_statistics=True,   # Enable displaying statistics\n    )\n    \npipeline = InferencePipeline.init(\n    model_id= model_name,\n    video_reference=RTSP_STREAM,\n    on_prediction=on_prediction,\n    api_key=API_KEY,\n    confidence=0.5,\n)\n\npipeline.start()\npipeline.join()\n</pre> # Create an instance of FPSMonitor fps_monitor = sv.FPSMonitor()  REGISTERED_ALIASES = {     \"yolov8n-640\": \"coco/3\",     \"yolov8n-1280\": \"coco/9\",     \"yolov8m-640\": \"coco/8\" }  API_KEY = \"API_KEY\" RTSP_STREAM = \"RTSP_URL\"  # Example alias alias = \"yolov8n-640\"  # Function to resolve an alias to the actual model ID def resolve_roboflow_model_alias(model_id: str) -&gt; str:     return REGISTERED_ALIASES.get(model_id, model_id)  # Resolve the alias to get the actual model ID model_name = resolve_roboflow_model_alias(alias)  # Modify the render_boxes function to enable displaying statistics def on_prediction(predictions, video_frame):     render_boxes(         predictions=predictions,         video_frame=video_frame,         fps_monitor=fps_monitor,  # Pass the FPS monitor object         display_statistics=True,   # Enable displaying statistics     )      pipeline = InferencePipeline.init(     model_id= model_name,     video_reference=RTSP_STREAM,     on_prediction=on_prediction,     api_key=API_KEY,     confidence=0.5, )  pipeline.start() pipeline.join() In\u00a0[\u00a0]: Copied! <pre>#ByteTrack &amp; Supervision\ntracker = sv.ByteTrack()\nannotator = sv.BoxAnnotator()\nframe_count = defaultdict(int)\ncolors = sv.ColorPalette.default()\n\n#define polygon zone of interest\npolygons = [\nnp.array([\n[390, 543],[1162, 503],[1510, 711],[410, 819],[298, 551],[394, 543]\n])\n]\n\n#create zones, zone_annotator, and box_annotator based on polygon zone of interest\nzones = [\n    sv.PolygonZone(\n        polygon=polygon,\n    )\n    for polygon\n    in polygons\n]\nzone_annotators = [\n    sv.PolygonZoneAnnotator(\n        zone=zone,\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=8,\n        text_scale=4\n    )\n    for index, zone\n    in enumerate(zones)\n]\nbox_annotators = [\n    sv.BoxAnnotator(\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=4,\n        text_scale=2\n        )\n    for index\n    in range(len(polygons))\n]\n\n\n#columns for csv output\ncolumns = ['trackerID', 'class_id', 'frame_count','entry_timestamp','exit_timestamp','time_in_zone']\nframe_count_df = pd.DataFrame(columns=columns)\n\n# Define a dictionary to store the first detection timestamp for each tracker_id\nfirst_detection_timestamps = {}\nlast_detection_timestamps = {}\n\ndef render(predictions: dict, video_frame) -&gt; None:\n    detections = sv.Detections.from_inference(predictions)\n    detections = tracker.update_with_detections(detections)\n    \n    for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n        mask = zone.trigger(detections=detections)\n        detections_filtered = detections[mask]\n        \n        image = box_annotator.annotate(scene=video_frame.image, detections=detections, skip_label=False)\n        image = zone_annotator.annotate(scene=image)\n        \n        for tracker_id, class_id in zip(detections_filtered.tracker_id, detections_filtered.class_id):\n            frame_count[tracker_id] += 1\n            \n            # Check if tracker_id is not in first_detection_timestamps, if not, add the timestamp\n            if tracker_id not in first_detection_timestamps:\n                first_detection_timestamps[tracker_id] = time.time()\n            \n            last_detection_timestamps[tracker_id] = time.time()\n            \n            time_difference = last_detection_timestamps[tracker_id] - first_detection_timestamps[tracker_id]\n            \n            # Add data to the DataFrame\n            frame_count_df.loc[tracker_id] = [tracker_id, class_id, frame_count[tracker_id], first_detection_timestamps[tracker_id],last_detection_timestamps[tracker_id], time_difference]\n    \n    frame_count_df.to_csv('demo.csv', index=False)\n    \n    cv2.imshow(\"Prediction\", image)\n    cv2.waitKey(1)\n    \n\n#Initialize &amp; Deploy InferencePipeline\npipeline = InferencePipeline.init(\n    model_id=\"coco/8\",\n    video_reference=\"RTSP_URL\",\n    on_prediction=render,\n    api_key = 'API_KEY',\n    confidence=0.5,\n)\npipeline.start()\npipeline.join()\n</pre> #ByteTrack &amp; Supervision tracker = sv.ByteTrack() annotator = sv.BoxAnnotator() frame_count = defaultdict(int) colors = sv.ColorPalette.default()  #define polygon zone of interest polygons = [ np.array([ [390, 543],[1162, 503],[1510, 711],[410, 819],[298, 551],[394, 543] ]) ]  #create zones, zone_annotator, and box_annotator based on polygon zone of interest zones = [     sv.PolygonZone(         polygon=polygon,     )     for polygon     in polygons ] zone_annotators = [     sv.PolygonZoneAnnotator(         zone=zone,         color=colors.by_idx(index),         thickness=4,         text_thickness=8,         text_scale=4     )     for index, zone     in enumerate(zones) ] box_annotators = [     sv.BoxAnnotator(         color=colors.by_idx(index),         thickness=4,         text_thickness=4,         text_scale=2         )     for index     in range(len(polygons)) ]   #columns for csv output columns = ['trackerID', 'class_id', 'frame_count','entry_timestamp','exit_timestamp','time_in_zone'] frame_count_df = pd.DataFrame(columns=columns)  # Define a dictionary to store the first detection timestamp for each tracker_id first_detection_timestamps = {} last_detection_timestamps = {}  def render(predictions: dict, video_frame) -&gt; None:     detections = sv.Detections.from_inference(predictions)     detections = tracker.update_with_detections(detections)          for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):         mask = zone.trigger(detections=detections)         detections_filtered = detections[mask]                  image = box_annotator.annotate(scene=video_frame.image, detections=detections, skip_label=False)         image = zone_annotator.annotate(scene=image)                  for tracker_id, class_id in zip(detections_filtered.tracker_id, detections_filtered.class_id):             frame_count[tracker_id] += 1                          # Check if tracker_id is not in first_detection_timestamps, if not, add the timestamp             if tracker_id not in first_detection_timestamps:                 first_detection_timestamps[tracker_id] = time.time()                          last_detection_timestamps[tracker_id] = time.time()                          time_difference = last_detection_timestamps[tracker_id] - first_detection_timestamps[tracker_id]                          # Add data to the DataFrame             frame_count_df.loc[tracker_id] = [tracker_id, class_id, frame_count[tracker_id], first_detection_timestamps[tracker_id],last_detection_timestamps[tracker_id], time_difference]          frame_count_df.to_csv('demo.csv', index=False)          cv2.imshow(\"Prediction\", image)     cv2.waitKey(1)       #Initialize &amp; Deploy InferencePipeline pipeline = InferencePipeline.init(     model_id=\"coco/8\",     video_reference=\"RTSP_URL\",     on_prediction=render,     api_key = 'API_KEY',     confidence=0.5, ) pipeline.start() pipeline.join()"},{"location":"notebooks/inference_pipeline_rtsp/#inferencepipeline-on-rtsp-stream","title":"InferencePipeline on RTSP Stream\u00b6","text":"<p>The Roboflow Inference Pipeline is a drop-in replacement for the Hosted Inference API that can be deployed on your own hardware. The Inference Pipeline interface is made for streaming and is likely the best route to go for real time use cases. It is an asynchronous interface that can consume many different video sources including local devices (like webcams), RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>We have optimized Inference Pipeline to get maximum performance from the NVIDIA Jetson line of edge-AI devices. We have done this by specifically tailoring the drivers, libraries, and binaries specifically to its CPU and GPU architectures.</p> <p>Let's begin!</p>"},{"location":"notebooks/inference_pipeline_rtsp/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>inference</code> and <code>supervision</code></p>"},{"location":"notebooks/inference_pipeline_rtsp/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/#run-inference-pipeline-with-coco-model-aliases-native-fps-monitor","title":"Run Inference Pipeline with COCO Model Aliases &amp; Native FPS Monitor\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/#time-in-zone-with-bytetrack-using-supervision-save-data-to-csv","title":"Time in Zone with Bytetrack using Supervision, save data to CSV\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/","title":"RGB Anomaly Detection","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install sklearn\n</pre> !pip install sklearn In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport numpy as np\nimport time\nimport base64\nimport requests\nimport os, glob\nfrom sklearn.cluster import KMeans\n</pre> import cv2 import numpy as np import time import base64 import requests import os, glob from sklearn.cluster import KMeans In\u00a0[\u00a0]: Copied! <pre>def parse_polygon_annotation(annotation_data, image_shape):\n    width, height = image_shape[1], image_shape[0]\n    return [(int(data['x']), int(data['y'])) for data in annotation_data]\n\ndef extract_polygon_area(image_path, polygon_points):\n    image = cv2.imread(image_path)\n    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n    cv2.drawContours(mask, [np.array(polygon_points)], -1, (255, 255, 255), -1)\n    return cv2.bitwise_and(image, image, mask=mask)\n\ndef compute_average_color(image):\n    mask = np.all(image != [0, 0, 0], axis=2)\n    avg_color = np.mean(image[mask], axis=0)\n    return avg_color\n\ndef color_difference(color1, color2):\n    return np.linalg.norm(np.array(color1) - np.array(color2))\n\ndef count_color_matches(dominant_colors, target_colors, threshold):\n    matches_count = {tuple(target): 0 for target in target_colors}\n    matched_colors = {tuple(target): [] for target in target_colors}\n    \n    for color in dominant_colors:\n        for target in target_colors:\n            difference = color_difference(color, target)\n            \n            if difference &lt; threshold:\n                matches_count[tuple(target)] += 1\n                matched_colors[tuple(target)].append(color)\n                \n    return matches_count, matched_colors\n\ndef get_dominant_colors(image, k=5):\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    image = image[np.any(image != [0, 0, 0], axis=1)]\n    kmeans = KMeans(n_clusters=k, n_init='auto')\n    kmeans.fit(image)\n    dominant_colors = kmeans.cluster_centers_\n    return dominant_colors\n\ndef extract_target_colors(target_image_path,inference_server_address, project_id, version_number):\n    target_image = cv2.imread(target_image_path)\n    with open(target_image_path, \"rb\") as f:\n        im_bytes = f.read()        \n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json'\n    }\n\n    params = {\n        'api_key': 'FFgkmScNUBERP9t3PJvV',\n    }\n\n    response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)\n    data = response.json()\n\n    for predictions in data['predictions']:\n        Pred_points = predictions['points']\n        target_image = cv2.imread(target_image_path)\n        polygon_points = parse_polygon_annotation(Pred_points, target_image.shape)\n        polygon_image = extract_polygon_area(target_image_path, polygon_points)\n        target_dominant_colors = get_dominant_colors(polygon_image)\n    \n    return target_dominant_colors\n\ndef match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold):\n    global prediction_counter, image_counter\n    total_matches = 0\n    matched_filepaths = []\n\n    extention_images = \".jpg\"\n    get_images = sorted(glob.glob(images_folder + '*' + extention_images))\n\n    for images in get_images:\n        t0 = time.time()\n        print(\"File path: \" + images)\n        img = cv2.imread(images)\n        with open(images, \"rb\") as f:\n            im_bytes = f.read()        \n        im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n        headers = {\n            'Content-Type': 'application/json',\n            'Accept': 'application/json'\n        }\n\n        params = {\n            'api_key': '',\n        }\n        \n        response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)\n        data = response.json()\n\n        for predictions in data['predictions']:\n            prediction_counter += 1\n            image_counter += 1\n            Pred_points = predictions['points']\n            image = cv2.imread(images)\n            polygon_points = parse_polygon_annotation(Pred_points, image.shape)\n            polygon_image = extract_polygon_area(images, polygon_points)\n            dominant_colors = get_dominant_colors(polygon_image)\n            matches, matched_colors_list = count_color_matches(dominant_colors, target_dominant_colors, color_threshold)\n        \n        all_matched = all(value &gt; 0 for value in matches.values())\n        \n        if all_matched:\n            matched_filepaths.append(images)\n            total_matches += 1\n\n    print(f\"\\nTotal images where all target colors matched: {total_matches}\")\n    print(f\"\\nMatched images where all target colors matched: {matched_filepaths}\")\n</pre> def parse_polygon_annotation(annotation_data, image_shape):     width, height = image_shape[1], image_shape[0]     return [(int(data['x']), int(data['y'])) for data in annotation_data]  def extract_polygon_area(image_path, polygon_points):     image = cv2.imread(image_path)     mask = np.zeros(image.shape[:2], dtype=np.uint8)     cv2.drawContours(mask, [np.array(polygon_points)], -1, (255, 255, 255), -1)     return cv2.bitwise_and(image, image, mask=mask)  def compute_average_color(image):     mask = np.all(image != [0, 0, 0], axis=2)     avg_color = np.mean(image[mask], axis=0)     return avg_color  def color_difference(color1, color2):     return np.linalg.norm(np.array(color1) - np.array(color2))  def count_color_matches(dominant_colors, target_colors, threshold):     matches_count = {tuple(target): 0 for target in target_colors}     matched_colors = {tuple(target): [] for target in target_colors}          for color in dominant_colors:         for target in target_colors:             difference = color_difference(color, target)                          if difference &lt; threshold:                 matches_count[tuple(target)] += 1                 matched_colors[tuple(target)].append(color)                      return matches_count, matched_colors  def get_dominant_colors(image, k=5):     image = image.reshape((image.shape[0] * image.shape[1], 3))     image = image[np.any(image != [0, 0, 0], axis=1)]     kmeans = KMeans(n_clusters=k, n_init='auto')     kmeans.fit(image)     dominant_colors = kmeans.cluster_centers_     return dominant_colors  def extract_target_colors(target_image_path,inference_server_address, project_id, version_number):     target_image = cv2.imread(target_image_path)     with open(target_image_path, \"rb\") as f:         im_bytes = f.read()             im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")      headers = {         'Content-Type': 'application/json',         'Accept': 'application/json'     }      params = {         'api_key': 'FFgkmScNUBERP9t3PJvV',     }      response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)     data = response.json()      for predictions in data['predictions']:         Pred_points = predictions['points']         target_image = cv2.imread(target_image_path)         polygon_points = parse_polygon_annotation(Pred_points, target_image.shape)         polygon_image = extract_polygon_area(target_image_path, polygon_points)         target_dominant_colors = get_dominant_colors(polygon_image)          return target_dominant_colors  def match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold):     global prediction_counter, image_counter     total_matches = 0     matched_filepaths = []      extention_images = \".jpg\"     get_images = sorted(glob.glob(images_folder + '*' + extention_images))      for images in get_images:         t0 = time.time()         print(\"File path: \" + images)         img = cv2.imread(images)         with open(images, \"rb\") as f:             im_bytes = f.read()                 im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")         headers = {             'Content-Type': 'application/json',             'Accept': 'application/json'         }          params = {             'api_key': '',         }                  response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)         data = response.json()          for predictions in data['predictions']:             prediction_counter += 1             image_counter += 1             Pred_points = predictions['points']             image = cv2.imread(images)             polygon_points = parse_polygon_annotation(Pred_points, image.shape)             polygon_image = extract_polygon_area(images, polygon_points)             dominant_colors = get_dominant_colors(polygon_image)             matches, matched_colors_list = count_color_matches(dominant_colors, target_dominant_colors, color_threshold)                  all_matched = all(value &gt; 0 for value in matches.values())                  if all_matched:             matched_filepaths.append(images)             total_matches += 1      print(f\"\\nTotal images where all target colors matched: {total_matches}\")     print(f\"\\nMatched images where all target colors matched: {matched_filepaths}\") In\u00a0[\u00a0]: Copied! <pre>def main():\n    target_image_path = \"TARGET_IMAGE_PATH\"\n    inference_server_address = \"http://detect.roboflow.com/\"\n    version_number = 1\n    project_id = \"PROJECT_ID\"\n    images_folder = \"IMAGE_FOLDER_PATH\"\n    # grab all the .jpg files\n    extention_images = \".jpg\"\n    get_images = sorted(glob.glob(images_folder + '*' + extention_images))\n    MAX_COLOR_DIFFERENCE = 3 * 256 # DO NOT EDIT\n    TARGET_COLOR_PERCENT_THRESHOLD= 0.08 # Value must be between 0 - 1 - DO EDIT\n    color_threshold = int(MAX_COLOR_DIFFERENCE * TARGET_COLOR_PERCENT_THRESHOLD)\n\n\n    target_dominant_colors = extract_target_colors(target_image_path,inference_server_address, project_id, version_number)\n    match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold)\n\nif __name__ == \"__main__\":\n    main()\n</pre> def main():     target_image_path = \"TARGET_IMAGE_PATH\"     inference_server_address = \"http://detect.roboflow.com/\"     version_number = 1     project_id = \"PROJECT_ID\"     images_folder = \"IMAGE_FOLDER_PATH\"     # grab all the .jpg files     extention_images = \".jpg\"     get_images = sorted(glob.glob(images_folder + '*' + extention_images))     MAX_COLOR_DIFFERENCE = 3 * 256 # DO NOT EDIT     TARGET_COLOR_PERCENT_THRESHOLD= 0.08 # Value must be between 0 - 1 - DO EDIT     color_threshold = int(MAX_COLOR_DIFFERENCE * TARGET_COLOR_PERCENT_THRESHOLD)       target_dominant_colors = extract_target_colors(target_image_path,inference_server_address, project_id, version_number)     match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold)  if __name__ == \"__main__\":     main()"},{"location":"notebooks/rgb_anomaly_detection/#rgb-anomaly-detection","title":"RGB Anomaly Detection\u00b6","text":"<p>In this cookbook, we identify color / RGB anomalies for segmented items. Capture a base image to extract your ground truth RGB with Roboflow and compare to neew data collected. In this scenario, we are assessing variations in logo color.</p> <p>Click the Open in Colab button to run the cookbook on Google Colab.</p> <p>Let's begin!</p>"},{"location":"notebooks/rgb_anomaly_detection/#install-required-packages","title":"Install required packages\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#extract-target-rgb-color-from-polygon-and-run-kmeans","title":"Extract target RGB color from polygon and run Kmeans\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#run-main-function-to-compare-base-color-logo-with-target-colors-and-run-anomaly-detection","title":"Run main function to compare base color logo with target colors and run anomaly detection\u00b6","text":""},{"location":"notebooks/workflow_schema_api/","title":"Workflow Schema API","text":"In\u00a0[\u00a0]: Copied! <pre>from google.colab import userdata\n\n\nWORKSPACE_NAME = \"nicks-workspace\"\nWORKFLOW_ID= \"detect-people\"\nINFERENCE_SERVER_URL = \"https://detect.roboflow.com\"\n\nWORKFLOW_SCHEMA_ENDPOINT = f\"{INFERENCE_SERVER_URL}/{WORKSPACE_NAME}/workflows/{WORKFLOW_ID}/describe_interface\"\nROBOFLOW_API_KEY = userdata.get(\"ROBOFLOW_API_KEY\")\n</pre> from google.colab import userdata   WORKSPACE_NAME = \"nicks-workspace\" WORKFLOW_ID= \"detect-people\" INFERENCE_SERVER_URL = \"https://detect.roboflow.com\"  WORKFLOW_SCHEMA_ENDPOINT = f\"{INFERENCE_SERVER_URL}/{WORKSPACE_NAME}/workflows/{WORKFLOW_ID}/describe_interface\" ROBOFLOW_API_KEY = userdata.get(\"ROBOFLOW_API_KEY\") In\u00a0[\u00a0]: Copied! <pre>import requests\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"api_key\": ROBOFLOW_API_KEY,\n}\n\nres = requests.post(WORKFLOW_SCHEMA_ENDPOINT, headers=headers, json=data)\n\nschema = res.json()\n\ninputs = schema[\"inputs\"]\noutputs = schema[\"outputs\"]\nkinds_schemas = schema[\"kinds_schemas\"]\ntyping_hints = schema[\"typing_hints\"]\n</pre> import requests  headers = {     \"Content-Type\": \"application/json\", }  data = {     \"api_key\": ROBOFLOW_API_KEY, }  res = requests.post(WORKFLOW_SCHEMA_ENDPOINT, headers=headers, json=data)  schema = res.json()  inputs = schema[\"inputs\"] outputs = schema[\"outputs\"] kinds_schemas = schema[\"kinds_schemas\"] typing_hints = schema[\"typing_hints\"]  In\u00a0[\u00a0]: Copied! <pre>from pprint import pprint\npprint(inputs)\npprint(outputs)\n</pre> from pprint import pprint pprint(inputs) pprint(outputs) <pre>{'image': ['image'], 'model_name': ['roboflow_model_id']}\n{'model_predictions': ['object_detection_prediction']}\n</pre> In\u00a0[\u00a0]: Copied! <pre>pprint(typing_hints)\n</pre> pprint(typing_hints) <pre>{'image': 'dict',\n 'object_detection_prediction': 'dict',\n 'roboflow_model_id': 'str'}\n</pre> In\u00a0[\u00a0]: Copied! <pre>pprint(kinds_schemas)\n</pre> pprint(kinds_schemas) <pre>{'image': {'properties': {'type': {'const': 'url',\n                                   'enum': ['url'],\n                                   'title': 'Type',\n                                   'type': 'string'},\n                          'value': {'description': 'Value depends on `type` - '\n                                                   'for url, one should '\n                                                   'provide URL to the file, '\n                                                   'for `file` - local path, '\n                                                   'for `base64` - base64 '\n                                                   'string.',\n                                    'title': 'Value'}},\n           'required': ['type', 'value'],\n           'title': 'ImageSchema',\n           'type': 'object'},\n 'object_detection_prediction': {'$defs': {'BoundingBoxSchema': {'properties': {'class': {'description': 'Name '\n                                                                                                         'of '\n                                                                                                         'the '\n                                                                                                         'class '\n                                                                                                         'associated '\n                                                                                                         'to '\n                                                                                                         'bounding '\n                                                                                                         'box',\n                                                                                          'title': 'class',\n                                                                                          'type': 'string'},\n                                                                                'class_id': {'description': 'Identifier '\n                                                                                                            'of '\n                                                                                                            'bounding '\n                                                                                                            'box '\n                                                                                                            'class',\n                                                                                             'title': 'Class '\n                                                                                                      'Id',\n                                                                                             'type': 'integer'},\n                                                                                'confidence': {'description': 'Model '\n                                                                                                              'confidence '\n                                                                                                              'for '\n                                                                                                              'bounding '\n                                                                                                              'box',\n                                                                                               'title': 'Confidence',\n                                                                                               'type': 'number'},\n                                                                                'detection_id': {'description': 'Identifier '\n                                                                                                                'of '\n                                                                                                                'detected '\n                                                                                                                'bounding '\n                                                                                                                'box',\n                                                                                                 'title': 'Detection '\n                                                                                                          'Id',\n                                                                                                 'type': 'string'},\n                                                                                'height': {'anyOf': [{'type': 'integer'},\n                                                                                                     {'type': 'number'}],\n                                                                                           'description': 'Height '\n                                                                                                          'of '\n                                                                                                          'bounding '\n                                                                                                          'box',\n                                                                                           'title': 'Height'},\n                                                                                'parent_id': {'anyOf': [{'type': 'string'},\n                                                                                                        {'type': 'null'}],\n                                                                                              'default': None,\n                                                                                              'description': 'Identifier '\n                                                                                                             'of '\n                                                                                                             'parent '\n                                                                                                             'image '\n                                                                                                             'region. '\n                                                                                                             'Useful '\n                                                                                                             'when '\n                                                                                                             'stack '\n                                                                                                             'of '\n                                                                                                             'detection-models '\n                                                                                                             'is '\n                                                                                                             'in '\n                                                                                                             'use '\n                                                                                                             'to '\n                                                                                                             'refer '\n                                                                                                             'the '\n                                                                                                             'RoI '\n                                                                                                             'being '\n                                                                                                             'the '\n                                                                                                             'input '\n                                                                                                             'to '\n                                                                                                             'inference',\n                                                                                              'title': 'Parent '\n                                                                                                       'Id'},\n                                                                                'width': {'anyOf': [{'type': 'integer'},\n                                                                                                    {'type': 'number'}],\n                                                                                          'description': 'Width '\n                                                                                                         'of '\n                                                                                                         'bounding '\n                                                                                                         'box',\n                                                                                          'title': 'Width'},\n                                                                                'x': {'anyOf': [{'type': 'integer'},\n                                                                                                {'type': 'number'}],\n                                                                                      'description': 'OX '\n                                                                                                     'coordinate '\n                                                                                                     'of '\n                                                                                                     'bounding '\n                                                                                                     'box '\n                                                                                                     'center',\n                                                                                      'title': 'X'},\n                                                                                'y': {'anyOf': [{'type': 'integer'},\n                                                                                                {'type': 'number'}],\n                                                                                      'description': 'OY '\n                                                                                                     'coordinate '\n                                                                                                     'of '\n                                                                                                     'bounding '\n                                                                                                     'box '\n                                                                                                     'center',\n                                                                                      'title': 'Y'}},\n                                                                 'required': ['width',\n                                                                              'height',\n                                                                              'x',\n                                                                              'y',\n                                                                              'confidence',\n                                                                              'class',\n                                                                              'class_id',\n                                                                              'detection_id'],\n                                                                 'title': 'BoundingBoxSchema',\n                                                                 'type': 'object'},\n                                           'ImageMetadataSchema': {'properties': {'height': {'anyOf': [{'type': 'integer'},\n                                                                                                       {'type': 'null'}],\n                                                                                             'description': 'The '\n                                                                                                            'original '\n                                                                                                            'height '\n                                                                                                            'of '\n                                                                                                            'the '\n                                                                                                            'image '\n                                                                                                            'used '\n                                                                                                            'in '\n                                                                                                            'inference',\n                                                                                             'title': 'Height'},\n                                                                                  'width': {'anyOf': [{'type': 'integer'},\n                                                                                                      {'type': 'null'}],\n                                                                                            'description': 'The '\n                                                                                                           'original '\n                                                                                                           'width '\n                                                                                                           'of '\n                                                                                                           'the '\n                                                                                                           'image '\n                                                                                                           'used '\n                                                                                                           'in '\n                                                                                                           'inference',\n                                                                                            'title': 'Width'}},\n                                                                   'required': ['width',\n                                                                                'height'],\n                                                                   'title': 'ImageMetadataSchema',\n                                                                   'type': 'object'}},\n                                 'properties': {'image': {'$ref': '#/$defs/ImageMetadataSchema'},\n                                                'predictions': {'items': {'$ref': '#/$defs/BoundingBoxSchema'},\n                                                                'title': 'Predictions',\n                                                                'type': 'array'}},\n                                 'required': ['image', 'predictions'],\n                                 'title': 'ObjectDetectionSchema',\n                                 'type': 'object'}}\n</pre>"},{"location":"notebooks/workflow_schema_api/#workflow-schema-api","title":"Workflow Schema API\u00b6","text":"<p>Prior to the workflow schema API it was a challenge to be able to programatically know what inputs a given workflow takes and what type of data a workflow will return. The Workflow Schema API aims to solve this problem by providng a list of inputs, outputs, typing hints, and schemas of kinds.</p>"},{"location":"notebooks/workflow_schema_api/#setup","title":"Setup\u00b6","text":"<p>First let's define a few variable for use in our api request. the workspace name and workflow ID can be found and configured on a workflow by clicking the workspace pencil icon.</p> <p></p> <p>You can also point this request to a self hosted, or dedicated deployment inference server url. Make sure to configure your ROBOFLOW_API_KEY in Google Colab secrets if you'd like to run this notebook aswell.</p>"},{"location":"notebooks/workflow_schema_api/#workflow-configuration","title":"Workflow Configuration\u00b6","text":"<p>For this example, let's show a simple workflow that takes one parameter for the model id, and runs the input image on a object detection model. It then outputs the predictions from the model.</p> <p></p>"},{"location":"notebooks/workflow_schema_api/#api-request","title":"API Request\u00b6","text":"<p>Now that we've got our workflow built, Let's take a look at the code required to hit the API.</p>"},{"location":"notebooks/workflow_schema_api/#inputs-and-outputs","title":"Inputs and Outputs\u00b6","text":"<p>The inputs and outputs keys show all of the inputs/outputs the workflow expects to run and return. This workflow requires an image and a model_name and will return a \"model_predictions\".</p>"},{"location":"notebooks/workflow_schema_api/#typing-hints","title":"Typing Hints\u00b6","text":"<p>The typing_hints key show the type of data being returned from the request from a python data type perspective.</p>"},{"location":"notebooks/workflow_schema_api/#kinds-schemas","title":"Kinds Schemas\u00b6","text":"<p>The kinds_schemas keys return an Open API specification with more detailed information about the data type being returned and how to parse it. For example the 'object_detection_prediction' contains information about the nested data that will be present.</p>"},{"location":"quickstart/aliases/","title":"Pre-Trained Models","text":"<p>Inference supports running any of the 50,000+ pre-trained public models hosted on Roboflow Universe, as well as fine-tuned models.</p> <p>We have defined IDs for common models for ease of use. These models do not require an API key for use unlike other public or private models.</p> <p>Using it in <code>inference</code> is as simple as:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\n\nresults = model.infer(\"https://media.roboflow.com/inference/people-walking.jpg\")\n</code></pre> <p>Tip</p> <p>See the Use a fine-tuned model guide for an example on how to deploy your own model.</p>"},{"location":"quickstart/aliases/#supported-pre-trained-models","title":"Supported Pre-Trained Models","text":"<p>You can click the link associated with a model below to test the model in your browser, and use the ID with Inference to deploy the model to the edge.</p> Model Size Task Model ID Test Model in Browser YOLOv8n 640 Object Detection yolov8n-640 Test in Browser YOLOv8n 1280 Object Detection yolov8n-1280 Test in Browser YOLOv8s 640 Object Detection yolov8s-640 Test in Browser YOLOv8s 1280 Object Detection yolov8s-1280 Test in Browser YOLOv8m 640 640 Object Detection yolov8m-640 Test in Browser YOLOv8m 1280 Object Detection yolov8m-1280 Test in Browser YOLOv8l 640 Object Detection yolov8l-640 Test in Browser YOLOv8l 1280 Object Detection yolov8l-1280 Test in Browser YOLOv8x 640 Object Detection yolov8x-640 Test in Browser YOLOv8x 1280 Object Detection yolov8x-1280 Test in Browser YOLO-NAS (small) 640 Object Detection yolo-nas-s-640 Test in Browser YOLO-NAS (medium) 640 Object Detection yolo-nas-m-640 Test in Browser YOLO-NAS (large) 640 Object Detection yolo-nas-l-640 Test in Browser YOLOv8n Instance Segmentation 640 Instance Segmentation yolov8n-seg-640 Test in Browser YOLOv8n Instance Segmentation 1280 Instance Segmentation yolov8n-seg-1280 Test in Browser YOLOv8s Instance Segmentation 640 Instance Segmentation yolov8s-seg-640 Test in Browser YOLOv8m Instance Segmentation 1280 Instance Segmentation yolov8s-seg-1280 Test in Browser YOLOv8m Instance Segmentation 640 Instance Segmentation yolov8m-seg-640 Test in Browser YOLOv8m Instance Segmentation 1280 Instance Segmentation yolov8m-seg-1280 Test in Browser YOLOv8l Instance Segmentation 640 Instance Segmentation yolov8l-seg-640 Test in Browser YOLOv8l Instance Segmentation 1280 Instance Segmentation yolov8l-seg-1280 Test in Browser YOLOv8x Instance Segmentation 640 Instance Segmentation yolov8x-seg-640 Test in Browser YOLOv8x Instance Segmentation 1280 Instance Segmentation yolov8x-seg-1280 Test in Browser YOLOv8x Keypoint Detection 1280 Keypoint Detection yolov8x-pose-1280 Test in Browser YOLOv8x Keypoint Detection 640 Keypoint Detection yolov8x-pose-640 Test in Browser YOLOv8l Keypoint Detection 640 Keypoint Detection yolov8l-pose-640 Test in Browser YOLOv8m Keypoint Detection 640 Keypoint Detection yolov8m-pose-640 Test in Browser YOLOv8s Keypoint Detection 640 Keypoint Detection yolov8s-pose-640 Test in Browser YOLOv8n Keypoint Detection 640 Keypoint Detection yolov8n-pose-640 Test in Browser YOLOv10n 640 Object Detection yolov10n-640 Test in Browser YOLOv10s 640 Object Detection yolov10s-640 Test in Browser YOLOv10m 640 Object Detection yolov10m-640 Test in Browser YOLOv10b 640 Object Detection yolov10b-640 Test in Browser YOLOv10l 640 Object Detection yolov10l-640 Test in Browser YOLOv10x 640 Object Detection yolov10x-640 Test in Browser YOLOv11n 640 Object Detection yolov11n-640 Test in Browser YOLOv11s 640 Object Detection yolov11s-640 Test in Browser YOLOv11m 640 Object Detection yolov11m-640 Test in Browser YOLOv11l 640 Object Detection yolov11l-640 Test in Browser YOLOv11x 640 Object Detection yolov11x-640 Test in Browser YOLOv11n 1280 Object Detection yolov11n-1280 Test in Browser YOLOv11s 1280 Object Detection yolov11s-1280 Test in Browser YOLOv11m 1280 Object Detection yolov11m-1280 Test in Browser YOLOv11l 1280 Object Detection yolov11l-1280 Test in Browser YOLOv11x 1280 Object Detection yolov11x-1280 Test in Browser YOLOv11n 640 Instance Segmentation yolov11n-seg-640 Test in Browser YOLOv11s 640 Instance Segmentation yolov11s-seg-640 Test in Browser YOLOv11m 640 Instance Segmentation yolov11m-seg-640 Test in Browser YOLOv11l 640 Instance Segmentation yolov11l-seg-640 Test in Browser YOLOv11x 640 Instance Segmentation yolov11x-seg-640 Test in Browser"},{"location":"quickstart/compatability_matrix/","title":"Model Compatability","text":"<p>The table below shows on what devices you can deploy models supported by Inference.</p> <p>See our Docker Getting Started guide for more information on how to deploy Inference on your device.</p> <p>Table key:</p> <ul> <li>\u2705 Fully supported</li> <li>\ud83d\udeab Not supported</li> <li>\ud83d\udea7 On roadmap, not currently supported</li> </ul> Model CPU GPU Jetson 4.5.x Jetson 4.6.x Jetson 5.x Roboflow Hosted Inference YOLOv8 Object Detection \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Classification \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Segmentation \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv5 Object Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Segmentation \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CLIP \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 DocTR \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Gaze \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \u2705 SAM \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab ViT Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLACT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"quickstart/configure_api_key/","title":"Roboflow API Key","text":"<p>Throughout these docs you will see references to your Roboflow API key. Using your Roboflow API key grants you access to the models you have trained on Roboflow, public models available on Roboflow Universe, and access to hosted inference API's.</p>"},{"location":"quickstart/configure_api_key/#access-your-roboflow-api-key","title":"Access Your Roboflow API Key","text":"<p>For some examples in the documentation you will need to provide your Roboflow API key. To access your Roboflow API key, you will need to create a free Roboflow account, then follow the docs to retrieve your key.</p>"},{"location":"quickstart/configure_api_key/#use-your-roboflow-api-key","title":"Use Your Roboflow API Key","text":"<p>There are several ways to configure your Roboflow API key when using Inference.</p>"},{"location":"quickstart/configure_api_key/#environment-variable","title":"Environment Variable","text":"<p>The recommended way is to set your Roboflow API key within your environment via the variable <code>ROBOFLOW_API_KEY</code>. In most terminals you can run:</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre> <p>Then, any command you run within that same terminal session will have access to the environment variable <code>ROBOFLOW_API_KEY</code>.</p>"},{"location":"quickstart/configure_api_key/#python","title":"Python","text":"<p>When using Inference within python, your Roboflow API key can be set via keyword arguments</p> <pre><code>from inference.models.utils import get_model\n\nmodel = get_model(model_id=\"...\", api_key=\"YOUR ROBOFLOW API KEY\")\n</code></pre> <p>Hint</p> <p>If you set your API key in your environment, you do not have to pass it as a keyword argument: <code>model = get_model(model_id=\"...\")</code></p>"},{"location":"quickstart/configure_api_key/#http-request-payload","title":"HTTP Request Payload","text":"<p>When using HTTP requests, your Roboflow API key should be passed as a url parameter, or as part of the request payload, depending on the route you are using.</p> <pre><code>import requests\n\nmy_api_key = \"YOUR ROBOFLOW API KEY\"\n\nurl = f\"http://localhost:9001/soccer-players-5fuqs/1?api_key={my_api_key}\"\nresponse = requests.post(url,...)\n\nurl = \"http://localhost:9001/infer/object_detection\"\npayload = {\n  \"api_key\": my_api_key,\n  \"model_id\": \"soccer-players-5fuqs/1\",\n  ...\n}\nresponse = requests.post(url,json=payload)\n</code></pre>"},{"location":"quickstart/configure_api_key/#docker-configuration","title":"Docker Configuration","text":"<p>If you are running the Roboflow Inference Server locally in a docker container, you can provide your Roboflow API key within the <code>docker run</code> command.</p> <pre><code>docker run -it --rm --network=host -e ROBOFLOW_API_KEY=YOUR_ROBOFLOW_API_KEY roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <p>Requests sent to this server can now omit <code>api_key</code> from the request payload.</p>"},{"location":"quickstart/devices/","title":"What Devices Can I Use?","text":"<p>You can deploy Inference on the edge, in your own cloud, or using the Roboflow hosted inference option.</p>"},{"location":"quickstart/devices/#supported-edge-devices","title":"Supported Edge Devices","text":"<p>You can set up a server to use computer vision models with Inference on the following devices:</p> <ul> <li>ARM CPU (macOS, Raspberry Pi)</li> <li>x86 CPU (macOS, Linux, Windows)</li> <li>NVIDIA GPU</li> <li>NVIDIA Jetson (JetPack 4.5.x, JetPack 4.6.x, JetPack 5.x, JetPack 6.x)</li> </ul>"},{"location":"quickstart/devices/#model-compatability","title":"Model Compatability","text":"<p>The table below shows on what devices you can deploy models supported by Inference.</p> <p>See our Docker Getting Started guide for more information on how to deploy Inference on your device.</p> <p>Table key:</p> <ul> <li>\u2705 Fully supported</li> <li>\ud83d\udeab Not supported</li> <li>\ud83d\udea7 On roadmap, not currently supported</li> </ul> Model CPU GPU Jetson 4.5.x Jetson 4.6.x Jetson 5.x Roboflow Hosted Inference YOLOv8 Object Detection \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Classification \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Segmentation \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv5 Object Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Segmentation \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CLIP \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 DocTR \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Gaze \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \u2705 SAM \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab ViT Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLACT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"quickstart/devices/#cloud-platform-support","title":"Cloud Platform Support","text":"<p>You can deploy Inference on any cloud platform such as AWS, GCP, or Azure.</p> <p>The installation and setup instructions are the same as for any edge device, once you have installed the relevant drivers on your cloud platform. We recommend deploying with an official \"Deep Learning\" image from your cloud provider if you are running inference on a GPU device. \"Deep Learning\" images should have the relevant drivers pre-installed so you can set up Inference without configuring GPU drivers manually</p>"},{"location":"quickstart/devices/#use-hosted-inference-from-roboflow","title":"Use Hosted Inference from Roboflow","text":"<p>You can also run your models in the cloud with the Roboflow hosted inference offering. The Roboflow hosted inference solution enables you to deploy your models in the cloud without having to manage your own infrastructure. Roboflow's hosted solution does not support all features available in Inference that you can run on your own infrastructure.</p> <p>To learn more about device compatibility with different models, refer to the model compatibility matrix.</p>"},{"location":"quickstart/docker/","title":"Running With Docker","text":""},{"location":"quickstart/docker/#setup","title":"Setup","text":"<p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If you haven't installed Docker yet, you can get it from Docker's official website.</p>"},{"location":"quickstart/docker/#set-up-a-docker-inference-server-via-inference-server-start","title":"Set up a Docker Inference Server via `inference server start``","text":"<p>Another easy way to run the Roboflow Inference Server with Docker is via the command line.</p> <p>First, Install the CLI.</p> <p>Running the Inference Server is as simple as running the following command:</p> <pre><code>inference server start\n</code></pre> <p>This will pull the appropriate Docker image for your machine and start the Inference Server on port 9001. You can then send requests to the server to get predictions from your model, as described in Quickstart Guide.</p> <p>Once you have your inference server running, you can check its status with the following command:</p> <pre><code>inference server status\n</code></pre> <p>Roboflow Inference CLI currently supports the following device targets:</p> <ul> <li>x86 CPU</li> <li>ARM64 CPU</li> <li>NVIDIA GPU</li> </ul> <p>For Jetson or TensorRT Runtime inference server images, pull the images directly following the instructions below.</p>"},{"location":"quickstart/docker/#manually-set-up-a-docker-container","title":"Manually Set Up a Docker Container","text":""},{"location":"quickstart/docker/#step-1-pull-from-docker-hub","title":"Step #1: Pull from Docker Hub","text":"<p>If you don't wish to build the Docker image locally or prefer to use the official releases, you can directly pull the pre-built images from the Docker Hub. These images are maintained by the Roboflow team and are optimized for various hardware configurations.</p> <p>docker pull</p> x86 CPUarm64 CPUGPU <p>Official Roboflow Inference Server Docker Image for x86 CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for ARM CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-gpu\n</code></pre> <p>=== \"Jetson 4.5.x\" (Deprecated)</p> <pre><code>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.5.x Targets.\n\n```\ndocker pull roboflow/roboflow-inference-server-jetson-4.5.0\n```\n</code></pre> <p>=== \"Jetson 4.6.x\" (Deprecated)</p> <pre><code>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.6.x Targets.\n\n```\ndocker pull roboflow/roboflow-inference-server-jetson-4.6.1\n```\n</code></pre> Jetson 5.xJetson 6.x <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 5.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-5.1.1\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 6.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-6.0.0\n</code></pre>"},{"location":"quickstart/docker/#step-2-run-the-docker-container","title":"Step #2: Run the Docker Container","text":"<p>Once you have a Docker image (either built locally or pulled from Docker Hub), you can run the Roboflow Inference Server in a container.</p> <p>docker run</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <pre><code>docker run -it --net=host \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -p 9001:9001 \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -it --network=host --gpus=all \\\nroboflow/roboflow-inference-server-gpu:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-4.5.0:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-4.6.1:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre> <p>Note: The Jetson images come with TensorRT dependencies. To use TensorRT acceleration with your model, pass an additional environment variable at runtime <code>-e ONNXRUNTIME_EXECUTION_PROVIDERS=TensorrtExecutionProvider</code>. This can improve inference speed, however, this also incurs a costly startup expense when the model is loaded. Note: On Windows and macOS, you may need to use <code>-p 9001:9001</code> instead of <code>--net=host</code> to expose the port to the host machine.</p> <p>You may add the flag <code>-e ROBOFLOW_API_KEY=&lt;YOUR API KEY&gt;</code> to your <code>docker run</code> command so that you do not need to provide a Roboflow API key in your requests. Substitute <code>&lt;YOUR API KEY&gt;</code> with your Roboflow API key. Learn how to retrieve your Roboflow API key here.</p> <p>You may add the flag <code>-v $(pwd)/cache:/tmp/cache</code> to create a cache folder on your home device so that you do not need to redownload or recompile model artifacts upon inference container reboot. You can also (preferably) store artificats in a docker volume named <code>inference-cache</code> by adding the flag <code>-v inference-cache:/tmp/cache</code>.</p>"},{"location":"quickstart/docker/#advanced-build-a-docker-container-from-scratch","title":"Advanced: Build a Docker Container from Scratch","text":"<p>To build a Docker image locally, first clone the Inference Server repository.</p> <pre><code>git clone https://github.com/roboflow/inference\n</code></pre> <p>Choose a Dockerfile from the following options, depending on the hardware you want to run Inference Server on.</p> <p>docker build</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.gpu \\\n-t roboflow/roboflow-inference-server-gpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-jetson-4.5.0 .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-jetson-4.6.1 .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson.5.1.1 \\\n-t roboflow/roboflow-inference-server-jetson-5.1.1 .\n</code></pre>"},{"location":"quickstart/docker_configuration_options/","title":"Docker Configuration Options","text":"<p>Inference servers have a number of configurable parameters which can be set using environment variables. To set an environment variable with the docker run command, use the -e flag with an argument, like this:</p> <pre><code>docker run -it --rm -e ENV_VAR_NAME=env_var_value -p 9001:9001 --gpus all roboflow/roboflow-inference-server-gpu:latest\n</code></pre>"},{"location":"quickstart/docker_configuration_options/#networking","title":"Networking","text":""},{"location":"quickstart/docker_configuration_options/#host","title":"Host","text":"<p>HOST: String (default = 0.0.0.0)</p> <p>Sets the host address used by HTTP interfaces.</p>"},{"location":"quickstart/docker_configuration_options/#inference-server-port","title":"Inference Server Port","text":"<p>PORT: Integer (default = 9001)</p> <p>Sets the port used by HTTP interfaces.</p>"},{"location":"quickstart/docker_configuration_options/#class-agnostic-nms","title":"Class Agnostic NMS","text":"<p>Variable: CLASS_AGNOSTIC_NMS</p> <p>Type: Boolean (default = False)</p> <p>Sets the default non-maximal suppression (NMS) behavior for detection type models (object detection, instance segmentation, etc.).  If True, the default NMS behavior will be class be class agnostic,  meaning overlapping detections from different classes may be removed based on the IoU threshold. If False, only overlapping detections from the same class will be considered for removal by NMS.</p>"},{"location":"quickstart/docker_configuration_options/#allow-origins","title":"Allow Origins","text":"<p>Variable: ALLOW_ORIGINS</p> <p>Type: String (default = \"*\")</p> <p>Sets the allow_origins property on the CORSMiddleware used with FastAPI for HTTP interfaces. Multiple values can be provided separated by a comma (ex. ALLOW_ORIGINS=orig1.com,orig2.com).</p>"},{"location":"quickstart/docker_configuration_options/#clip-model-options","title":"CLIP Model Options","text":""},{"location":"quickstart/docker_configuration_options/#clip-version","title":"CLIP Version","text":"<p>Variable: CLIP_VERSION_ID</p> <p>Type: String (default = ViT-B-16)</p> <p>Sets the OpenAI CLIP version for use by all /clip routes. Available model versions are: RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, BiT-B-32, BiT-L-14-336px, and ViT-L-14.</p>"},{"location":"quickstart/docker_configuration_options/#clip-batch-size","title":"CLIP Batch Size","text":"<p>Variable: CLIP_MAX_BATCH_SIZE</p> <p>Type: Integer (default = 8)</p> <p>Sets the max batch size accepted by the clip model inference functions.</p>"},{"location":"quickstart/docker_configuration_options/#batch-size","title":"Batch Size","text":"<p>FIX_BATCH_SIZE: Boolean (default = False)</p> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p>"},{"location":"quickstart/docker_configuration_options/#license-server","title":"License Server","text":"<p>LICENSE_SERVER: String (default = None)</p> <p>Sets the address of a Roboflow license server.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-active-models","title":"Maximum Active Models","text":"<p>MAX_ACTIVE_MODELS: Integer (default = 8)</p> <p>Sets the maximum number of models the internal model manager will store in memory at one time. By default, the model queue will remove the least recently accessed model when making space for a new model.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-candidates","title":"Maximum Candidates","text":"<p>MAX_CANDIDATES: Integer (default = 3000)</p> <p>The maximum number of candidates for detection.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-detections","title":"Maximum Detections","text":"<p>MAX_DETECTIONS: Integer (default = 300)</p> <p>Sets the maximum number of detections returned by a model.</p>"},{"location":"quickstart/docker_configuration_options/#model-cache-directory","title":"Model Cache Directory","text":"<p>MODEL_CACHE_DIR: String (default = /tmp/cache)</p> <p>Sets the container path for the root model cache directory.</p>"},{"location":"quickstart/docker_configuration_options/#number-of-workers","title":"Number of Workers","text":"<p>NUM_WORKERS: Integer (default = 1)</p> <p>Sets the number of workers used by HTTP interfaces. </p>"},{"location":"quickstart/docker_configuration_options/#tensorrt-cache-directory","title":"TensorRT Cache Directory","text":"<p>TENSORRT_CACHE_PATH: String (default = MODEL_CACHE_DIR)</p> <p>Sets the container path to the TensorRT cache directory. Setting this path in conjunction with mounting a host volume can reduce the cold start time of TensorRT based servers.</p>"},{"location":"quickstart/explore_models/","title":"Explore models","text":"<p>With Inference, you can run private, fine-tuned models that you have trained or uploaded to Roboflow.</p> <p>All models run on your own hardware.</p>"},{"location":"quickstart/explore_models/#run-a-private-fine-tuned-model","title":"Run a Private, Fine-Tuned Model","text":"<p>To run a model, first go to your Roboflow dashboard. Then, choose the model you want to run.</p> <p></p> <p>Click the \"Deploy\" link in the sidebar to find the information you will need to use your model with Inference.</p> <p>Copy the model ID on the page (in this case, <code>taylor-swift-records/3</code>).</p> <p></p> <p>Then, create a new Python file and add the following code:</p> <pre><code>from inference import get_model\nimport supervision as sv\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"taylor-swift-album-1989.jpeg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"taylor-swift-records/3\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)[0]\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results)\n\n# create supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>The <code>taylor-swift-album-1989.jpeg</code> file is hosted here.</p> <p>Replace <code>taylor-swift-records/3</code> with the model ID from your private model and ensure your API key is in your environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>You should see your model's predictions visualized on your screen.</p> <p></p>"},{"location":"quickstart/http_inference/","title":"HTTP Inference","text":"<p>The Roboflow Inference Server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we show how to run inference on object detection, classification, and segmentation models using the Inference Server.</p> <p>Currently, the server is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>To run inference with the server, we will:</p> <ol> <li>Install the server</li> <li>Download a model for use on the server</li> <li>Run inference</li> </ol>"},{"location":"quickstart/http_inference/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Start the server using <code>inference server start</code>. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"quickstart/http_inference/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions.</p> <p>There are two generations of routes in a Roboflow inference server To see what routes are available for a running inference server instance, visit the <code>/docs</code> route in a browser. Roboflow hosted inference endpoints (<code>detect.roboflow.com</code>) only support V1 routes.</p>"},{"location":"quickstart/http_inference/#run-inference-on-a-v2-route","title":"Run Inference on a v2 Route","text":"<p>Run</p> URLBase64 ImageBatch InferenceNumpy Array <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\"\nconfidence = 0.75\niou_thresh = 0.5\napi_key = \"YOUR API KEY\"\ntask = \"object_detection\"\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"url\",\n        \"value\": image_url,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>image_url</code>: The URL of the image you want to run inference on.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": img_str,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Object detection models support batching. Utilize batch inference by passing a list of image objects in a request payload:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        }\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\nimg_str = base64.b64encode(numpy_data)\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": img_str,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre>"},{"location":"quickstart/http_inference/#run-inference-on-a-v1-route","title":"Run Inference on a v1 Route","text":"<p>Run</p> URL <pre><code>The Roboflow hosted API uses the V1 route and requests take a slightly different form:\n\n```python\nimport requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\n\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}&amp;image={image_url}\",\n)\n\npredictions = res.json()\nprint(predictions)\n```\n\nAbove, specify:\n\n1. `project_id`, `model_version`: Your project ID and model version number. &lt;a href=\"https://docs.roboflow.com/api-reference/workspace-and-project-ids\" target=\"_blank\"&gt;Learn how to retrieve your project ID and model version number&lt;/a&gt;.\n2. `confidence`: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.\n3. `api_key`: Your Roboflow API key. &lt;a href=\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key\" target=\"_blank\"&gt;Learn how to retrieve your Roboflow API key&lt;/a&gt;.\n4. `task`: The type of task you want to run. Choose from `object_detection`, `classification`, or `instance_segmentation`.\n5. `filename`: The path to the image you want to run inference on.\n\nThen, run the Python script:\n\n```\npython app.py\n```\n</code></pre> Base64 ImageNumPy ArrayBatch Inference <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}\",\n    data=img_str,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Numpy arrays can be pickled and passed to the inference server for quicker processing. Note, Roboflow hosted APIs to not accept numpy inputs for security reasons:</p> <pre><code>import requests\nimport cv2\nimport pickle\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\napi_key = \"YOUR API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\n\nres = requests.post(\n    f\"http://localhost:9001/{project_id}/{model_version}?api_key={api_key}&amp;image_type=numpy\",\n    data=numpy_data,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Batch inference is not currently supported by V1 routes.</p> <p>The code snippets above will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> or <code>localhost:9001/redoc</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/http_inference/#batching-requests","title":"Batching Requests","text":"<p>Object detection models trained with Roboflow support batching, which allow you to upload multiple images of any type at once:</p> <pre><code>infer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"url\",\n            \"value\": image_url_1,\n        },\n        {\n            \"type\": \"url\",\n            \"value\": image_url_2,\n        },\n        {\n            \"type\": \"url\",\n            \"value\": image_url_3,\n        },\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n</code></pre>"},{"location":"quickstart/inference_101/","title":"How Do I Run Inference?","text":"<p>There are three ways to run Inference:</p> <ul> <li>Using the Python SDK (Images and Videos)</li> <li>Using the Python HTTP SDK (Images)</li> <li>Using the HTTP SDK (Images for all languages)</li> </ul> <p>We document every method in the \"Inputs\" section of the Inference documentation.</p> <p>Below, we talk about when you would want to use each method.</p>"},{"location":"quickstart/inference_101/#using-the-python-sdk-images-and-videos","title":"Using the Python SDK (Images and Videos)","text":"<p>You can use the Python SDK to run models on images and videos directly using the Inference code, without using Docker.</p> <p>Any code example that imports from <code>inference.models</code> uses the model directly.</p> <p>To use the Python SDK, you need to install:</p> <pre><code>pip install inference\n</code></pre>"},{"location":"quickstart/inference_101/#python-http-sdk","title":"Python HTTP SDK","text":"<p>You can use the Python HTTP SDK to run models using Inference with Docker.</p> <p>Any code example that imports from <code>inference_sdk</code> uses the HTTP API.</p> <p>To use this method, you will need an Inference server running, or you can use the Roboflow endpoint for your model.</p>"},{"location":"quickstart/inference_101/#self-hosted-inference-server","title":"Self-Hosted Inference Server","text":"<p>You can set up and install and Inference server using:</p> <pre><code>pip install inference\ninference server start\n</code></pre>"},{"location":"quickstart/inference_101/#roboflow-hosted-api","title":"Roboflow Hosted API","text":"<p>First, run:</p> <pre><code>pip install inference inference-sdk\n</code></pre> <p>Then, use your Roboflow hosted API endpoint to access your model. You can find this in the Deploy tab of your Roboflow model.</p>"},{"location":"quickstart/inference_101/#http-sdk","title":"HTTP SDK","text":"<p>You can deploy your model with Inference and Docker and use the API in any programming language (i.e. Swift, Node.js, and more).</p> <p>To use this method, you will need an Inference server running. You can set up and install and Inference server using:</p>"},{"location":"quickstart/inference_101/#self-hosted-inference-server_1","title":"Self-Hosted Inference Server","text":"<p>You can set up and install and Inference server using:</p> <pre><code>pip install inference\ninference server start\n</code></pre>"},{"location":"quickstart/inference_101/#roboflow-hosted-api_1","title":"Roboflow Hosted API","text":"<p>Use your Roboflow hosted API endpoint to access your model. You can find this in the Deploy tab of your Roboflow model.</p>"},{"location":"quickstart/inference_101/#benefits-of-using-inference-over-http","title":"Benefits of Using Inference Over HTTP","text":"<p>You can run Inference directly from your codebase or using a HTTP microservice deployed with Docker.</p> <p>Running Inference this way can have several advantages:</p> <ul> <li>No Dependency Management: When running Inference within one of Roboflow's published Inference Server containers, all the dependencies are built and isolated so they wont interfere with other dependencies in your code.</li> <li>Microservice Architecture: Running Inference as a separate service allows you to operate and maintain your computer vision models separate from other logic within your software, including scaling up and down to meet dynamic loads.</li> <li>Roboflow Hosted API: Roboflow hosts a powerful and infinitely scalable version of the Roboflow Inference Server. This makes it even easier to integrate computer vision models into your software without adding any maintenance burden. And, since the Roboflow hosted APIs are running using the Inference package, it's easy to switch between using a hosted server and an on prem server without having to reinvent your client code.</li> <li>Non-Python Clients: Running Inference within an HTTP server allows you to interact with it from any language you prefer.</li> </ul>"},{"location":"quickstart/inference_101/#advanced-usage-interfaces","title":"Advanced Usage &amp; Interfaces","text":"<p>There are several advanced interfaces that enhance the capabilities of the base Inference package.</p> <ul> <li>Active Learning: Active learning helps improve your model over time by contributing real world data back to your Roboflow dataset in real time.</li> <li>Parallel HTTP API: A highly parallel server capable of accepting requests from many different clients and batching them dynamically in real time to make the most efficient use of the host hardware. Docs and Examples</li> <li>Stream Manager API: An API for starting, stopping, and managing Inference Pipeline instances. This interfaces combines the advantages of running Inference realtime on a stream while also fitting nicely into a microservice architecture. Docs and Examples</li> </ul> <p>To learn more, contact the Roboflow sales team.</p>"},{"location":"quickstart/inference_gpu_windows/","title":"Install the <code>inference-gpu</code> Python Package and NVIDIA CUDA on Windows","text":"<p>Warning</p> <p>We strongly recommend installing Inference with Docker on Windows. The guide below should only be used if you are unable to use Docker on your system.</p> <p>You can use Inference with <code>inference-gpu</code> and NVIDIA CUDA on Windows devices.</p> <p>This guide walks through how to configure your Windows GPU setup.</p>"},{"location":"quickstart/inference_gpu_windows/#prerequisites","title":"Prerequisites","text":"<p>To follow this guide, you must have a Windows machine that runs either Windows 10 or Windows 11. Your machine must have an NVIDIA GPU.</p>"},{"location":"quickstart/inference_gpu_windows/#step-1-install-python","title":"Step #1: Install Python","text":"<p>Download and the latest Python 3.11.x from the Python Windows version list.</p> <p>Do not install the Python version from the windows store as it is not compatible with onnxruntime.</p> <p>Click the \"Windows Installer (64 Bit)\" link and follow instructions to install python on the machine.</p> <p>When the installation is finished, type <code>py --version</code> to ensure your Python installation was successful.</p> <p>You should see a message showing your Python version.</p>"},{"location":"quickstart/inference_gpu_windows/#step-2-install-inference-gpu","title":"Step #2: Install Inference GPU","text":"<p>In a powershell terminal, run:</p> <pre><code>py -m pip install inference-gpu\n</code></pre>"},{"location":"quickstart/inference_gpu_windows/#step-3-install-cuda-toolkit-118","title":"Step #3: Install CUDA Toolkit 11.8","text":"<p>Next, we need to install CUDA Toolkit 11.8. This software will allow Inference to use CUDA.</p> <p>Download CUDA Toolkit.</p> <p>From the download page, choose the correct parameters for your system, and then choose \"exe (Network)\" and follow the link to download the toolkit.</p> <p></p> <p>Open the installation file and accept all defaults in order to install the toolkit.</p>"},{"location":"quickstart/inference_gpu_windows/#step-3-install-cudnn","title":"Step #3: Install cuDNN","text":"<p>Next, we need to install cuDNN.</p> <p>Navigate to the cuDNN installation page on the NVIDIA website.</p> <p>Select \"cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\"</p> <p>Choose the link for 11.x, as others will not be compatible with your CUDA version and will fail.</p> <p>Choose \"Local Installer for Windows (Zip)\" and continue to download the ZIP file. You will need to sign up for an NVIDIA account in order to download the software.</p> <p>Open the file in downloads folder, right click and choose \"Extract All\" to extract all files to the download folder.</p> <p>Type ctrl+n to open a new Explorer window, and in this window navigate to <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8</code>.</p> <p>Copy all .dll files from the bin/ folder of the cuDNN download into the bin/ folder of the CUDA toolkit:</p> <p></p> <p>Copy all .h files from the include/ folder of the cuDNN download into the include/ folder of the CUDA toolkit:</p> <p></p> <p>Copy the x64 folder from the lib/ directory of the cuDNN download into the lib/ directory of the CUDA installation:</p> <p></p> <p>Right click the start menu and choose System \u2192 Advanced system settings \u2192 Environment Variables.</p> <p>Create a new environment variable called CUDNN with the value:</p> <pre><code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\lib;\n</code></pre>"},{"location":"quickstart/inference_gpu_windows/#step-4-install-zlib","title":"Step #4: Install zlib.","text":"<p>Find the file C:\\Program Files\\NVIDIA Corporation\\Nsight Systems 2022.4.2\\host-windows-x64\\zlib.dll Right click and choose \"Copy\".</p> <p>Now navigate to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin in the finder.</p> <p>Paste the zlib.dll file into this folder and rename to zlibwapi.dll.</p>"},{"location":"quickstart/inference_gpu_windows/#step-5-install-visual-studio-2019-c-runtime","title":"Step #5: Install Visual Studio 2019 C++ Runtime","text":"<p>Finally, install the Visual Studio 2019 C++ runtime (download link).</p> <p>Create a new file with the following contents:</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init(\n    api_key=\"REPLACE API KEY\", ## &lt;- update API KEY\n    model_id=\"rock-paper-scissors-sxsw/11\",\n    video_reference='https://media.roboflow.com/rock-paper-scissors.mp4',\n    on_prediction=render_boxes,\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>Using a text editor (we recommend Visual Studio Code), add your Roboflow API key in the string on line 5.</p> <p>Open a PowerShell terminal in the location of the file, and type py infer.py. If the installation is successful, you should see a few frames of annotated images displayed, with no errors or warnings in the console.</p>"},{"location":"quickstart/inference_notebook/","title":"Inference notebook","text":"<p>Roboflow Inference Servers come equipped with a built in Jupyterlab environment. This environment is the fastest way to get up and running with inference for development and testing. To use it, first start an inference server.</p> <p>The easiest way to start an inference server is with the inference CLI. Install it via pip:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Now run the <code>inference sever start</code> command. Be sure to specify the <code>--dev</code> flag so that the notebook environment is enabled (it is disabled by default).</p> <pre><code>inference server start --dev\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. This page contains links to resources and examples related to <code>inference</code>. It also contains a link to the built in Jupyterlab environment.</p> <p>From the landing page, select the button labeled \"Jump Into an Inference Enabled Notebook\" to open a new tab for the Jupyterlab environment. </p> <p>This Jupyterlab environment comes preloaded with several example notebooks and all of the dependencies needed to run <code>inference</code>.</p>"},{"location":"quickstart/licensing/","title":"Licensing","text":""},{"location":"quickstart/licensing/#inference-source-code-license","title":"Inference Source Code License","text":"<p>See Roboflow Licensing for information on how Inference and models supported in Inference are licensed.</p>"},{"location":"quickstart/load_from_universe/","title":"Universe Models","text":"<p>With Inference, you can run any of the 50,000+ models available on Roboflow Universe.</p> <p>All models run on your own hardware.</p>"},{"location":"quickstart/load_from_universe/#run-a-model-from-roboflow-universe","title":"Run a Model From Roboflow Universe","text":"<p>In the first example, we showed how to run a people detection model. This model was hosted on Universe. Let's find another model to try.</p> <p>Go to the Roboflow Universe homepage and use the search bar to find a model.</p> <p></p> <p>Info</p> <p>Add \"model\" to your search query to only find models.</p> <p>Browse the search page to find a model.</p> <p></p> <p>When you have found a model, click on the model card to learn more. Click the \"Model\" link in the sidebar to get the information you need to use the model.</p> <p>Then, install Inference and supervision, which we will use to run our model and handle model predictions, respectively:</p> <pre><code>pip install inference supervision\n</code></pre> <p>Next, create a new Python file and add the following code:</p> <pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n# import supervision to visualize our results\nimport supervision as sv\n# import cv2 to helo load our image\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"people-walking.jpg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results[0].dict(by_alias=True, exclude_none=True))\n\n# create supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>Tip</p> <p>To see more models, check out the Pre-Trained Models page and Roboflow Universe.</p> <p>The <code>people-walking.jpg</code> file is hosted here.</p> <p>Replace <code>yolov8n-640</code> with the model ID you found on Universe, replace <code>image</code> with the image of your choosing, and be sure to export your API key:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>You should see your model's predictions visualized on your screen.</p> <p></p>"},{"location":"quickstart/roboflow_ecosystem/","title":"Introduction to Roboflow Ecosystem","text":"<p>Roboflow provides everything you need to label, train, and deploy computer vision solutions. It helps you manage and refine datasets, provides tools to streamline and speed up data labelling, helps train and deploy models in the cloud and on edge devices.</p> <p>Inference is what allows you to deploy and run computer vision models. It enables you to perform object detection, classification, instance segmentation and keypoint detection, and utilize foundation models like CLIP, Segment Anything, and YOLO-World, through a Python-native package, a self-hosted inference server, or a fully managed API.</p> <p>Roboflow offers both a free tier, and paid plans, encompassing all of its products.</p> <ul> <li>Over half of Fortune 100 companies build with Roboflow. If you're an enterprise customer and interested in a custom solution, parallel processing, active learning or licenses for more than one cloud instance or edge device - Reach Out to our sales team!</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#related-products","title":"Related Products","text":"<p>Inference is commonly used with several other roboflow products.</p>"},{"location":"quickstart/roboflow_ecosystem/#roboflow-app","title":"Roboflow App","text":"<p>Roboflow App This is your central dashboard. Here you can upload data, annotate images, define datasets, train and deploy models. You can find the API key (scoped to workspace)</p> <ul> <li>Roboflow App</li> <li>Docs: Getting Started with Roboflow</li> <li>App: API key</li> <li>Docs: How to Retrieve the API key</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#roboflow-package","title":"<code>roboflow</code> Package","text":"<p>Your workspace can be managed via the dashboard UI. If you'd like to do it via Python, install the <code>roboflow</code> package. This lets you manage your workspace, upload datasets and model weights, and even run model inference (a bit outdated).</p> <p>However, If all you need is to run a deployed model, you likely won't need <code>roboflow</code> at all. Where possible, we recommend <code>inference</code>. That's what we use on our servers!</p> <p>If you wish to use the <code>roboflow</code> package, instructions can be found in Roboflow Python Package Docs.</p>"},{"location":"quickstart/roboflow_ecosystem/#universe","title":"Universe","text":"<p>Universe is our space for sharing datasets and models.</p> <p>Search for models, test out their performance on your images, track model versions, access in inference, build on top.</p> <p>The <code>model_id</code> you pass into inference can be a <code>model_id</code> from Universe.</p> <ul> <li>Run Model from Roboflow Universe</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#supervision","title":"Supervision","text":"<p>What happens when you infer some results from a model? <code>supervision</code> lets you plot bounding boxes and segmentation masks, track objects, merge various detections. With supervision tools such as <code>InferenceSlicer</code> you can detect small objects in an image by running the model on smaller patches.</p> <p>It also encodes model results from various sources - <code>inference</code>, Hugging Face, Ultralytics and more, into a common format. https://supervision.roboflow.com/latest/</p> <ul> <li>Get started with Supervision</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#workflows","title":"Workflows","text":"<p>Workflows are an new <code>inference</code> feature. Instead of writing code, you may chain together blocks to build your computer vision algorithms from scratch. There's an expanding library of blocks available - see if you find anything you like!</p>"},{"location":"quickstart/run_a_model/","title":"Run a Model","text":"<p>Let's run a computer vision model with Inference. The quickest way to get started with Inference is to simply load a model, and then call the model's <code>infer(...)</code> method.</p>"},{"location":"quickstart/run_a_model/#install-inference","title":"Install Inference","text":"<p>First, we need to install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>To help us visualize our results in the example below, we will install Supervision:</p> <pre><code>pip install supervision\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p>"},{"location":"quickstart/run_a_model/#load-a-model-and-run-inference","title":"Load a Model and Run Inference","text":"<pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n\n# define the image url to use for inference\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n</code></pre> <p>In the code above, we loaded a model and then we used that model's <code>infer(...)</code> method to run an image through it.</p> <p>Tip</p> <p>When you run inference on an image, the same augmentations you applied when you generated a version in Roboflow will be applied at inference time. This helps improve model performance.</p>"},{"location":"quickstart/run_a_model/#visualize-results","title":"Visualize Results","text":"<p>Running inference is fun but it's not much to look at. Let's add some code to visualize our results.</p> <pre><code>from io import BytesIO\n\nimport requests\nimport supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom PIL.ImageFile import ImageFile\n\n\ndef load_image_from_url(url: str) -&gt; ImageFile:\n    response = requests.get(url)\n    response.raise_for_status()  # check if the request was successful\n    image = Image.open(BytesIO(response.content))\n    return image\n\n\n# load the image from an url\nimage = load_image_from_url(\"https://media.roboflow.com/inference/people-walking.jpg\")\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)[0]\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results)\n\n# create supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p></p>"},{"location":"quickstart/run_a_model/#summary","title":"Summary","text":"<p>Huzzah! We used Inference to load a computer vision model, run inference on an image, then visualize the results! But this is just the start. There are many different ways to use Inference and how you use it is likely to depend on your specific use case and deployment environment. Learn more about how to use inference here.</p>"},{"location":"quickstart/run_keypoint_detection/","title":"Keypoint Detection","text":"<p>Running a keypoint detection model on Roboflow is very similar to segmentation or detection.</p> <p>You may run it locally, hosted on our inference servers, or using a docker container.</p> \ud83d\udca1 model weights  In all cases, model weights need to be downloaded from Roboflow's servers first.  If you have the weights locally, you may upload the weights to our servers using the [From Local Weights](https://inference.roboflow.com/models/from_local_weights/) guide.  For offline usage, run inference with the Python API once. The weights will be downloaded and cached in the format our inference runtime can parse."},{"location":"quickstart/run_keypoint_detection/#quickstart","title":"Quickstart","text":"<p>Install dependencies:</p> <pre><code>pip install inference\n</code></pre> <p>Set up the API key:</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre> <p>Run with Python API:</p> <pre><code>from inference import get_model\n\n\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nmodel = get_model(model_id=\"yolov8x-pose-640\")\nresults = model.infer(image)[0]\n</code></pre>"},{"location":"quickstart/run_keypoint_detection/#details","title":"Details","text":""},{"location":"quickstart/run_keypoint_detection/#inference-setup","title":"Inference Setup","text":"<p>In all cases, you'll need the <code>inference</code> package.</p> <pre><code>pip install inference\n</code></pre> <p>By default, it runs on the CPU. Instead, you may install the GPU module with the following command:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"quickstart/run_keypoint_detection/#api-keys","title":"API Keys","text":"<p>You'll need the API key to access the fine-tuned models or models on the Roboflow Universe. A guide can be found in Retrieve Your API Key.</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre>"},{"location":"quickstart/run_keypoint_detection/#available-pretrained-models","title":"Available Pretrained Models","text":"<p>You may use keypoint detection models available on the Universe. Alternatively, here's a few <code>model_ids</code> that we support out-of-the-box:</p> <ul> <li><code>yolov8x-pose-1280</code> (largest)</li> <li><code>yolov8x-pose-640</code></li> <li><code>yolov8l-pose-640</code></li> <li><code>yolov8m-pose-640</code></li> <li><code>yolov8s-pose-640</code></li> <li><code>yolov8n-pose-640</code> (smallest)</li> </ul> <p>Run</p> Python API - ImageInference Pipeline - StreamHostedDocker server <p>Run the model locally, without needing to set up a docker container. This pulls the model from roboflow servers and runs it on your machine. It can take both images and videos as input.</p> <p>Run:</p> <pre><code>from inference import get_model\n\n\n# This can be a URL, a np.ndarray or a PIL image.\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nmodel = get_model(model_id=\"yolov8x-pose-640\")\nresults = model.infer(image)[0]\n</code></pre> <p>Inference Pipeline allows running inference on videos, webcams and RTSP streams. You may define a custom sink to extract pose results.</p> <p>More details can be found on Predict on a Video, Webcam or RTSP Stream</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    print(predictions)\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-pose-640\", # Roboflow model to use\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=my_custom_sink, # Function to run after each prediction\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>Send an image to our servers and get the detected keypoint response. Only images are supported (URL, <code>np.ndarray</code>, <code>PIL</code>).</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\n\n# This can be a URL, a np.ndarray or a PIL image.\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\nresults = client.infer(image, model_id=\"yolov8x-pose-640\")\n</code></pre> <p>With this method, you may self-host a server container, similar to Hosted model API. Only images are supported (URL, <code>np.ndarray</code>, <code>PIL</code>).</p> <p>Note that the model weights still need to be retrieved from our servers at least once. Check out From Local Weights for instructions on how to upload yours.</p> <p>Start the inference server:</p> <pre><code>inference server start\n</code></pre> <p>Run: <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\n\n# This can be a URL, a np.ndarray or a PIL image.\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\nresults = client.infer(image, model_id=\"yolov8x-pose-640\")\n</code></pre></p>"},{"location":"quickstart/run_keypoint_detection/#visualize","title":"Visualize","text":"<p>With supervision you may visualize the results, carry out post-processing. Supervision library standardizes results from various keypoint detection and pose estimation models into a consistent format, using adaptors such as <code>from_inference</code>.</p> <p>Example usage:</p> <pre><code>import os\nimport cv2\nfrom inference import get_model\nimport supervision as sv\n\n\n# Model accepts URLs, np.arrays (cv2.imread), and PIL images.\n# Annotators accept np.arrays (cv2.imread), and PIL images\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nmodel = get_model(model_id=\"yolov8x-pose-640\")\nresults = model.infer(image)[0]\n\n# Any results object would work, regardless of which inference API is used\nkeypoints = sv.KeyPoints.from_inference(results)\n\n# Convert to numpy image\nimg_name = \"people-walking.jpg\"\nif not os.path.exists(img_name):\n    os.system(f\"wget -O {img_name} {image}\")\nimage_np = cv2.imread(img_name)\n\nannotated_image = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n).annotate(image, keypoints)\n</code></pre>"},{"location":"quickstart/run_model_on_image/","title":"Predict on an Image Over HTTP","text":"<p>A Roboflow Inference server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we show how to run inference on object detection, classification, and segmentation models using Inference.</p> <p>Note</p> <p>Inference is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>You can run inference on images from:</p> <ol> <li>URLs, which will be downloaded from the internet</li> <li>File names, which will be read from disk</li> <li>PIL images</li> <li>NumPy arrays</li> </ol>"},{"location":"quickstart/run_model_on_image/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Start the server using <code>inference server start</code>. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"quickstart/run_model_on_image/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions.</p> <p>There are two generations of routes in a Roboflow inference server To see what routes are available for a running inference server instance, visit the <code>/docs</code> route in a browser. Roboflow hosted inference endpoints (<code>detect.roboflow.com</code>) only support V1 routes.</p>"},{"location":"quickstart/run_model_on_image/#run-inference-on-a-v2-route","title":"Run Inference on a v2 Route","text":"<p>Run</p> URLPIL ImageNumPy ArrayBatch Inference <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference_sdk\nfrom inference_sdk import InferenceHTTPClient,\n# import os to get the API_KEY from the environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\n# create a client object\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# run inference on the image\nresults = client.infer(image_url, model_id=f\"{project_id}/{model_version}\")\n\n# print the results\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>image_url</code>: The URL of the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference sdk\nfrom inference_sdk import InferenceHTTPClient\n# import PIL for loading image\nfrom PIL import Image\n# import os for getting api key from environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nfilename = \"path/to/local/image.jpg\"\n\n# create a client object\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# load the image\npil_image = Image.open(filename)\n\n# run inference\nresults = client.infer(pil_image, model_id=f\"{project_id}/{model_version}\")\n\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference-sdk\nfrom inference_sdk import InferenceHTTPClient\n# import opencv for image loading\nimport cv2\n# import os to read api key from environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nfilename = \"path/to/local/image.jpg\"\n\n# create client\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# load image with opencv\nnumpy_image = cv2.imread(filename)\n\n# run inference\nresults = client.infer(numpy_image, model_id=f\"{project_id}/{model_version}\")\n\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Object detection models support batching. Utilize batch inference by passing a list of image objects in a request payload:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        }\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>See more docs on the inference-sdk</p>"},{"location":"quickstart/run_model_on_image/#run-inference-on-a-v1-route","title":"Run Inference on a v1 Route","text":"<p>Run</p> URLBase64 ImageNumPy ArrayBatch Inference <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\"\n\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}&amp;image={image_url}\",\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}\",\n    data=img_str,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Numpy arrays can be pickled and passed to the inference server for quicker processing. Note, Roboflow hosted APIs to not accept numpy inputs for security reasons:</p> <pre><code>import requests\nimport cv2\nimport pickle\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\napi_key = \"YOUR API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\n\nres = requests.post(\n    f\"http://localhost:9001/{project_id}/{model_version}?api_key={api_key}&amp;image_type=numpy\",\n    data=numpy_data,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>instance_segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Batch inference is not currently supported by V1 routes.</p> <p>The code snippets above will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> or <code>localhost:9001/redoc</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/","title":"Predict on a Video, Webcam or RTSP Stream","text":"<p>You can run computer vision models on webcam stream frames, RTSP stream frames, and video frames with Inference.</p> <p>Follow our Run a Fine-Tuned Model on Images guide to learn how to find a model to run.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#installation","title":"Installation","text":"<p>To use fine-tuned models with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Learn more about using Roboflow API keys in Inference</p> <p>Then, install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"quickstart/run_model_on_rtsp_webcam/#inference-on-video","title":"Inference on Video","text":"<p>Next, create an Inference Pipeline. Once you have selected a model to run, create a new Python file and add the following code:</p> <pre><code># Import the InferencePipeline object\nfrom inference import InferencePipeline\n# Import the built in render_boxes sink for visualizing results\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\n# initialize a pipeline object\npipeline = InferencePipeline.init(\n    model_id=\"rock-paper-scissors-sxsw/11\", # Roboflow model to use\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=render_boxes, # Function to run after each prediction\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>This code will run a model on frames from a webcam stream. To use RTSP, set the <code>video_reference</code> value to an RTSP stream URL. To use video, set the <code>video_reference</code> value to a video file path.</p> <p>Predictions are annotated using the <code>render_boxes</code> helper function. You can specify any function to process each prediction in the <code>on_prediction</code> parameter.</p> <p>Replace <code>rock-paper-scissors-sxsw/11</code> with the model ID associated with the model you want to run.</p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Your webcam will open and you can see the model running:</p> <p>Tip</p> <p>When you run inference on an image, the same augmentations you applied when you generated a version in Roboflow will be applied at inference time. This helps improve model performance.</p> <p>Presto! We used an InferencePipeline to run inference on our webcam and learned how we could modify it to run on other video sources (like video files or RTSP streams). See the Inference Pipeline docs to learn more about other configurable parameters and built in sinks.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#define-custom-prediction-logic","title":"Define Custom Prediction Logic","text":"<p>In Inference a sink is a function used to execute logic on the inference results within an <code>InferencePipeline</code>. Inference has some built in sinks for convenience. We used one above to plot bounding boxes.</p> <p>Below, we describe how to define custom prediction logic.</p> <p>The <code>on_prediction</code> parameter in the <code>InferencePipeline</code> constructor allows you to define custom prediction handlers. You can use this to define custom logic for how predictions are processed.</p> <p>This function provides two parameters:</p> <ul> <li><code>predictions</code>: A dictionary that contains all predictions returned by the model for the frame, and;</li> <li><code>video_frame</code>: A dataclass</li> </ul> <p>A VideoFrame object contains:</p> <ul> <li><code>image</code>: The video frame as a NumPy array,</li> <li><code>frame_id</code>: The frame ID, and;</li> <li><code>frame_timestamp</code>: The timestamp of the frame.</li> <li><code>source_id</code>: The index of the video_reference element which was passed to InferencePipeline (useful when multiple streams are passed to InferencePipeline).</li> </ul> <p>Let's start by just printing the frame ID to the console.</p> <pre><code>from inference import InferencePipeline\n# import VideoFrame for type hinting\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\n# define sink function\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    # print the frame ID of the video_frame object\n    print(f\"Frame ID: {video_frame.frame_id}\")\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\",\n    on_prediction=my_custom_sink,\n)\n\npipeline.start()\npipeline.join()\n</code></pre> <p>The output should look something like:</p> <pre><code>Frame ID: 1\nFrame ID: 2\nFrame ID: 3\n</code></pre> <p>Now let's do something a little more useful and use our custom sink to visualize our predictions.</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\n# import opencv to display our annotated images\nimport cv2\n# import supervision to help visualize our predictions\nimport supervision as sv\n\n# create a bounding box annotator and label annotator to use in our custom sink\nlabel_annotator = sv.LabelAnnotator()\nbox_annotator = sv.BoxAnnotator()\n\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    # get the text labels for each prediction\n    labels = [p[\"class\"] for p in predictions[\"predictions\"]]\n    # load our predictions into the Supervision Detections api\n    detections = sv.Detections.from_inference(predictions)\n    # annotate the frame using our supervision annotator, the video_frame, the predictions (as supervision Detections), and the prediction labels\n    image = label_annotator.annotate(\n        scene=video_frame.image.copy(), detections=detections, labels=labels\n    )\n    image = box_annotator.annotate(image, detections=detections)\n    # display the annotated image\n    cv2.imshow(\"Predictions\", image)\n    cv2.waitKey(1)\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\",\n    on_prediction=my_custom_sink,\n)\n\npipeline.start()\npipeline.join()\n</code></pre> <p>You should see something like this on your screen:</p> <p>And there you have it! We created a custom sink that takes the outputs of our Inference Pipeline, annotates an image, and displays it to our screen. See the Inference Pipeline docs to learn more about other configurable parameters and built in sinks.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#existing-video-sinks","title":"Existing Video Sinks","text":""},{"location":"quickstart/run_model_on_rtsp_webcam/#built-in-sinks","title":"Built In Sinks","text":"<p>Inference has several sinks built in that are ready to use.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#render_boxes","title":"<code>render_boxes(...)</code>","text":"<p>The render boxes sink is made to visualize predictions and overlay them on a stream. It uses Supervision annotators to render the predictions and display the annotated frame. It only works for Roboflow models that yields detection-based output (<code>object-detection</code>, <code>instance-segmentation</code>, <code>keypoint-detection</code>), yet not all details of predictions may be  displayed by default (like detected key-points).</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#udpsink","title":"<code>UDPSink(...)</code>","text":"<p>The UDP sink is made to broadcast predictions with a UDP port. This port can be listened to by client code for further processing. It uses Python-default json serialisation - so predictions must be serializable, otherwise error will be thrown.  </p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#multi_sink","title":"<code>multi_sink(...)</code>","text":"<p>The Multi-Sink is a way to combine multiple sinks so that multiple actions can happen on a single inference result.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#videofilesink","title":"<code>VideoFileSink(...)</code>","text":"<p>The Video File Sink visualizes predictions, similar to the <code>render_boxes(...)</code> sink, however, instead of displaying the annotated frames, it saves them to a video file. All constraints related to <code>render_boxes(...)</code> apply.</p>"},{"location":"quickstart/run_model_over_udp/","title":"Run model over udp","text":"<p>You can run Inference directly on frames using UDP.</p> <p>This is ideal for real-time use cases where reducing latency is essential (i.e. sports broadcasting).</p> <p>This feature only works on devices with a CUDA-enabled GPU.</p> <p>Inference has been used at sports broadcasting events around the world for real-time object detection.</p> <p>Follow our Run a Fine-Tuned Model on Images guide to learn how to find a model to run.</p>"},{"location":"quickstart/run_model_over_udp/#run-a-vision-model-on-a-udp-stream","title":"Run a Vision Model on a UDP Stream","text":"<p>To run inference on frames from a UDP stream, you will need to:</p> <ol> <li>Set up a listening server to receive predictions from Inference, and;</li> <li>Run Inference, connected directly to your stream.</li> </ol>"},{"location":"quickstart/run_model_over_udp/#authenticate-with-roboflow","title":"Authenticate with Roboflow","text":"<p>To use Inference with a UDP stream, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre>"},{"location":"quickstart/run_model_over_udp/#configure-a-listening-server","title":"Configure a Listening Server","text":"<p>You need a server to receive predictions from Inference. This server is where you can write custom logic to process predictions.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import socket\nimport json\nimport time\n\n\nHOST = \"localhost\"\nPORT = 8000\n\nfps_array = []\n\n# Create a datagram (UDP) socket\nUDPClientSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n\n# Bind to the given IP address and port\nUDPClientSocket.bind((HOST, PORT))\n\nprint(f\"UDP server up and listening on http://{HOST}:{PORT}\")\n\n# Listen for incoming datagrams\nwhile True:\n    t0 = time.time()\n\n    bytesAddressPair = UDPClientSocket.recvfrom(1024)\n    message = bytesAddressPair[0]\n    address = bytesAddressPair[1]\n\n    clientMsg = json.loads(message)\n    clientIP = \"Client IP Address:{}\".format(address)\n\n    print(clientMsg)\n    print(clientIP)\n\n    t = time.time() - t0\n    fps_array.append(1 / t)\n    fps_array[-150:]\n    fps_average = sum(fps_array) / len(fps_array)\n    print(\"AVERAGE FPS: \" + str(fps_average))\n</code></pre> <p>Above, replace <code>port</code> with the port on which you want to run your server.</p>"},{"location":"quickstart/run_model_over_udp/#run-a-broadcasting-server","title":"Run a Broadcasting Server","text":"<ul> <li>set up socket</li> <li>render will broadcast</li> <li>https://hub.docker.com/repository/docker/roboflow/roboflow-inference-server-udp-gpu/general</li> </ul>"},{"location":"reference/inference/","title":"Index","text":"<ul> <li>inference<ul> <li>core<ul> <li>active_learning<ul> <li>accounting</li> <li>configuration</li> </ul> </li> <li>cache<ul> <li>base</li> <li>memory</li> <li>model_artifacts</li> <li>redis</li> </ul> </li> <li>devices<ul> <li>utils</li> </ul> </li> <li>entities<ul> <li>requests<ul> <li>clip</li> <li>doctr</li> <li>dynamic_class_base</li> <li>easy_ocr</li> <li>gaze</li> <li>groundingdino</li> <li>inference</li> <li>moondream2</li> <li>owlv2</li> <li>perception_encoder</li> <li>sam</li> <li>sam2</li> <li>server_state</li> <li>trocr</li> <li>yolo_world</li> </ul> </li> <li>responses<ul> <li>clip</li> <li>gaze</li> <li>inference</li> <li>notebooks</li> <li>ocr</li> <li>perception_encoder</li> <li>sam</li> <li>sam2</li> <li>server_state</li> </ul> </li> </ul> </li> <li>exceptions</li> <li>interfaces<ul> <li>base</li> <li>camera<ul> <li>camera</li> <li>entities</li> <li>utils</li> <li>video_source</li> </ul> </li> <li>http<ul> <li>builder<ul> <li>routes</li> </ul> </li> <li>error_handlers</li> <li>http_api</li> <li>middlewares<ul> <li>cors</li> </ul> </li> </ul> </li> <li>stream<ul> <li>sinks</li> <li>stream</li> <li>watchdog</li> </ul> </li> <li>udp<ul> <li>udp_stream</li> </ul> </li> </ul> </li> <li>logging<ul> <li>memory_handler</li> </ul> </li> <li>managers<ul> <li>base</li> <li>decorators<ul> <li>base</li> <li>locked_load</li> <li>logger</li> </ul> </li> <li>metrics</li> <li>pingback</li> <li>prometheus</li> </ul> </li> <li>models<ul> <li>base</li> <li>classification_base</li> <li>instance_segmentation_base</li> <li>keypoints_detection_base</li> <li>object_detection_base</li> <li>roboflow</li> <li>utils<ul> <li>keypoints</li> </ul> </li> </ul> </li> <li>nms</li> <li>registries<ul> <li>base</li> <li>roboflow</li> </ul> </li> <li>roboflow_api</li> <li>usage</li> <li>utils<ul> <li>container</li> <li>environment</li> <li>file_system</li> <li>image_utils</li> <li>onnx</li> <li>postprocess</li> <li>preprocess</li> </ul> </li> <li>workflows<ul> <li>core_steps<ul> <li>classical_cv<ul> <li>camera_focus<ul> <li>v1</li> </ul> </li> <li>contours<ul> <li>v1</li> </ul> </li> <li>distance_measurement<ul> <li>v1</li> </ul> </li> <li>image_blur<ul> <li>v1</li> </ul> </li> <li>pixel_color_count<ul> <li>v1</li> </ul> </li> <li>sift<ul> <li>v1</li> </ul> </li> <li>sift_comparison<ul> <li>v2</li> </ul> </li> <li>size_measurement<ul> <li>v1</li> </ul> </li> <li>threshold<ul> <li>v1</li> </ul> </li> </ul> </li> <li>common<ul> <li>utils</li> </ul> </li> <li>fusion<ul> <li>detections_stitch<ul> <li>v1</li> </ul> </li> </ul> </li> <li>models<ul> <li>foundation<ul> <li>gaze<ul> <li>v1</li> </ul> </li> <li>openai<ul> <li>v3</li> </ul> </li> <li>stability_ai<ul> <li>inpainting<ul> <li>v1</li> </ul> </li> <li>outpainting<ul> <li>v1</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>sinks<ul> <li>roboflow<ul> <li>dataset_upload<ul> <li>v1</li> </ul> </li> </ul> </li> </ul> </li> <li>transformations<ul> <li>detections_merge<ul> <li>v1</li> </ul> </li> <li>image_slicer<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>qr_code_generator<ul> <li>v1</li> </ul> </li> <li>stitch_ocr_detections<ul> <li>v1</li> </ul> </li> </ul> </li> <li>visualizations<ul> <li>classification_label<ul> <li>v1</li> </ul> </li> <li>common<ul> <li>annotators<ul> <li>background_color</li> <li>halo</li> <li>model_comparison</li> <li>polygon</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>execution_engine<ul> <li>v1<ul> <li>compiler<ul> <li>cache</li> <li>graph_traversal</li> </ul> </li> <li>dynamic_blocks<ul> <li>block_assembler</li> <li>modal_executor</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>enterprise<ul> <li>parallel<ul> <li>dispatch_manager</li> <li>infer</li> <li>utils</li> </ul> </li> <li>workflows<ul> <li>enterprise_blocks<ul> <li>sinks<ul> <li>PLC_modbus<ul> <li>v1</li> </ul> </li> <li>PLCethernetIP<ul> <li>v1</li> </ul> </li> <li>microsoft_sql_server<ul> <li>v1</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>models<ul> <li>clip<ul> <li>clip_model</li> </ul> </li> <li>doctr<ul> <li>doctr_model</li> </ul> </li> <li>easy_ocr<ul> <li>easy_ocr</li> </ul> </li> <li>florence2<ul> <li>utils</li> </ul> </li> <li>gaze<ul> <li>gaze</li> <li>l2cs</li> </ul> </li> <li>grounding_dino<ul> <li>grounding_dino</li> </ul> </li> <li>owlv2<ul> <li>owlv2</li> </ul> </li> <li>paligemma<ul> <li>paligemma</li> </ul> </li> <li>perception_encoder<ul> <li>perception_encoder</li> <li>vision_encoder<ul> <li>config</li> <li>pe</li> <li>rope</li> <li>tokenizer</li> </ul> </li> </ul> </li> <li>qwen25vl<ul> <li>qwen25vl</li> </ul> </li> <li>resnet<ul> <li>resnet_classification</li> </ul> </li> <li>rfdetr<ul> <li>rfdetr</li> </ul> </li> <li>sam<ul> <li>segment_anything</li> </ul> </li> <li>sam2<ul> <li>segment_anything2</li> </ul> </li> <li>vit<ul> <li>vit_classification</li> </ul> </li> <li>yolact<ul> <li>yolact_instance_segmentation</li> </ul> </li> <li>yolo_world<ul> <li>yolo_world</li> </ul> </li> <li>yolov10<ul> <li>yolov10_object_detection</li> </ul> </li> <li>yolov5<ul> <li>yolov5_instance_segmentation</li> <li>yolov5_object_detection</li> </ul> </li> <li>yolov7<ul> <li>yolov7_instance_segmentation</li> </ul> </li> <li>yolov8<ul> <li>yolov8_instance_segmentation</li> <li>yolov8_keypoints_detection</li> <li>yolov8_object_detection</li> </ul> </li> <li>yolov9<ul> <li>yolov9_object_detection</li> </ul> </li> </ul> </li> <li>usage_tracking<ul> <li>redis_queue</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/inference/core/exceptions/","title":"Exceptions","text":""},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.ContentTypeInvalid","title":"<code>ContentTypeInvalid</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the content type is invalid.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class ContentTypeInvalid(Exception):\n    \"\"\"Raised when the content type is invalid.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.ContentTypeMissing","title":"<code>ContentTypeMissing</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the content type is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class ContentTypeMissing(Exception):\n    \"\"\"Raised when the content type is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.EngineIgnitionFailure","title":"<code>EngineIgnitionFailure</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the engine fails to ignite.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class EngineIgnitionFailure(Exception):\n    \"\"\"Raised when the engine fails to ignite.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.InferenceModelNotFound","title":"<code>InferenceModelNotFound</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the inference model is not found.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InferenceModelNotFound(Exception):\n    \"\"\"Raised when the inference model is not found.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.InvalidEnvironmentVariableError","title":"<code>InvalidEnvironmentVariableError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an environment variable is invalid.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidEnvironmentVariableError(Exception):\n    \"\"\"Raised when an environment variable is invalid.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.InvalidMaskDecodeArgument","title":"<code>InvalidMaskDecodeArgument</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an invalid argument is provided for mask decoding.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidMaskDecodeArgument(Exception):\n    \"\"\"Raised when an invalid argument is provided for mask decoding.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.InvalidNumpyInput","title":"<code>InvalidNumpyInput</code>","text":"<p>               Bases: <code>InputImageLoadError</code></p> <p>Raised when the input is an invalid NumPy array.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidNumpyInput(InputImageLoadError):\n    \"\"\"Raised when the input is an invalid NumPy array.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the API key is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Raised when the API key is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.MissingServiceSecretError","title":"<code>MissingServiceSecretError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the service secret is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class MissingServiceSecretError(Exception):\n    \"\"\"Raised when the service secret is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.OnnxProviderNotAvailable","title":"<code>OnnxProviderNotAvailable</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the ONNX provider is not available.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class OnnxProviderNotAvailable(Exception):\n    \"\"\"Raised when the ONNX provider is not available.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/exceptions/#inference.core.exceptions.WorkspaceLoadError","title":"<code>WorkspaceLoadError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there is an error loading the workspace.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class WorkspaceLoadError(Exception):\n    \"\"\"Raised when there is an error loading the workspace.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"reference/inference/core/nms/","title":"Nms","text":""},{"location":"reference/inference/core/nms/#inference.core.nms.non_max_suppression_fast","title":"<code>non_max_suppression_fast(boxes, overlapThresh)</code>","text":"<p>Applies non-maximum suppression to bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>ndarray</code> <p>Array of bounding boxes with confidence scores.</p> required <code>overlapThresh</code> <code>float</code> <p>Overlap threshold for suppression.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of bounding boxes after non-maximum suppression.</p> Source code in <code>inference/core/nms.py</code> <pre><code>def non_max_suppression_fast(boxes, overlapThresh):\n    \"\"\"Applies non-maximum suppression to bounding boxes.\n\n    Args:\n        boxes (np.ndarray): Array of bounding boxes with confidence scores.\n        overlapThresh (float): Overlap threshold for suppression.\n\n    Returns:\n        list: List of bounding boxes after non-maximum suppression.\n    \"\"\"\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n    # initialize the list of picked indexes\n    pick = []\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    conf = boxes[:, 4]\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    idxs = np.argsort(conf)\n    # keep looping while some indexes still remain in the indexes\n    # list\n    while len(idxs) &gt; 0:\n        # grab the last index in the indexes list and add the\n        # index value to the list of picked indexes\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n        # find the largest (x, y) coordinates for the start of\n        # the bounding box and the smallest (x, y) coordinates\n        # for the end of the bounding box\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n        # compute the width and height of the bounding box\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n        # compute the ratio of overlap\n        overlap = (w * h) / area[idxs[:last]]\n        # delete all indexes from the index list that have\n        idxs = np.delete(\n            idxs, np.concatenate(([last], np.where(overlap &gt; overlapThresh)[0]))\n        )\n    # return only the bounding boxes that were picked using the\n    # integer data type\n    return boxes[pick].astype(\"float\")\n</code></pre>"},{"location":"reference/inference/core/nms/#inference.core.nms.w_np_non_max_suppression","title":"<code>w_np_non_max_suppression(prediction, conf_thresh=0.25, iou_thresh=0.45, class_agnostic=False, max_detections=300, max_candidate_detections=3000, timeout_seconds=None, num_masks=0, box_format='xywh')</code>","text":"<p>Applies non-maximum suppression to predictions.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>ndarray</code> <p>Array of predictions. Format for single prediction is [bbox x 4, max_class_confidence, (confidence) x num_of_classes, additional_element x num_masks]</p> required <code>conf_thresh</code> <code>float</code> <p>Confidence threshold. Defaults to 0.25.</p> <code>0.25</code> <code>iou_thresh</code> <code>float</code> <p>IOU threshold. Defaults to 0.45.</p> <code>0.45</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to ignore class labels. Defaults to False.</p> <code>False</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections. Defaults to 300.</p> <code>300</code> <code>max_candidate_detections</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>3000</code> <code>timeout_seconds</code> <code>Optional[int]</code> <p>Timeout in seconds. Defaults to None.</p> <code>None</code> <code>num_masks</code> <code>int</code> <p>Number of masks. Defaults to 0.</p> <code>0</code> <code>box_format</code> <code>str</code> <p>Format of bounding boxes. Either 'xywh' or 'xyxy'. Defaults to 'xywh'.</p> <code>'xywh'</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of filtered predictions after non-maximum suppression. Format of a single result is: [bbox x 4, max_class_confidence, max_class_confidence, id_of_class_with_max_confidence, additional_element x num_masks]</p> Source code in <code>inference/core/nms.py</code> <pre><code>def w_np_non_max_suppression(\n    prediction,\n    conf_thresh: float = 0.25,\n    iou_thresh: float = 0.45,\n    class_agnostic: bool = False,\n    max_detections: int = 300,\n    max_candidate_detections: int = 3000,\n    timeout_seconds: Optional[int] = None,\n    num_masks: int = 0,\n    box_format: str = \"xywh\",\n):\n    \"\"\"Applies non-maximum suppression to predictions.\n\n    Args:\n        prediction (np.ndarray): Array of predictions. Format for single prediction is\n            [bbox x 4, max_class_confidence, (confidence) x num_of_classes, additional_element x num_masks]\n        conf_thresh (float, optional): Confidence threshold. Defaults to 0.25.\n        iou_thresh (float, optional): IOU threshold. Defaults to 0.45.\n        class_agnostic (bool, optional): Whether to ignore class labels. Defaults to False.\n        max_detections (int, optional): Maximum number of detections. Defaults to 300.\n        max_candidate_detections (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        timeout_seconds (Optional[int], optional): Timeout in seconds. Defaults to None.\n        num_masks (int, optional): Number of masks. Defaults to 0.\n        box_format (str, optional): Format of bounding boxes. Either 'xywh' or 'xyxy'. Defaults to 'xywh'.\n\n    Returns:\n        list: List of filtered predictions after non-maximum suppression. Format of a single result is:\n            [bbox x 4, max_class_confidence, max_class_confidence, id_of_class_with_max_confidence,\n            additional_element x num_masks]\n    \"\"\"\n    num_classes = prediction.shape[2] - 5 - num_masks\n\n    if box_format == \"xywh\":\n        pred_view = prediction[:, :, :4]\n\n        # Calculate all values without allocating a new array\n        x1 = pred_view[:, :, 0] - pred_view[:, :, 2] / 2\n        y1 = pred_view[:, :, 1] - pred_view[:, :, 3] / 2\n        x2 = pred_view[:, :, 0] + pred_view[:, :, 2] / 2\n        y2 = pred_view[:, :, 1] + pred_view[:, :, 3] / 2\n\n        # Assign directly to the view\n        pred_view[:, :, 0] = x1\n        pred_view[:, :, 1] = y1\n        pred_view[:, :, 2] = x2\n        pred_view[:, :, 3] = y2\n    elif box_format != \"xyxy\":\n        raise ValueError(\n            \"box_format must be either 'xywh' or 'xyxy', got {}\".format(box_format)\n        )\n\n    batch_predictions = []\n\n    # Pre-allocate space for class confidence and class prediction arrays\n    cls_confs_shape = (prediction.shape[1], 1)\n\n    for np_image_i, np_image_pred in enumerate(prediction):\n        np_conf_mask = np_image_pred[:, 4] &gt;= conf_thresh\n        if not np.any(np_conf_mask):  # Quick check if no boxes pass threshold\n            batch_predictions.append([])\n            continue\n\n        np_image_pred = np_image_pred[np_conf_mask]\n\n        # Handle empty case after filtering\n        if np_image_pred.shape[0] == 0:\n            batch_predictions.append([])\n            continue\n\n        cls_confs = np_image_pred[:, 5 : num_classes + 5]\n        # Check for empty classes after slicing\n        if cls_confs.shape[1] == 0:\n            batch_predictions.append([])\n            continue\n\n        np_class_conf = np.max(cls_confs, axis=1, keepdims=True)\n        np_class_pred = np.argmax(cls_confs, axis=1, keepdims=True)\n        # Extract mask predictions if any\n        if num_masks &gt; 0:\n            np_mask_pred = np_image_pred[:, 5 + num_classes :]\n            # Construct final detections array directly\n            np_detections = np.concatenate(\n                [\n                    np_image_pred[:, :5],\n                    np_class_conf,\n                    np_class_pred.astype(np.float32),\n                    np_mask_pred,\n                ],\n                axis=1,\n            )\n        else:\n            # Optimization: Avoid concatenation when no masks are present\n            np_detections = np.concatenate(\n                [np_image_pred[:, :5], np_class_conf, np_class_pred.astype(np.float32)],\n                axis=1,\n            )\n        filtered_predictions = []\n        if class_agnostic:\n            # Sort by confidence directly\n            sorted_indices = np.argsort(-np_detections[:, 4])\n            np_detections_sorted = np_detections[sorted_indices]\n            # Directly pass to optimized NMS\n            filtered_predictions.extend(\n                non_max_suppression_fast(np_detections_sorted, iou_thresh)\n            )\n        else:\n            np_unique_labels = np.unique(np_class_pred)\n\n            # Process each class\n            for c in np_unique_labels:\n                class_mask = np.atleast_1d(np_class_pred.squeeze() == c)\n                np_detections_class = np_detections[class_mask]\n\n                # Skip empty arrays\n                if np_detections_class.shape[0] == 0:\n                    continue\n\n                # Sort by confidence (highest first)\n                sorted_indices = np.argsort(-np_detections_class[:, 4])\n                np_detections_sorted = np_detections_class[sorted_indices]\n\n                # Apply optimized NMS and extend filtered predictions\n                filtered_predictions.extend(\n                    non_max_suppression_fast(np_detections_sorted, iou_thresh)\n                )\n\n        # Sort final predictions by confidence and limit to max_detections\n        if filtered_predictions:\n            # Use numpy sort for better performance\n            filtered_np = np.array(filtered_predictions)\n            idx = np.argsort(-filtered_np[:, 4])\n            filtered_np = filtered_np[idx]\n\n            # Limit to max_detections\n            if len(filtered_np) &gt; max_detections:\n                filtered_np = filtered_np[:max_detections]\n\n            batch_predictions.append(list(filtered_np))\n        else:\n            batch_predictions.append([])\n\n    return batch_predictions\n</code></pre>"},{"location":"reference/inference/core/roboflow_api/","title":"Roboflow api","text":""},{"location":"reference/inference/core/roboflow_api/#inference.core.roboflow_api.post_to_roboflow_api","title":"<code>post_to_roboflow_api(endpoint, api_key, payload=None, params=None)</code>","text":"<p>Generic function to make a POST request to the Roboflow API.</p> Source code in <code>inference/core/roboflow_api.py</code> <pre><code>@wrap_roboflow_api_errors()\ndef post_to_roboflow_api(\n    endpoint: str,\n    api_key: Optional[str],\n    payload: Optional[dict] = None,\n    params: Optional[List[Tuple[str, str]]] = None,\n) -&gt; dict:\n    \"\"\"Generic function to make a POST request to the Roboflow API.\"\"\"\n    url_params = []\n    if api_key:\n        url_params.append((\"api_key\", api_key))\n    if params:\n        url_params.extend(params)\n\n    full_url = _add_params_to_url(\n        url=f\"{API_BASE_URL}/{endpoint.strip('/')}\", params=url_params\n    )\n    wrapped_url = wrap_url(full_url)\n\n    headers = build_roboflow_api_headers()\n\n    response = requests.post(\n        url=wrapped_url,\n        json=payload,\n        headers=headers,\n        timeout=ROBOFLOW_API_REQUEST_TIMEOUT,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference/core/usage/","title":"Usage","text":""},{"location":"reference/inference/core/usage/#inference.core.usage.trackUsage","title":"<code>trackUsage(endpoint, actor, n=1)</code>","text":"<p>Tracks the usage of an endpoint by an actor.</p> <p>This function increments the usage count for a given endpoint by an actor. It also handles initialization if the count does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint being accessed.</p> required <code>actor</code> <code>str</code> <p>The actor accessing the endpoint.</p> required <code>n</code> <code>int</code> <p>The number of times the endpoint was accessed. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>None</code> <p>This function does not return anything but updates the memcache client.</p> Source code in <code>inference/core/usage.py</code> <pre><code>def trackUsage(endpoint, actor, n=1):\n    \"\"\"Tracks the usage of an endpoint by an actor.\n\n    This function increments the usage count for a given endpoint by an actor.\n    It also handles initialization if the count does not exist.\n\n    Args:\n        endpoint (str): The endpoint being accessed.\n        actor (str): The actor accessing the endpoint.\n        n (int, optional): The number of times the endpoint was accessed. Defaults to 1.\n\n    Returns:\n        None: This function does not return anything but updates the memcache client.\n    \"\"\"\n    # count an inference\n    try:\n        job = endpoint + \"endpoint:::actor\" + actor\n        current_infers = memcache_client.incr(job, n)\n        if current_infers is None:  # not yet set; initialize at 1\n            memcache_client.set(job, n)\n            current_infers = n\n\n            # store key\n            job_keys = memcache_client.get(\"JOB_KEYS\")\n            if job_keys is None:\n                memcache_client.add(\"JOB_KEYS\", json.dumps([job]))\n            else:\n                decoded = json.loads(job_keys)\n                decoded.append(job)\n                decoded = list(set(decoded))\n                memcache_client.set(\"JOB_KEYS\", json.dumps(decoded))\n\n            actor_keys = memcache_client.get(\"ACTOR_KEYS\")\n            if actor_keys is None:\n                ak = {}\n                ak[actor] = n\n                memcache_client.add(\"ACTOR_KEYS\", json.dumps(ak))\n            else:\n                decoded = json.loads(actor_keys)\n                if actor in actor_keys:\n                    actor_keys[actor] += n\n                else:\n                    actor_keys[actor] = n\n                memcache_client.set(\"ACTOR_KEYS\", json.dumps(actor_keys))\n\n    except Exception as e:\n        logger.debug(\"WARNING: there was an error in counting this inference\")\n        logger.debug(e)\n</code></pre>"},{"location":"reference/inference/core/active_learning/accounting/","title":"Accounting","text":""},{"location":"reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.get_images_in_labeling_jobs_of_specific_batch","title":"<code>get_images_in_labeling_jobs_of_specific_batch(all_labeling_jobs, batch_id)</code>","text":"<p>Get the number of images in labeling jobs of a specific batch.</p> <p>Parameters:</p> Name Type Description Default <code>all_labeling_jobs</code> <code>List[dict]</code> <p>All labeling jobs.</p> required <code>batch_id</code> <code>str</code> <p>ID of the batch.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of images in labeling jobs of the batch.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def get_images_in_labeling_jobs_of_specific_batch(\n    all_labeling_jobs: List[dict],\n    batch_id: str,\n) -&gt; int:\n    \"\"\"Get the number of images in labeling jobs of a specific batch.\n\n    Args:\n        all_labeling_jobs: All labeling jobs.\n        batch_id: ID of the batch.\n\n    Returns:\n        The number of images in labeling jobs of the batch.\n\n    \"\"\"\n\n    matching_jobs = []\n    for labeling_job in all_labeling_jobs:\n        if batch_id in labeling_job[\"sourceBatch\"]:\n            matching_jobs.append(labeling_job)\n    return sum(job[\"numImages\"] for job in matching_jobs)\n</code></pre>"},{"location":"reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.get_matching_labeling_batch","title":"<code>get_matching_labeling_batch(all_labeling_batches, batch_name)</code>","text":"<p>Get the matching labeling batch.</p> <p>Parameters:</p> Name Type Description Default <code>all_labeling_batches</code> <code>List[dict]</code> <p>All labeling batches.</p> required <code>batch_name</code> <code>str</code> <p>Name of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>The matching labeling batch if found, None otherwise.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def get_matching_labeling_batch(\n    all_labeling_batches: List[dict],\n    batch_name: str,\n) -&gt; Optional[dict]:\n    \"\"\"Get the matching labeling batch.\n\n    Args:\n        all_labeling_batches: All labeling batches.\n        batch_name: Name of the batch.\n\n    Returns:\n        The matching labeling batch if found, None otherwise.\n\n    \"\"\"\n    matching_batch = None\n    for labeling_batch in all_labeling_batches:\n        if labeling_batch[\"name\"] == batch_name:\n            matching_batch = labeling_batch\n            break\n    return matching_batch\n</code></pre>"},{"location":"reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.image_can_be_submitted_to_batch","title":"<code>image_can_be_submitted_to_batch(batch_name, workspace_id, dataset_id, max_batch_images, api_key)</code>","text":"<p>Check if an image can be submitted to a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_name</code> <code>str</code> <p>Name of the batch.</p> required <code>workspace_id</code> <code>WorkspaceID</code> <p>ID of the workspace.</p> required <code>dataset_id</code> <code>DatasetID</code> <p>ID of the dataset.</p> required <code>max_batch_images</code> <code>Optional[int]</code> <p>Maximum number of images allowed in the batch.</p> required <code>api_key</code> <code>str</code> <p>API key to use for the request.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the image can be submitted to the batch, False otherwise.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def image_can_be_submitted_to_batch(\n    batch_name: str,\n    workspace_id: WorkspaceID,\n    dataset_id: DatasetID,\n    max_batch_images: Optional[int],\n    api_key: str,\n) -&gt; bool:\n    \"\"\"Check if an image can be submitted to a batch.\n\n    Args:\n        batch_name: Name of the batch.\n        workspace_id: ID of the workspace.\n        dataset_id: ID of the dataset.\n        max_batch_images: Maximum number of images allowed in the batch.\n        api_key: API key to use for the request.\n\n    Returns:\n        True if the image can be submitted to the batch, False otherwise.\n    \"\"\"\n    if max_batch_images is None:\n        return True\n    labeling_batches = get_roboflow_labeling_batches(\n        api_key=api_key,\n        workspace_id=workspace_id,\n        dataset_id=dataset_id,\n    )\n    matching_labeling_batch = get_matching_labeling_batch(\n        all_labeling_batches=labeling_batches[\"batches\"],\n        batch_name=batch_name,\n    )\n    if matching_labeling_batch is None:\n        return max_batch_images &gt; 0\n    batch_images_under_labeling = 0\n    if matching_labeling_batch[\"numJobs\"] &gt; 0:\n        labeling_jobs = get_roboflow_labeling_jobs(\n            api_key=api_key, workspace_id=workspace_id, dataset_id=dataset_id\n        )\n        batch_images_under_labeling = get_images_in_labeling_jobs_of_specific_batch(\n            all_labeling_jobs=labeling_jobs[\"jobs\"],\n            batch_id=matching_labeling_batch[\"id\"],\n        )\n    total_batch_images = matching_labeling_batch[\"images\"] + batch_images_under_labeling\n    return max_batch_images &gt; total_batch_images\n</code></pre>"},{"location":"reference/inference/core/active_learning/configuration/","title":"Configuration","text":""},{"location":"reference/inference/core/active_learning/configuration/#inference.core.active_learning.configuration.predictions_incompatible_with_dataset","title":"<code>predictions_incompatible_with_dataset(model_type, dataset_type)</code>","text":"<p>The incompatibility occurs when we mix classification with detection - as detection-based predictions are partially compatible (for instance - for key-points detection we may register bboxes from object detection and manually provide key-points annotations)</p> Source code in <code>inference/core/active_learning/configuration.py</code> <pre><code>def predictions_incompatible_with_dataset(\n    model_type: str,\n    dataset_type: str,\n) -&gt; bool:\n    \"\"\"\n    The incompatibility occurs when we mix classification with detection - as detection-based\n    predictions are partially compatible (for instance - for key-points detection we may register bboxes\n    from object detection and manually provide key-points annotations)\n    \"\"\"\n    model_is_classifier = CLASSIFICATION_TASK in model_type\n    dataset_is_of_type_classification = CLASSIFICATION_TASK in dataset_type\n    return model_is_classifier != dataset_is_of_type_classification\n</code></pre>"},{"location":"reference/inference/core/cache/base/","title":"Base","text":""},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache","title":"<code>BaseCache</code>","text":"<p>BaseCache is an abstract base class that defines the interface for a cache.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>class BaseCache:\n    \"\"\"\n    BaseCache is an abstract base class that defines the interface for a cache.\n    \"\"\"\n\n    def get(self, key: str):\n        \"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def set(self, key: str, value: str, expire: float = None):\n        \"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zadd(self, key: str, value: str, score: float, expire: float = None):\n        \"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n        \"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting index of the range. Defaults to -1.\n            stop (int, optional): The ending index of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        start: Optional[int] = -1,\n        stop: Optional[int] = float(\"inf\"),\n    ):\n        \"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def acquire_lock(self, key: str, expire: float = None) -&gt; Any:\n        raise NotImplementedError()\n\n    @contextmanager\n    def lock(self, key: str, expire: float = None) -&gt; Any:\n        logger.debug(f\"Acquiring lock at cache key: {key}\")\n        l = self.acquire_lock(key, expire=expire)\n        try:\n            yield l\n        finally:\n            logger.debug(f\"Releasing lock at cache key: {key}\")\n            l.release()\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        \"\"\"\n        Caches a numpy array.\n\n        Args:\n            key (str): The key to store the value.\n            value (Any): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_numpy(self, key: str) -&gt; Any:\n        \"\"\"\n        Retrieves a numpy array from the cache.\n\n        Args:\n            key (str): The key of the value to retrieve.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def get(self, key: str):\n    \"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.get_numpy","title":"<code>get_numpy(key)</code>","text":"<p>Retrieves a numpy array from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the value to retrieve.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def get_numpy(self, key: str) -&gt; Any:\n    \"\"\"\n    Retrieves a numpy array from the cache.\n\n    Args:\n        key (str): The key of the value to retrieve.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n    \"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.set_numpy","title":"<code>set_numpy(key, value, expire=None)</code>","text":"<p>Caches a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def set_numpy(self, key: str, value: Any, expire: float = None):\n    \"\"\"\n    Caches a numpy array.\n\n    Args:\n        key (str): The key to store the value.\n        value (Any): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zadd(self, key: str, value: str, score: float, expire: float = None):\n    \"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting index of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending index of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n    \"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting index of the range. Defaults to -1.\n        stop (int, optional): The ending index of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zremrangebyscore","title":"<code>zremrangebyscore(key, start=-1, stop=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> <code>-1</code> <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> <code>float('inf')</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    start: Optional[int] = -1,\n    stop: Optional[int] = float(\"inf\"),\n):\n    \"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/inference/core/cache/memory/","title":"Memory","text":""},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache","title":"<code>MemoryCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>MemoryCache is an in-memory cache that implements the BaseCache interface.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>dict</code> <p>A dictionary to store the cache values.</p> <code>expires</code> <code>dict</code> <p>A dictionary to store the expiration times of the cache values.</p> <code>zexpires</code> <code>dict</code> <p>A dictionary to store the expiration times of the sorted set values.</p> <code>_expire_thread</code> <code>Thread</code> <p>A thread that runs the _expire method.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>class MemoryCache(BaseCache):\n    \"\"\"\n    MemoryCache is an in-memory cache that implements the BaseCache interface.\n\n    Attributes:\n        cache (dict): A dictionary to store the cache values.\n        expires (dict): A dictionary to store the expiration times of the cache values.\n        zexpires (dict): A dictionary to store the expiration times of the sorted set values.\n        _expire_thread (threading.Thread): A thread that runs the _expire method.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initializes a new instance of the MemoryCache class.\n        \"\"\"\n        self.cache = dict()\n        self.expires = dict()\n        self.zexpires = dict()\n\n        self._expire_thread = threading.Thread(target=self._expire)\n        self._expire_thread.daemon = True\n        self._expire_thread.start()\n\n    def _expire(self):\n        \"\"\"\n        Removes the expired keys from the cache and zexpires dictionaries.\n\n        This method runs in an infinite loop and sleeps for MEMORY_CACHE_EXPIRE_INTERVAL seconds between each iteration.\n        \"\"\"\n        while True:\n            now = time.time()\n            keys_to_delete = []\n            for k, v in self.expires.copy().items():\n                if v &lt; now:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                del self.cache[k]\n                del self.expires[k]\n            keys_to_delete = []\n            for k, v in self.zexpires.copy().items():\n                if v &lt; now:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                del self.cache[k[0]][k[1]]\n                del self.zexpires[k]\n            while time.time() - now &lt; MEMORY_CACHE_EXPIRE_INTERVAL:\n                time.sleep(0.1)\n\n    def get(self, key: str):\n        \"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Returns:\n            str: The value associated with the key, or None if the key does not exist or is expired.\n        \"\"\"\n        if key in self.expires:\n            if self.expires[key] &lt; time.time():\n                del self.cache[key]\n                del self.expires[key]\n                return None\n        return self.cache.get(key)\n\n    def set(self, key: str, value: str, expire: float = None):\n        \"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        self.cache[key] = value\n        if expire:\n            self.expires[key] = expire + time.time()\n\n    def zadd(self, key: str, value: Any, score: float, expire: float = None):\n        \"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        if not key in self.cache:\n            self.cache[key] = dict()\n        self.cache[key][score] = value\n        if expire:\n            self.zexpires[(key, score)] = expire + time.time()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n        \"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting score of the range. Defaults to -1.\n            stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Returns:\n            list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n        \"\"\"\n        if not key in self.cache:\n            return []\n        keys = sorted([k for k in self.cache[key].keys() if min &lt;= k &lt;= max])\n        if withscores:\n            return [(self.cache[key][k], k) for k in keys]\n        else:\n            return [self.cache[key][k] for k in keys]\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n    ):\n        \"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Returns:\n            int: The number of members removed from the sorted set.\n        \"\"\"\n        res = self.zrangebyscore(key, min=min, max=max, withscores=True)\n        keys_to_delete = [k[1] for k in res]\n        for k in keys_to_delete:\n            del self.cache[key][k]\n        return len(keys_to_delete)\n\n    def acquire_lock(self, key: str, expire=None) -&gt; Any:\n        lock: Optional[Lock] = self.get(key)\n        if lock is None:\n            lock = Lock()\n            self.set(key, lock, expire=expire)\n        if expire is None:\n            expire = -1\n        acquired = lock.acquire(timeout=expire)\n        if not acquired:\n            raise TimeoutError()\n        # refresh the lock\n        self.set(key, lock, expire=expire)\n        return lock\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        return self.set(key, value, expire=expire)\n\n    def get_numpy(self, key: str):\n        return self.get(key)\n</code></pre>"},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new instance of the MemoryCache class.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initializes a new instance of the MemoryCache class.\n    \"\"\"\n    self.cache = dict()\n    self.expires = dict()\n    self.zexpires = dict()\n\n    self._expire_thread = threading.Thread(target=self._expire)\n    self._expire_thread.daemon = True\n    self._expire_thread.start()\n</code></pre>"},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The value associated with the key, or None if the key does not exist or is expired.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def get(self, key: str):\n    \"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Returns:\n        str: The value associated with the key, or None if the key does not exist or is expired.\n    \"\"\"\n    if key in self.expires:\n        if self.expires[key] &lt; time.time():\n            del self.cache[key]\n            del self.expires[key]\n            return None\n    return self.cache.get(key)\n</code></pre>"},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n    \"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    self.cache[key] = value\n    if expire:\n        self.expires[key] = expire + time.time()\n</code></pre>"},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zadd(self, key: str, value: Any, score: float, expire: float = None):\n    \"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    if not key in self.cache:\n        self.cache[key] = dict()\n    self.cache[key][score] = value\n    if expire:\n        self.zexpires[(key, score)] = expire + time.time()\n</code></pre>"},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending score of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values (or value-score pairs if withscores is True) in the specified score range.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n    \"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting score of the range. Defaults to -1.\n        stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Returns:\n        list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n    \"\"\"\n    if not key in self.cache:\n        return []\n    keys = sorted([k for k in self.cache[key].keys() if min &lt;= k &lt;= max])\n    if withscores:\n        return [(self.cache[key][k], k) for k in keys]\n    else:\n        return [self.cache[key][k] for k in keys]\n</code></pre>"},{"location":"reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zremrangebyscore","title":"<code>zremrangebyscore(key, min=-1, max=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of members removed from the sorted set.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n):\n    \"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Returns:\n        int: The number of members removed from the sorted set.\n    \"\"\"\n    res = self.zrangebyscore(key, min=min, max=max, withscores=True)\n    keys_to_delete = [k[1] for k in res]\n    for k in keys_to_delete:\n        del self.cache[key][k]\n    return len(keys_to_delete)\n</code></pre>"},{"location":"reference/inference/core/cache/model_artifacts/","title":"Model artifacts","text":""},{"location":"reference/inference/core/cache/model_artifacts/#inference.core.cache.model_artifacts.clear_cache","title":"<code>clear_cache(model_id=None, delete_from_disk=True)</code>","text":"<p>Clear the cache for a specific model or the entire cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>Optional[str]</code> <p>The model ID to clear cache for. If None, clears entire cache. Defaults to None.</p> <code>None</code> <code>delete_from_disk</code> <code>bool</code> <p>Whether to delete cached files from disk. Defaults to False.</p> <code>True</code> Source code in <code>inference/core/cache/model_artifacts.py</code> <pre><code>def clear_cache(model_id: Optional[str] = None, delete_from_disk: bool = True) -&gt; None:\n    \"\"\"Clear the cache for a specific model or the entire cache directory.\n\n    Args:\n        model_id (Optional[str], optional): The model ID to clear cache for. If None, clears entire cache. Defaults to None.\n        delete_from_disk (bool, optional): Whether to delete cached files from disk. Defaults to False.\n    \"\"\"\n    if not delete_from_disk:\n        return\n    cache_dir = get_cache_dir(model_id=model_id)\n    if not os.path.exists(cache_dir):\n        return\n    lock_dir = MODEL_CACHE_DIR + \"/_file_locks\"  # Dedicated lock directory\n    os.makedirs(lock_dir, exist_ok=True)  # ensure lock directory exists.\n\n    # Use the last 2 levels of the cache directory path as the lock file name suffix\n    parts = os.path.normpath(cache_dir).split(os.sep)\n    suffix = (\n        os.path.join(*parts[-2:]) if len(parts) &gt;= 2 else os.path.basename(cache_dir)\n    )\n    lock_file = os.path.join(lock_dir, f\"{suffix}.lock\")\n\n    try:\n        lock = FileLock(lock_file, timeout=10)  # 10 second timeout\n        with lock:\n            if not os.path.exists(cache_dir):  # Check again after acquiring lock\n                return  # Already deleted by another process\n\n            max_retries = 3\n            retry_delay = 1  # Initial delay in seconds\n\n            for attempt in range(max_retries):\n                try:\n                    shutil.rmtree(cache_dir, onerror=_rmtree_onerror)\n                    return  # Success\n                except FileNotFoundError:\n                    return  # Already deleted by another process\n                except Exception as e:\n                    if attempt &lt; max_retries - 1:\n                        logger.warning(\n                            f\"Error deleting cache %s: %s, retrying in %s seconds...\",\n                            cache_dir,\n                            e,\n                            retry_delay,\n                        )\n                        time.sleep(retry_delay)\n                        retry_delay *= 2  # Exponential backoff\n                    else:\n                        logger.warning(\n                            f\"Error deleting cache %s: %s, max retries exceeded.\",\n                            cache_dir,\n                            e,\n                        )\n                        return\n    except Exception as e:\n        logger.warning(\n            f\"Error acquiring lock for cache %s, skipping cache cleanup. %s\",\n            cache_dir,\n            e,\n        )\n    finally:\n        try:\n            if os.path.exists(lock_file):\n                os.unlink(lock_file)  # Clean up lock file\n        except OSError:\n            pass  # Best effort cleanup\n</code></pre>"},{"location":"reference/inference/core/cache/redis/","title":"Redis","text":""},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache","title":"<code>RedisCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>MemoryCache is an in-memory cache that implements the BaseCache interface.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>dict</code> <p>A dictionary to store the cache values.</p> <code>expires</code> <code>dict</code> <p>A dictionary to store the expiration times of the cache values.</p> <code>zexpires</code> <code>dict</code> <p>A dictionary to store the expiration times of the sorted set values.</p> <code>_expire_thread</code> <code>Thread</code> <p>A thread that runs the _expire method.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>class RedisCache(BaseCache):\n    \"\"\"\n    MemoryCache is an in-memory cache that implements the BaseCache interface.\n\n    Attributes:\n        cache (dict): A dictionary to store the cache values.\n        expires (dict): A dictionary to store the expiration times of the cache values.\n        zexpires (dict): A dictionary to store the expiration times of the sorted set values.\n        _expire_thread (threading.Thread): A thread that runs the _expire method.\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        ssl: bool = False,\n        timeout: float = 2.0,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a new instance of the MemoryCache class.\n        \"\"\"\n        self.client = redis.Redis(\n            host=host,\n            port=port,\n            db=db,\n            decode_responses=False,\n            ssl=ssl,\n            socket_timeout=timeout,\n            socket_connect_timeout=timeout,\n        )\n        logger.debug(\"Attempting to diagnose Redis connection...\")\n        self.client.ping()\n        logger.debug(\"Redis connection established.\")\n        self.zexpires = dict()\n\n        self._expire_thread = threading.Thread(target=self._expire, daemon=True)\n        self._expire_thread.start()\n\n    def _expire(self):\n        \"\"\"\n        Removes the expired keys from the cache and zexpires dictionaries.\n\n        This method runs in an infinite loop and sleeps for MEMORY_CACHE_EXPIRE_INTERVAL seconds between each iteration.\n        \"\"\"\n        while True:\n            now = time.time()\n            for k, v in copy(list(self.zexpires.items())):\n                if v &lt; now:\n                    tolerance_factor = 1e-14  # floating point accuracy\n                    self.zremrangebyscore(\n                        k[0], k[1] - tolerance_factor, k[1] + tolerance_factor\n                    )\n                    del self.zexpires[k]\n            sleep_time = MEMORY_CACHE_EXPIRE_INTERVAL - (time.time() - now)\n            time.sleep(max(sleep_time, 0))\n\n    def get(self, key: str):\n        \"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Returns:\n            str: The value associated with the key, or None if the key does not exist or is expired.\n        \"\"\"\n        item = self.client.get(key)\n        if item is not None:\n            try:\n                return json.loads(item)\n            except (TypeError, ValueError):\n                return item\n\n    def set(self, key: str, value: str, expire: float = None):\n        \"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        if not isinstance(value, bytes):\n            value = json.dumps(value)\n        self.client.set(key, value, ex=expire)\n\n    def zadd(self, key: str, value: Any, score: float, expire: float = None):\n        \"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        # serializable_value = self.ensure_serializable(value)\n        value = json.dumps(value)\n        self.client.zadd(key, {value: score})\n        if expire:\n            self.zexpires[(key, score)] = expire + time.time()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n        \"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting score of the range. Defaults to -1.\n            stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Returns:\n            list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n        \"\"\"\n        res = self.client.zrangebyscore(key, min, max, withscores=withscores)\n        if withscores:\n            return [(json.loads(x), y) for x, y in res]\n        else:\n            return [json.loads(x) for x in res]\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n    ):\n        \"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Returns:\n            int: The number of members removed from the sorted set.\n        \"\"\"\n        return self.client.zremrangebyscore(key, min, max)\n\n    def ensure_serializable(self, value: Any):\n        if isinstance(value, dict):\n            for k, v in value.items():\n                if isinstance(v, Exception):\n                    value[k] = str(v)\n                elif inspect.isclass(v) and isinstance(v, InferenceResponseImage):\n                    value[k] = v.dict()\n        return value\n\n    def acquire_lock(self, key: str, expire=None) -&gt; Any:\n        l = self.client.lock(key, blocking=True, timeout=expire)\n        acquired = l.acquire(blocking_timeout=expire)\n        if not acquired:\n            raise TimeoutError(\"Couldn't get lock\")\n        # refresh the lock\n        if expire is not None:\n            l.extend(expire)\n        return l\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        serialized_value = pickle.dumps(value)\n        self.set(key, serialized_value, expire=expire)\n\n    def get_numpy(self, key: str) -&gt; Any:\n        serialized_value = self.get(key)\n        if serialized_value is not None:\n            return pickle.loads(serialized_value)\n        else:\n            return None\n</code></pre>"},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.__init__","title":"<code>__init__(host='localhost', port=6379, db=0, ssl=False, timeout=2.0)</code>","text":"<p>Initializes a new instance of the MemoryCache class.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def __init__(\n    self,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    ssl: bool = False,\n    timeout: float = 2.0,\n) -&gt; None:\n    \"\"\"\n    Initializes a new instance of the MemoryCache class.\n    \"\"\"\n    self.client = redis.Redis(\n        host=host,\n        port=port,\n        db=db,\n        decode_responses=False,\n        ssl=ssl,\n        socket_timeout=timeout,\n        socket_connect_timeout=timeout,\n    )\n    logger.debug(\"Attempting to diagnose Redis connection...\")\n    self.client.ping()\n    logger.debug(\"Redis connection established.\")\n    self.zexpires = dict()\n\n    self._expire_thread = threading.Thread(target=self._expire, daemon=True)\n    self._expire_thread.start()\n</code></pre>"},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The value associated with the key, or None if the key does not exist or is expired.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def get(self, key: str):\n    \"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Returns:\n        str: The value associated with the key, or None if the key does not exist or is expired.\n    \"\"\"\n    item = self.client.get(key)\n    if item is not None:\n        try:\n            return json.loads(item)\n        except (TypeError, ValueError):\n            return item\n</code></pre>"},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n    \"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    if not isinstance(value, bytes):\n        value = json.dumps(value)\n    self.client.set(key, value, ex=expire)\n</code></pre>"},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zadd(self, key: str, value: Any, score: float, expire: float = None):\n    \"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    # serializable_value = self.ensure_serializable(value)\n    value = json.dumps(value)\n    self.client.zadd(key, {value: score})\n    if expire:\n        self.zexpires[(key, score)] = expire + time.time()\n</code></pre>"},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending score of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values (or value-score pairs if withscores is True) in the specified score range.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n    \"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting score of the range. Defaults to -1.\n        stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Returns:\n        list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n    \"\"\"\n    res = self.client.zrangebyscore(key, min, max, withscores=withscores)\n    if withscores:\n        return [(json.loads(x), y) for x, y in res]\n    else:\n        return [json.loads(x) for x in res]\n</code></pre>"},{"location":"reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zremrangebyscore","title":"<code>zremrangebyscore(key, min=-1, max=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of members removed from the sorted set.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n):\n    \"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Returns:\n        int: The number of members removed from the sorted set.\n    \"\"\"\n    return self.client.zremrangebyscore(key, min, max)\n</code></pre>"},{"location":"reference/inference/core/devices/utils/","title":"Utils","text":""},{"location":"reference/inference/core/devices/utils/#inference.core.devices.utils.get_cpu_id","title":"<code>get_cpu_id()</code>","text":"<p>Fetches the CPU ID based on the operating system.</p> <p>Attempts to get the CPU ID for Windows, Linux, and MacOS. In case of any error or an unsupported OS, returns None.</p> <p>Returns:</p> Type Description <p>Optional[str]: CPU ID string if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_cpu_id():\n    \"\"\"Fetches the CPU ID based on the operating system.\n\n    Attempts to get the CPU ID for Windows, Linux, and MacOS.\n    In case of any error or an unsupported OS, returns None.\n\n    Returns:\n        Optional[str]: CPU ID string if available, None otherwise.\n    \"\"\"\n    try:\n        if platform.system() == \"Windows\":\n            return os.popen(\"wmic cpu get ProcessorId\").read().strip()\n        elif platform.system() == \"Linux\":\n            return (\n                open(\"/proc/cpuinfo\").read().split(\"processor\")[0].split(\":\")[1].strip()\n            )\n        elif platform.system() == \"Darwin\":\n            import subprocess\n\n            return (\n                subprocess.check_output([\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"])\n                .strip()\n                .decode()\n            )\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"reference/inference/core/devices/utils/#inference.core.devices.utils.get_device_hostname","title":"<code>get_device_hostname()</code>","text":"<p>Fetches the device's hostname.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The device's hostname.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_device_hostname():\n    \"\"\"Fetches the device's hostname.\n\n    Returns:\n        str: The device's hostname.\n    \"\"\"\n    return platform.node()\n</code></pre>"},{"location":"reference/inference/core/devices/utils/#inference.core.devices.utils.get_gpu_id","title":"<code>get_gpu_id()</code>","text":"<p>Fetches the GPU ID if a GPU is present.</p> <p>Tries to import and use the <code>pynvml</code> (delivered by nvidia-ml-py) module to retrieve the GPU information.</p> <p>Returns:</p> Type Description <p>Optional[int]: GPU ID if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_gpu_id():\n    \"\"\"Fetches the GPU ID if a GPU is present.\n\n    Tries to import and use the `pynvml` (delivered by nvidia-ml-py) module to retrieve the GPU information.\n\n    Returns:\n        Optional[int]: GPU ID if available, None otherwise.\n    \"\"\"\n    try:\n        from pynvml import nvmlDeviceGetCount, nvmlInit\n\n        nvmlInit()\n        gpus_count = nvmlDeviceGetCount()\n        if gpus_count:\n            return 0\n    except ImportError:\n        return None\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/inference/core/devices/utils/#inference.core.devices.utils.get_inference_server_id","title":"<code>get_inference_server_id()</code>","text":"<p>Fetches a unique device ID.</p> <p>Tries to get the GPU ID first, then falls back to CPU ID. If the application is running inside Docker, the Docker container ID is appended to the hostname.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A unique string representing the device. If unable to determine, returns \"UNKNOWN\".</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_inference_server_id():\n    \"\"\"Fetches a unique device ID.\n\n    Tries to get the GPU ID first, then falls back to CPU ID.\n    If the application is running inside Docker, the Docker container ID is appended to the hostname.\n\n    Returns:\n        str: A unique string representing the device. If unable to determine, returns \"UNKNOWN\".\n    \"\"\"\n    try:\n        if INFERENCE_SERVER_ID is not None:\n            return INFERENCE_SERVER_ID\n        id = random_string(6)\n        gpu_id = get_gpu_id()\n        if gpu_id is not None:\n            return f\"{id}-GPU-{gpu_id}\"\n        jetson_id = get_jetson_id()\n        if jetson_id is not None:\n            return f\"{id}-JETSON-{jetson_id}\"\n        return id\n    except Exception as e:\n        return \"UNKNOWN\"\n</code></pre>"},{"location":"reference/inference/core/devices/utils/#inference.core.devices.utils.get_jetson_id","title":"<code>get_jetson_id()</code>","text":"<p>Fetches the Jetson device's serial number.</p> <p>Attempts to read the serial number from the device tree. In case of any error, returns None.</p> <p>Returns:</p> Type Description <p>Optional[str]: Jetson device serial number if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_jetson_id():\n    \"\"\"Fetches the Jetson device's serial number.\n\n    Attempts to read the serial number from the device tree.\n    In case of any error, returns None.\n\n    Returns:\n        Optional[str]: Jetson device serial number if available, None otherwise.\n    \"\"\"\n    try:\n        # Fetch the device's serial number\n        if not os.path.exists(\"/proc/device-tree/serial-number\"):\n            return None\n        serial_number = os.popen(\"cat /proc/device-tree/serial-number\").read().strip()\n        if serial_number == \"\":\n            return None\n        return serial_number\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"reference/inference/core/devices/utils/#inference.core.devices.utils.is_running_in_docker","title":"<code>is_running_in_docker()</code>","text":"<p>Checks if the current process is running inside a Docker container.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if running inside a Docker container, False otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def is_running_in_docker():\n    \"\"\"Checks if the current process is running inside a Docker container.\n\n    Returns:\n        bool: True if running inside a Docker container, False otherwise.\n    \"\"\"\n    return os.path.exists(\"/.dockerenv\")\n</code></pre>"},{"location":"reference/inference/core/entities/requests/clip/","title":"Clip","text":""},{"location":"reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipCompareRequest","title":"<code>ClipCompareRequest</code>","text":"<p>               Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>subject</code> <code>Union[InferenceRequestImage, str]</code> <p>The type of image data provided, one of 'url' or 'base64'.</p> <code>subject_type</code> <code>str</code> <p>The type of subject, one of 'image' or 'text'.</p> <code>prompt</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]</code> <p>The prompt for comparison.</p> <code>prompt_type</code> <code>str</code> <p>The type of prompt, one of 'image' or 'text'.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipCompareRequest(ClipInferenceRequest):\n    \"\"\"Request for CLIP comparison.\n\n    Attributes:\n        subject (Union[InferenceRequestImage, str]): The type of image data provided, one of 'url' or 'base64'.\n        subject_type (str): The type of subject, one of 'image' or 'text'.\n        prompt (Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]): The prompt for comparison.\n        prompt_type (str): The type of prompt, one of 'image' or 'text'.\n    \"\"\"\n\n    subject: Union[InferenceRequestImage, str] = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url' or 'base64'\",\n    )\n    subject_type: str = Field(\n        default=\"image\",\n        examples=[\"image\"],\n        description=\"The type of subject, one of 'image' or 'text'\",\n    )\n    prompt: Union[\n        List[InferenceRequestImage],\n        InferenceRequestImage,\n        str,\n        List[str],\n        Dict[str, Union[InferenceRequestImage, str]],\n    ]\n    prompt_type: str = Field(\n        default=\"text\",\n        examples=[\"text\"],\n        description=\"The type of prompt, one of 'image' or 'text'\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipImageEmbeddingRequest","title":"<code>ClipImageEmbeddingRequest</code>","text":"<p>               Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP image embedding.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) to be embedded.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipImageEmbeddingRequest(ClipInferenceRequest):\n    \"\"\"Request for CLIP image embedding.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) to be embedded.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n</code></pre>"},{"location":"reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipInferenceRequest","title":"<code>ClipInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for CLIP inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>clip_version_id</code> <code>Optional[str]</code> <p>The version ID of CLIP to be used for this request.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipInferenceRequest(BaseRequest):\n    \"\"\"Request for CLIP inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        clip_version_id (Optional[str]): The version ID of CLIP to be used for this request.\n    \"\"\"\n\n    clip_version_id: Optional[str] = Field(\n        default=CLIP_VERSION_ID,\n        examples=[\"ViT-B-16\"],\n        description=\"The version ID of CLIP to be used for this request. Must be one of RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, ViT-B-32, ViT-L-14-336px, and ViT-L-14.\",\n    )\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"clip_version_id\") is None:\n            return None\n        return f\"clip/{values['clip_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipTextEmbeddingRequest","title":"<code>ClipTextEmbeddingRequest</code>","text":"<p>               Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP text embedding.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Union[List[str], str]</code> <p>A string or list of strings.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipTextEmbeddingRequest(ClipInferenceRequest):\n    \"\"\"Request for CLIP text embedding.\n\n    Attributes:\n        text (Union[List[str], str]): A string or list of strings.\n    \"\"\"\n\n    text: Union[List[str], str] = Field(\n        examples=[\"The quick brown fox jumps over the lazy dog\"],\n        description=\"A string or list of strings\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/doctr/","title":"Doctr","text":""},{"location":"reference/inference/core/entities/requests/doctr/#inference.core.entities.requests.doctr.DoctrOCRInferenceRequest","title":"<code>DoctrOCRInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>DocTR inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> Source code in <code>inference/core/entities/requests/doctr.py</code> <pre><code>class DoctrOCRInferenceRequest(BaseRequest):\n    \"\"\"\n    DocTR inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    doctr_version_id: Optional[str] = \"default\"\n    model_id: Optional[str] = Field(None)\n    # flag to generate bounding box data rather than just a string, set to False for backwards compatibility\n    generate_bounding_boxes: Optional[bool] = False\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"doctr_version_id\") is None:\n            return None\n        return f\"doctr/{values['doctr_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/dynamic_class_base/","title":"Dynamic class base","text":""},{"location":"reference/inference/core/entities/requests/dynamic_class_base/#inference.core.entities.requests.dynamic_class_base.DynamicClassBaseInferenceRequest","title":"<code>DynamicClassBaseInferenceRequest</code>","text":"<p>               Bases: <code>CVInferenceRequest</code></p> <p>Request for zero-shot object detection models (with dynamic class lists).</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/dynamic_class_base.py</code> <pre><code>class DynamicClassBaseInferenceRequest(CVInferenceRequest):\n    \"\"\"Request for zero-shot object detection models (with dynamic class lists).\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    model_id: Optional[str] = Field(None)\n    text: List[str] = Field(\n        examples=[[\"person\", \"dog\", \"cat\"]],\n        description=\"A list of strings\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/easy_ocr/","title":"Easy ocr","text":""},{"location":"reference/inference/core/entities/requests/easy_ocr/#inference.core.entities.requests.easy_ocr.EasyOCRInferenceRequest","title":"<code>EasyOCRInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>EasyOCR inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> Source code in <code>inference/core/entities/requests/easy_ocr.py</code> <pre><code>class EasyOCRInferenceRequest(BaseRequest):\n    \"\"\"\n    EasyOCR inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    easy_ocr_version_id: Optional[str] = EASYOCR_VERSION_ID\n    model_id: Optional[str] = Field(None)\n    language_codes: Optional[List[str]] = Field(default=[\"en\"])\n    quantize: Optional[bool] = Field(\n        default=False,\n        description=\"Quantized models are smaller and faster, but may be less accurate and won't work correctly on all hardware.\",\n    )\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"easy_ocr_version_id\") is None:\n            return None\n        return f\"easy_ocr/{values['easy_ocr_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/gaze/","title":"Gaze","text":""},{"location":"reference/inference/core/entities/requests/gaze/#inference.core.entities.requests.gaze.GazeDetectionInferenceRequest","title":"<code>GazeDetectionInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>gaze_version_id</code> <code>Optional[str]</code> <p>The version ID of Gaze to be used for this request.</p> <code>do_run_face_detection</code> <code>Optional[bool]</code> <p>If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection.</p> <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> Source code in <code>inference/core/entities/requests/gaze.py</code> <pre><code>class GazeDetectionInferenceRequest(BaseRequest):\n    \"\"\"Request for gaze detection inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        gaze_version_id (Optional[str]): The version ID of Gaze to be used for this request.\n        do_run_face_detection (Optional[bool]): If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection.\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n    \"\"\"\n\n    gaze_version_id: Optional[str] = Field(\n        default=GAZE_VERSION_ID,\n        examples=[\"L2CS\"],\n        description=\"The version ID of Gaze to be used for this request. Must be one of l2cs.\",\n    )\n\n    do_run_face_detection: Optional[bool] = Field(\n        default=True,\n        examples=[False],\n        description=\"If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection\",\n    )\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"gaze_version_id\") is None:\n            return None\n        return f\"gaze/{values['gaze_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/groundingdino/","title":"Groundingdino","text":""},{"location":"reference/inference/core/entities/requests/groundingdino/#inference.core.entities.requests.groundingdino.GroundingDINOInferenceRequest","title":"<code>GroundingDINOInferenceRequest</code>","text":"<p>               Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Grounding DINO zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/groundingdino.py</code> <pre><code>class GroundingDINOInferenceRequest(DynamicClassBaseInferenceRequest):\n    \"\"\"Request for Grounding DINO zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    box_threshold: Optional[float] = 0.5\n    grounding_dino_version_id: Optional[str] = \"default\"\n    text_threshold: Optional[float] = 0.5\n    class_agnostic_nms: Optional[bool] = CLASS_AGNOSTIC_NMS\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/","title":"Inference","text":""},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.BaseRequest","title":"<code>BaseRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base request for inference.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str_</code> <p>A unique request identifier.</p> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key that will be passed to the model during initialization for artifact retrieval.</p> <code>start</code> <code>Optional[float]</code> <p>start time of request</p> <code>disable_model_monitoring</code> <code>Optional[bool]</code> <p>If true, disables model monitoring for this request.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class BaseRequest(BaseModel):\n    \"\"\"Base request for inference.\n\n    Attributes:\n        id (str_): A unique request identifier.\n        api_key (Optional[str]): Roboflow API Key that will be passed to the model during initialization for artifact retrieval.\n        start (Optional[float]): start time of request\n        disable_model_monitoring (Optional[bool]): If true, disables model monitoring for this request.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        kwargs[\"id\"] = kwargs.get(\"id\", str(uuid4()))\n        super().__init__(**kwargs)\n\n    model_config = ConfigDict(protected_namespaces=())\n    id: str\n    api_key: Optional[str] = ApiKey\n    usage_billable: bool = True\n    start: Optional[float] = None\n    source: Optional[str] = None\n    source_info: Optional[str] = None\n    disable_model_monitoring: Optional[bool] = Field(\n        default=False, description=\"If true, disables model monitoring for this request\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.CVInferenceRequest","title":"<code>CVInferenceRequest</code>","text":"<p>               Bases: <code>InferenceRequest</code></p> <p>Computer Vision inference request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> <code>disable_preproc_auto_orient</code> <code>Optional[bool]</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_contrast</code> <code>Optional[bool]</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_grayscale</code> <code>Optional[bool]</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_static_crop</code> <code>Optional[bool]</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class CVInferenceRequest(InferenceRequest):\n    \"\"\"Computer Vision inference request.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n        disable_preproc_auto_orient (Optional[bool]): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (Optional[bool]): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (Optional[bool]): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (Optional[bool]): If true, the static crop preprocessing step is disabled for this call. Default is False.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    disable_preproc_auto_orient: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the auto orient preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_contrast: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the auto contrast preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_grayscale: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the grayscale preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_static_crop: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the static crop preprocessing step is disabled for this call.\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.ClassificationInferenceRequest","title":"<code>ClassificationInferenceRequest</code>","text":"<p>               Bases: <code>CVInferenceRequest</code></p> <p>Classification inference request.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class ClassificationInferenceRequest(CVInferenceRequest):\n    \"\"\"Classification inference request.\n\n    Attributes:\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        kwargs[\"model_type\"] = \"classification\"\n        super().__init__(**kwargs)\n\n    confidence: Optional[float] = Field(\n        default=0.4,\n        examples=[0.5],\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n    disable_active_learning: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n    )\n    active_learning_target_dataset: Optional[str] = Field(\n        default=None,\n        examples=[\"my_dataset\"],\n        description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.DepthEstimationRequest","title":"<code>DepthEstimationRequest</code>","text":"<p>               Bases: <code>InferenceRequest</code></p> <p>Request for depth estimation.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) to be estimated.</p> <code>model_id</code> <code>str</code> <p>The model ID to use for depth estimation.</p> <code>depth_version_id</code> <code>Optional[str]</code> <p>The version ID of the depth estimation model.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class DepthEstimationRequest(InferenceRequest):\n    \"\"\"Request for depth estimation.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) to be estimated.\n        model_id (str): The model ID to use for depth estimation.\n        depth_version_id (Optional[str]): The version ID of the depth estimation model.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    model_id: Optional[str] = Field(None)\n    depth_version_id: Optional[str] = Field(\n        default=\"small\",\n        examples=[\"small\"],\n        description=\"The version ID of the depth estimation model\",\n    )\n\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"depth_version_id\") is None:\n            return None\n        return f\"depth-anything-v2/{values['depth_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InferenceRequest","title":"<code>InferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Base request for inference.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> <code>model_type</code> <code>Optional[str]</code> <p>The type of the model, usually referring to what task the model performs.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InferenceRequest(BaseRequest):\n    \"\"\"Base request for inference.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n        model_type (Optional[str]): The type of the model, usually referring to what task the model performs.\n    \"\"\"\n\n    model_id: Optional[str] = ModelID\n    model_type: Optional[str] = ModelType\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InferenceRequestImage","title":"<code>InferenceRequestImage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Image data for inference request.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of image data provided, one of 'url', 'base64', or 'numpy'.</p> <code>value</code> <code>Optional[Any]</code> <p>Image data corresponding to the image type.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InferenceRequestImage(BaseModel):\n    \"\"\"Image data for inference request.\n\n    Attributes:\n        type (str): The type of image data provided, one of 'url', 'base64', or 'numpy'.\n        value (Optional[Any]): Image data corresponding to the image type.\n    \"\"\"\n\n    type: str = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url', 'base64', or 'numpy'\",\n    )\n    value: Optional[Any] = Field(\n        None,\n        examples=[\"http://www.example-image-url.com\"],\n        description=\"Image data corresponding to the image type, if type = 'url' then value is a string containing the url of an image, else if type = 'base64' then value is a string containing base64 encoded image data, else if type = 'numpy' then value is binary numpy data serialized using pickle.dumps(); array should 3 dimensions, channels last, with values in the range [0,255].\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InstanceSegmentationInferenceRequest","title":"<code>InstanceSegmentationInferenceRequest</code>","text":"<p>               Bases: <code>ObjectDetectionInferenceRequest</code></p> <p>Instance Segmentation inference request.</p> <p>Attributes:</p> Name Type Description <code>mask_decode_mode</code> <code>Optional[str]</code> <p>The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.</p> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>The amount to tradeoff between 0='fast' and 1='accurate'.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InstanceSegmentationInferenceRequest(ObjectDetectionInferenceRequest):\n    \"\"\"Instance Segmentation inference request.\n\n    Attributes:\n        mask_decode_mode (Optional[str]): The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.\n        tradeoff_factor (Optional[float]): The amount to tradeoff between 0='fast' and 1='accurate'.\n    \"\"\"\n\n    mask_decode_mode: Optional[str] = Field(\n        default=\"accurate\",\n        examples=[\"accurate\"],\n        description=\"The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'\",\n    )\n    tradeoff_factor: Optional[float] = Field(\n        default=0.0,\n        examples=[0.5],\n        description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.ObjectDetectionInferenceRequest","title":"<code>ObjectDetectionInferenceRequest</code>","text":"<p>               Bases: <code>CVInferenceRequest</code></p> <p>Object Detection inference request.</p> <p>Attributes:</p> Name Type Description <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>If true, NMS is applied to all detections at once, if false, NMS is applied per class.</p> <code>class_filter</code> <code>Optional[List[str]]</code> <p>If provided, only predictions for the listed classes will be returned.</p> <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>fix_batch_size</code> <code>Optional[bool]</code> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p> <code>iou_threshold</code> <code>Optional[float]</code> <p>The IoU threshold that must be met for a box pair to be considered duplicate during NMS.</p> <code>max_detections</code> <code>Optional[int]</code> <p>The maximum number of detections that will be returned.</p> <code>max_candidates</code> <code>Optional[int]</code> <p>The maximum number of candidate detections passed to NMS.</p> <code>visualization_labels</code> <code>Optional[bool]</code> <p>If true, labels will be rendered on prediction visualizations.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class ObjectDetectionInferenceRequest(CVInferenceRequest):\n    \"\"\"Object Detection inference request.\n\n    Attributes:\n        class_agnostic_nms (Optional[bool]): If true, NMS is applied to all detections at once, if false, NMS is applied per class.\n        class_filter (Optional[List[str]]): If provided, only predictions for the listed classes will be returned.\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        fix_batch_size (Optional[bool]): If true, the batch size will be fixed to the maximum batch size configured for this server.\n        iou_threshold (Optional[float]): The IoU threshold that must be met for a box pair to be considered duplicate during NMS.\n        max_detections (Optional[int]): The maximum number of detections that will be returned.\n        max_candidates (Optional[int]): The maximum number of candidate detections passed to NMS.\n        visualization_labels (Optional[bool]): If true, labels will be rendered on prediction visualizations.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    class_agnostic_nms: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, NMS is applied to all detections at once, if false, NMS is applied per class\",\n    )\n    class_filter: Optional[List[str]] = Field(\n        default=None,\n        examples=[[\"class-1\", \"class-2\", \"class-n\"]],\n        description=\"If provided, only predictions for the listed classes will be returned\",\n    )\n    confidence: Optional[float] = Field(\n        default=0.4,\n        examples=[0.5],\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    fix_batch_size: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the batch size will be fixed to the maximum batch size configured for this server\",\n    )\n    iou_threshold: Optional[float] = Field(\n        default=0.3,\n        examples=[0.5],\n        description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n    )\n    max_detections: Optional[int] = Field(\n        default=300,\n        examples=[300],\n        description=\"The maximum number of detections that will be returned\",\n    )\n    max_candidates: Optional[int] = Field(\n        default=3000,\n        description=\"The maximum number of candidate detections passed to NMS\",\n    )\n    visualization_labels: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, labels will be rendered on prediction visualizations\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n    disable_active_learning: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n    )\n    active_learning_target_dataset: Optional[str] = Field(\n        default=None,\n        examples=[\"my_dataset\"],\n        description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.request_from_type","title":"<code>request_from_type(model_type, request_dict)</code>","text":"<p>Uses original request id</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>def request_from_type(model_type, request_dict):\n    \"\"\"Uses original request id\"\"\"\n    if model_type == \"classification\":\n        request = ClassificationInferenceRequest(**request_dict)\n    elif model_type == \"instance-segmentation\":\n        request = InstanceSegmentationInferenceRequest(**request_dict)\n    elif model_type == \"object-detection\":\n        request = ObjectDetectionInferenceRequest(**request_dict)\n    else:\n        raise ValueError(f\"Unknown task type {model_type}\")\n    request.id = request_dict.get(\"id\", request.id)\n    return request\n</code></pre>"},{"location":"reference/inference/core/entities/requests/moondream2/","title":"Moondream2","text":""},{"location":"reference/inference/core/entities/requests/moondream2/#inference.core.entities.requests.moondream2.Moondream2InferenceRequest","title":"<code>Moondream2InferenceRequest</code>","text":"<p>               Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Moondream 2 zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/moondream2.py</code> <pre><code>class Moondream2InferenceRequest(DynamicClassBaseInferenceRequest):\n    \"\"\"Request for Moondream 2 zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    prompt: str\n</code></pre>"},{"location":"reference/inference/core/entities/requests/owlv2/","title":"Owlv2","text":""},{"location":"reference/inference/core/entities/requests/owlv2/#inference.core.entities.requests.owlv2.OwlV2InferenceRequest","title":"<code>OwlV2InferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>owlv2_version_id</code> <code>Optional[str]</code> <p>The version ID of Gaze to be used for this request.</p> <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> <code>training_data</code> <code>List[TrainingImage]</code> <p>Training data to ground the model on</p> <code>confidence</code> <code>float</code> <p>Confidence threshold to filter predictions by</p> Source code in <code>inference/core/entities/requests/owlv2.py</code> <pre><code>class OwlV2InferenceRequest(BaseRequest):\n    \"\"\"Request for gaze detection inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        owlv2_version_id (Optional[str]): The version ID of Gaze to be used for this request.\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n        training_data (List[TrainingImage]): Training data to ground the model on\n        confidence (float): Confidence threshold to filter predictions by\n    \"\"\"\n\n    owlv2_version_id: Optional[str] = Field(\n        default=OWLV2_VERSION_ID,\n        examples=[\"owlv2-base-patch16-ensemble\"],\n        description=\"The version ID of owlv2 to be used for this request.\",\n    )\n    model_id: Optional[str] = Field(\n        default=None, description=\"Model id to be used in the request.\"\n    )\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage] = Field(\n        description=\"Images to run the model on\"\n    )\n    training_data: List[TrainingImage] = Field(\n        description=\"Training images for the owlvit model to learn form\"\n    )\n    confidence: Optional[float] = Field(\n        default=0.99,\n        examples=[0.99],\n        description=\"Default confidence threshold for owlvit predictions. \"\n        \"Needs to be much higher than you're used to, probably 0.99 - 0.9999\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, return visualized predictions as a base64 string\",\n    )\n    visualization_labels: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, labels will be rendered on prediction visualizations\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"owl2_version_id\") is None:\n            return None\n        return f\"google/{values['owl2_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/perception_encoder/","title":"Perception encoder","text":""},{"location":"reference/inference/core/entities/requests/perception_encoder/#inference.core.entities.requests.perception_encoder.PerceptionEncoderCompareRequest","title":"<code>PerceptionEncoderCompareRequest</code>","text":"<p>               Bases: <code>PerceptionEncoderInferenceRequest</code></p> <p>Request for PERCEPTION_ENCODER comparison.</p> <p>Attributes:</p> Name Type Description <code>subject</code> <code>Union[InferenceRequestImage, str]</code> <p>The type of image data provided, one of 'url' or 'base64'.</p> <code>subject_type</code> <code>str</code> <p>The type of subject, one of 'image' or 'text'.</p> <code>prompt</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]</code> <p>The prompt for comparison.</p> <code>prompt_type</code> <code>str</code> <p>The type of prompt, one of 'image' or 'text'.</p> Source code in <code>inference/core/entities/requests/perception_encoder.py</code> <pre><code>class PerceptionEncoderCompareRequest(PerceptionEncoderInferenceRequest):\n    \"\"\"Request for PERCEPTION_ENCODER comparison.\n\n    Attributes:\n        subject (Union[InferenceRequestImage, str]): The type of image data provided, one of 'url' or 'base64'.\n        subject_type (str): The type of subject, one of 'image' or 'text'.\n        prompt (Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]): The prompt for comparison.\n        prompt_type (str): The type of prompt, one of 'image' or 'text'.\n    \"\"\"\n\n    subject: Union[InferenceRequestImage, str] = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url' or 'base64'\",\n    )\n    subject_type: str = Field(\n        default=\"image\",\n        examples=[\"image\"],\n        description=\"The type of subject, one of 'image' or 'text'\",\n    )\n    prompt: Union[\n        List[InferenceRequestImage],\n        InferenceRequestImage,\n        str,\n        List[str],\n        Dict[str, Union[InferenceRequestImage, str]],\n    ]\n    prompt_type: str = Field(\n        default=\"text\",\n        examples=[\"text\"],\n        description=\"The type of prompt, one of 'image' or 'text'\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/perception_encoder/#inference.core.entities.requests.perception_encoder.PerceptionEncoderImageEmbeddingRequest","title":"<code>PerceptionEncoderImageEmbeddingRequest</code>","text":"<p>               Bases: <code>PerceptionEncoderInferenceRequest</code></p> <p>Request for PERCEPTION_ENCODER image embedding.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) to be embedded.</p> Source code in <code>inference/core/entities/requests/perception_encoder.py</code> <pre><code>class PerceptionEncoderImageEmbeddingRequest(PerceptionEncoderInferenceRequest):\n    \"\"\"Request for PERCEPTION_ENCODER image embedding.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) to be embedded.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n</code></pre>"},{"location":"reference/inference/core/entities/requests/perception_encoder/#inference.core.entities.requests.perception_encoder.PerceptionEncoderInferenceRequest","title":"<code>PerceptionEncoderInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for PERCEPTION_ENCODER inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>clip_version_id</code> <code>Optional[str]</code> <p>The version ID of PERCEPTION_ENCODER to be used for this request.</p> Source code in <code>inference/core/entities/requests/perception_encoder.py</code> <pre><code>class PerceptionEncoderInferenceRequest(BaseRequest):\n    \"\"\"Request for PERCEPTION_ENCODER inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        clip_version_id (Optional[str]): The version ID of PERCEPTION_ENCODER to be used for this request.\n    \"\"\"\n\n    perception_encoder_version_id: Optional[str] = Field(\n        default=PERCEPTION_ENCODER_VERSION_ID,\n        examples=[\"PE-Core-L14-336\"],\n        description=\"The version ID of PERCEPTION_ENCODER to be used for this request. Must be one of RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, ViT-B-32, ViT-L-14-336px, and ViT-L-14.\",\n    )\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"perception_encoder_version_id\") is None:\n            return None\n        return f\"perception_encoder/{values['perception_encoder_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/perception_encoder/#inference.core.entities.requests.perception_encoder.PerceptionEncoderTextEmbeddingRequest","title":"<code>PerceptionEncoderTextEmbeddingRequest</code>","text":"<p>               Bases: <code>PerceptionEncoderInferenceRequest</code></p> <p>Request for PERCEPTION_ENCODER text embedding.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Union[List[str], str]</code> <p>A string or list of strings.</p> Source code in <code>inference/core/entities/requests/perception_encoder.py</code> <pre><code>class PerceptionEncoderTextEmbeddingRequest(PerceptionEncoderInferenceRequest):\n    \"\"\"Request for PERCEPTION_ENCODER text embedding.\n\n    Attributes:\n        text (Union[List[str], str]): A string or list of strings.\n    \"\"\"\n\n    text: Union[List[str], str] = Field(\n        examples=[\"The quick brown fox jumps over the lazy dog\"],\n        description=\"A string or list of strings\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/sam/","title":"Sam","text":""},{"location":"reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamEmbeddingRequest","title":"<code>SamEmbeddingRequest</code>","text":"<p>               Bases: <code>SamInferenceRequest</code></p> <p>SAM embedding request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be embedded.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be embedded used to cache the embedding.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response. Must be one of json or binary.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamEmbeddingRequest(SamInferenceRequest):\n    \"\"\"SAM embedding request.\n\n    Attributes:\n        image (Optional[inference.core.entities.requests.inference.InferenceRequestImage]): The image to be embedded.\n        image_id (Optional[str]): The ID of the image to be embedded used to cache the embedding.\n        format (Optional[str]): The format of the response. Must be one of json or binary.\n    \"\"\"\n\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be embedded\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be embedded used to cache the embedding.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, embedding is returned as a binary numpy array.\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamInferenceRequest","title":"<code>SamInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>SAM inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>sam_version_id</code> <code>Optional[str]</code> <p>The version ID of SAM to be used for this request.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamInferenceRequest(BaseRequest):\n    \"\"\"SAM inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        sam_version_id (Optional[str]): The version ID of SAM to be used for this request.\n    \"\"\"\n\n    sam_version_id: Optional[str] = Field(\n        default=SAM_VERSION_ID,\n        examples=[\"vit_h\"],\n        description=\"The version ID of SAM to be used for this request. Must be one of vit_h, vit_l, or vit_b.\",\n    )\n\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"sam_version_id\") is None:\n            return None\n        return f\"sam/{values['sam_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamSegmentationRequest","title":"<code>SamSegmentationRequest</code>","text":"<p>               Bases: <code>SamInferenceRequest</code></p> <p>SAM segmentation request.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Optional[Union[List[List[List[List[float]]]], Any]]</code> <p>The embeddings to be decoded.</p> <code>embeddings_format</code> <code>Optional[str]</code> <p>The format of the embeddings.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response.</p> <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be segmented.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be segmented used to retrieve cached embeddings.</p> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Whether or not the request includes a mask input.</p> <code>mask_input</code> <code>Optional[Union[List[List[List[float]]], Any]]</code> <p>The set of output masks.</p> <code>mask_input_format</code> <code>Optional[str]</code> <p>The format of the mask input.</p> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>The original size of the image used to generate the embeddings.</p> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>The coordinates of the interactive points used during decoding.</p> <code>point_labels</code> <code>Optional[List[float]]</code> <p>The labels of the interactive points used during decoding.</p> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Whether or not to use the mask input cache.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamSegmentationRequest(SamInferenceRequest):\n    \"\"\"SAM segmentation request.\n\n    Attributes:\n        embeddings (Optional[Union[List[List[List[List[float]]]], Any]]): The embeddings to be decoded.\n        embeddings_format (Optional[str]): The format of the embeddings.\n        format (Optional[str]): The format of the response.\n        image (Optional[InferenceRequestImage]): The image to be segmented.\n        image_id (Optional[str]): The ID of the image to be segmented used to retrieve cached embeddings.\n        has_mask_input (Optional[bool]): Whether or not the request includes a mask input.\n        mask_input (Optional[Union[List[List[List[float]]], Any]]): The set of output masks.\n        mask_input_format (Optional[str]): The format of the mask input.\n        orig_im_size (Optional[List[int]]): The original size of the image used to generate the embeddings.\n        point_coords (Optional[List[List[float]]]): The coordinates of the interactive points used during decoding.\n        point_labels (Optional[List[float]]): The labels of the interactive points used during decoding.\n        use_mask_input_cache (Optional[bool]): Whether or not to use the mask input cache.\n    \"\"\"\n\n    embeddings: Optional[Union[List[List[List[List[float]]]], Any]] = Field(\n        None,\n        examples=[\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\"],\n        description=\"The embeddings to be decoded. The dimensions of the embeddings are 1 x 256 x 64 x 64. If embeddings is not provided, image must be provided.\",\n    )\n    embeddings_format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the embeddings. Must be one of json or binary. If binary, embeddings are expected to be a binary numpy array.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, masks are returned as binary numpy arrays. If json, masks are converted to polygons, then returned as json.\",\n    )\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be segmented. Only required if embeddings are not provided.\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be segmented used to retrieve cached embeddings. If an embedding is cached, it will be used instead of generating a new embedding. If no embedding is cached, a new embedding will be generated and cached.\",\n    )\n    has_mask_input: Optional[bool] = Field(\n        default=False,\n        examples=[True],\n        description=\"Whether or not the request includes a mask input. If true, the mask input must be provided.\",\n    )\n    mask_input: Optional[Union[List[List[List[float]]], Any]] = Field(\n        default=None,\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256. This is the same as the output, low resolution mask from the previous inference.\",\n    )\n    mask_input_format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the mask input. Must be one of json or binary. If binary, mask input is expected to be a binary numpy array.\",\n    )\n    orig_im_size: Optional[List[int]] = Field(\n        default=None,\n        examples=[[640, 320]],\n        description=\"The original size of the image used to generate the embeddings. This is only required if the image is not provided.\",\n    )\n    point_coords: Optional[List[List[float]]] = Field(\n        default=[[0.0, 0.0]],\n        examples=[[[10.0, 10.0]]],\n        description=\"The coordinates of the interactive points used during decoding. Each point (x,y pair) corresponds to a label in point_labels.\",\n    )\n    point_labels: Optional[List[float]] = Field(\n        default=[-1],\n        examples=[[1]],\n        description=\"The labels of the interactive points used during decoding. A 1 represents a positive point (part of the object to be segmented). A -1 represents a negative point (not part of the object to be segmented). Each label corresponds to a point in point_coords.\",\n    )\n    use_mask_input_cache: Optional[bool] = Field(\n        default=True,\n        examples=[True],\n        description=\"Whether or not to use the mask input cache. If true, the mask input cache will be used if it exists. If false, the mask input cache will not be used.\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/sam2/","title":"Sam2","text":""},{"location":"reference/inference/core/entities/requests/sam2/#inference.core.entities.requests.sam2.Sam2EmbeddingRequest","title":"<code>Sam2EmbeddingRequest</code>","text":"<p>               Bases: <code>Sam2InferenceRequest</code></p> <p>SAM embedding request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be embedded.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be embedded used to cache the embedding.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response. Must be one of json or binary.</p> Source code in <code>inference/core/entities/requests/sam2.py</code> <pre><code>class Sam2EmbeddingRequest(Sam2InferenceRequest):\n    \"\"\"SAM embedding request.\n\n    Attributes:\n        image (Optional[inference.core.entities.requests.inference.InferenceRequestImage]): The image to be embedded.\n        image_id (Optional[str]): The ID of the image to be embedded used to cache the embedding.\n        format (Optional[str]): The format of the response. Must be one of json or binary.\n    \"\"\"\n\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be embedded\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be embedded used to cache the embedding.\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/sam2/#inference.core.entities.requests.sam2.Sam2InferenceRequest","title":"<code>Sam2InferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>SAM2 inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>sam2_version_id</code> <code>Optional[str]</code> <p>The version ID of SAM2 to be used for this request.</p> Source code in <code>inference/core/entities/requests/sam2.py</code> <pre><code>class Sam2InferenceRequest(BaseRequest):\n    \"\"\"SAM2 inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        sam2_version_id (Optional[str]): The version ID of SAM2 to be used for this request.\n    \"\"\"\n\n    sam2_version_id: Optional[str] = Field(\n        default=SAM2_VERSION_ID,\n        examples=[\"hiera_large\"],\n        description=\"The version ID of SAM to be used for this request. Must be one of hiera_tiny, hiera_small, hiera_large, hiera_b_plus\",\n    )\n\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"sam_version_id\") is None:\n            return None\n        return f\"sam2/{values['sam_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/sam2/#inference.core.entities.requests.sam2.Sam2SegmentationRequest","title":"<code>Sam2SegmentationRequest</code>","text":"<p>               Bases: <code>Sam2InferenceRequest</code></p> <p>SAM segmentation request.</p> <p>Attributes:</p> Name Type Description <code>format</code> <code>Optional[str]</code> <p>The format of the response.</p> <code>image</code> <code>InferenceRequestImage</code> <p>The image to be segmented.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be segmented used to retrieve cached embeddings.</p> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>The coordinates of the interactive points used during decoding.</p> <code>point_labels</code> <code>Optional[List[float]]</code> <p>The labels of the interactive points used during decoding.</p> Source code in <code>inference/core/entities/requests/sam2.py</code> <pre><code>class Sam2SegmentationRequest(Sam2InferenceRequest):\n    \"\"\"SAM segmentation request.\n\n    Attributes:\n        format (Optional[str]): The format of the response.\n        image (InferenceRequestImage): The image to be segmented.\n        image_id (Optional[str]): The ID of the image to be segmented used to retrieve cached embeddings.\n        point_coords (Optional[List[List[float]]]): The coordinates of the interactive points used during decoding.\n        point_labels (Optional[List[float]]): The labels of the interactive points used during decoding.\n    \"\"\"\n\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of 'json', 'rle', or 'binary'. If binary, masks are returned as binary numpy arrays. If json, masks are converted to polygons. If rle, masks are converted to RLE format.\",\n    )\n    image: InferenceRequestImage = Field(\n        description=\"The image to be segmented.\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be segmented used to retrieve cached embeddings. If an embedding is cached, it will be used instead of generating a new embedding. If no embedding is cached, a new embedding will be generated and cached.\",\n    )\n    prompts: Sam2PromptSet = Field(\n        default=Sam2PromptSet(prompts=None),\n        example=[{\"prompts\": [{\"points\": [{\"x\": 100, \"y\": 100, \"positive\": True}]}]}],\n        description=\"A list of prompts for masks to predict. Each prompt can include a bounding box and / or a set of postive or negative points\",\n    )\n    multimask_output: bool = Field(\n        default=True,\n        examples=[True],\n        description=\"If true, the model will return three masks. \"\n        \"For ambiguous input prompts (such as a single click), this will often \"\n        \"produce better masks than a single prediction. If only a single \"\n        \"mask is needed, the model's predicted quality score can be used \"\n        \"to select the best mask. For non-ambiguous prompts, such as multiple \"\n        \"input prompts, multimask_output=False can give better results.\",\n    )\n\n    save_logits_to_cache: bool = Field(\n        default=False,\n        description=\"If True, saves the low-resolution logits to the cache for potential future use. \"\n        \"This can speed up subsequent requests with similar prompts on the same image. \"\n        \"This feature is ignored if DISABLE_SAM2_LOGITS_CACHE env variable is set True\",\n    )\n    load_logits_from_cache: bool = Field(\n        default=False,\n        description=\"If True, attempts to load previously cached low-resolution logits for the given image and prompt set. \"\n        \"This can significantly speed up inference when making multiple similar requests on the same image. \"\n        \"This feature is ignored if DISABLE_SAM2_LOGITS_CACHE env variable is set True\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/requests/server_state/","title":"Server state","text":""},{"location":"reference/inference/core/entities/requests/server_state/#inference.core.entities.requests.server_state.AddModelRequest","title":"<code>AddModelRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request to add a model to the inference server.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> <code>model_type</code> <code>Optional[str]</code> <p>The type of the model, usually referring to what task the model performs.</p> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key that will be passed to the model during initialization for artifact retrieval.</p> Source code in <code>inference/core/entities/requests/server_state.py</code> <pre><code>class AddModelRequest(BaseModel):\n    \"\"\"Request to add a model to the inference server.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n        model_type (Optional[str]): The type of the model, usually referring to what task the model performs.\n        api_key (Optional[str]): Roboflow API Key that will be passed to the model during initialization for artifact retrieval.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    model_id: str = ModelID\n    model_type: Optional[str] = ModelType\n    api_key: Optional[str] = ApiKey\n</code></pre>"},{"location":"reference/inference/core/entities/requests/server_state/#inference.core.entities.requests.server_state.ClearModelRequest","title":"<code>ClearModelRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request to clear a model from the inference server.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> Source code in <code>inference/core/entities/requests/server_state.py</code> <pre><code>class ClearModelRequest(BaseModel):\n    \"\"\"Request to clear a model from the inference server.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    model_id: str = ModelID\n</code></pre>"},{"location":"reference/inference/core/entities/requests/trocr/","title":"Trocr","text":""},{"location":"reference/inference/core/entities/requests/trocr/#inference.core.entities.requests.trocr.TrOCRInferenceRequest","title":"<code>TrOCRInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>TrOCR inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> Source code in <code>inference/core/entities/requests/trocr.py</code> <pre><code>class TrOCRInferenceRequest(BaseRequest):\n    \"\"\"\n    TrOCR inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    trocr_version_id: Optional[str] = \"trocr-base-printed\"\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"trocr_version_id\") is None:\n            return None\n        return f\"trocr/{values['trocr_version_id']}\"\n</code></pre>"},{"location":"reference/inference/core/entities/requests/yolo_world/","title":"Yolo world","text":""},{"location":"reference/inference/core/entities/requests/yolo_world/#inference.core.entities.requests.yolo_world.YOLOWorldInferenceRequest","title":"<code>YOLOWorldInferenceRequest</code>","text":"<p>               Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Grounding DINO zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/yolo_world.py</code> <pre><code>class YOLOWorldInferenceRequest(DynamicClassBaseInferenceRequest):\n    \"\"\"Request for Grounding DINO zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    yolo_world_version_id: Optional[str] = \"l\"\n    confidence: Optional[float] = DEFAULT_CONFIDENCE\n</code></pre>"},{"location":"reference/inference/core/entities/responses/clip/","title":"Clip","text":""},{"location":"reference/inference/core/entities/responses/clip/#inference.core.entities.responses.clip.ClipCompareResponse","title":"<code>ClipCompareResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Response for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>similarity</code> <code>Union[List[float], Dict[str, float]]</code> <p>Similarity scores.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the similarity scores including preprocessing.</p> Source code in <code>inference/core/entities/responses/clip.py</code> <pre><code>class ClipCompareResponse(InferenceResponse):\n    \"\"\"Response for CLIP comparison.\n\n    Attributes:\n        similarity (Union[List[float], Dict[str, float]]): Similarity scores.\n        time (float): The time in seconds it took to produce the similarity scores including preprocessing.\n    \"\"\"\n\n    similarity: Union[List[float], Dict[str, float]]\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the similarity scores including preprocessing\",\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/clip/#inference.core.entities.responses.clip.ClipEmbeddingResponse","title":"<code>ClipEmbeddingResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Response for CLIP embedding.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>List[List[float]]</code> <p>A list of embeddings, each embedding is a list of floats.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/clip.py</code> <pre><code>class ClipEmbeddingResponse(InferenceResponse):\n    \"\"\"Response for CLIP embedding.\n\n    Attributes:\n        embeddings (List[List[float]]): A list of embeddings, each embedding is a list of floats.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: List[List[float]] = Field(\n        examples=[\"[[0.12, 0.23, 0.34, ..., 0.43]]\"],\n        description=\"A list of embeddings, each embedding is a list of floats\",\n    )\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/gaze/","title":"Gaze","text":""},{"location":"reference/inference/core/entities/responses/gaze/#inference.core.entities.responses.gaze.GazeDetectionInferenceResponse","title":"<code>GazeDetectionInferenceResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[GazeDetectionPrediction]</code> <p>List of gaze detection predictions.</p> <code>time</code> <code>float</code> <p>The processing time (second).</p> Source code in <code>inference/core/entities/responses/gaze.py</code> <pre><code>class GazeDetectionInferenceResponse(BaseModel):\n    \"\"\"Response for gaze detection inference.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.gaze.GazeDetectionPrediction]): List of gaze detection predictions.\n        time (float): The processing time (second).\n    \"\"\"\n\n    predictions: List[GazeDetectionPrediction]\n\n    time: float = Field(description=\"The processing time (second)\")\n    time_face_det: Optional[float] = Field(\n        None, description=\"The face detection time (second)\"\n    )\n    time_gaze_det: Optional[float] = Field(\n        None, description=\"The gaze detection time (second)\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/gaze/#inference.core.entities.responses.gaze.GazeDetectionPrediction","title":"<code>GazeDetectionPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gaze Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>face</code> <code>FaceDetectionPrediction</code> <p>The face prediction.</p> <code>yaw</code> <code>float</code> <p>Yaw (radian) of the detected face.</p> <code>pitch</code> <code>float</code> <p>Pitch (radian) of the detected face.</p> Source code in <code>inference/core/entities/responses/gaze.py</code> <pre><code>class GazeDetectionPrediction(BaseModel):\n    \"\"\"Gaze Detection prediction.\n\n    Attributes:\n        face (inference.core.entities.responses.inference.FaceDetectionPrediction): The face prediction.\n        yaw (float): Yaw (radian) of the detected face.\n        pitch (float): Pitch (radian) of the detected face.\n    \"\"\"\n\n    face: FaceDetectionPrediction\n\n    yaw: float = Field(description=\"Yaw (radian) of the detected face\")\n    pitch: float = Field(description=\"Pitch (radian) of the detected face\")\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/","title":"Inference","text":""},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ClassificationInferenceResponse","title":"<code>ClassificationInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Classification inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[ClassificationPrediction]</code> <p>List of classification predictions.</p> <code>top</code> <code>str</code> <p>The top predicted class label.</p> <code>confidence</code> <code>float</code> <p>The confidence of the top predicted class label.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ClassificationInferenceResponse(CvInferenceResponse, WithVisualizationResponse):\n    \"\"\"Classification inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.ClassificationPrediction]): List of classification predictions.\n        top (str): The top predicted class label.\n        confidence (float): The confidence of the top predicted class label.\n    \"\"\"\n\n    predictions: List[ClassificationPrediction]\n    top: str = Field(\n        description=\"The top predicted class label\", default=\"\"\n    )  # Not making this field optional to avoid breaking change - in other parts of the codebase `model_dump` is called with `exclude_none=True`\n    confidence: float = Field(\n        description=\"The confidence of the top predicted class label\",\n        default=0.0,\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ClassificationPrediction","title":"<code>ClassificationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Classification prediction.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_id</code> <code>int</code> <p>Numeric ID associated with the class label.</p> <code>confidence</code> <code>float</code> <p>The class label confidence as a fraction between 0 and 1.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ClassificationPrediction(BaseModel):\n    \"\"\"Classification prediction.\n\n    Attributes:\n        class_name (str): The predicted class label.\n        class_id (int): Numeric ID associated with the class label.\n        confidence (float): The class label confidence as a fraction between 0 and 1.\n    \"\"\"\n\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n    class_id: int = Field(description=\"Numeric ID associated with the class label\")\n    confidence: float = Field(\n        description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.CvInferenceResponse","title":"<code>CvInferenceResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Computer Vision inference response.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceResponseImage], InferenceResponseImage]</code> <p>Image(s) used in inference.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class CvInferenceResponse(InferenceResponse):\n    \"\"\"Computer Vision inference response.\n\n    Attributes:\n        image (Union[List[inference.core.entities.responses.inference.InferenceResponseImage], inference.core.entities.responses.inference.InferenceResponseImage]): Image(s) used in inference.\n    \"\"\"\n\n    image: Union[List[InferenceResponseImage], InferenceResponseImage]\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.DepthEstimationResponse","title":"<code>DepthEstimationResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response for depth estimation inference.</p> <p>Attributes:</p> Name Type Description <code>normalized_depth</code> <code>List[List[float]]</code> <p>The normalized depth map as a 2D array of floats between 0 and 1.</p> <code>image</code> <code>Optional[str]</code> <p>Base64 encoded visualization of the depth map if visualize_predictions is True.</p> <code>time</code> <code>float</code> <p>The processing time in seconds.</p> <code>visualization</code> <code>Optional[str]</code> <p>Base64 encoded visualization of the depth map if visualize_predictions is True.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class DepthEstimationResponse(BaseModel):\n    \"\"\"Response for depth estimation inference.\n\n    Attributes:\n        normalized_depth (List[List[float]]): The normalized depth map as a 2D array of floats between 0 and 1.\n        image (Optional[str]): Base64 encoded visualization of the depth map if visualize_predictions is True.\n        time (float): The processing time in seconds.\n        visualization (Optional[str]): Base64 encoded visualization of the depth map if visualize_predictions is True.\n    \"\"\"\n\n    normalized_depth: List[List[float]] = Field(\n        description=\"The normalized depth map as a 2D array of floats between 0 and 1\"\n    )\n    image: Optional[str] = Field(\n        None,\n        description=\"Base64 encoded visualization of the depth map if visualize_predictions is True\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.FaceDetectionPrediction","title":"<code>FaceDetectionPrediction</code>","text":"<p>               Bases: <code>ObjectDetectionPrediction</code></p> <p>Face Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>fixed value \"face\".</p> <code>landmarks</code> <code>Union[List[Point], List[Point3D]]</code> <p>The detected face landmarks.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class FaceDetectionPrediction(ObjectDetectionPrediction):\n    \"\"\"Face Detection prediction.\n\n    Attributes:\n        class_name (str): fixed value \"face\".\n        landmarks (Union[List[inference.core.entities.responses.inference.Point], List[inference.core.entities.responses.inference.Point3D]]): The detected face landmarks.\n    \"\"\"\n\n    class_id: Optional[int] = Field(\n        description=\"The class id of the prediction\", default=0\n    )\n    class_name: str = Field(\n        alias=\"class\", default=\"face\", description=\"The predicted class label\"\n    )\n    landmarks: Union[List[Point], List[Point3D]]\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InferenceResponse","title":"<code>InferenceResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base inference response.</p> <p>Attributes:</p> Name Type Description <code>inference_id</code> <code>Optional[str]</code> <p>Unique identifier of inference</p> <code>frame_id</code> <code>Optional[int]</code> <p>The frame id of the image used in inference if the input was a video.</p> <code>time</code> <code>Optional[float]</code> <p>The time in seconds it took to produce the predictions including image preprocessing.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InferenceResponse(BaseModel):\n    \"\"\"Base inference response.\n\n    Attributes:\n        inference_id (Optional[str]): Unique identifier of inference\n        frame_id (Optional[int]): The frame id of the image used in inference if the input was a video.\n        time (Optional[float]): The time in seconds it took to produce the predictions including image preprocessing.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    inference_id: Optional[str] = Field(\n        description=\"Unique identifier of inference\", default=None\n    )\n    frame_id: Optional[int] = Field(\n        default=None,\n        description=\"The frame id of the image used in inference if the input was a video\",\n    )\n    time: Optional[float] = Field(\n        default=None,\n        description=\"The time in seconds it took to produce the predictions including image preprocessing\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InferenceResponseImage","title":"<code>InferenceResponseImage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Inference response image information.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>The original width of the image used in inference.</p> <code>height</code> <code>int</code> <p>The original height of the image used in inference.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InferenceResponseImage(BaseModel):\n    \"\"\"Inference response image information.\n\n    Attributes:\n        width (int): The original width of the image used in inference.\n        height (int): The original height of the image used in inference.\n    \"\"\"\n\n    width: int = Field(description=\"The original width of the image used in inference\")\n    height: int = Field(\n        description=\"The original height of the image used in inference\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InstanceSegmentationInferenceResponse","title":"<code>InstanceSegmentationInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Instance Segmentation inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[InstanceSegmentationPrediction]</code> <p>List of instance segmentation predictions.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InstanceSegmentationInferenceResponse(\n    CvInferenceResponse, WithVisualizationResponse\n):\n    \"\"\"Instance Segmentation inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.InstanceSegmentationPrediction]): List of instance segmentation predictions.\n    \"\"\"\n\n    predictions: List[InstanceSegmentationPrediction]\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InstanceSegmentationPrediction","title":"<code>InstanceSegmentationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Instance Segmentation prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The center x-axis pixel coordinate of the prediction.</p> <code>y</code> <code>float</code> <p>The center y-axis pixel coordinate of the prediction.</p> <code>width</code> <code>float</code> <p>The width of the prediction bounding box in number of pixels.</p> <code>height</code> <code>float</code> <p>The height of the prediction bounding box in number of pixels.</p> <code>confidence</code> <code>float</code> <p>The detection confidence as a fraction between 0 and 1.</p> <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_confidence</code> <code>Union[float, None]</code> <p>The class label confidence as a fraction between 0 and 1.</p> <code>points</code> <code>List[Point]</code> <p>The list of points that make up the instance polygon.</p> <code>class_id</code> <code>int</code> <p>int = Field(description=\"The class id of the prediction\")</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InstanceSegmentationPrediction(BaseModel):\n    \"\"\"Instance Segmentation prediction.\n\n    Attributes:\n        x (float): The center x-axis pixel coordinate of the prediction.\n        y (float): The center y-axis pixel coordinate of the prediction.\n        width (float): The width of the prediction bounding box in number of pixels.\n        height (float): The height of the prediction bounding box in number of pixels.\n        confidence (float): The detection confidence as a fraction between 0 and 1.\n        class_name (str): The predicted class label.\n        class_confidence (Union[float, None]): The class label confidence as a fraction between 0 and 1.\n        points (List[Point]): The list of points that make up the instance polygon.\n        class_id: int = Field(description=\"The class id of the prediction\")\n    \"\"\"\n\n    x: float = Field(description=\"The center x-axis pixel coordinate of the prediction\")\n    y: float = Field(description=\"The center y-axis pixel coordinate of the prediction\")\n    width: float = Field(\n        description=\"The width of the prediction bounding box in number of pixels\"\n    )\n    height: float = Field(\n        description=\"The height of the prediction bounding box in number of pixels\"\n    )\n    confidence: float = Field(\n        description=\"The detection confidence as a fraction between 0 and 1\"\n    )\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n\n    class_confidence: Union[float, None] = Field(\n        None, description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    points: List[Point] = Field(\n        description=\"The list of points that make up the instance polygon\"\n    )\n    class_id: int = Field(description=\"The class id of the prediction\")\n    detection_id: str = Field(\n        description=\"Unique identifier of detection\",\n        default_factory=lambda: str(uuid4()),\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.MultiLabelClassificationInferenceResponse","title":"<code>MultiLabelClassificationInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Multi-label Classification inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>Dict[str, MultiLabelClassificationPrediction]</code> <p>Dictionary of multi-label classification predictions.</p> <code>predicted_classes</code> <code>List[str]</code> <p>The list of predicted classes.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class MultiLabelClassificationInferenceResponse(\n    CvInferenceResponse, WithVisualizationResponse\n):\n    \"\"\"Multi-label Classification inference response.\n\n    Attributes:\n        predictions (Dict[str, inference.core.entities.responses.inference.MultiLabelClassificationPrediction]): Dictionary of multi-label classification predictions.\n        predicted_classes (List[str]): The list of predicted classes.\n    \"\"\"\n\n    predictions: Dict[str, MultiLabelClassificationPrediction]\n    predicted_classes: List[str] = Field(description=\"The list of predicted classes\")\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.MultiLabelClassificationPrediction","title":"<code>MultiLabelClassificationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Multi-label Classification prediction.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>float</code> <p>The class label confidence as a fraction between 0 and 1.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class MultiLabelClassificationPrediction(BaseModel):\n    \"\"\"Multi-label Classification prediction.\n\n    Attributes:\n        confidence (float): The class label confidence as a fraction between 0 and 1.\n    \"\"\"\n\n    confidence: float = Field(\n        description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    class_id: int = Field(description=\"Numeric ID associated with the class label\")\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ObjectDetectionInferenceResponse","title":"<code>ObjectDetectionInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Object Detection inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[ObjectDetectionPrediction]</code> <p>List of object detection predictions.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ObjectDetectionInferenceResponse(CvInferenceResponse, WithVisualizationResponse):\n    \"\"\"Object Detection inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.ObjectDetectionPrediction]): List of object detection predictions.\n    \"\"\"\n\n    predictions: List[ObjectDetectionPrediction]\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ObjectDetectionPrediction","title":"<code>ObjectDetectionPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Object Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The center x-axis pixel coordinate of the prediction.</p> <code>y</code> <code>float</code> <p>The center y-axis pixel coordinate of the prediction.</p> <code>width</code> <code>float</code> <p>The width of the prediction bounding box in number of pixels.</p> <code>height</code> <code>float</code> <p>The height of the prediction bounding box in number of pixels.</p> <code>confidence</code> <code>float</code> <p>The detection confidence as a fraction between 0 and 1.</p> <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_confidence</code> <code>Union[float, None]</code> <p>The class label confidence as a fraction between 0 and 1.</p> <code>class_id</code> <code>int</code> <p>The class id of the prediction</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ObjectDetectionPrediction(BaseModel):\n    \"\"\"Object Detection prediction.\n\n    Attributes:\n        x (float): The center x-axis pixel coordinate of the prediction.\n        y (float): The center y-axis pixel coordinate of the prediction.\n        width (float): The width of the prediction bounding box in number of pixels.\n        height (float): The height of the prediction bounding box in number of pixels.\n        confidence (float): The detection confidence as a fraction between 0 and 1.\n        class_name (str): The predicted class label.\n        class_confidence (Union[float, None]): The class label confidence as a fraction between 0 and 1.\n        class_id (int): The class id of the prediction\n    \"\"\"\n\n    x: float = Field(description=\"The center x-axis pixel coordinate of the prediction\")\n    y: float = Field(description=\"The center y-axis pixel coordinate of the prediction\")\n    width: float = Field(\n        description=\"The width of the prediction bounding box in number of pixels\"\n    )\n    height: float = Field(\n        description=\"The height of the prediction bounding box in number of pixels\"\n    )\n    confidence: float = Field(\n        description=\"The detection confidence as a fraction between 0 and 1\"\n    )\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n\n    class_confidence: Union[float, None] = Field(\n        None, description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    class_id: int = Field(description=\"The class id of the prediction\")\n    tracker_id: Optional[int] = Field(\n        description=\"The tracker id of the prediction if tracking is enabled\",\n        default=None,\n    )\n    detection_id: str = Field(\n        description=\"Unique identifier of detection\",\n        default_factory=lambda: str(uuid4()),\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.Point","title":"<code>Point</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Point coordinates.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The x-axis pixel coordinate of the point.</p> <code>y</code> <code>float</code> <p>The y-axis pixel coordinate of the point.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class Point(BaseModel):\n    \"\"\"Point coordinates.\n\n    Attributes:\n        x (float): The x-axis pixel coordinate of the point.\n        y (float): The y-axis pixel coordinate of the point.\n    \"\"\"\n\n    x: float = Field(description=\"The x-axis pixel coordinate of the point\")\n    y: float = Field(description=\"The y-axis pixel coordinate of the point\")\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.Point3D","title":"<code>Point3D</code>","text":"<p>               Bases: <code>Point</code></p> <p>3D Point coordinates.</p> <p>Attributes:</p> Name Type Description <code>z</code> <code>float</code> <p>The z-axis pixel coordinate of the point.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class Point3D(Point):\n    \"\"\"3D Point coordinates.\n\n    Attributes:\n        z (float): The z-axis pixel coordinate of the point.\n    \"\"\"\n\n    z: float = Field(description=\"The z-axis pixel coordinate of the point\")\n</code></pre>"},{"location":"reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.WithVisualizationResponse","title":"<code>WithVisualizationResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response with visualization.</p> <p>Attributes:</p> Name Type Description <code>visualization</code> <code>Optional[Any]</code> <p>Base64 encoded string containing prediction visualization image data.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class WithVisualizationResponse(BaseModel):\n    \"\"\"Response with visualization.\n\n    Attributes:\n        visualization (Optional[Any]): Base64 encoded string containing prediction visualization image data.\n    \"\"\"\n\n    visualization: Optional[Any] = Field(\n        default=None,\n        description=\"Base64 encoded string containing prediction visualization image data\",\n    )\n\n    @field_serializer(\"visualization\", when_used=\"json\")\n    def serialize_visualisation(self, visualization: Optional[Any]) -&gt; Optional[str]:\n        if visualization is None:\n            return None\n        return base64.b64encode(visualization).decode(\"utf-8\")\n</code></pre>"},{"location":"reference/inference/core/entities/responses/notebooks/","title":"Notebooks","text":""},{"location":"reference/inference/core/entities/responses/notebooks/#inference.core.entities.responses.notebooks.NotebookStartResponse","title":"<code>NotebookStartResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for notebook start request</p> Source code in <code>inference/core/entities/responses/notebooks.py</code> <pre><code>class NotebookStartResponse(BaseModel):\n    \"\"\"Response model for notebook start request\"\"\"\n\n    success: str = Field(..., description=\"Status of the request\")\n    message: str = Field(..., description=\"Message of the request\", optional=True)\n</code></pre>"},{"location":"reference/inference/core/entities/responses/ocr/","title":"Ocr","text":""},{"location":"reference/inference/core/entities/responses/ocr/#inference.core.entities.responses.ocr.OCRInferenceResponse","title":"<code>OCRInferenceResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>OCR Inference response.</p> <p>Attributes:</p> Name Type Description <code>result</code> <code>str</code> <p>The combined OCR recognition result.</p> <code>predictions</code> <code>List[ObjectDetectionPrediction]</code> <p>List of objects detected by OCR</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the inference including preprocessing</p> Source code in <code>inference/core/entities/responses/ocr.py</code> <pre><code>class OCRInferenceResponse(BaseModel):\n    \"\"\"\n    OCR Inference response.\n\n    Attributes:\n        result (str): The combined OCR recognition result.\n        predictions (List[ObjectDetectionPrediction]): List of objects detected by OCR\n        time (float): The time in seconds it took to produce the inference including preprocessing\n    \"\"\"\n\n    result: str = Field(description=\"The combined OCR recognition result.\")\n    image: Optional[InferenceResponseImage] = Field(\n        description=\"Metadata about input image dimensions\", default=None\n    )\n    predictions: Optional[List[ObjectDetectionPrediction]] = Field(\n        description=\"List of objects detected by OCR\",\n        default=None,\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the inference including preprocessing.\"\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/perception_encoder/","title":"Perception encoder","text":""},{"location":"reference/inference/core/entities/responses/perception_encoder/#inference.core.entities.responses.perception_encoder.PerceptionEncoderCompareResponse","title":"<code>PerceptionEncoderCompareResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Response for PERCEPTION_ENCODER comparison.</p> <p>Attributes:</p> Name Type Description <code>similarity</code> <code>Union[List[float], Dict[str, float]]</code> <p>Similarity scores.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the similarity scores including preprocessing.</p> Source code in <code>inference/core/entities/responses/perception_encoder.py</code> <pre><code>class PerceptionEncoderCompareResponse(InferenceResponse):\n    \"\"\"Response for PERCEPTION_ENCODER comparison.\n\n    Attributes:\n        similarity (Union[List[float], Dict[str, float]]): Similarity scores.\n        time (float): The time in seconds it took to produce the similarity scores including preprocessing.\n    \"\"\"\n\n    similarity: Union[List[float], Dict[str, float]]\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the similarity scores including preprocessing\",\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/perception_encoder/#inference.core.entities.responses.perception_encoder.PerceptionEncoderEmbeddingResponse","title":"<code>PerceptionEncoderEmbeddingResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Response for PERCEPTION_ENCODER embedding.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>List[List[float]]</code> <p>A list of embeddings, each embedding is a list of floats.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/perception_encoder.py</code> <pre><code>class PerceptionEncoderEmbeddingResponse(InferenceResponse):\n    \"\"\"Response for PERCEPTION_ENCODER embedding.\n\n    Attributes:\n        embeddings (List[List[float]]): A list of embeddings, each embedding is a list of floats.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: List[List[float]] = Field(\n        examples=[\"[[0.12, 0.23, 0.34, ..., 0.43]]\"],\n        description=\"A list of embeddings, each embedding is a list of floats\",\n    )\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\",\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/sam/","title":"Sam","text":""},{"location":"reference/inference/core/entities/responses/sam/#inference.core.entities.responses.sam.SamEmbeddingResponse","title":"<code>SamEmbeddingResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM embedding response.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Union[List[List[List[List[float]]]], Any]</code> <p>The SAM embedding.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam.py</code> <pre><code>class SamEmbeddingResponse(BaseModel):\n    \"\"\"SAM embedding response.\n\n    Attributes:\n        embeddings (Union[List[List[List[List[float]]]], Any]): The SAM embedding.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: Union[List[List[List[List[float]]]], Any] = Field(\n        examples=[\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\"],\n        description=\"If request format is json, embeddings is a series of nested lists representing the SAM embedding. If request format is binary, embeddings is a binary numpy array. The dimensions of the embedding are 1 x 256 x 64 x 64.\",\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/sam/#inference.core.entities.responses.sam.SamSegmentationResponse","title":"<code>SamSegmentationResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM segmentation response.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output masks.</p> <code>low_res_masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output low-resolution masks.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the segmentation including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam.py</code> <pre><code>class SamSegmentationResponse(BaseModel):\n    \"\"\"SAM segmentation response.\n\n    Attributes:\n        masks (Union[List[List[List[int]]], Any]): The set of output masks.\n        low_res_masks (Union[List[List[List[int]]], Any]): The set of output low-resolution masks.\n        time (float): The time in seconds it took to produce the segmentation including preprocessing.\n    \"\"\"\n\n    masks: Union[List[List[List[int]]], Any] = Field(\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are the same as the dimensions of the input image.\",\n    )\n    low_res_masks: Union[List[List[List[int]]], Any] = Field(\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256\",\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the segmentation including preprocessing\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/sam2/","title":"Sam2","text":""},{"location":"reference/inference/core/entities/responses/sam2/#inference.core.entities.responses.sam2.Sam2EmbeddingResponse","title":"<code>Sam2EmbeddingResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM embedding response.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Union[List[List[List[List[float]]]], Any]</code> <p>The SAM embedding.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam2.py</code> <pre><code>class Sam2EmbeddingResponse(BaseModel):\n    \"\"\"SAM embedding response.\n\n    Attributes:\n        embeddings (Union[List[List[List[List[float]]]], Any]): The SAM embedding.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    image_id: str = Field(description=\"Image id embeddings are cached to\")\n    time: float = Field(\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/sam2/#inference.core.entities.responses.sam2.Sam2SegmentationPrediction","title":"<code>Sam2SegmentationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM segmentation prediction.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>Union[List[List[List[int]]], Dict[str, Any], Any]</code> <p>Mask data - either polygon coordinates or RLE encoding.</p> <code>confidence</code> <code>float</code> <p>Masks confidences.</p> <code>format</code> <code>Optional[str]</code> <p>Format of the mask data: 'polygon' or 'rle'.</p> Source code in <code>inference/core/entities/responses/sam2.py</code> <pre><code>class Sam2SegmentationPrediction(BaseModel):\n    \"\"\"SAM segmentation prediction.\n\n    Attributes:\n        masks (Union[List[List[List[int]]], Dict[str, Any], Any]): Mask data - either polygon coordinates or RLE encoding.\n        confidence (float): Masks confidences.\n        format (Optional[str]): Format of the mask data: 'polygon' or 'rle'.\n    \"\"\"\n\n    masks: Union[List[List[List[int]]], Dict[str, Any]] = Field(\n        description=\"If polygon format, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If rle format, masks is a dictionary with the keys 'size' and 'counts' containing the size and counts of the RLE encoding.\"\n    )\n    confidence: float = Field(description=\"Masks confidences\")\n    format: Optional[str] = Field(\n        default=\"polygon\", description=\"Format of the mask data: 'polygon' or 'rle'\"\n    )\n</code></pre>"},{"location":"reference/inference/core/entities/responses/server_state/","title":"Server state","text":""},{"location":"reference/inference/core/entities/responses/server_state/#inference.core.entities.responses.server_state.ServerVersionInfo","title":"<code>ServerVersionInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Server version information.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Server name.</p> <code>version</code> <code>str</code> <p>Server version.</p> <code>uuid</code> <code>str</code> <p>Server UUID.</p> Source code in <code>inference/core/entities/responses/server_state.py</code> <pre><code>class ServerVersionInfo(BaseModel):\n    \"\"\"Server version information.\n\n    Attributes:\n        name (str): Server name.\n        version (str): Server version.\n        uuid (str): Server UUID.\n    \"\"\"\n\n    name: str = Field(examples=[\"Roboflow Inference Server\"])\n    version: str = Field(examples=[\"0.0.1\"])\n    uuid: str = Field(examples=[\"9c18c6f4-2266-41fb-8a0f-c12ae28f6fbe\"])\n</code></pre>"},{"location":"reference/inference/core/interfaces/base/","title":"Base","text":""},{"location":"reference/inference/core/interfaces/base/#inference.core.interfaces.base.BaseInterface","title":"<code>BaseInterface</code>","text":"<p>Base interface class which accepts a model manager on initialization</p> Source code in <code>inference/core/interfaces/base.py</code> <pre><code>class BaseInterface:\n    \"\"\"Base interface class which accepts a model manager on initialization\"\"\"\n\n    def __init__(self, model_manager: ModelManager) -&gt; None:\n        self.model_manager = model_manager\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/camera/","title":"Camera","text":""},{"location":"reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream","title":"<code>WebcamStream</code>","text":"<p>Class to handle webcam streaming using a separate thread.</p> <p>Attributes:</p> Name Type Description <code>stream_id</code> <code>int</code> <p>The ID of the webcam stream.</p> <code>frame_id</code> <code>int</code> <p>A counter for the current frame.</p> <code>vcap</code> <code>VideoCapture</code> <p>OpenCV video capture object.</p> <code>width</code> <code>int</code> <p>The width of the video frame.</p> <code>height</code> <code>int</code> <p>The height of the video frame.</p> <code>fps_input_stream</code> <code>int</code> <p>Frames per second of the input stream.</p> <code>grabbed</code> <code>bool</code> <p>A flag indicating if a frame was successfully grabbed.</p> <code>frame</code> <code>array</code> <p>The current frame as a NumPy array.</p> <code>pil_image</code> <code>Image</code> <p>The current frame as a PIL image.</p> <code>stopped</code> <code>bool</code> <p>A flag indicating if the stream is stopped.</p> <code>t</code> <code>Thread</code> <p>The thread used to update the stream.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>class WebcamStream:\n    \"\"\"Class to handle webcam streaming using a separate thread.\n\n    Attributes:\n        stream_id (int): The ID of the webcam stream.\n        frame_id (int): A counter for the current frame.\n        vcap (VideoCapture): OpenCV video capture object.\n        width (int): The width of the video frame.\n        height (int): The height of the video frame.\n        fps_input_stream (int): Frames per second of the input stream.\n        grabbed (bool): A flag indicating if a frame was successfully grabbed.\n        frame (array): The current frame as a NumPy array.\n        pil_image (Image): The current frame as a PIL image.\n        stopped (bool): A flag indicating if the stream is stopped.\n        t (Thread): The thread used to update the stream.\n    \"\"\"\n\n    def __init__(self, stream_id=0, enforce_fps=False):\n        \"\"\"Initialize the webcam stream.\n\n        Args:\n            stream_id (int, optional): The ID of the webcam stream. Defaults to 0.\n        \"\"\"\n        self.stream_id = stream_id\n        self.enforce_fps = enforce_fps\n        self.frame_id = 0\n        self.vcap = cv2.VideoCapture(self.stream_id)\n\n        for key in os.environ:\n            if key.startswith(\"CV2_CAP_PROP\"):\n                opencv_prop = key[4:]\n                opencv_constant = getattr(cv2, opencv_prop, None)\n                if opencv_constant is not None:\n                    value = int(os.getenv(key))\n                    self.vcap.set(opencv_constant, value)\n                    logger.info(f\"set {opencv_prop} to {value}\")\n                else:\n                    logger.warning(f\"Property {opencv_prop} not found in cv2\")\n\n        self.width = int(self.vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.height = int(self.vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        self.file_mode = self.vcap.get(cv2.CAP_PROP_FRAME_COUNT) &gt; 0\n        if self.enforce_fps and not self.file_mode:\n            logger.warning(\n                \"Ignoring enforce_fps flag for this stream. It is not compatible with streams and will cause the process to crash\"\n            )\n            self.enforce_fps = False\n        self.max_fps = None\n        if self.vcap.isOpened() is False:\n            logger.debug(\"[Exiting]: Error accessing webcam stream.\")\n            exit(0)\n        self.fps_input_stream = int(self.vcap.get(cv2.CAP_PROP_FPS))\n        logger.debug(\n            \"FPS of webcam hardware/input stream: {}\".format(self.fps_input_stream)\n        )\n        self.grabbed, self.frame = self.vcap.read()\n        self.pil_image = Image.fromarray(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB))\n        if self.grabbed is False:\n            logger.debug(\"[Exiting] No more frames to read\")\n            exit(0)\n        self.stopped = True\n        self.t = Thread(target=self.update, args=())\n        self.t.daemon = True\n\n    def start(self):\n        \"\"\"Start the thread for reading frames.\"\"\"\n        self.stopped = False\n        self.t.start()\n\n    def update(self):\n        \"\"\"Update the frame by reading from the webcam.\"\"\"\n        frame_id = 0\n        next_frame_time = 0\n        t0 = time.perf_counter()\n        while True:\n            t1 = time.perf_counter()\n            if self.stopped is True:\n                break\n\n            self.grabbed = self.vcap.grab()\n            if self.grabbed is False:\n                logger.debug(\"[Exiting] No more frames to read\")\n                self.stopped = True\n                break\n            frame_id += 1\n            # We can't retrieve each frame on nano and other lower powered devices quickly enough to keep up with the stream.\n            # By default, we will only retrieve frames when we'll be ready process them (determined by self.max_fps).\n            if t1 &gt; next_frame_time:\n                ret, frame = self.vcap.retrieve()\n                if frame is None:\n                    logger.debug(\"[Exiting] Frame not available for read\")\n                    self.stopped = True\n                    break\n                logger.debug(\n                    f\"retrieved frame {frame_id}, effective FPS: {frame_id / (t1 - t0):.2f}\"\n                )\n                self.frame_id = frame_id\n                self.frame = frame\n                while self.file_mode and self.enforce_fps and self.max_fps is None:\n                    # sleep until we have processed the first frame and we know what our FPS should be\n                    time.sleep(0.01)\n                if self.max_fps is None:\n                    self.max_fps = 30\n                next_frame_time = t1 + (1 / self.max_fps) + 0.02\n            if self.file_mode:\n                t2 = time.perf_counter()\n                if self.enforce_fps:\n                    # when enforce_fps is true, grab video frames 1:1 with inference speed\n                    time_to_sleep = next_frame_time - t2\n                else:\n                    # otherwise, grab at native FPS of the video file\n                    time_to_sleep = (1 / self.fps_input_stream) - (t2 - t1)\n                if time_to_sleep &gt; 0:\n                    time.sleep(time_to_sleep)\n        self.vcap.release()\n\n    def read_opencv(self):\n        \"\"\"Read the current frame using OpenCV.\n\n        Returns:\n            array, int: The current frame as a NumPy array, and the frame ID.\n        \"\"\"\n        return self.frame, self.frame_id\n\n    def stop(self):\n        \"\"\"Stop the webcam stream.\"\"\"\n        self.stopped = True\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.__init__","title":"<code>__init__(stream_id=0, enforce_fps=False)</code>","text":"<p>Initialize the webcam stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_id</code> <code>int</code> <p>The ID of the webcam stream. Defaults to 0.</p> <code>0</code> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def __init__(self, stream_id=0, enforce_fps=False):\n    \"\"\"Initialize the webcam stream.\n\n    Args:\n        stream_id (int, optional): The ID of the webcam stream. Defaults to 0.\n    \"\"\"\n    self.stream_id = stream_id\n    self.enforce_fps = enforce_fps\n    self.frame_id = 0\n    self.vcap = cv2.VideoCapture(self.stream_id)\n\n    for key in os.environ:\n        if key.startswith(\"CV2_CAP_PROP\"):\n            opencv_prop = key[4:]\n            opencv_constant = getattr(cv2, opencv_prop, None)\n            if opencv_constant is not None:\n                value = int(os.getenv(key))\n                self.vcap.set(opencv_constant, value)\n                logger.info(f\"set {opencv_prop} to {value}\")\n            else:\n                logger.warning(f\"Property {opencv_prop} not found in cv2\")\n\n    self.width = int(self.vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.height = int(self.vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.file_mode = self.vcap.get(cv2.CAP_PROP_FRAME_COUNT) &gt; 0\n    if self.enforce_fps and not self.file_mode:\n        logger.warning(\n            \"Ignoring enforce_fps flag for this stream. It is not compatible with streams and will cause the process to crash\"\n        )\n        self.enforce_fps = False\n    self.max_fps = None\n    if self.vcap.isOpened() is False:\n        logger.debug(\"[Exiting]: Error accessing webcam stream.\")\n        exit(0)\n    self.fps_input_stream = int(self.vcap.get(cv2.CAP_PROP_FPS))\n    logger.debug(\n        \"FPS of webcam hardware/input stream: {}\".format(self.fps_input_stream)\n    )\n    self.grabbed, self.frame = self.vcap.read()\n    self.pil_image = Image.fromarray(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB))\n    if self.grabbed is False:\n        logger.debug(\"[Exiting] No more frames to read\")\n        exit(0)\n    self.stopped = True\n    self.t = Thread(target=self.update, args=())\n    self.t.daemon = True\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.read_opencv","title":"<code>read_opencv()</code>","text":"<p>Read the current frame using OpenCV.</p> <p>Returns:</p> Type Description <p>array, int: The current frame as a NumPy array, and the frame ID.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def read_opencv(self):\n    \"\"\"Read the current frame using OpenCV.\n\n    Returns:\n        array, int: The current frame as a NumPy array, and the frame ID.\n    \"\"\"\n    return self.frame, self.frame_id\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.start","title":"<code>start()</code>","text":"<p>Start the thread for reading frames.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def start(self):\n    \"\"\"Start the thread for reading frames.\"\"\"\n    self.stopped = False\n    self.t.start()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.stop","title":"<code>stop()</code>","text":"<p>Stop the webcam stream.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the webcam stream.\"\"\"\n    self.stopped = True\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.update","title":"<code>update()</code>","text":"<p>Update the frame by reading from the webcam.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def update(self):\n    \"\"\"Update the frame by reading from the webcam.\"\"\"\n    frame_id = 0\n    next_frame_time = 0\n    t0 = time.perf_counter()\n    while True:\n        t1 = time.perf_counter()\n        if self.stopped is True:\n            break\n\n        self.grabbed = self.vcap.grab()\n        if self.grabbed is False:\n            logger.debug(\"[Exiting] No more frames to read\")\n            self.stopped = True\n            break\n        frame_id += 1\n        # We can't retrieve each frame on nano and other lower powered devices quickly enough to keep up with the stream.\n        # By default, we will only retrieve frames when we'll be ready process them (determined by self.max_fps).\n        if t1 &gt; next_frame_time:\n            ret, frame = self.vcap.retrieve()\n            if frame is None:\n                logger.debug(\"[Exiting] Frame not available for read\")\n                self.stopped = True\n                break\n            logger.debug(\n                f\"retrieved frame {frame_id}, effective FPS: {frame_id / (t1 - t0):.2f}\"\n            )\n            self.frame_id = frame_id\n            self.frame = frame\n            while self.file_mode and self.enforce_fps and self.max_fps is None:\n                # sleep until we have processed the first frame and we know what our FPS should be\n                time.sleep(0.01)\n            if self.max_fps is None:\n                self.max_fps = 30\n            next_frame_time = t1 + (1 / self.max_fps) + 0.02\n        if self.file_mode:\n            t2 = time.perf_counter()\n            if self.enforce_fps:\n                # when enforce_fps is true, grab video frames 1:1 with inference speed\n                time_to_sleep = next_frame_time - t2\n            else:\n                # otherwise, grab at native FPS of the video file\n                time_to_sleep = (1 / self.fps_input_stream) - (t2 - t1)\n            if time_to_sleep &gt; 0:\n                time.sleep(time_to_sleep)\n    self.vcap.release()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/entities/","title":"Entities","text":""},{"location":"reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.StatusUpdate","title":"<code>StatusUpdate</code>  <code>dataclass</code>","text":"<p>Represents a status update event in the system.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>datetime</code> <p>The timestamp when the status update was created.</p> <code>severity</code> <code>UpdateSeverity</code> <p>The severity level of the update.</p> <code>event_type</code> <code>str</code> <p>A string representing the type of the event.</p> <code>payload</code> <code>dict</code> <p>A dictionary containing data relevant to the update.</p> <code>context</code> <code>str</code> <p>A string providing additional context about the update.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass StatusUpdate:\n    \"\"\"Represents a status update event in the system.\n\n    Attributes:\n        timestamp (datetime): The timestamp when the status update was created.\n        severity (UpdateSeverity): The severity level of the update.\n        event_type (str): A string representing the type of the event.\n        payload (dict): A dictionary containing data relevant to the update.\n        context (str): A string providing additional context about the update.\n    \"\"\"\n\n    timestamp: datetime\n    severity: UpdateSeverity\n    event_type: str\n    payload: dict\n    context: str\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.UpdateSeverity","title":"<code>UpdateSeverity</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for defining different levels of update severity.</p> <p>Attributes:</p> Name Type Description <code>DEBUG</code> <code>int</code> <p>A debugging severity level.</p> <code>INFO</code> <code>int</code> <p>An informational severity level.</p> <code>WARNING</code> <code>int</code> <p>A warning severity level.</p> <code>ERROR</code> <code>int</code> <p>An error severity level.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>class UpdateSeverity(Enum):\n    \"\"\"Enumeration for defining different levels of update severity.\n\n    Attributes:\n        DEBUG (int): A debugging severity level.\n        INFO (int): An informational severity level.\n        WARNING (int): A warning severity level.\n        ERROR (int): An error severity level.\n    \"\"\"\n\n    DEBUG = logging.DEBUG\n    INFO = logging.INFO\n    WARNING = logging.WARNING\n    ERROR = logging.ERROR\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.VideoFrame","title":"<code>VideoFrame</code>  <code>dataclass</code>","text":"<p>Represents a single frame of video data.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>ndarray</code> <p>The image data of the frame as a NumPy array.</p> <code>frame_id</code> <code>FrameID</code> <p>A unique identifier for the frame.</p> <code>frame_timestamp</code> <code>FrameTimestamp</code> <p>The timestamp when the frame was captured.</p> <code>source_id</code> <code>int</code> <p>The index of the video_reference element which was passed to InferencePipeline for this frame (useful when multiple streams are passed to InferencePipeline).</p> <code>fps</code> <code>Optional[float]</code> <p>declared FPS of source (if possible to be acquired)</p> <code>measured_fps</code> <code>Optional[float]</code> <p>measured FPS of live stream</p> <code>comes_from_video_file</code> <code>Optional[bool]</code> <p>flag to determine if frame comes from video file</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass VideoFrame:\n    \"\"\"Represents a single frame of video data.\n\n    Attributes:\n        image (np.ndarray): The image data of the frame as a NumPy array.\n        frame_id (FrameID): A unique identifier for the frame.\n        frame_timestamp (FrameTimestamp): The timestamp when the frame was captured.\n        source_id (int): The index of the video_reference element which was passed to InferencePipeline for this frame\n            (useful when multiple streams are passed to InferencePipeline).\n        fps (Optional[float]): declared FPS of source (if possible to be acquired)\n        measured_fps (Optional[float]): measured FPS of live stream\n        comes_from_video_file (Optional[bool]): flag to determine if frame comes from video file\n    \"\"\"\n\n    image: np.ndarray\n    frame_id: FrameID\n    frame_timestamp: FrameTimestamp\n    # TODO: in next major version of inference replace `fps` with `declared_fps`\n    fps: Optional[float] = None\n    measured_fps: Optional[float] = None\n    source_id: Optional[int] = None\n    comes_from_video_file: Optional[bool] = None\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/utils/","title":"Utils","text":""},{"location":"reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.RateLimiter","title":"<code>RateLimiter</code>","text":"<p>Implements rate upper-bound rate limiting by ensuring estimate_next_tick_delay() to be at min 1 / desired_fps, not letting the client obeying outcomes to exceed assumed rate.</p> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>class RateLimiter:\n    \"\"\"\n    Implements rate upper-bound rate limiting by ensuring estimate_next_tick_delay()\n    to be at min 1 / desired_fps, not letting the client obeying outcomes to exceed\n    assumed rate.\n    \"\"\"\n\n    def __init__(self, desired_fps: Union[float, int]):\n        self._desired_fps = max(desired_fps, MINIMAL_FPS)\n        self._last_tick: Optional[float] = None\n\n    def tick(self) -&gt; None:\n        self._last_tick = time.monotonic()\n\n    def estimate_next_action_delay(self) -&gt; float:\n        if self._last_tick is None:\n            return 0.0\n        desired_delay = 1 / self._desired_fps\n        time_since_last_tick = time.monotonic() - self._last_tick\n        return max(desired_delay - time_since_last_tick, 0.0)\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.VideoSourcesManager","title":"<code>VideoSourcesManager</code>","text":"<p>This class should be treated as internal building block of stream multiplexer - not for external use.</p> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>class VideoSourcesManager:\n    \"\"\"\n    This class should be treated as internal building block of stream multiplexer - not for external use.\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        video_sources: VideoSources,\n        should_stop: Callable[[], bool],\n        on_reconnection_error: Callable[[Optional[int], SourceConnectionError], None],\n    ) -&gt; \"VideoSourcesManager\":\n        return cls(\n            video_sources=video_sources,\n            should_stop=should_stop,\n            on_reconnection_error=on_reconnection_error,\n        )\n\n    def __init__(\n        self,\n        video_sources: VideoSources,\n        should_stop: Callable[[], bool],\n        on_reconnection_error: Callable[[Optional[int], SourceConnectionError], None],\n    ):\n        self._video_sources = video_sources\n        self._reconnection_threads: Dict[int, Thread] = {}\n        self._external_should_stop = should_stop\n        self._on_reconnection_error = on_reconnection_error\n        self._enforce_stop: Dict[int, bool] = {}\n        self._ended_sources: Set[int] = set()\n        self._threads_to_join: Set[int] = set()\n        self._last_batch_yielded_time = datetime.now()\n\n    def retrieve_frames_from_sources(\n        self,\n        batch_collection_timeout: Optional[float],\n    ) -&gt; Optional[List[VideoFrame]]:\n        batch_frames = []\n        if batch_collection_timeout is not None:\n            batch_timeout_moment = self._last_batch_yielded_time + timedelta(\n                seconds=batch_collection_timeout\n            )\n        else:\n            batch_timeout_moment = None\n        for source_ord, (source, source_should_reconnect) in enumerate(\n            zip(self._video_sources.all_sources, self._video_sources.allow_reconnection)\n        ):\n            if self._external_should_stop():\n                self.join_all_reconnection_threads(include_not_finished=True)\n                return None\n            if self._is_source_inactive(source_ord=source_ord):\n                continue\n            batch_time_left = (\n                None\n                if batch_timeout_moment is None\n                else max((batch_timeout_moment - datetime.now()).total_seconds(), 0.0)\n            )\n            try:\n                frame = source.read_frame(timeout=batch_time_left)\n                if frame is not None:\n                    batch_frames.append(frame)\n            except EndOfStreamError:\n                self._register_end_of_stream(source_ord=source_ord)\n        self.join_all_reconnection_threads()\n        self._last_batch_yielded_time = datetime.now()\n        return batch_frames\n\n    def all_sources_ended(self) -&gt; bool:\n        return len(self._ended_sources) &gt;= len(self._video_sources.all_sources)\n\n    def join_all_reconnection_threads(self, include_not_finished: bool = False) -&gt; None:\n        for source_ord in copy(self._threads_to_join):\n            self._purge_reconnection_thread(source_ord=source_ord)\n        if not include_not_finished:\n            return None\n        for source_ord in list(self._reconnection_threads.keys()):\n            self._purge_reconnection_thread(source_ord=source_ord)\n\n    def _is_source_inactive(self, source_ord: int) -&gt; bool:\n        return (\n            source_ord in self._ended_sources\n            or source_ord in self._reconnection_threads\n        )\n\n    def _register_end_of_stream(self, source_ord: int) -&gt; None:\n        source_should_reconnect = self._video_sources.allow_reconnection[source_ord]\n        if source_should_reconnect:\n            self._reconnect_source(source_ord=source_ord)\n        else:\n            self._ended_sources.add(source_ord)\n\n    def _reconnect_source(self, source_ord: int) -&gt; None:\n        if source_ord in self._reconnection_threads:\n            return None\n        self._reconnection_threads[source_ord] = Thread(\n            target=_attempt_reconnect,\n            args=(\n                self._video_sources.all_sources[source_ord],\n                partial(self._should_stop, source_ord=source_ord),\n                self._on_reconnection_error,\n                partial(self._register_thread_to_join, source_ord=source_ord),\n                partial(self._register_reconnection_fatal_error, source_ord=source_ord),\n            ),\n        )\n        self._reconnection_threads[source_ord].start()\n\n    def _register_reconnection_fatal_error(self, source_ord: int) -&gt; None:\n        self._register_thread_to_join(source_ord=source_ord)\n        self._ended_sources.add(source_ord)\n\n    def _register_thread_to_join(self, source_ord: int) -&gt; None:\n        self._threads_to_join.add(source_ord)\n\n    def _purge_reconnection_thread(self, source_ord: int) -&gt; None:\n        if source_ord not in self._reconnection_threads:\n            return None\n        self._enforce_stop[source_ord] = True\n        self._reconnection_threads[source_ord].join()\n        del self._reconnection_threads[source_ord]\n        self._enforce_stop[source_ord] = False\n        if source_ord in self._threads_to_join:\n            self._threads_to_join.remove(source_ord)\n\n    def _should_stop(self, source_ord: int) -&gt; bool:\n        if self._external_should_stop():\n            return True\n        return self._enforce_stop.get(source_ord, False)\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.get_video_frames_generator","title":"<code>get_video_frames_generator(video, max_fps=None, limiter_strategy=None)</code>","text":"<p>Util function to create a frames generator from <code>VideoSource</code> with possibility to limit FPS of consumed frames and dictate what to do if frames are produced to fast.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Union[VideoSource, str, int]</code> <p>Either instance of VideoSource or video reference accepted by VideoSource.init(...)</p> required <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>value of maximum FPS rate of generated frames - can be used to limit generation frequency</p> <code>None</code> <code>limiter_strategy</code> <code>Optional[FPSLimiterStrategy]</code> <p>strategy used to deal with frames decoding exceeding limit of <code>max_fps</code>. By default - for files, in the interest of processing all frames - generation will be awaited, for streams - frames will be dropped on the floor.</p> <code>None</code> <p>Returns: generator of <code>VideoFrame</code></p> Example <pre><code>from inference.core.interfaces.camera.utils import get_video_frames_generator\n\nfor frame in get_video_frames_generator(\n    video=\"./some.mp4\",\n    max_fps=50,\n):\n     pass\n</code></pre> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>def get_video_frames_generator(\n    video: Union[VideoSource, str, int],\n    max_fps: Optional[Union[float, int]] = None,\n    limiter_strategy: Optional[FPSLimiterStrategy] = None,\n) -&gt; Generator[VideoFrame, None, None]:\n    \"\"\"\n    Util function to create a frames generator from `VideoSource` with possibility to\n    limit FPS of consumed frames and dictate what to do if frames are produced to fast.\n\n    Args:\n        video (Union[VideoSource, str, int]): Either instance of VideoSource or video reference accepted\n            by VideoSource.init(...)\n        max_fps (Optional[Union[float, int]]): value of maximum FPS rate of generated frames - can be used to limit\n            generation frequency\n        limiter_strategy (Optional[FPSLimiterStrategy]): strategy used to deal with frames decoding exceeding\n            limit of `max_fps`. By default - for files, in the interest of processing all frames -\n            generation will be awaited, for streams - frames will be dropped on the floor.\n    Returns: generator of `VideoFrame`\n\n    Example:\n        ```python\n        from inference.core.interfaces.camera.utils import get_video_frames_generator\n\n        for frame in get_video_frames_generator(\n            video=\"./some.mp4\",\n            max_fps=50,\n        ):\n             pass\n        ```\n    \"\"\"\n    is_managed_source = False\n    if issubclass(type(video), str) or issubclass(type(video), int):\n        video = VideoSource.init(\n            video_reference=video,\n        )\n        video.start()\n        is_managed_source = True\n    if max_fps is None:\n        yield from video\n        if is_managed_source:\n            video.terminate(purge_frames_buffer=True)\n        return None\n    limiter_strategy = resolve_limiter_strategy(\n        explicitly_defined_strategy=limiter_strategy,\n        source_properties=video.describe_source().source_properties,\n    )\n    yield from limit_frame_rate(\n        frames_generator=video, max_fps=max_fps, strategy=limiter_strategy\n    )\n    if is_managed_source:\n        video.terminate(purge_frames_buffer=True)\n    return None\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.multiplex_videos","title":"<code>multiplex_videos(videos, max_fps=None, limiter_strategy=None, batch_collection_timeout=None, force_stream_reconnection=True, should_stop=never_stop, on_reconnection_error=log_error)</code>","text":"<p>Function that is supposed to provide a generator over frames from multiple video sources. It is capable to initialise <code>VideoSource</code> from references to video files or streams and grab frames from all the sources - each running individual decoding on separate thread. In each cycle it attempts to grab frames from all sources (and wait at max <code>batch_collection_timeout</code> for whole batch to be collected). If frame from specific source cannot be collected in that time - it is simply not included in returned list. If after batch collection list of frames is empty - new collection start immediately. Collection does not account for sources that lost connectivity (example: streams that went offline). If that does not happen and stream has large latency - without reasonable <code>batch_collection_timeout</code> it will slow down processing - so please set it up in PROD solutions. In case of video streams (not video files) - given that <code>force_stream_reconnection=True</code> function will attempt to re-connect to disconnected source using background thread, not impairing batch frames collection and that source is not going to block frames retrieval even if infinite <code>batch_collection_timeout=None</code> is set. Similarly, when processing files - video file that is shorter than other passed into processing will not block the whole flow after End Of Stream (EOS).</p> <p>All sources must be accessible on start - if that's not the case - logic function raises <code>SourceConnectionError</code> and closes all video sources it opened on it own. Disconnections at later stages are handled by re-connection mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>List[Union[VideoSource, str, int]]</code> <p>List with references to video sources. Elements can be pre-initialised <code>VideoSource</code> instances, str with stream URI or file location or int representing camera device attached to the PC/server running the code.</p> required <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Upper-bound of processing speed - to be used when one wants at max <code>max_fps</code> video frames per second to be yielded from all sources by the generator.</p> <code>None</code> <code>limiter_strategy</code> <code>Optional[FPSLimiterStrategy]</code> <p>strategy used to deal with frames decoding exceeding limit of <code>max_fps</code>. For video files, in the interest of processing all frames - we recommend WAIT mode,  for streams - frames should be dropped on the floor with DROP strategy. Not setting the strategy equals  using automatic mode - WAIT if all sources are files and DROP otherwise</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>maximum await time to get batch of predictions from all sources. <code>None</code> means infinite timeout.</p> <code>None</code> <code>force_stream_reconnection</code> <code>bool</code> <p>Flag to decide on reconnection to streams (files are never re-connected)</p> <code>True</code> <code>should_stop</code> <code>Callable[[], bool]</code> <p>external stop signal that is periodically checked - to denote that video consumption stopped - make the function to return True</p> <code>never_stop</code> <code>on_reconnection_error</code> <code>Callable[[Optional[int], SourceConnectionError], None]</code> <p>Function that will be called whenever source cannot re-connect after disconnection. First parameter is source_id, second is connection error instance.</p> <code>log_error</code> <p>Returns Generator[List[VideoFrame], None, None]: allowing to iterate through frames from multiple video sources.</p> <p>Raises:</p> Type Description <code>SourceConnectionError</code> <p>when one or more source is not reachable at start of generation</p> Example <pre><code>from inference.core.interfaces.camera.utils import multiplex_videos\n\nfor frames in multiplex_videos(videos=[\"./some.mp4\", \"./other.mp4\"]):\n     for frame in frames:\n        pass  # do something with frame\n</code></pre> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>def multiplex_videos(\n    videos: List[Union[VideoSource, str, int]],\n    max_fps: Optional[Union[float, int]] = None,\n    limiter_strategy: Optional[FPSLimiterStrategy] = None,\n    batch_collection_timeout: Optional[float] = None,\n    force_stream_reconnection: bool = True,\n    should_stop: Callable[[], bool] = never_stop,\n    on_reconnection_error: Callable[\n        [Optional[int], SourceConnectionError], None\n    ] = log_error,\n) -&gt; Generator[List[VideoFrame], None, None]:\n    \"\"\"\n    Function that is supposed to provide a generator over frames from multiple video sources. It is capable to\n    initialise `VideoSource` from references to video files or streams and grab frames from all the sources -\n    each running individual decoding on separate thread. In each cycle it attempts to grab frames from all sources\n    (and wait at max `batch_collection_timeout` for whole batch to be collected). If frame from specific source\n    cannot be collected in that time - it is simply not included in returned list. If after batch collection list of\n    frames is empty - new collection start immediately. Collection does not account for\n    sources that lost connectivity (example: streams that went offline). If that does not happen and stream has\n    large latency - without reasonable `batch_collection_timeout` it will slow down processing - so please\n    set it up in PROD solutions. In case of video streams (not video files) - given that\n    `force_stream_reconnection=True` function will attempt to re-connect to disconnected source using background thread,\n    not impairing batch frames collection and that source is not going to block frames retrieval even if infinite\n    `batch_collection_timeout=None` is set. Similarly, when processing files - video file that is shorter than other\n    passed into processing will not block the whole flow after End Of Stream (EOS).\n\n    All sources must be accessible on start - if that's not the case - logic function raises `SourceConnectionError`\n    and closes all video sources it opened on it own. Disconnections at later stages are handled by re-connection\n    mechanism.\n\n    Args:\n        videos (List[Union[VideoSource, str, int]]): List with references to video sources. Elements can be\n            pre-initialised `VideoSource` instances, str with stream URI or file location or int representing\n            camera device attached to the PC/server running the code.\n        max_fps (Optional[Union[float, int]]): Upper-bound of processing speed - to be used when one wants at max\n            `max_fps` video frames per second to be yielded from all sources by the generator.\n        limiter_strategy (Optional[FPSLimiterStrategy]): strategy used to deal with frames decoding exceeding\n            limit of `max_fps`. For video files, in the interest of processing all frames - we recommend WAIT mode,\n             for streams - frames should be dropped on the floor with DROP strategy. Not setting the strategy equals\n             using automatic mode - WAIT if all sources are files and DROP otherwise\n        batch_collection_timeout (Optional[float]): maximum await time to get batch of predictions from all sources.\n            `None` means infinite timeout.\n        force_stream_reconnection (bool): Flag to decide on reconnection to streams (files are never re-connected)\n        should_stop (Callable[[], bool]): external stop signal that is periodically checked - to denote that\n            video consumption stopped - make the function to return True\n        on_reconnection_error (Callable[[Optional[int], SourceConnectionError], None]): Function that will be\n            called whenever source cannot re-connect after disconnection. First parameter is source_id, second\n            is connection error instance.\n\n    Returns Generator[List[VideoFrame], None, None]: allowing to iterate through frames from multiple video sources.\n\n    Raises:\n        SourceConnectionError: when one or more source is not reachable at start of generation\n\n    Example:\n        ```python\n        from inference.core.interfaces.camera.utils import multiplex_videos\n\n        for frames in multiplex_videos(videos=[\"./some.mp4\", \"./other.mp4\"]):\n             for frame in frames:\n                pass  # do something with frame\n        ```\n    \"\"\"\n    video_sources = _prepare_video_sources(\n        videos=videos, force_stream_reconnection=force_stream_reconnection\n    )\n    if any(rule is None for rule in video_sources.allow_reconnection):\n        logger.warning(\"Could not connect to all sources.\")\n        return None\n    generator = _multiplex_videos(\n        video_sources=video_sources,\n        batch_collection_timeout=batch_collection_timeout,\n        should_stop=should_stop,\n        on_reconnection_error=on_reconnection_error,\n    )\n    if max_fps is None:\n        yield from generator\n        return None\n    max_fps = max_fps / len(videos)\n    if limiter_strategy is None:\n        limiter_strategy = negotiate_rate_limiter_strategy_for_multiple_sources(\n            video_sources=video_sources.all_sources,\n        )\n    yield from limit_frame_rate(\n        frames_generator=generator, max_fps=max_fps, strategy=limiter_strategy\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/","title":"Video source","text":""},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoConsumer","title":"<code>VideoConsumer</code>","text":"<p>This class should be consumed as part of internal implementation. It provides abstraction around stream consumption strategies.</p> <p>It must always be given the same video source for consecutive invocations, otherwise the internal state does not make sense.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>class VideoConsumer:\n    \"\"\"\n    This class should be consumed as part of internal implementation.\n    It provides abstraction around stream consumption strategies.\n\n    It must always be given the same video source for consecutive invocations,\n    otherwise the internal state does not make sense.\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        buffer_filling_strategy: Optional[BufferFillingStrategy],\n        adaptive_mode_stream_pace_tolerance: float,\n        adaptive_mode_reader_pace_tolerance: float,\n        minimum_adaptive_mode_samples: int,\n        maximum_adaptive_frames_dropped_in_row: int,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        desired_fps: Optional[Union[float, int]] = None,\n    ) -&gt; \"VideoConsumer\":\n        minimum_adaptive_mode_samples = max(minimum_adaptive_mode_samples, 2)\n        reader_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        stream_consumption_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        decoding_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        return cls(\n            buffer_filling_strategy=buffer_filling_strategy,\n            adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n            adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n            minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n            maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n            status_update_handlers=status_update_handlers,\n            reader_pace_monitor=reader_pace_monitor,\n            stream_consumption_pace_monitor=stream_consumption_pace_monitor,\n            decoding_pace_monitor=decoding_pace_monitor,\n            desired_fps=desired_fps,\n        )\n\n    def __init__(\n        self,\n        buffer_filling_strategy: Optional[BufferFillingStrategy],\n        adaptive_mode_stream_pace_tolerance: float,\n        adaptive_mode_reader_pace_tolerance: float,\n        minimum_adaptive_mode_samples: int,\n        maximum_adaptive_frames_dropped_in_row: int,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        reader_pace_monitor: sv.FPSMonitor,\n        stream_consumption_pace_monitor: sv.FPSMonitor,\n        decoding_pace_monitor: sv.FPSMonitor,\n        desired_fps: Optional[Union[float, int]],\n    ):\n        self._buffer_filling_strategy = buffer_filling_strategy\n        self._frame_counter = 0\n        self._adaptive_mode_stream_pace_tolerance = adaptive_mode_stream_pace_tolerance\n        self._adaptive_mode_reader_pace_tolerance = adaptive_mode_reader_pace_tolerance\n        self._minimum_adaptive_mode_samples = minimum_adaptive_mode_samples\n        self._maximum_adaptive_frames_dropped_in_row = (\n            maximum_adaptive_frames_dropped_in_row\n        )\n        self._adaptive_frames_dropped_in_row = 0\n        self._reader_pace_monitor = reader_pace_monitor\n        self._stream_consumption_pace_monitor = stream_consumption_pace_monitor\n        self._decoding_pace_monitor = decoding_pace_monitor\n        self._desired_fps = desired_fps\n        self._declared_source_fps = None\n        self._is_source_video_file = None\n        self._timestamp_created: Optional[datetime] = None\n        self._status_update_handlers = status_update_handlers\n        self._next_frame_from_video_to_accept = 1\n\n    @property\n    def buffer_filling_strategy(self) -&gt; Optional[BufferFillingStrategy]:\n        return self._buffer_filling_strategy\n\n    def reset(self, source_properties: SourceProperties) -&gt; None:\n        if source_properties.is_file:\n            self._set_file_mode_buffering_strategies()\n        else:\n            self._set_stream_mode_buffering_strategies()\n        self._reader_pace_monitor.reset()\n        self.reset_stream_consumption_pace()\n        self._decoding_pace_monitor.reset()\n        self._adaptive_frames_dropped_in_row = 0\n        self._next_frame_from_video_to_accept = self._frame_counter + 1\n\n    def reset_stream_consumption_pace(self) -&gt; None:\n        self._stream_consumption_pace_monitor.reset()\n\n    def notify_frame_consumed(self) -&gt; None:\n        self._reader_pace_monitor.tick()\n\n    def consume_frame(\n        self,\n        video: VideoFrameProducer,\n        declared_source_fps: Optional[float],\n        is_source_video_file: Optional[bool],\n        buffer: Queue,\n        frames_buffering_allowed: bool,\n        source_id: Optional[int] = None,\n    ) -&gt; bool:\n        if self._is_source_video_file is None:\n            source_properties = video.discover_source_properties()\n            self._is_source_video_file = source_properties.is_file\n            self._declared_source_fps = source_properties.fps\n            self._timestamp_created = source_properties.timestamp_created\n\n        if self._timestamp_created:\n            frame_timestamp = self._timestamp_created + timedelta(\n                seconds=self._frame_counter / self._declared_source_fps\n            )\n        else:\n            frame_timestamp = datetime.now()\n\n        success = video.grab()\n        self._stream_consumption_pace_monitor.tick()\n        if not success:\n            return False\n        self._frame_counter += 1\n        if self._status_update_handlers:\n            send_video_source_status_update(\n                severity=UpdateSeverity.DEBUG,\n                event_type=FRAME_CAPTURED_EVENT,\n                payload={\n                    \"frame_timestamp\": frame_timestamp,\n                    \"frame_id\": self._frame_counter,\n                    \"source_id\": source_id,\n                },\n                status_update_handlers=self._status_update_handlers,\n            )\n        measured_source_fps = declared_source_fps\n        if not is_source_video_file:\n            if hasattr(self._stream_consumption_pace_monitor, \"fps\"):\n                measured_source_fps = self._stream_consumption_pace_monitor.fps\n            else:\n                measured_source_fps = self._stream_consumption_pace_monitor()\n\n        if self._video_fps_should_be_sub_sampled():\n            return True\n        return self._consume_stream_frame(\n            video=video,\n            declared_source_fps=declared_source_fps,\n            measured_source_fps=measured_source_fps,\n            is_source_video_file=is_source_video_file,\n            frame_timestamp=frame_timestamp,\n            buffer=buffer,\n            frames_buffering_allowed=frames_buffering_allowed,\n            source_id=source_id,\n        )\n\n    def _set_file_mode_buffering_strategies(self) -&gt; None:\n        if self._buffer_filling_strategy is None:\n            self._buffer_filling_strategy = BufferFillingStrategy.WAIT\n\n    def _set_stream_mode_buffering_strategies(self) -&gt; None:\n        if self._buffer_filling_strategy is None:\n            self._buffer_filling_strategy = BufferFillingStrategy.ADAPTIVE_DROP_OLDEST\n\n    def _video_fps_should_be_sub_sampled(self) -&gt; bool:\n        if self._desired_fps is None:\n            return False\n        if self._is_source_video_file:\n            actual_fps = self._declared_source_fps\n        else:\n            fraction_of_pace_monitor_samples = (\n                len(self._stream_consumption_pace_monitor.all_timestamps)\n                / self._stream_consumption_pace_monitor.all_timestamps.maxlen\n            )\n            if fraction_of_pace_monitor_samples &lt; 0.9:\n                actual_fps = self._declared_source_fps\n            elif hasattr(self._stream_consumption_pace_monitor, \"fps\"):\n                actual_fps = self._stream_consumption_pace_monitor.fps\n            else:\n                actual_fps = self._stream_consumption_pace_monitor()\n        if self._frame_counter == self._next_frame_from_video_to_accept:\n            stride = calculate_video_file_stride(\n                actual_fps=actual_fps,\n                desired_fps=self._desired_fps,\n            )\n            self._next_frame_from_video_to_accept += stride\n            return False\n        # skipping frame\n        return True\n\n    def _consume_stream_frame(\n        self,\n        video: VideoFrameProducer,\n        declared_source_fps: Optional[float],\n        measured_source_fps: Optional[float],\n        is_source_video_file: Optional[bool],\n        frame_timestamp: datetime,\n        buffer: Queue,\n        frames_buffering_allowed: bool,\n        source_id: Optional[int],\n    ) -&gt; bool:\n        \"\"\"\n        Returns: boolean flag with success status\n        \"\"\"\n        if not frames_buffering_allowed:\n            send_frame_drop_update(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                cause=\"Buffering not allowed at the moment\",\n                status_update_handlers=self._status_update_handlers,\n                source_id=source_id,\n            )\n            return True\n        if self._frame_should_be_adaptively_dropped(\n            declared_source_fps=declared_source_fps\n        ):\n            self._adaptive_frames_dropped_in_row += 1\n            send_frame_drop_update(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                cause=\"ADAPTIVE strategy\",\n                status_update_handlers=self._status_update_handlers,\n                source_id=source_id,\n            )\n            return True\n        self._adaptive_frames_dropped_in_row = 0\n        if (\n            not buffer.full()\n            or self._buffer_filling_strategy is BufferFillingStrategy.WAIT\n        ):\n            return decode_video_frame_to_buffer(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                video=video,\n                buffer=buffer,\n                decoding_pace_monitor=self._decoding_pace_monitor,\n                source_id=source_id,\n                declared_source_fps=declared_source_fps,\n                measured_source_fps=measured_source_fps,\n                comes_from_video_file=is_source_video_file,\n            )\n        if self._buffer_filling_strategy in DROP_OLDEST_STRATEGIES:\n            return self._process_stream_frame_dropping_oldest(\n                frame_timestamp=frame_timestamp,\n                video=video,\n                buffer=buffer,\n                source_id=source_id,\n                is_video_file=is_source_video_file,\n            )\n        send_frame_drop_update(\n            frame_timestamp=frame_timestamp,\n            frame_id=self._frame_counter,\n            cause=\"DROP_LATEST strategy\",\n            status_update_handlers=self._status_update_handlers,\n            source_id=source_id,\n        )\n        return True\n\n    def _frame_should_be_adaptively_dropped(\n        self, declared_source_fps: Optional[float]\n    ) -&gt; bool:\n        if self._buffer_filling_strategy not in ADAPTIVE_STRATEGIES:\n            return False\n        if (\n            self._adaptive_frames_dropped_in_row\n            &gt;= self._maximum_adaptive_frames_dropped_in_row\n        ):\n            return False\n        if (\n            len(self._stream_consumption_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ):\n            # not enough observations\n            return False\n        if hasattr(self._stream_consumption_pace_monitor, \"fps\"):\n            stream_consumption_pace = self._stream_consumption_pace_monitor.fps\n        else:\n            stream_consumption_pace = self._stream_consumption_pace_monitor()\n        announced_stream_fps = stream_consumption_pace\n        if declared_source_fps is not None and declared_source_fps &gt; 0:\n            announced_stream_fps = declared_source_fps\n        if (\n            announced_stream_fps - stream_consumption_pace\n            &gt; self._adaptive_mode_stream_pace_tolerance\n        ):\n            # cannot keep up with stream emission\n            return True\n        if (\n            len(self._reader_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ) or (\n            len(self._decoding_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ):\n            # not enough observations\n            return False\n        actual_reader_pace = get_fps_if_tick_happens_now(\n            fps_monitor=self._reader_pace_monitor\n        )\n        if hasattr(self._decoding_pace_monitor, \"fps\"):\n            decoding_pace = self._decoding_pace_monitor.fps\n        else:\n            decoding_pace = self._decoding_pace_monitor()\n        if (\n            decoding_pace - actual_reader_pace\n            &gt; self._adaptive_mode_reader_pace_tolerance\n        ):\n            # we are too fast for the reader - time to save compute on decoding\n            return True\n        return False\n\n    def _process_stream_frame_dropping_oldest(\n        self,\n        frame_timestamp: datetime,\n        video: VideoFrameProducer,\n        buffer: Queue,\n        source_id: Optional[int],\n        is_video_file: bool,\n    ) -&gt; bool:\n        drop_single_frame_from_buffer(\n            buffer=buffer,\n            cause=\"DROP_OLDEST strategy\",\n            status_update_handlers=self._status_update_handlers,\n        )\n        return decode_video_frame_to_buffer(\n            frame_timestamp=frame_timestamp,\n            frame_id=self._frame_counter,\n            video=video,\n            buffer=buffer,\n            decoding_pace_monitor=self._decoding_pace_monitor,\n            source_id=source_id,\n            comes_from_video_file=is_video_file,\n        )\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource","title":"<code>VideoSource</code>","text":"Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>class VideoSource:\n    @classmethod\n    def init(\n        cls,\n        video_reference: VideoSourceIdentifier,\n        buffer_size: int = DEFAULT_BUFFER_SIZE,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        adaptive_mode_stream_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE,\n        adaptive_mode_reader_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE,\n        minimum_adaptive_mode_samples: int = DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES,\n        maximum_adaptive_frames_dropped_in_row: int = DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        source_id: Optional[int] = None,\n        desired_fps: Optional[Union[float, int]] = None,\n    ):\n        \"\"\"\n        This class is meant to represent abstraction over video sources - both video files and\n        on-line streams that are possible to be consumed and used by other components of `inference`\n        library.\n\n        Before digging into details of the class behaviour, it is advised to familiarise with the following\n        concepts and implementation assumptions:\n\n        1. Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by\n            its processing capabilities. If processing is faster than the frame rate of video, operations\n            may be executed in a time shorter than the time of video playback. In the opposite case - consumer\n            may freely decode and process frames in its own pace, without risk for failures due to temporal\n            dependencies of processing - this is classical offline processing example.\n        2. Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -\n            in other words - this is on-line processing example. Consumer being faster than incoming stream\n            frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.\n            Slow consumer, however, may not be able to process everything on time and to keep up with the pace\n            of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of\n            sync with the stream causing decoding failures or unpredictable behavior.\n\n        To fit those two types of video sources, `VideoSource` introduces the concept of buffered decoding of\n        video stream (like at the YouTube - player buffers some frames that are soon to be displayed).\n        The way on how buffer is filled and consumed dictates the behavior of `VideoSource`.\n\n        Starting from `BufferFillingStrategy` - we have 3 basic options:\n        * WAIT: in case of slow video consumption, when buffer is full - `VideoSource` will wait for\n        the empty spot in buffer before next frame will be processed - this is suitable in cases when\n        we want to ensure EACH FRAME of the video to be processed\n        * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped -\n        this is suitable for cases when we want to process the most recent frames possible\n        * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when\n        it is expected to have processing performance drops, but we would like to consume portions of\n        video that are locally smooth - but this is probably the least common use-case.\n\n        On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST,\n        which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion\n        of that mode will be described later.\n\n        Naturally, decoded frames must also be consumed. `VideoSource` provides a handy interface for reading\n        a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via\n        `BufferConsumptionStrategy`:\n        * LAZY - consume all the frames from decoding buffer one-by-one\n        * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent\n\n        In consequence - there are various combinations of `BufferFillingStrategy` and `BufferConsumptionStrategy`.\n        The most popular would be:\n        * `BufferFillingStrategy.WAIT` and `BufferConsumptionStrategy.LAZY` - to always decode and process each and\n            every frame of the source (useful while processing video files - and default behaviour enforced by\n            `inference` if there is no explicit configuration)\n        * `BufferFillingStrategy.DROP_OLDEST` and `BufferConsumptionStrategy.EAGER` - to always process the most\n            recent frames of source (useful while processing video streams when low latency [real-time experience]\n            is required - ADAPTIVE version of this is default for streams)\n\n        ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume\n        video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing\n        against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as `DROP_OLDEST`\n        and `DROP_LATEST` strategies, but there are two more conditions that may influence frame drop:\n        * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that\n        MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing\n        deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner\n        * consumption rate - in resource constraints environment, not only decoding is problematic from the performance\n        perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources\n        for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently\n        than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just\n        grabbed and dropped on the floor.\n        ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases.\n        Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source,\n        reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients,\n        with reasonable defaults being set.\n\n        `VideoSource` emits events regarding its activity - which can be intercepted by custom handlers. Take\n        into account that they are always executed in context of thread invoking them (and should be fast to complete,\n        otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.\n\n        `VideoSource` implementation is naturally multithreading, with different thread decoding video and different\n        one consuming it and manipulating source state. Implementation of user interface is thread-safe, although\n        stream it is meant to be consumed by a single thread only.\n\n        ENV variables involved:\n        * VIDEO_SOURCE_BUFFER_SIZE - default: 64\n        * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1\n        * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0\n        * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10\n        * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n\n        Args:\n            video_reference (Union[str, int]): Either str with file or stream reference, or int representing device ID\n            buffer_size (int): size of decoding buffer\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers for status updates\n            buffer_filling_strategy (Optional[BufferFillingStrategy]): Settings for buffer filling strategy - if not\n                given - automatic choice regarding source type will be applied\n            buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Settings for buffer consumption strategy,\n                if not given - automatic choice regarding source type will be applied\n            adaptive_mode_stream_pace_tolerance (float): Maximum deviation between frames grabbing pace and stream pace\n                that will not trigger adaptive mode frame drop\n            adaptive_mode_reader_pace_tolerance (float): Maximum deviation between decoding pace and stream consumption\n                pace that will not trigger adaptive mode frame drop\n            minimum_adaptive_mode_samples (int): Minimal number of frames to be used to establish actual pace of\n                processing, before adaptive mode can drop any frame\n            maximum_adaptive_frames_dropped_in_row (int): Maximum number of frames dropped in row due to application of\n                adaptive strategy\n            video_source_properties (Optional[dict[str, float]]): Optional dictionary with video source properties\n                corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.\n            source_id (Optional[int]): Optional identifier of video source - mainly useful to recognise specific source\n                when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised\n                to keep it unique within all sources in use.\n\n        Returns: Instance of `VideoSource` class\n        \"\"\"\n        frames_buffer = Queue(maxsize=buffer_size)\n        if status_update_handlers is None:\n            status_update_handlers = []\n        video_consumer = VideoConsumer.init(\n            buffer_filling_strategy=buffer_filling_strategy,\n            adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n            adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n            minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n            maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n            status_update_handlers=status_update_handlers,\n            desired_fps=desired_fps,\n        )\n        return cls(\n            stream_reference=video_reference,\n            frames_buffer=frames_buffer,\n            status_update_handlers=status_update_handlers,\n            buffer_consumption_strategy=buffer_consumption_strategy,\n            video_consumer=video_consumer,\n            video_source_properties=video_source_properties,\n            source_id=source_id,\n        )\n\n    def __init__(\n        self,\n        stream_reference: VideoSourceIdentifier,\n        frames_buffer: Queue,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        buffer_consumption_strategy: Optional[BufferConsumptionStrategy],\n        video_consumer: \"VideoConsumer\",\n        video_source_properties: Optional[Dict[str, float]],\n        source_id: Optional[int],\n    ):\n        self._stream_reference = stream_reference\n        self._video: Optional[VideoFrameProducer] = None\n        self._source_properties: Optional[SourceProperties] = None\n        self._frames_buffer = frames_buffer\n        self._status_update_handlers = status_update_handlers\n        self._buffer_consumption_strategy = buffer_consumption_strategy\n        self._video_consumer = video_consumer\n        self._state = StreamState.NOT_STARTED\n        self._playback_allowed = Event()\n        self._frames_buffering_allowed = True\n        self._stream_consumption_thread: Optional[Thread] = None\n        self._state_change_lock = Lock()\n        self._video_source_properties = video_source_properties or {}\n        self._source_id = source_id\n        self._last_frame_timestamp: int = time.time_ns()\n        self._fps: Optional[float] = None\n        self._is_file: Optional[bool] = None\n\n    @property\n    def source_id(self) -&gt; Optional[int]:\n        return self._source_id\n\n    @lock_state_transition\n    def restart(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Method to restart source consumption. Eligible to be used in states:\n        [MUTED, RUNNING, PAUSED, ENDED, ERROR].\n        End state:\n        * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n        * ERROR - if it was not possible to connect with source\n\n        Thread safe - only one transition of states possible at the time.\n\n        Args:\n            wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n                completion of this operation.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n            * SourceConnectionError: if source cannot be connected\n        \"\"\"\n        if self._state not in RESTART_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not RESTART stream in state: {self._state}\"\n            )\n        self._restart(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n\n    @lock_state_transition\n    def start(self) -&gt; None:\n        \"\"\"\n        Method to be used to start source consumption. Eligible to be used in states:\n        [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)]\n        End state:\n        * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n        * ERROR - if it was not possible to connect with source\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n            * SourceConnectionError: if source cannot be connected\n        \"\"\"\n        if self._state not in START_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not START stream in state: {self._state}\"\n            )\n        self._start()\n\n    @lock_state_transition\n    def terminate(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Method to be used to terminate source consumption. Eligible to be used in states:\n        [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)]\n        End state:\n        * ENDED - indicating success of the process\n        * ERROR - if error with processing occurred\n\n        Must be used to properly dispose resources at the end.\n\n        Thread safe - only one transition of states possible at the time.\n\n        Args:\n            wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n                completion of this operation.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in TERMINATE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not TERMINATE stream in state: {self._state}\"\n            )\n        self._terminate(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n\n    @lock_state_transition\n    def pause(self) -&gt; None:\n        \"\"\"\n        Method to be used to pause source consumption. During pause - no new frames are consumed.\n        Used on on-line streams for too long may cause stream disconnection.\n        Eligible to be used in states:\n        [RUNNING]\n        End state:\n        * PAUSED\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in PAUSE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not PAUSE stream in state: {self._state}\"\n            )\n        self._pause()\n\n    @lock_state_transition\n    def mute(self) -&gt; None:\n        \"\"\"\n        Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where\n        frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing\n        intermediate frames to be dropped. May be also used against files, although arguably less useful.\n        Eligible to be used in states:\n        [RUNNING]\n        End state:\n        * MUTED\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in MUTE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not MUTE stream in state: {self._state}\"\n            )\n        self._mute()\n\n    @lock_state_transition\n    def resume(self) -&gt; None:\n        \"\"\"\n        Method to recover from pause or mute into running state.\n        [PAUSED, MUTED]\n        End state:\n        * RUNNING\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in RESUME_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not RESUME stream in state: {self._state}\"\n            )\n        self._resume()\n\n    def get_state(self) -&gt; StreamState:\n        \"\"\"\n        Method to get current state of the `VideoSource`\n\n        Returns: StreamState\n        \"\"\"\n        return self._state\n\n    def frame_ready(self) -&gt; bool:\n        \"\"\"\n        Method to check if decoded frame is ready for consumer\n\n        Returns: boolean flag indicating frame readiness\n        \"\"\"\n        return not self._frames_buffer.empty()\n\n    def read_frame(self, timeout: Optional[float] = None) -&gt; Optional[VideoFrame]:\n        \"\"\"\n        Method to be used by the consumer to get decoded source frame.\n\n        Returns: VideoFrame object with decoded frame and its metadata.\n        Throws:\n            * EndOfStreamError: when trying to get the frame from closed source.\n        \"\"\"\n        if self._is_file is None:\n            source_metadata: SourceMetadata = self.describe_source()\n            self._is_file = source_metadata.source_properties.is_file\n            self._fps = source_metadata.source_properties.fps\n            if not self._fps or self._fps &lt;= 0 or self._fps &gt; 1000:\n                self._fps = 30  # sane default\n        video_frame: Optional[Union[VideoFrame, str]] = get_from_queue(\n            queue=self._frames_buffer,\n            on_successful_read=self._video_consumer.notify_frame_consumed,\n            timeout=timeout,\n            purge=self._buffer_consumption_strategy is BufferConsumptionStrategy.EAGER,\n        )\n        if video_frame == POISON_PILL:\n            raise EndOfStreamError(\n                \"Attempted to retrieve frame from stream that already ended.\"\n            )\n        if video_frame is not None and self._status_update_handlers:\n            send_video_source_status_update(\n                severity=UpdateSeverity.DEBUG,\n                event_type=FRAME_CONSUMED_EVENT,\n                payload={\n                    \"frame_timestamp\": video_frame.frame_timestamp,\n                    \"frame_id\": video_frame.frame_id,\n                    \"source_id\": video_frame.source_id,\n                },\n                status_update_handlers=self._status_update_handlers,\n            )\n        return video_frame\n\n    def describe_source(self) -&gt; SourceMetadata:\n        serialized_source_reference = self._stream_reference\n        if callable(serialized_source_reference):\n            serialized_source_reference = str(self._stream_reference)\n        return SourceMetadata(\n            source_properties=self._source_properties,\n            source_reference=serialized_source_reference,\n            buffer_size=self._frames_buffer.maxsize,\n            state=self._state,\n            buffer_filling_strategy=self._video_consumer.buffer_filling_strategy,\n            buffer_consumption_strategy=self._buffer_consumption_strategy,\n            source_id=self._source_id,\n        )\n\n    def _restart(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        self._terminate(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n        self._change_state(target_state=StreamState.RESTARTING)\n        self._playback_allowed = Event()\n        self._frames_buffering_allowed = True\n        self._video: Optional[VideoFrameProducer] = None\n        self._source_properties: Optional[SourceProperties] = None\n        self._start()\n\n    def _start(self) -&gt; None:\n        self._change_state(target_state=StreamState.INITIALISING)\n        if callable(self._stream_reference):\n            self._video = self._stream_reference()\n        else:\n            self._video = CV2VideoFrameProducer(self._stream_reference)\n        if not self._video.isOpened():\n            self._change_state(target_state=StreamState.ERROR)\n            raise SourceConnectionError(\n                f\"Cannot connect to video source under reference: {self._stream_reference}\"\n            )\n        self._video.initialize_source_properties(self._video_source_properties)\n        self._source_properties = self._video.discover_source_properties()\n        self._video_consumer.reset(source_properties=self._source_properties)\n        if self._source_properties.is_file:\n            self._set_file_mode_consumption_strategies()\n        else:\n            self._set_stream_mode_consumption_strategies()\n        self._playback_allowed.set()\n        self._stream_consumption_thread = Thread(target=self._consume_video)\n        self._stream_consumption_thread.start()\n\n    def _terminate(\n        self, wait_on_frames_consumption: bool, purge_frames_buffer: bool\n    ) -&gt; None:\n        if self._state in RESUME_ELIGIBLE_STATES:\n            self._resume()\n        previous_state = self._state\n        self._change_state(target_state=StreamState.TERMINATING)\n        if purge_frames_buffer:\n            _ = get_from_queue(queue=self._frames_buffer, timeout=0.0, purge=True)\n        if self._stream_consumption_thread is not None:\n            self._stream_consumption_thread.join()\n        if wait_on_frames_consumption:\n            self._frames_buffer.join()\n        if previous_state is not StreamState.ERROR:\n            self._change_state(target_state=StreamState.ENDED)\n\n    def _pause(self) -&gt; None:\n        self._playback_allowed.clear()\n        self._change_state(target_state=StreamState.PAUSED)\n\n    def _mute(self) -&gt; None:\n        self._frames_buffering_allowed = False\n        self._change_state(target_state=StreamState.MUTED)\n\n    def _resume(self) -&gt; None:\n        previous_state = self._state\n        self._change_state(target_state=StreamState.RUNNING)\n        if previous_state is StreamState.PAUSED:\n            self._video_consumer.reset_stream_consumption_pace()\n            self._playback_allowed.set()\n        if previous_state is StreamState.MUTED:\n            self._frames_buffering_allowed = True\n\n    def _set_file_mode_consumption_strategies(self) -&gt; None:\n        if self._buffer_consumption_strategy is None:\n            self._buffer_consumption_strategy = BufferConsumptionStrategy.LAZY\n\n    def _set_stream_mode_consumption_strategies(self) -&gt; None:\n        if self._buffer_consumption_strategy is None:\n            self._buffer_consumption_strategy = BufferConsumptionStrategy.EAGER\n\n    def _consume_video(self) -&gt; None:\n        send_video_source_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=VIDEO_CONSUMPTION_STARTED_EVENT,\n            status_update_handlers=self._status_update_handlers,\n            payload={\"source_id\": self._source_id},\n        )\n        logger.info(f\"Video consumption started\")\n        try:\n            if self._state is not StreamState.TERMINATING:\n                self._change_state(target_state=StreamState.RUNNING)\n            declared_source_fps, is_video_file = None, None\n            if self._source_properties is not None:\n                declared_source_fps = self._source_properties.fps\n                is_video_file = self._source_properties.is_file\n            while self._video.isOpened():\n                if self._state is StreamState.TERMINATING:\n                    break\n                self._playback_allowed.wait()\n                success = self._video_consumer.consume_frame(\n                    video=self._video,\n                    declared_source_fps=declared_source_fps,\n                    is_source_video_file=is_video_file,\n                    buffer=self._frames_buffer,\n                    frames_buffering_allowed=self._frames_buffering_allowed,\n                    source_id=self._source_id,\n                )\n                if not success:\n                    break\n            self._frames_buffer.put(POISON_PILL)\n            self._video.release()\n            self._change_state(target_state=StreamState.ENDED)\n            send_video_source_status_update(\n                severity=UpdateSeverity.INFO,\n                event_type=VIDEO_CONSUMPTION_FINISHED_EVENT,\n                status_update_handlers=self._status_update_handlers,\n                payload={\"source_id\": self._source_id},\n            )\n            logger.info(f\"Video consumption finished\")\n        except Exception as error:\n            self._change_state(target_state=StreamState.ERROR)\n            payload = {\n                \"source_id\": self._source_id,\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"stream_consumer_thread\",\n            }\n            send_video_source_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=SOURCE_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.exception(\"Encountered error in video consumption thread\")\n\n    def _change_state(self, target_state: StreamState) -&gt; None:\n        payload = {\n            \"previous_state\": self._state,\n            \"new_state\": target_state,\n            \"source_id\": self._source_id,\n        }\n        self._state = target_state\n        send_video_source_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=SOURCE_STATE_UPDATE_EVENT,\n            payload=payload,\n            status_update_handlers=self._status_update_handlers,\n        )\n\n    def __iter__(self) -&gt; \"VideoSource\":\n        return self\n\n    def __next__(self) -&gt; VideoFrame:\n        \"\"\"\n        Method allowing to use `VideoSource` convenient to read frames\n\n        Returns: VideoFrame\n\n        Example:\n            ```python\n            source = VideoSource.init(video_reference=\"./some.mp4\")\n            source.start()\n\n            for frame in source:\n                 pass\n            ```\n        \"\"\"\n        try:\n            return self.read_frame()\n        except EndOfStreamError:\n            raise StopIteration()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.__next__","title":"<code>__next__()</code>","text":"<p>Method allowing to use <code>VideoSource</code> convenient to read frames</p> <p>Returns: VideoFrame</p> Example <pre><code>source = VideoSource.init(video_reference=\"./some.mp4\")\nsource.start()\n\nfor frame in source:\n     pass\n</code></pre> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def __next__(self) -&gt; VideoFrame:\n    \"\"\"\n    Method allowing to use `VideoSource` convenient to read frames\n\n    Returns: VideoFrame\n\n    Example:\n        ```python\n        source = VideoSource.init(video_reference=\"./some.mp4\")\n        source.start()\n\n        for frame in source:\n             pass\n        ```\n    \"\"\"\n    try:\n        return self.read_frame()\n    except EndOfStreamError:\n        raise StopIteration()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.frame_ready","title":"<code>frame_ready()</code>","text":"<p>Method to check if decoded frame is ready for consumer</p> <p>Returns: boolean flag indicating frame readiness</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def frame_ready(self) -&gt; bool:\n    \"\"\"\n    Method to check if decoded frame is ready for consumer\n\n    Returns: boolean flag indicating frame readiness\n    \"\"\"\n    return not self._frames_buffer.empty()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.get_state","title":"<code>get_state()</code>","text":"<p>Method to get current state of the <code>VideoSource</code></p> <p>Returns: StreamState</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def get_state(self) -&gt; StreamState:\n    \"\"\"\n    Method to get current state of the `VideoSource`\n\n    Returns: StreamState\n    \"\"\"\n    return self._state\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.init","title":"<code>init(video_reference, buffer_size=DEFAULT_BUFFER_SIZE, status_update_handlers=None, buffer_filling_strategy=None, buffer_consumption_strategy=None, adaptive_mode_stream_pace_tolerance=DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE, adaptive_mode_reader_pace_tolerance=DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE, minimum_adaptive_mode_samples=DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES, maximum_adaptive_frames_dropped_in_row=DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW, video_source_properties=None, source_id=None, desired_fps=None)</code>  <code>classmethod</code>","text":"<p>This class is meant to represent abstraction over video sources - both video files and on-line streams that are possible to be consumed and used by other components of <code>inference</code> library.</p> <p>Before digging into details of the class behaviour, it is advised to familiarise with the following concepts and implementation assumptions:</p> <ol> <li>Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by     its processing capabilities. If processing is faster than the frame rate of video, operations     may be executed in a time shorter than the time of video playback. In the opposite case - consumer     may freely decode and process frames in its own pace, without risk for failures due to temporal     dependencies of processing - this is classical offline processing example.</li> <li>Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -     in other words - this is on-line processing example. Consumer being faster than incoming stream     frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.     Slow consumer, however, may not be able to process everything on time and to keep up with the pace     of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of     sync with the stream causing decoding failures or unpredictable behavior.</li> </ol> <p>To fit those two types of video sources, <code>VideoSource</code> introduces the concept of buffered decoding of video stream (like at the YouTube - player buffers some frames that are soon to be displayed). The way on how buffer is filled and consumed dictates the behavior of <code>VideoSource</code>.</p> <p>Starting from <code>BufferFillingStrategy</code> - we have 3 basic options: * WAIT: in case of slow video consumption, when buffer is full - <code>VideoSource</code> will wait for the empty spot in buffer before next frame will be processed - this is suitable in cases when we want to ensure EACH FRAME of the video to be processed * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped - this is suitable for cases when we want to process the most recent frames possible * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when it is expected to have processing performance drops, but we would like to consume portions of video that are locally smooth - but this is probably the least common use-case.</p> <p>On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST, which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion of that mode will be described later.</p> <p>Naturally, decoded frames must also be consumed. <code>VideoSource</code> provides a handy interface for reading a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via <code>BufferConsumptionStrategy</code>: * LAZY - consume all the frames from decoding buffer one-by-one * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent</p> <p>In consequence - there are various combinations of <code>BufferFillingStrategy</code> and <code>BufferConsumptionStrategy</code>. The most popular would be: * <code>BufferFillingStrategy.WAIT</code> and <code>BufferConsumptionStrategy.LAZY</code> - to always decode and process each and     every frame of the source (useful while processing video files - and default behaviour enforced by     <code>inference</code> if there is no explicit configuration) * <code>BufferFillingStrategy.DROP_OLDEST</code> and <code>BufferConsumptionStrategy.EAGER</code> - to always process the most     recent frames of source (useful while processing video streams when low latency [real-time experience]     is required - ADAPTIVE version of this is default for streams)</p> <p>ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as <code>DROP_OLDEST</code> and <code>DROP_LATEST</code> strategies, but there are two more conditions that may influence frame drop: * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner * consumption rate - in resource constraints environment, not only decoding is problematic from the performance perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just grabbed and dropped on the floor. ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases. Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source, reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients, with reasonable defaults being set.</p> <p><code>VideoSource</code> emits events regarding its activity - which can be intercepted by custom handlers. Take into account that they are always executed in context of thread invoking them (and should be fast to complete, otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.</p> <p><code>VideoSource</code> implementation is naturally multithreading, with different thread decoding video and different one consuming it and manipulating source state. Implementation of user interface is thread-safe, although stream it is meant to be consumed by a single thread only.</p> <p>ENV variables involved: * VIDEO_SOURCE_BUFFER_SIZE - default: 64 * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1 * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0 * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10 * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int]</code> <p>Either str with file or stream reference, or int representing device ID</p> required <code>buffer_size</code> <code>int</code> <p>size of decoding buffer</p> <code>DEFAULT_BUFFER_SIZE</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers for status updates</p> <code>None</code> <code>buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Settings for buffer filling strategy - if not given - automatic choice regarding source type will be applied</p> <code>None</code> <code>buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Settings for buffer consumption strategy, if not given - automatic choice regarding source type will be applied</p> <code>None</code> <code>adaptive_mode_stream_pace_tolerance</code> <code>float</code> <p>Maximum deviation between frames grabbing pace and stream pace that will not trigger adaptive mode frame drop</p> <code>DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE</code> <code>adaptive_mode_reader_pace_tolerance</code> <code>float</code> <p>Maximum deviation between decoding pace and stream consumption pace that will not trigger adaptive mode frame drop</p> <code>DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE</code> <code>minimum_adaptive_mode_samples</code> <code>int</code> <p>Minimal number of frames to be used to establish actual pace of processing, before adaptive mode can drop any frame</p> <code>DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES</code> <code>maximum_adaptive_frames_dropped_in_row</code> <code>int</code> <p>Maximum number of frames dropped in row due to application of adaptive strategy</p> <code>DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW</code> <code>video_source_properties</code> <code>Optional[dict[str, float]]</code> <p>Optional dictionary with video source properties corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.</p> <code>None</code> <code>source_id</code> <code>Optional[int]</code> <p>Optional identifier of video source - mainly useful to recognise specific source when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised to keep it unique within all sources in use.</p> <code>None</code> <p>Returns: Instance of <code>VideoSource</code> class</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_reference: VideoSourceIdentifier,\n    buffer_size: int = DEFAULT_BUFFER_SIZE,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    adaptive_mode_stream_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE,\n    adaptive_mode_reader_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE,\n    minimum_adaptive_mode_samples: int = DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES,\n    maximum_adaptive_frames_dropped_in_row: int = DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    source_id: Optional[int] = None,\n    desired_fps: Optional[Union[float, int]] = None,\n):\n    \"\"\"\n    This class is meant to represent abstraction over video sources - both video files and\n    on-line streams that are possible to be consumed and used by other components of `inference`\n    library.\n\n    Before digging into details of the class behaviour, it is advised to familiarise with the following\n    concepts and implementation assumptions:\n\n    1. Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by\n        its processing capabilities. If processing is faster than the frame rate of video, operations\n        may be executed in a time shorter than the time of video playback. In the opposite case - consumer\n        may freely decode and process frames in its own pace, without risk for failures due to temporal\n        dependencies of processing - this is classical offline processing example.\n    2. Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -\n        in other words - this is on-line processing example. Consumer being faster than incoming stream\n        frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.\n        Slow consumer, however, may not be able to process everything on time and to keep up with the pace\n        of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of\n        sync with the stream causing decoding failures or unpredictable behavior.\n\n    To fit those two types of video sources, `VideoSource` introduces the concept of buffered decoding of\n    video stream (like at the YouTube - player buffers some frames that are soon to be displayed).\n    The way on how buffer is filled and consumed dictates the behavior of `VideoSource`.\n\n    Starting from `BufferFillingStrategy` - we have 3 basic options:\n    * WAIT: in case of slow video consumption, when buffer is full - `VideoSource` will wait for\n    the empty spot in buffer before next frame will be processed - this is suitable in cases when\n    we want to ensure EACH FRAME of the video to be processed\n    * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped -\n    this is suitable for cases when we want to process the most recent frames possible\n    * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when\n    it is expected to have processing performance drops, but we would like to consume portions of\n    video that are locally smooth - but this is probably the least common use-case.\n\n    On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST,\n    which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion\n    of that mode will be described later.\n\n    Naturally, decoded frames must also be consumed. `VideoSource` provides a handy interface for reading\n    a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via\n    `BufferConsumptionStrategy`:\n    * LAZY - consume all the frames from decoding buffer one-by-one\n    * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent\n\n    In consequence - there are various combinations of `BufferFillingStrategy` and `BufferConsumptionStrategy`.\n    The most popular would be:\n    * `BufferFillingStrategy.WAIT` and `BufferConsumptionStrategy.LAZY` - to always decode and process each and\n        every frame of the source (useful while processing video files - and default behaviour enforced by\n        `inference` if there is no explicit configuration)\n    * `BufferFillingStrategy.DROP_OLDEST` and `BufferConsumptionStrategy.EAGER` - to always process the most\n        recent frames of source (useful while processing video streams when low latency [real-time experience]\n        is required - ADAPTIVE version of this is default for streams)\n\n    ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume\n    video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing\n    against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as `DROP_OLDEST`\n    and `DROP_LATEST` strategies, but there are two more conditions that may influence frame drop:\n    * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that\n    MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing\n    deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner\n    * consumption rate - in resource constraints environment, not only decoding is problematic from the performance\n    perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources\n    for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently\n    than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just\n    grabbed and dropped on the floor.\n    ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases.\n    Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source,\n    reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients,\n    with reasonable defaults being set.\n\n    `VideoSource` emits events regarding its activity - which can be intercepted by custom handlers. Take\n    into account that they are always executed in context of thread invoking them (and should be fast to complete,\n    otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.\n\n    `VideoSource` implementation is naturally multithreading, with different thread decoding video and different\n    one consuming it and manipulating source state. Implementation of user interface is thread-safe, although\n    stream it is meant to be consumed by a single thread only.\n\n    ENV variables involved:\n    * VIDEO_SOURCE_BUFFER_SIZE - default: 64\n    * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1\n    * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0\n    * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10\n    * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n\n    Args:\n        video_reference (Union[str, int]): Either str with file or stream reference, or int representing device ID\n        buffer_size (int): size of decoding buffer\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers for status updates\n        buffer_filling_strategy (Optional[BufferFillingStrategy]): Settings for buffer filling strategy - if not\n            given - automatic choice regarding source type will be applied\n        buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Settings for buffer consumption strategy,\n            if not given - automatic choice regarding source type will be applied\n        adaptive_mode_stream_pace_tolerance (float): Maximum deviation between frames grabbing pace and stream pace\n            that will not trigger adaptive mode frame drop\n        adaptive_mode_reader_pace_tolerance (float): Maximum deviation between decoding pace and stream consumption\n            pace that will not trigger adaptive mode frame drop\n        minimum_adaptive_mode_samples (int): Minimal number of frames to be used to establish actual pace of\n            processing, before adaptive mode can drop any frame\n        maximum_adaptive_frames_dropped_in_row (int): Maximum number of frames dropped in row due to application of\n            adaptive strategy\n        video_source_properties (Optional[dict[str, float]]): Optional dictionary with video source properties\n            corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.\n        source_id (Optional[int]): Optional identifier of video source - mainly useful to recognise specific source\n            when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised\n            to keep it unique within all sources in use.\n\n    Returns: Instance of `VideoSource` class\n    \"\"\"\n    frames_buffer = Queue(maxsize=buffer_size)\n    if status_update_handlers is None:\n        status_update_handlers = []\n    video_consumer = VideoConsumer.init(\n        buffer_filling_strategy=buffer_filling_strategy,\n        adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n        adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n        minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n        maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n        status_update_handlers=status_update_handlers,\n        desired_fps=desired_fps,\n    )\n    return cls(\n        stream_reference=video_reference,\n        frames_buffer=frames_buffer,\n        status_update_handlers=status_update_handlers,\n        buffer_consumption_strategy=buffer_consumption_strategy,\n        video_consumer=video_consumer,\n        video_source_properties=video_source_properties,\n        source_id=source_id,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.mute","title":"<code>mute()</code>","text":"<p>Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing intermediate frames to be dropped. May be also used against files, although arguably less useful. Eligible to be used in states: [RUNNING] End state: * MUTED</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef mute(self) -&gt; None:\n    \"\"\"\n    Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where\n    frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing\n    intermediate frames to be dropped. May be also used against files, although arguably less useful.\n    Eligible to be used in states:\n    [RUNNING]\n    End state:\n    * MUTED\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in MUTE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not MUTE stream in state: {self._state}\"\n        )\n    self._mute()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.pause","title":"<code>pause()</code>","text":"<p>Method to be used to pause source consumption. During pause - no new frames are consumed. Used on on-line streams for too long may cause stream disconnection. Eligible to be used in states: [RUNNING] End state: * PAUSED</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef pause(self) -&gt; None:\n    \"\"\"\n    Method to be used to pause source consumption. During pause - no new frames are consumed.\n    Used on on-line streams for too long may cause stream disconnection.\n    Eligible to be used in states:\n    [RUNNING]\n    End state:\n    * PAUSED\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in PAUSE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not PAUSE stream in state: {self._state}\"\n        )\n    self._pause()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.read_frame","title":"<code>read_frame(timeout=None)</code>","text":"<p>Method to be used by the consumer to get decoded source frame.</p> <p>Throws:     * EndOfStreamError: when trying to get the frame from closed source.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def read_frame(self, timeout: Optional[float] = None) -&gt; Optional[VideoFrame]:\n    \"\"\"\n    Method to be used by the consumer to get decoded source frame.\n\n    Returns: VideoFrame object with decoded frame and its metadata.\n    Throws:\n        * EndOfStreamError: when trying to get the frame from closed source.\n    \"\"\"\n    if self._is_file is None:\n        source_metadata: SourceMetadata = self.describe_source()\n        self._is_file = source_metadata.source_properties.is_file\n        self._fps = source_metadata.source_properties.fps\n        if not self._fps or self._fps &lt;= 0 or self._fps &gt; 1000:\n            self._fps = 30  # sane default\n    video_frame: Optional[Union[VideoFrame, str]] = get_from_queue(\n        queue=self._frames_buffer,\n        on_successful_read=self._video_consumer.notify_frame_consumed,\n        timeout=timeout,\n        purge=self._buffer_consumption_strategy is BufferConsumptionStrategy.EAGER,\n    )\n    if video_frame == POISON_PILL:\n        raise EndOfStreamError(\n            \"Attempted to retrieve frame from stream that already ended.\"\n        )\n    if video_frame is not None and self._status_update_handlers:\n        send_video_source_status_update(\n            severity=UpdateSeverity.DEBUG,\n            event_type=FRAME_CONSUMED_EVENT,\n            payload={\n                \"frame_timestamp\": video_frame.frame_timestamp,\n                \"frame_id\": video_frame.frame_id,\n                \"source_id\": video_frame.source_id,\n            },\n            status_update_handlers=self._status_update_handlers,\n        )\n    return video_frame\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.restart","title":"<code>restart(wait_on_frames_consumption=True, purge_frames_buffer=False)</code>","text":"<p>Method to restart source consumption. Eligible to be used in states: [MUTED, RUNNING, PAUSED, ENDED, ERROR]. End state: * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed * ERROR - if it was not possible to connect with source</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Parameters:</p> Name Type Description Default <code>wait_on_frames_consumption</code> <code>bool</code> <p>Flag telling if all frames from buffer must be consumed before completion of this operation.</p> <code>True</code> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source     * SourceConnectionError: if source cannot be connected</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef restart(\n    self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n) -&gt; None:\n    \"\"\"\n    Method to restart source consumption. Eligible to be used in states:\n    [MUTED, RUNNING, PAUSED, ENDED, ERROR].\n    End state:\n    * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n    * ERROR - if it was not possible to connect with source\n\n    Thread safe - only one transition of states possible at the time.\n\n    Args:\n        wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n            completion of this operation.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        * SourceConnectionError: if source cannot be connected\n    \"\"\"\n    if self._state not in RESTART_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not RESTART stream in state: {self._state}\"\n        )\n    self._restart(\n        wait_on_frames_consumption=wait_on_frames_consumption,\n        purge_frames_buffer=purge_frames_buffer,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.resume","title":"<code>resume()</code>","text":"<p>Method to recover from pause or mute into running state. [PAUSED, MUTED] End state: * RUNNING</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef resume(self) -&gt; None:\n    \"\"\"\n    Method to recover from pause or mute into running state.\n    [PAUSED, MUTED]\n    End state:\n    * RUNNING\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in RESUME_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not RESUME stream in state: {self._state}\"\n        )\n    self._resume()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.start","title":"<code>start()</code>","text":"<p>Method to be used to start source consumption. Eligible to be used in states: [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)] End state: * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed * ERROR - if it was not possible to connect with source</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source     * SourceConnectionError: if source cannot be connected</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef start(self) -&gt; None:\n    \"\"\"\n    Method to be used to start source consumption. Eligible to be used in states:\n    [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)]\n    End state:\n    * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n    * ERROR - if it was not possible to connect with source\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        * SourceConnectionError: if source cannot be connected\n    \"\"\"\n    if self._state not in START_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not START stream in state: {self._state}\"\n        )\n    self._start()\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.terminate","title":"<code>terminate(wait_on_frames_consumption=True, purge_frames_buffer=False)</code>","text":"<p>Method to be used to terminate source consumption. Eligible to be used in states: [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)] End state: * ENDED - indicating success of the process * ERROR - if error with processing occurred</p> <p>Must be used to properly dispose resources at the end.</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Parameters:</p> Name Type Description Default <code>wait_on_frames_consumption</code> <code>bool</code> <p>Flag telling if all frames from buffer must be consumed before completion of this operation.</p> <code>True</code> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef terminate(\n    self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n) -&gt; None:\n    \"\"\"\n    Method to be used to terminate source consumption. Eligible to be used in states:\n    [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)]\n    End state:\n    * ENDED - indicating success of the process\n    * ERROR - if error with processing occurred\n\n    Must be used to properly dispose resources at the end.\n\n    Thread safe - only one transition of states possible at the time.\n\n    Args:\n        wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n            completion of this operation.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in TERMINATE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not TERMINATE stream in state: {self._state}\"\n        )\n    self._terminate(\n        wait_on_frames_consumption=wait_on_frames_consumption,\n        purge_frames_buffer=purge_frames_buffer,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.get_from_queue","title":"<code>get_from_queue(queue, timeout=None, on_successful_read=lambda: None, purge=False)</code>","text":"<p>Function is supposed to take element from the queue waiting on the first element to appear using <code>timeout</code> parameter. One may ask to go to the very last element of the queue and return it - then <code>purge</code> should be set to True. No additional wait on new elements to appear happen and the purge stops once queue is free returning last element consumed. queue.task_done() and on_successful_read(...) will be called on each received element.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def get_from_queue(\n    queue: Queue,\n    timeout: Optional[float] = None,\n    on_successful_read: Callable[[], None] = lambda: None,\n    purge: bool = False,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Function is supposed to take element from the queue waiting on the first element to appear using `timeout`\n    parameter. One may ask to go to the very last element of the queue and return it - then `purge` should be set\n    to True. No additional wait on new elements to appear happen and the purge stops once queue is free returning last\n    element consumed.\n    queue.task_done() and on_successful_read(...) will be called on each received element.\n    \"\"\"\n    result = None\n    if queue.empty() or not purge:\n        try:\n            result = queue.get(timeout=timeout)\n            queue.task_done()\n            on_successful_read()\n        except Empty:\n            pass\n    while not queue.empty() and purge:\n        result = queue.get()\n        queue.task_done()\n        on_successful_read()\n    return result\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/error_handlers/","title":"Error handlers","text":""},{"location":"reference/inference/core/interfaces/http/error_handlers/#inference.core.interfaces.http.error_handlers.with_route_exceptions","title":"<code>with_route_exceptions(route)</code>","text":"<p>A decorator that wraps a FastAPI route to handle specific exceptions. If an exception is caught, it returns a JSON response with the error message.</p> <p>Parameters:</p> Name Type Description Default <code>route</code> <code>Callable</code> <p>The FastAPI route to be wrapped.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The wrapped route.</p> Source code in <code>inference/core/interfaces/http/error_handlers.py</code> <pre><code>def with_route_exceptions(route):\n    \"\"\"\n    A decorator that wraps a FastAPI route to handle specific exceptions. If an exception\n    is caught, it returns a JSON response with the error message.\n\n    Args:\n        route (Callable): The FastAPI route to be wrapped.\n\n    Returns:\n        Callable: The wrapped route.\n    \"\"\"\n\n    @wraps(route)\n    def wrapped_route(*args, **kwargs):\n        try:\n            return route(*args, **kwargs)\n        except ContentTypeInvalid as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid Content-Type header provided with request.\"\n                },\n            )\n        except ContentTypeMissing as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Content-Type header not provided with request.\"},\n            )\n        except InputImageLoadError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": f\"Could not load input image. Cause: {error.get_public_error_details()}\"\n                },\n            )\n        except InvalidModelIDError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Invalid Model ID sent in request.\"},\n            )\n        except InvalidMaskDecodeArgument as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid mask decode argument sent. tradeoff_factor must be in [0.0, 1.0], \"\n                    \"mask_decode_mode: must be one of ['accurate', 'fast', 'tradeoff']\"\n                },\n            )\n        except MissingApiKeyError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Required Roboflow API key is missing. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n        except (\n            WorkflowSyntaxError,\n            InvalidReferenceTargetError,\n            ExecutionGraphStructureError,\n            StepInputDimensionalityError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            content = WorkflowErrorResponse(\n                message=str(error.public_message),\n                error_type=error.__class__.__name__,\n                context=str(error.context),\n                inner_error_type=str(error.inner_error_type),\n                inner_error_message=str(error.inner_error),\n                blocks_errors=error.blocks_errors,\n            )\n            resp = JSONResponse(status_code=400, content=content.model_dump())\n        except (\n            WorkflowDefinitionError,\n            ReferenceTypeError,\n            RuntimeInputError,\n            InvalidInputTypeError,\n            OperationTypeNotRecognisedError,\n            DynamicBlockError,\n            WorkflowExecutionEngineVersionError,\n            NotSupportedExecutionEngineError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"context\": error.context,\n                    \"inner_error_type\": error.inner_error_type,\n                    \"inner_error_message\": str(error.inner_error),\n                },\n            )\n        except (\n            ProcessesManagerInvalidPayload,\n            MalformedPayloadError,\n            MessageToBigError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except (\n            RoboflowAPINotAuthorizedError,\n            ProcessesManagerAuthorisationError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=401,\n                content={\n                    \"message\": \"Unauthorized access to roboflow API - check API key and make sure the key is valid for \"\n                    \"workspace you use. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n        except RoboflowAPIForbiddenError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=403,\n                content={\n                    \"message\": \"Unauthorized access to roboflow API - check API key and make sure the key is valid and \"\n                    \"have required scopes. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n        except RoboflowAPINotNotFoundError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": \"Requested Roboflow resource not found. Make sure that workspace, project or model \"\n                    \"you referred in request exists.\"\n                },\n            )\n        except ProcessesManagerNotFoundError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except (\n            InvalidEnvironmentVariableError,\n            MissingServiceSecretError,\n            ServiceConfigurationError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Service misconfiguration.\"}\n            )\n        except (\n            PreProcessingError,\n            PostProcessingError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": \"Model configuration related to pre- or post-processing is invalid.\"\n                },\n            )\n        except ModelArtefactError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Model package is broken.\"}\n            )\n        except OnnxProviderNotAvailable as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=501,\n                content={\n                    \"message\": \"Could not find requested ONNX Runtime Provider. Check that you are using \"\n                    \"the correct docker image on a supported device.\"\n                },\n            )\n        except (\n            MalformedRoboflowAPIResponseError,\n            RoboflowAPIUnsuccessfulRequestError,\n            WorkspaceLoadError,\n            MalformedWorkflowResponseError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=502,\n                content={\"message\": \"Internal error. Request to Roboflow API failed.\"},\n            )\n        except InferenceModelNotFound as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=503,\n                content={\"message\": \"Model is temporarily not ready - retry request.\"},\n                headers={\"Retry-After\": \"1\"},\n            )\n        except RoboflowAPIConnectionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=503,\n                content={\n                    \"message\": \"Internal error. Could not connect to Roboflow API.\"\n                },\n            )\n        except ModelManagerLockAcquisitionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=503,\n                content={\n                    \"message\": \"Could not acquire model manager lock due to other request performing \"\n                    \"blocking operation. Try again....\"\n                },\n                headers={\"Retry-After\": \"1\"},\n            )\n        except RoboflowAPITimeoutError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=504,\n                content={\n                    \"message\": \"Timeout when attempting to connect to Roboflow API.\"\n                },\n            )\n        except ClientCausedStepExecutionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            content = WorkflowErrorResponse(\n                message=str(error.public_message),\n                error_type=error.__class__.__name__,\n                context=str(error.context),\n                inner_error_type=str(error.inner_error_type),\n                inner_error_message=str(error.inner_error),\n                blocks_errors=[\n                    WorkflowBlockError(\n                        block_id=error.block_id,\n                    ),\n                ],\n            )\n            resp = JSONResponse(\n                status_code=error.status_code,\n                content=content.model_dump(),\n            )\n        except StepExecutionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            content = WorkflowErrorResponse(\n                message=str(error.public_message),\n                error_type=error.__class__.__name__,\n                context=str(error.context),\n                inner_error_type=str(error.inner_error_type),\n                inner_error_message=str(error.inner_error),\n                blocks_errors=[\n                    WorkflowBlockError(\n                        block_id=error.block_id,\n                        block_type=error.block_type,\n                    ),\n                ],\n            )\n            resp = JSONResponse(\n                status_code=500,\n                content=content.model_dump(),\n            )\n        except WorkflowError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"context\": error.context,\n                    \"inner_error_type\": error.inner_error_type,\n                    \"inner_error_message\": str(error.inner_error),\n                },\n            )\n        except (\n            ProcessesManagerClientError,\n            CommunicationProtocolError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except Exception as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(status_code=500, content={\"message\": \"Internal error.\"})\n        return resp\n\n    return wrapped_route\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/error_handlers/#inference.core.interfaces.http.error_handlers.with_route_exceptions_async","title":"<code>with_route_exceptions_async(route)</code>","text":"<p>A decorator that wraps a FastAPI route to handle specific exceptions. If an exception is caught, it returns a JSON response with the error message.</p> <p>Parameters:</p> Name Type Description Default <code>route</code> <code>Callable</code> <p>The FastAPI route to be wrapped.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The wrapped route.</p> Source code in <code>inference/core/interfaces/http/error_handlers.py</code> <pre><code>def with_route_exceptions_async(route):\n    \"\"\"\n    A decorator that wraps a FastAPI route to handle specific exceptions. If an exception\n    is caught, it returns a JSON response with the error message.\n\n    Args:\n        route (Callable): The FastAPI route to be wrapped.\n\n    Returns:\n        Callable: The wrapped route.\n    \"\"\"\n\n    @wraps(route)\n    async def wrapped_route(*args, **kwargs):\n        try:\n            return await route(*args, **kwargs)\n        except ContentTypeInvalid as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid Content-Type header provided with request.\"\n                },\n            )\n        except ContentTypeMissing as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Content-Type header not provided with request.\"},\n            )\n        except InputImageLoadError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": f\"Could not load input image. Cause: {error.get_public_error_details()}\"\n                },\n            )\n        except InvalidModelIDError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Invalid Model ID sent in request.\"},\n            )\n        except InvalidMaskDecodeArgument as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid mask decode argument sent. tradeoff_factor must be in [0.0, 1.0], \"\n                    \"mask_decode_mode: must be one of ['accurate', 'fast', 'tradeoff']\"\n                },\n            )\n        except MissingApiKeyError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Required Roboflow API key is missing. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n        except (\n            WorkflowSyntaxError,\n            InvalidReferenceTargetError,\n            ExecutionGraphStructureError,\n            StepInputDimensionalityError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            content = WorkflowErrorResponse(\n                message=str(error.public_message),\n                error_type=error.__class__.__name__,\n                context=str(error.context),\n                inner_error_type=str(error.inner_error_type),\n                inner_error_message=str(error.inner_error),\n                blocks_errors=error.blocks_errors,\n            )\n            resp = JSONResponse(status_code=400, content=content.model_dump())\n        except (\n            WorkflowDefinitionError,\n            ReferenceTypeError,\n            RuntimeInputError,\n            InvalidInputTypeError,\n            OperationTypeNotRecognisedError,\n            DynamicBlockError,\n            WorkflowExecutionEngineVersionError,\n            NotSupportedExecutionEngineError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"context\": error.context,\n                    \"inner_error_type\": error.inner_error_type,\n                    \"inner_error_message\": str(error.inner_error),\n                },\n            )\n        except (\n            ProcessesManagerInvalidPayload,\n            MalformedPayloadError,\n            MessageToBigError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except (\n            RoboflowAPINotAuthorizedError,\n            ProcessesManagerAuthorisationError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=401,\n                content={\n                    \"message\": \"Unauthorized access to roboflow API - check API key and make sure the key is valid for \"\n                    \"workspace you use. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n        except RoboflowAPIForbiddenError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=403,\n                content={\n                    \"message\": \"Unauthorized access to roboflow API - check API key and make sure the key is valid and \"\n                    \"have required scopes. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n        except RoboflowAPINotNotFoundError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": \"Requested Roboflow resource not found. Make sure that workspace, project or model \"\n                    \"you referred in request exists.\"\n                },\n            )\n        except ProcessesManagerNotFoundError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except (\n            InvalidEnvironmentVariableError,\n            MissingServiceSecretError,\n            ServiceConfigurationError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Service misconfiguration.\"}\n            )\n        except (\n            PreProcessingError,\n            PostProcessingError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": \"Model configuration related to pre- or post-processing is invalid.\"\n                },\n            )\n        except ModelArtefactError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Model package is broken.\"}\n            )\n        except OnnxProviderNotAvailable as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=501,\n                content={\n                    \"message\": \"Could not find requested ONNX Runtime Provider. Check that you are using \"\n                    \"the correct docker image on a supported device.\"\n                },\n            )\n        except (\n            MalformedRoboflowAPIResponseError,\n            RoboflowAPIUnsuccessfulRequestError,\n            WorkspaceLoadError,\n            MalformedWorkflowResponseError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=502,\n                content={\"message\": \"Internal error. Request to Roboflow API failed.\"},\n            )\n        except InferenceModelNotFound as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=503,\n                content={\"message\": \"Model is temporarily not ready - retry request.\"},\n                headers={\"Retry-After\": \"1\"},\n            )\n        except RoboflowAPIConnectionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=503,\n                content={\n                    \"message\": \"Internal error. Could not connect to Roboflow API.\"\n                },\n            )\n        except ModelManagerLockAcquisitionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=503,\n                content={\n                    \"message\": \"Could not acquire model manager lock due to other request performing \"\n                    \"blocking operation. Try again....\"\n                },\n                headers={\"Retry-After\": \"1\"},\n            )\n        except RoboflowAPITimeoutError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=504,\n                content={\n                    \"message\": \"Timeout when attempting to connect to Roboflow API.\"\n                },\n            )\n        except ClientCausedStepExecutionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            content = WorkflowErrorResponse(\n                message=str(error.public_message),\n                error_type=error.__class__.__name__,\n                context=str(error.context),\n                inner_error_type=str(error.inner_error_type),\n                inner_error_message=str(error.inner_error),\n                blocks_errors=[\n                    WorkflowBlockError(\n                        block_id=error.block_id,\n                    ),\n                ],\n            )\n            resp = JSONResponse(\n                status_code=error.status_code,\n                content=content.model_dump(),\n            )\n        except StepExecutionError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            content = WorkflowErrorResponse(\n                message=str(error.public_message),\n                error_type=error.__class__.__name__,\n                context=str(error.context),\n                inner_error_type=str(error.inner_error_type),\n                inner_error_message=str(error.inner_error),\n                blocks_errors=[\n                    WorkflowBlockError(\n                        block_id=error.block_id,\n                        block_type=error.block_type,\n                    ),\n                ],\n            )\n            resp = JSONResponse(\n                status_code=500,\n                content=content.model_dump(),\n            )\n        except WorkflowError as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"context\": error.context,\n                    \"inner_error_type\": error.inner_error_type,\n                    \"inner_error_message\": str(error.inner_error),\n                },\n            )\n        except (\n            ProcessesManagerClientError,\n            CommunicationProtocolError,\n        ) as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except Exception as error:\n            logger.exception(\"%s: %s\", type(error).__name__, error)\n            resp = JSONResponse(status_code=500, content={\"message\": \"Internal error.\"})\n        return resp\n\n    return wrapped_route\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/http_api/","title":"Http api","text":""},{"location":"reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.HttpInterface","title":"<code>HttpInterface</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Roboflow defined HTTP interface for a general-purpose inference server.</p> <p>This class sets up the FastAPI application and adds necessary middleware, as well as initializes the model manager and model registry for the inference server.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>class HttpInterface(BaseInterface):\n    \"\"\"Roboflow defined HTTP interface for a general-purpose inference server.\n\n    This class sets up the FastAPI application and adds necessary middleware,\n    as well as initializes the model manager and model registry for the inference server.\n\n    Attributes:\n        app (FastAPI): The FastAPI application instance.\n        model_manager (ModelManager): The manager for handling different models.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: ModelManager,\n        root_path: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the HttpInterface with given model manager and model registry.\n\n        Args:\n            model_manager (ModelManager): The manager for handling different models.\n            root_path (Optional[str]): The root path for the FastAPI application.\n\n        Description:\n            Deploy Roboflow trained models to nearly any compute environment!\n        \"\"\"\n\n        description = \"Roboflow inference server\"\n\n        app = FastAPI(\n            title=\"Roboflow Inference Server\",\n            description=description,\n            version=__version__,\n            terms_of_service=\"https://roboflow.com/terms\",\n            contact={\n                \"name\": \"Roboflow Inc.\",\n                \"url\": \"https://roboflow.com/contact\",\n                \"email\": \"help@roboflow.com\",\n            },\n            license_info={\n                \"name\": \"Apache 2.0\",\n                \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n            },\n            root_path=root_path,\n        )\n        # Ensure in-memory logging is initialized as early as possible for all runtimes\n        try:\n            from inference.core.logging.memory_handler import setup_memory_logging\n\n            setup_memory_logging()\n        except Exception:\n            pass\n\n        app.mount(\n            \"/static\",\n            StaticFiles(directory=\"./inference/landing/out/static\", html=True),\n            name=\"static\",\n        )\n        app.mount(\n            \"/_next/static\",\n            StaticFiles(directory=\"./inference/landing/out/_next/static\", html=True),\n            name=\"_next_static\",\n        )\n\n        @app.on_event(\"shutdown\")\n        async def on_shutdown():\n            logger.info(\"Shutting down %s\", description)\n            await usage_collector.async_push_usage_payloads()\n\n        InferenceInstrumentator(app, model_manager=model_manager, endpoint=\"/metrics\")\n        if LAMBDA:\n            app.add_middleware(LambdaMiddleware)\n        if GCP_SERVERLESS:\n            app.add_middleware(GCPServerlessMiddleware)\n\n        if len(ALLOW_ORIGINS) &gt; 0:\n            # Add CORS Middleware (but not for /build**, which is controlled separately)\n            app.add_middleware(\n                PathAwareCORSMiddleware,\n                match_paths=r\"^(?!/build).*\",\n                allow_origins=ALLOW_ORIGINS,\n                allow_credentials=True,\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n                expose_headers=[PROCESSING_TIME_HEADER],\n            )\n\n        # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n        if PROFILE:\n            app.add_middleware(\n                CProfileMiddleware,\n                enable=True,\n                server_app=app,\n                filename=\"/profile/output.pstats\",\n                strip_dirs=False,\n                sort_by=\"cumulative\",\n            )\n        if API_LOGGING_ENABLED:\n            app.add_middleware(\n                asgi_correlation_id.CorrelationIdMiddleware,\n                header_name=CORRELATION_ID_HEADER,\n                update_request_header=True,\n                generator=lambda: uuid4().hex,\n                validator=lambda a: True,\n                transformer=lambda a: a,\n            )\n        else:\n            app.add_middleware(asgi_correlation_id.CorrelationIdMiddleware)\n\n        if METRICS_ENABLED:\n\n            @app.middleware(\"http\")\n            async def count_errors(request: Request, call_next):\n                \"\"\"Middleware to count errors.\n\n                Args:\n                    request (Request): The incoming request.\n                    call_next (Callable): The next middleware or endpoint to call.\n\n                Returns:\n                    Response: The response from the next middleware or endpoint.\n                \"\"\"\n                response = await call_next(request)\n                if self.model_manager.pingback and response.status_code &gt;= 400:\n                    self.model_manager.num_errors += 1\n                return response\n\n        if not (LAMBDA or GCP_SERVERLESS):\n\n            @app.get(\"/device/stats\")\n            def device_stats():\n                not_configured_error_message = {\n                    \"error\": \"Device statistics endpoint is not enabled.\",\n                    \"hint\": \"Mount the Docker socket and point its location when running the docker \"\n                    \"container to collect device stats \"\n                    \"(i.e. `docker run ... -v /var/run/docker.sock:/var/run/docker.sock \"\n                    \"-e DOCKER_SOCKET_PATH=/var/run/docker.sock ...`).\",\n                }\n                if not DOCKER_SOCKET_PATH:\n                    return JSONResponse(\n                        status_code=404,\n                        content=not_configured_error_message,\n                    )\n                if not is_docker_socket_mounted(docker_socket_path=DOCKER_SOCKET_PATH):\n                    return JSONResponse(\n                        status_code=500,\n                        content=not_configured_error_message,\n                    )\n                container_stats = get_container_stats(\n                    docker_socket_path=DOCKER_SOCKET_PATH\n                )\n                return JSONResponse(status_code=200, content=container_stats)\n\n        cached_api_keys = dict()\n\n        if GCP_SERVERLESS:\n\n            @app.middleware(\"http\")\n            async def check_authorization_serverless(request: Request, call_next):\n                # exclusions\n                skip_check = (\n                    request.method not in [\"GET\", \"POST\"]\n                    or request.url.path\n                    in [\n                        \"/\",\n                        \"/docs\",\n                        \"/info\",\n                        \"/healthz\",  # health check endpoint for liveness probe\n                        \"/readiness\",\n                        \"/metrics\",\n                        \"/openapi.json\",  # needed for /docs and /redoc\n                        \"/model/registry\",  # dont auth this route, usually not used on serverlerless, but queue based serverless uses it internally (not accessible from outside)\n                    ]\n                    or request.url.path.startswith(\"/static/\")\n                    or request.url.path.startswith(\"/_next/\")\n                )\n\n                # for these routes we only want to auth if dynamic python modules are provided\n                if request.url.path in [\n                    \"/workflows/blocks/describe\",\n                    \"/workflows/definition/schema\",\n                ]:\n                    if request.method == \"GET\":\n                        skip_check = True\n\n                    elif (\n                        get_content_type(request) == \"application/json\"\n                        and int(request.headers.get(\"content-length\", 0)) &gt; 0\n                    ):\n                        json_params = await request.json()\n                        dynamic_blocks_definitions = json_params.get(\n                            \"dynamic_blocks_definitions\", None\n                        )\n                        if not dynamic_blocks_definitions:\n                            skip_check = True\n\n                if skip_check:\n                    return await call_next(request)\n\n                def _unauthorized_response(msg):\n                    return JSONResponse(\n                        status_code=401,\n                        content={\n                            \"status\": 401,\n                            \"message\": msg,\n                        },\n                    )\n\n                req_params = request.query_params\n                json_params = dict()\n                api_key = req_params.get(\"api_key\", None)\n                if (\n                    api_key is None\n                    and get_content_type(request) == \"application/json\"\n                    and int(request.headers.get(\"content-length\", 0)) &gt; 0\n                ):\n                    # have to try catch here, because some legacy endpoints that abuse Content-Type header but dont actually receive json\n                    try:\n                        json_params = await request.json()\n                    except Exception:\n                        pass\n                api_key = json_params.get(\"api_key\", api_key)\n\n                if api_key is None:\n                    return _unauthorized_response(\"Unauthorized api_key\")\n\n                if cached_api_keys.get(api_key, 0) &lt; time.time():\n                    try:\n                        await get_roboflow_workspace_async(api_key=api_key)\n                        cached_api_keys[api_key] = (\n                            time.time() + 3600\n                        )  # expired after 1 hour\n                    except (RoboflowAPINotAuthorizedError, WorkspaceLoadError):\n                        return _unauthorized_response(\"Unauthorized api_key\")\n\n                return await call_next(request)\n\n        if DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n\n            @app.middleware(\"http\")\n            async def check_authorization(request: Request, call_next):\n                # exclusions\n                skip_check = (\n                    request.method not in [\"GET\", \"POST\"]\n                    or request.url.path\n                    in [\n                        \"/\",\n                        \"/docs\",\n                        \"/redoc\",\n                        \"/info\",\n                        \"/healthz\",  # health check endpoint for liveness probe\n                        \"/readiness\",\n                        \"/metrics\",\n                        \"/openapi.json\",  # needed for /docs and /redoc\n                    ]\n                    or request.url.path.startswith(\"/static/\")\n                    or request.url.path.startswith(\"/_next/\")\n                )\n                if skip_check:\n                    return await call_next(request)\n\n                def _unauthorized_response(msg):\n                    return JSONResponse(\n                        status_code=401,\n                        content={\n                            \"status\": 401,\n                            \"message\": msg,\n                        },\n                    )\n\n                # check api_key\n                req_params = request.query_params\n                json_params = dict()\n                api_key = req_params.get(\"api_key\", None)\n                if (\n                    api_key is None\n                    and get_content_type(request) == \"application/json\"\n                    and int(request.headers.get(\"content-length\", 0)) &gt; 0\n                ):\n                    # have to try catch here, because some legacy endpoints that abuse Content-Type header but dont actually receive json\n                    try:\n                        json_params = await request.json()\n                    except Exception:\n                        pass\n                api_key = json_params.get(\"api_key\", api_key)\n\n                if api_key is None:\n                    return _unauthorized_response(\"Unauthorized api_key\")\n\n                if cached_api_keys.get(api_key, 0) &lt; time.time():\n                    try:\n                        # TODO: make this request async!\n                        if api_key is None:\n                            workspace_url = None\n                        else:\n                            workspace_url = await get_roboflow_workspace_async(\n                                api_key=api_key\n                            )\n\n                        if workspace_url != DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n                            return _unauthorized_response(\"Unauthorized api_key\")\n\n                        cached_api_keys[api_key] = (\n                            time.time() + 3600\n                        )  # expired after 1 hour\n                    except (RoboflowAPINotAuthorizedError, WorkspaceLoadError):\n                        return _unauthorized_response(\"Unauthorized api_key\")\n\n                return await call_next(request)\n\n        self.app = app\n        self.model_manager = model_manager\n        self.stream_manager_client: Optional[StreamManagerClient] = None\n\n        if ENABLE_STREAM_API:\n            operations_timeout = os.getenv(\"STREAM_MANAGER_OPERATIONS_TIMEOUT\")\n            if operations_timeout is not None:\n                operations_timeout = float(operations_timeout)\n            self.stream_manager_client = StreamManagerClient.init(\n                host=os.getenv(\"STREAM_MANAGER_HOST\", \"127.0.0.1\"),\n                port=int(os.getenv(\"STREAM_MANAGER_PORT\", \"7070\")),\n                operations_timeout=operations_timeout,\n            )\n\n        def process_inference_request(\n            inference_request: InferenceRequest,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n            **kwargs,\n        ) -&gt; InferenceResponse:\n            \"\"\"Processes an inference request by calling the appropriate model.\n\n            Args:\n                inference_request (InferenceRequest): The request containing model ID and other inference details.\n                countinference (Optional[bool]): Whether to count inference for usage.\n                service_secret (Optional[str]): The service secret.\n\n            Returns:\n                InferenceResponse: The response containing the inference results.\n            \"\"\"\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=inference_request.model_id\n            )\n            self.model_manager.add_model(\n                de_aliased_model_id,\n                inference_request.api_key,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n            resp = self.model_manager.infer_from_request_sync(\n                de_aliased_model_id, inference_request, **kwargs\n            )\n            return orjson_response(resp)\n\n        def process_workflow_inference_request(\n            workflow_request: WorkflowInferenceRequest,\n            workflow_specification: dict,\n            background_tasks: Optional[BackgroundTasks],\n            profiler: WorkflowsProfiler,\n        ) -&gt; WorkflowInferenceResponse:\n\n            workflow_init_parameters = {\n                \"workflows_core.model_manager\": model_manager,\n                \"workflows_core.api_key\": workflow_request.api_key,\n                \"workflows_core.background_tasks\": background_tasks,\n            }\n            execution_engine = ExecutionEngine.init(\n                workflow_definition=workflow_specification,\n                init_parameters=workflow_init_parameters,\n                max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                prevent_local_images_loading=True,\n                profiler=profiler,\n                workflow_id=workflow_request.workflow_id,\n            )\n            is_preview = False\n            if hasattr(workflow_request, \"is_preview\"):\n                is_preview = workflow_request.is_preview\n            workflow_results = execution_engine.run(\n                runtime_parameters=workflow_request.inputs,\n                serialize_results=True,\n                _is_preview=is_preview,\n            )\n            with profiler.profile_execution_phase(\n                name=\"workflow_results_filtering\",\n                categories=[\"inference_package_operation\"],\n            ):\n                outputs = filter_out_unwanted_workflow_outputs(\n                    workflow_results=workflow_results,\n                    excluded_fields=workflow_request.excluded_fields,\n                )\n            profiler_trace = profiler.export_trace()\n            response = WorkflowInferenceResponse(\n                outputs=outputs,\n                profiler_trace=profiler_trace,\n            )\n            return orjson_response(response=response)\n\n        def load_core_model(\n            inference_request: InferenceRequest,\n            api_key: Optional[str] = None,\n            core_model: str = None,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ) -&gt; None:\n            \"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n            Args:\n                inference_request (InferenceRequest): The request containing version and other details.\n                api_key (Optional[str]): The API key for the request.\n                core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n                countinference (Optional[bool]): Whether to count inference or not.\n                service_secret (Optional[str]): The service secret for the request.\n\n            Returns:\n                str: The core model ID.\n            \"\"\"\n            if api_key:\n                inference_request.api_key = api_key\n            version_id_field = f\"{core_model}_version_id\"\n            core_model_id = (\n                f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n            )\n            self.model_manager.add_model(\n                core_model_id,\n                inference_request.api_key,\n                endpoint_type=ModelEndpointType.CORE_MODEL,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n            return core_model_id\n\n        load_clip_model = partial(load_core_model, core_model=\"clip\")\n        \"\"\"Loads the CLIP model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The CLIP model ID.\n        \"\"\"\n\n        load_pe_model = partial(load_core_model, core_model=\"perception_encoder\")\n        \"\"\"Loads the Perception Encoder model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The Perception Encoder model ID.\n        \"\"\"\n\n        load_sam_model = partial(load_core_model, core_model=\"sam\")\n        \"\"\"Loads the SAM model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The SAM model ID.\n        \"\"\"\n        load_sam2_model = partial(load_core_model, core_model=\"sam2\")\n        \"\"\"Loads the SAM2 model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The SAM2 model ID.\n        \"\"\"\n\n        load_gaze_model = partial(load_core_model, core_model=\"gaze\")\n        \"\"\"Loads the GAZE model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The GAZE model ID.\n        \"\"\"\n\n        load_doctr_model = partial(load_core_model, core_model=\"doctr\")\n        \"\"\"Loads the DocTR model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The DocTR model ID.\n        \"\"\"\n\n        load_easy_ocr_model = partial(load_core_model, core_model=\"easy_ocr\")\n        \"\"\"Loads the EasyOCR model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The EasyOCR model ID.\n        \"\"\"\n\n        load_paligemma_model = partial(load_core_model, core_model=\"paligemma\")\n\n        load_grounding_dino_model = partial(\n            load_core_model, core_model=\"grounding_dino\"\n        )\n        \"\"\"Loads the Grounding DINO model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The Grounding DINO model ID.\n        \"\"\"\n\n        load_yolo_world_model = partial(load_core_model, core_model=\"yolo_world\")\n        load_owlv2_model = partial(load_core_model, core_model=\"owlv2\")\n        \"\"\"Loads the YOLO World model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The YOLO World model ID.\n        \"\"\"\n\n        load_trocr_model = partial(load_core_model, core_model=\"trocr\")\n        \"\"\"Loads the TrOCR model into the model manager.\n\n        Args:\n        Same as `load_core_model`.\n\n        Returns:\n        The TrOCR model ID.\n        \"\"\"\n\n        @app.get(\n            \"/info\",\n            response_model=ServerVersionInfo,\n            summary=\"Info\",\n            description=\"Get the server name and version number\",\n        )\n        def root():\n            \"\"\"Endpoint to get the server name and version number.\n\n            Returns:\n                ServerVersionInfo: The server version information.\n            \"\"\"\n            return ServerVersionInfo(\n                name=\"Roboflow Inference Server\",\n                version=__version__,\n                uuid=GLOBAL_INFERENCE_SERVER_ID,\n            )\n\n        @app.get(\n            \"/logs\",\n            summary=\"Get Recent Logs\",\n            description=\"Get recent application logs for debugging\",\n        )\n        def get_logs(\n            limit: Optional[int] = Query(\n                100, description=\"Maximum number of log entries to return\"\n            ),\n            level: Optional[str] = Query(\n                None,\n                description=\"Filter by log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\",\n            ),\n            since: Optional[str] = Query(\n                None, description=\"Return logs since this ISO timestamp\"\n            ),\n        ):\n            \"\"\"Get recent application logs from memory.\n\n            Only available when ENABLE_IN_MEMORY_LOGS environment variable is set to 'true'.\n\n            Args:\n                limit: Maximum number of log entries (default 100)\n                level: Filter by log level\n                since: ISO timestamp to filter logs since\n\n            Returns:\n                List of log entries with timestamp, level, logger, and message\n            \"\"\"\n            # Check if in-memory logging is enabled\n            from inference.core.logging.memory_handler import (\n                get_recent_logs,\n                is_memory_logging_enabled,\n            )\n\n            if not is_memory_logging_enabled():\n                raise HTTPException(\n                    status_code=404, detail=\"Logs endpoint not available\"\n                )\n\n            try:\n                logs = get_recent_logs(limit=limit or 100, level=level, since=since)\n                return {\"logs\": logs, \"total_count\": len(logs)}\n            except (ImportError, ModuleNotFoundError):\n                raise HTTPException(\n                    status_code=500, detail=\"Logging system not properly initialized\"\n                )\n\n        if not LAMBDA and GET_MODEL_REGISTRY_ENABLED:\n\n            @app.get(\n                \"/model/registry\",\n                response_model=ModelsDescriptions,\n                summary=\"Get model keys\",\n                description=\"Get the ID of each loaded model\",\n            )\n            def registry():\n                \"\"\"Get the ID of each loaded model in the registry.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/registry\")\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n        # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n        if not (LAMBDA or GCP_SERVERLESS):\n\n            @app.post(\n                \"/model/add\",\n                response_model=ModelsDescriptions,\n                summary=\"Load a model\",\n                description=\"Load the model with the given model ID\",\n            )\n            @with_route_exceptions\n            def model_add(\n                request: AddModelRequest,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"Load the model with the given model ID into the model manager.\n\n                Args:\n                    request (AddModelRequest): The request containing the model ID and optional API key.\n                    countinference (Optional[bool]): Whether to count inference or not.\n                    service_secret (Optional[str]): The service secret for the request.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/add\")\n                de_aliased_model_id = resolve_roboflow_model_alias(\n                    model_id=request.model_id\n                )\n                logger.info(f\"Loading model: {de_aliased_model_id}\")\n                self.model_manager.add_model(\n                    de_aliased_model_id,\n                    request.api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/remove\",\n                response_model=ModelsDescriptions,\n                summary=\"Remove a model\",\n                description=\"Remove the model with the given model ID\",\n            )\n            @with_route_exceptions\n            def model_remove(request: ClearModelRequest):\n                \"\"\"Remove the model with the given model ID from the model manager.\n\n                Args:\n                    request (ClearModelRequest): The request containing the model ID to be removed.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/remove\")\n                de_aliased_model_id = resolve_roboflow_model_alias(\n                    model_id=request.model_id\n                )\n                self.model_manager.remove(de_aliased_model_id)\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/clear\",\n                response_model=ModelsDescriptions,\n                summary=\"Remove all models\",\n                description=\"Remove all loaded models\",\n            )\n            @with_route_exceptions\n            def model_clear():\n                \"\"\"Remove all loaded models from the model manager.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/clear\")\n                self.model_manager.clear()\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n        # these NEW endpoints need authentication protection\n        if not LAMBDA and not GCP_SERVERLESS:\n\n            @app.post(\n                \"/infer/object_detection\",\n                response_model=Union[\n                    ObjectDetectionInferenceResponse,\n                    List[ObjectDetectionInferenceResponse],\n                    StubResponse,\n                ],\n                summary=\"Object detection infer\",\n                description=\"Run inference with the specified object detection model\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_object_detection(\n                inference_request: ObjectDetectionInferenceRequest,\n                background_tasks: BackgroundTasks,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"Run inference with the specified object detection model.\n\n                Args:\n                    inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/object_detection\")\n                return process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n            @app.post(\n                \"/infer/instance_segmentation\",\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse, StubResponse\n                ],\n                summary=\"Instance segmentation infer\",\n                description=\"Run inference with the specified instance segmentation model\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_instance_segmentation(\n                inference_request: InstanceSegmentationInferenceRequest,\n                background_tasks: BackgroundTasks,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"Run inference with the specified instance segmentation model.\n\n                Args:\n                    inference_request (InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    InstanceSegmentationInferenceResponse: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/instance_segmentation\")\n                return process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n            @app.post(\n                \"/infer/classification\",\n                response_model=Union[\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                ],\n                summary=\"Classification infer\",\n                description=\"Run inference with the specified classification model\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_classification(\n                inference_request: ClassificationInferenceRequest,\n                background_tasks: BackgroundTasks,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"Run inference with the specified classification model.\n\n                Args:\n                    inference_request (ClassificationInferenceRequest): The request containing the necessary details for classification.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/classification\")\n                return process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n            @app.post(\n                \"/infer/keypoints_detection\",\n                response_model=Union[KeypointsDetectionInferenceResponse, StubResponse],\n                summary=\"Keypoints detection infer\",\n                description=\"Run inference with the specified keypoints detection model\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_keypoints(\n                inference_request: KeypointsDetectionInferenceRequest,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"Run inference with the specified keypoints detection model.\n\n                Args:\n                    inference_request (KeypointsDetectionInferenceRequest): The request containing the necessary details for keypoints detection.\n\n                Returns:\n                    Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/keypoints_detection\")\n                return process_inference_request(\n                    inference_request,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n            if LMM_ENABLED or MOONDREAM2_ENABLED:\n\n                @app.post(\n                    \"/infer/lmm\",\n                    response_model=Union[\n                        LMMInferenceResponse,\n                        List[LMMInferenceResponse],\n                        StubResponse,\n                    ],\n                    summary=\"Large multi-modal model infer\",\n                    description=\"Run inference with the specified large multi-modal model\",\n                    response_model_exclude_none=True,\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def infer_lmm(\n                    inference_request: LMMInferenceRequest,\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"Run inference with the specified object detection model.\n\n                    Args:\n                        inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                        background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                    Returns:\n                        Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                    \"\"\"\n                    logger.debug(f\"Reached /infer/lmm\")\n                    return process_inference_request(\n                        inference_request,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n\n        if not DISABLE_WORKFLOW_ENDPOINTS:\n\n            @app.post(\n                \"/{workspace_name}/workflows/{workflow_id}/describe_interface\",\n                response_model=DescribeInterfaceResponse,\n                summary=\"Endpoint to describe interface of predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - describes workflow inputs and outputs\",\n            )\n            @with_route_exceptions\n            def describe_predefined_workflow_interface(\n                workspace_name: str,\n                workflow_id: str,\n                workflow_request: PredefinedWorkflowDescribeInterfaceRequest,\n            ) -&gt; DescribeInterfaceResponse:\n                workflow_specification = get_workflow_specification(\n                    api_key=workflow_request.api_key,\n                    workspace_id=workspace_name,\n                    workflow_id=workflow_id,\n                    use_cache=workflow_request.use_cache,\n                )\n                return handle_describe_workflows_interface(\n                    definition=workflow_specification,\n                )\n\n            @app.post(\n                \"/workflows/describe_interface\",\n                response_model=DescribeInterfaceResponse,\n                summary=\"Endpoint to describe interface of workflow given in request\",\n                description=\"Parses workflow definition and retrieves describes inputs and outputs\",\n            )\n            @with_route_exceptions\n            def describe_workflow_interface(\n                workflow_request: WorkflowSpecificationDescribeInterfaceRequest,\n            ) -&gt; DescribeInterfaceResponse:\n                return handle_describe_workflows_interface(\n                    definition=workflow_request.specification,\n                )\n\n            @app.post(\n                \"/{workspace_name}/workflows/{workflow_id}\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"Endpoint to run predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body\",\n            )\n            @app.post(\n                \"/infer/workflows/{workspace_name}/{workflow_id}\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"[LEGACY] Endpoint to run predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024\",\n                deprecated=True,\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_from_predefined_workflow(\n                workspace_name: str,\n                workflow_id: str,\n                workflow_request: PredefinedWorkflowInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ) -&gt; WorkflowInferenceResponse:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                    profiler = BaseWorkflowsProfiler.init(\n                        max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                    )\n                else:\n                    profiler = NullWorkflowsProfiler.init()\n                with profiler.profile_execution_phase(\n                    name=\"workflow_definition_fetching\",\n                    categories=[\"inference_package_operation\"],\n                ):\n                    workflow_specification = get_workflow_specification(\n                        api_key=workflow_request.api_key,\n                        workspace_id=workspace_name,\n                        workflow_id=workflow_id,\n                        use_cache=workflow_request.use_cache,\n                    )\n                if not workflow_request.workflow_id:\n                    workflow_request.workflow_id = workflow_id\n                if not workflow_specification.get(\"id\"):\n                    logger.warning(\n                        \"Internal workflow ID missing in specification for '%s'\",\n                        workflow_id,\n                    )\n                return process_workflow_inference_request(\n                    workflow_request=workflow_request,\n                    workflow_specification=workflow_specification,\n                    background_tasks=(\n                        background_tasks if not (LAMBDA or GCP_SERVERLESS) else None\n                    ),\n                    profiler=profiler,\n                )\n\n            @app.post(\n                \"/workflows/run\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"Endpoint to run workflow specification provided in payload\",\n                description=\"Parses and executes workflow specification, injecting runtime parameters from request body.\",\n            )\n            @app.post(\n                \"/infer/workflows\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"[LEGACY] Endpoint to run workflow specification provided in payload\",\n                description=\"Parses and executes workflow specification, injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024.\",\n                deprecated=True,\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_from_workflow(\n                workflow_request: WorkflowSpecificationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ) -&gt; WorkflowInferenceResponse:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                    profiler = BaseWorkflowsProfiler.init(\n                        max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                    )\n                else:\n                    profiler = NullWorkflowsProfiler.init()\n                return process_workflow_inference_request(\n                    workflow_request=workflow_request,\n                    workflow_specification=workflow_request.specification,\n                    background_tasks=(\n                        background_tasks if not (LAMBDA or GCP_SERVERLESS) else None\n                    ),\n                    profiler=profiler,\n                )\n\n            @app.get(\n                \"/workflows/execution_engine/versions\",\n                response_model=ExecutionEngineVersions,\n                summary=\"Returns available Execution Engine versions sorted from oldest to newest\",\n                description=\"Returns available Execution Engine versions sorted from oldest to newest\",\n            )\n            @with_route_exceptions\n            def get_execution_engine_versions() -&gt; ExecutionEngineVersions:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                versions = get_available_versions()\n                return ExecutionEngineVersions(versions=versions)\n\n            @app.get(\n                \"/workflows/blocks/describe\",\n                response_model=WorkflowsBlocksDescription,\n                summary=\"[LEGACY] Endpoint to get definition of workflows blocks that are accessible\",\n                description=\"Endpoint provides detailed information about workflows building blocks that are \"\n                \"accessible in the inference server. This information could be used to programmatically \"\n                \"build / display workflows.\",\n                deprecated=True,\n            )\n            @with_route_exceptions\n            def describe_workflows_blocks(\n                request: Request,\n            ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n                result = handle_describe_workflows_blocks_request()\n                return gzip_response_if_requested(request=request, response=result)\n\n            @app.post(\n                \"/workflows/blocks/describe\",\n                response_model=WorkflowsBlocksDescription,\n                summary=\"[EXPERIMENTAL] Endpoint to get definition of workflows blocks that are accessible\",\n                description=\"Endpoint provides detailed information about workflows building blocks that are \"\n                \"accessible in the inference server. This information could be used to programmatically \"\n                \"build / display workflows. Additionally - in request body one can specify list of \"\n                \"dynamic blocks definitions which will be transformed into blocks and used to generate \"\n                \"schemas and definitions of connections\",\n            )\n            @with_route_exceptions\n            def describe_workflows_blocks(\n                request: Request,\n                request_payload: Optional[DescribeBlocksRequest] = None,\n            ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                dynamic_blocks_definitions = None\n                requested_execution_engine_version = None\n                api_key = None\n                if request_payload is not None:\n                    dynamic_blocks_definitions = (\n                        request_payload.dynamic_blocks_definitions\n                    )\n                    requested_execution_engine_version = (\n                        request_payload.execution_engine_version\n                    )\n                    api_key = request_payload.api_key or request.query_params.get(\n                        \"api_key\", None\n                    )\n                result = handle_describe_workflows_blocks_request(\n                    dynamic_blocks_definitions=dynamic_blocks_definitions,\n                    requested_execution_engine_version=requested_execution_engine_version,\n                    api_key=api_key,\n                )\n                return gzip_response_if_requested(request=request, response=result)\n\n            @app.get(\n                \"/workflows/definition/schema\",\n                response_model=WorkflowsBlocksSchemaDescription,\n                summary=\"Endpoint to fetch the workflows block schema\",\n                description=\"Endpoint to fetch the schema of all available blocks. This information can be \"\n                \"used to validate workflow definitions and suggest syntax in the JSON editor.\",\n            )\n            @with_route_exceptions\n            def get_workflow_schema(\n                request: Request,\n            ) -&gt; WorkflowsBlocksSchemaDescription:\n                result = get_workflow_schema_description()\n                return gzip_response_if_requested(request, response=result)\n\n            @app.post(\n                \"/workflows/blocks/dynamic_outputs\",\n                response_model=List[OutputDefinition],\n                summary=\"[EXPERIMENTAL] Endpoint to get definition of dynamic output for workflow step\",\n                description=\"Endpoint to be used when step outputs can be discovered only after \"\n                \"filling manifest with data.\",\n            )\n            @with_route_exceptions\n            def get_dynamic_block_outputs(\n                step_manifest: Dict[str, Any],\n            ) -&gt; List[OutputDefinition]:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                # Potentially TODO: dynamic blocks do not support dynamic outputs, but if it changes\n                # we need to provide dynamic blocks manifests here\n                dummy_workflow_definition = {\n                    \"version\": \"1.0\",\n                    \"inputs\": [],\n                    \"steps\": [step_manifest],\n                    \"outputs\": [],\n                }\n                available_blocks = load_workflow_blocks()\n                parsed_definition = parse_workflow_definition(\n                    raw_workflow_definition=dummy_workflow_definition,\n                    available_blocks=available_blocks,\n                )\n                parsed_manifest = parsed_definition.steps[0]\n                return parsed_manifest.get_actual_outputs()\n\n            @app.post(\n                \"/workflows/validate\",\n                response_model=WorkflowValidationStatus,\n                summary=\"[EXPERIMENTAL] Endpoint to validate\",\n                description=\"Endpoint provides a way to check validity of JSON workflow definition.\",\n            )\n            @with_route_exceptions\n            def validate_workflow(\n                specification: dict,\n            ) -&gt; WorkflowValidationStatus:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                step_execution_mode = StepExecutionMode(WORKFLOWS_STEP_EXECUTION_MODE)\n                workflow_init_parameters = {\n                    \"workflows_core.model_manager\": model_manager,\n                    \"workflows_core.api_key\": None,\n                    \"workflows_core.background_tasks\": None,\n                    \"workflows_core.step_execution_mode\": step_execution_mode,\n                }\n                _ = ExecutionEngine.init(\n                    workflow_definition=specification,\n                    init_parameters=workflow_init_parameters,\n                    max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                    prevent_local_images_loading=True,\n                )\n                return WorkflowValidationStatus(status=\"ok\")\n\n        if WEBRTC_WORKER_ENABLED:\n\n            @app.post(\n                \"/initialise_webrtc_worker\",\n                response_model=InitializeWebRTCResponse,\n                summary=\"[EXPERIMENTAL] Establishes WebRTC peer connection and processes video stream in spawned process or modal function\",\n                description=\"[EXPERIMENTAL] Establishes WebRTC peer connection and processes video stream in spawned process or modal function\",\n            )\n            @with_route_exceptions_async\n            async def initialise_webrtc_worker(\n                request: WebRTCWorkerRequest,\n            ) -&gt; InitializeWebRTCResponse:\n                logger.debug(\"Received initialise_webrtc_worker request\")\n                worker_result: WebRTCWorkerResult = await start_worker(\n                    webrtc_request=request,\n                )\n                if worker_result.exception_type is not None:\n                    if worker_result.exception_type == \"WorkflowSyntaxError\":\n                        raise WorkflowSyntaxError(\n                            public_message=worker_result.error_message,\n                            context=worker_result.error_context,\n                            inner_error=worker_result.inner_error,\n                        )\n                    expected_exceptions = {\n                        \"Exception\": Exception,\n                        \"KeyError\": KeyError,\n                        \"MissingApiKeyError\": MissingApiKeyError,\n                        \"NotImplementedError\": NotImplementedError,\n                        \"RoboflowAPINotAuthorizedError\": RoboflowAPINotAuthorizedError,\n                        \"RoboflowAPINotNotFoundError\": RoboflowAPINotNotFoundError,\n                        \"ValidationError\": ValidationError,\n                    }\n                    exc = expected_exceptions.get(\n                        worker_result.exception_type, Exception\n                    )(worker_result.error_message)\n                    logger.error(\n                        f\"Initialise webrtc worker failed with %s: %s\",\n                        worker_result.exception_type,\n                        worker_result.error_message,\n                    )\n                    raise exc\n                logger.debug(\"Returning initialise_webrtc_worker response\")\n                return InitializeWebRTCResponse(\n                    context=CommandContext(),\n                    status=OperationStatus.SUCCESS,\n                    sdp=worker_result.answer.sdp,\n                    type=worker_result.answer.type,\n                )\n\n        if ENABLE_STREAM_API:\n\n            @app.get(\n                \"/inference_pipelines/list\",\n                response_model=ListPipelinesResponse,\n                summary=\"[EXPERIMENTAL] List active InferencePipelines\",\n                description=\"[EXPERIMENTAL] Listing all active InferencePipelines processing videos\",\n            )\n            @with_route_exceptions_async\n            async def list_pipelines(_: Request) -&gt; ListPipelinesResponse:\n                return await self.stream_manager_client.list_pipelines()\n\n            @app.get(\n                \"/inference_pipelines/{pipeline_id}/status\",\n                response_model=InferencePipelineStatusResponse,\n                summary=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n                description=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n            )\n            @with_route_exceptions_async\n            async def get_status(pipeline_id: str) -&gt; InferencePipelineStatusResponse:\n                return await self.stream_manager_client.get_status(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.post(\n                \"/inference_pipelines/initialise\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n                description=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n            )\n            @with_route_exceptions_async\n            async def initialise(request: InitialisePipelinePayload) -&gt; CommandResponse:\n                return await self.stream_manager_client.initialise_pipeline(\n                    initialisation_request=request\n                )\n\n            @app.post(\n                \"/inference_pipelines/initialise_webrtc\",\n                response_model=InitializeWebRTCPipelineResponse,\n                summary=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n                description=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n            )\n            @with_route_exceptions_async\n            async def initialise_webrtc_inference_pipeline(\n                request: InitialiseWebRTCPipelinePayload,\n            ) -&gt; CommandResponse:\n                logger.debug(\"Received initialise webrtc inference pipeline request\")\n                resp = await self.stream_manager_client.initialise_webrtc_pipeline(\n                    initialisation_request=request\n                )\n                logger.debug(\"Returning initialise webrtc inference pipeline response\")\n                return resp\n\n            @app.post(\n                \"/inference_pipelines/{pipeline_id}/pause\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n                description=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n            )\n            @with_route_exceptions_async\n            async def pause(pipeline_id: str) -&gt; CommandResponse:\n                return await self.stream_manager_client.pause_pipeline(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.post(\n                \"/inference_pipelines/{pipeline_id}/resume\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n                description=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n            )\n            @with_route_exceptions_async\n            async def resume(pipeline_id: str) -&gt; CommandResponse:\n                return await self.stream_manager_client.resume_pipeline(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.post(\n                \"/inference_pipelines/{pipeline_id}/terminate\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n                description=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n            )\n            @with_route_exceptions_async\n            async def terminate(pipeline_id: str) -&gt; CommandResponse:\n                return await self.stream_manager_client.terminate_pipeline(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.get(\n                \"/inference_pipelines/{pipeline_id}/consume\",\n                response_model=ConsumePipelineResponse,\n                summary=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n                description=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n            )\n            @with_route_exceptions_async\n            async def consume(\n                pipeline_id: str,\n                request: Optional[ConsumeResultsPayload] = None,\n            ) -&gt; ConsumePipelineResponse:\n                if request is None:\n                    request = ConsumeResultsPayload()\n                return await self.stream_manager_client.consume_pipeline_result(\n                    pipeline_id=pipeline_id,\n                    excluded_fields=request.excluded_fields,\n                )\n\n        # Enable preloading models at startup\n        if (\n            (PRELOAD_MODELS or DEDICATED_DEPLOYMENT_WORKSPACE_URL)\n            and API_KEY\n            and not (LAMBDA or GCP_SERVERLESS)\n        ):\n\n            class ModelInitState:\n                \"\"\"Class to track model initialization state.\"\"\"\n\n                def __init__(self):\n                    self.is_ready = False\n                    self.lock = Lock()  # For thread-safe updates\n                    self.initialization_errors = []  # Track errors per model\n\n            model_init_state = ModelInitState()\n\n            def initialize_models(state: ModelInitState):\n                \"\"\"Perform asynchronous initialization tasks to load models.\"\"\"\n                # Limit the number of concurrent tasks to prevent resource exhaustion\n\n                def load_model(model_id):\n                    logger.debug(f\"load_model({model_id}) - starting\", flush=True)\n                    try:\n                        # TODO: how to add timeout here? Probably best to timeout model loading?\n                        model_add(\n                            AddModelRequest(\n                                model_id=model_id,\n                                model_type=None,\n                                api_key=API_KEY,\n                            )\n                        )\n                        logger.info(f\"Model {model_id} loaded successfully.\")\n                    except Exception as e:\n                        error_msg = f\"Error loading model {model_id}: {e}\"\n                        logger.error(error_msg)\n                        with state.lock:\n                            state.initialization_errors.append((model_id, str(e)))\n                    logger.debug(f\"load_model({model_id}) - finished\", flush=True)\n\n                if PRELOAD_MODELS:\n                    # Create tasks for each model to be loaded\n                    model_loading_executor = ThreadPoolExecutor(max_workers=2)\n                    loaded_futures: List[Tuple[str, Future]] = []\n                    for model_id in PRELOAD_MODELS:\n                        future = model_loading_executor.submit(\n                            load_model, model_id=model_id\n                        )\n                        loaded_futures.append((model_id, future))\n\n                    for model_id, future in loaded_futures:\n                        try:\n                            future.result(timeout=300)\n                        except (\n                            TimeoutError,\n                            CancelledError,\n                            concurrent.futures.TimeoutError,\n                        ):\n                            state.initialization_errors.append(\n                                (\n                                    model_id,\n                                    \"Could not finalise model loading before timeout\",\n                                )\n                            )\n                            future.cancel()\n\n                # Update the readiness state in a thread-safe manner\n                with state.lock:\n                    state.is_ready = True\n\n            @app.on_event(\"startup\")\n            def startup_model_init():\n                \"\"\"Initialize the models on startup.\"\"\"\n                startup_thread = Thread(\n                    target=initialize_models, args=(model_init_state,), daemon=True\n                )\n                startup_thread.start()\n                logger.info(\"Model initialization started in the background.\")\n\n            @app.get(\"/readiness\", status_code=200)\n            def readiness(\n                state: ModelInitState = Depends(lambda: model_init_state),\n            ):\n                \"\"\"Readiness endpoint for Kubernetes readiness probe.\"\"\"\n                with state.lock:\n                    if state.is_ready:\n                        return {\"status\": \"ready\"}\n                    else:\n                        return JSONResponse(\n                            content={\"status\": \"not ready\"}, status_code=503\n                        )\n\n            @app.get(\"/healthz\", status_code=200)\n            def healthz():\n                \"\"\"Health endpoint for Kubernetes liveness probe.\"\"\"\n                return {\"status\": \"healthy\"}\n\n        if CORE_MODELS_ENABLED:\n            if CORE_MODEL_CLIP_ENABLED:\n\n                @app.post(\n                    \"/clip/embed_image\",\n                    response_model=ClipEmbeddingResponse,\n                    summary=\"CLIP Image Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed image data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def clip_embed_image(\n                    inference_request: ClipImageEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipEmbeddingResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/embed_image\")\n                    clip_model_id = load_clip_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/embed_text\",\n                    response_model=ClipEmbeddingResponse,\n                    summary=\"CLIP Text Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed text data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def clip_embed_text(\n                    inference_request: ClipTextEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds text data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipEmbeddingResponse: The response containing the embedded text.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/embed_text\")\n                    clip_model_id = load_clip_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/compare\",\n                    response_model=ClipCompareResponse,\n                    summary=\"CLIP Compare\",\n                    description=\"Run the Open AI CLIP model to compute similarity scores.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def clip_compare(\n                    inference_request: ClipCompareRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Computes similarity scores using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipCompareRequest): The request containing the data to be compared.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipCompareResponse: The response containing the similarity scores.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/compare\")\n                    clip_model_id = load_clip_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor, n=2)\n                    return response\n\n            if CORE_MODEL_PE_ENABLED:\n\n                @app.post(\n                    \"/perception_encoder/embed_image\",\n                    response_model=PerceptionEncoderEmbeddingResponse,\n                    summary=\"PE Image Embeddings\",\n                    description=\"Run the Meta Perception Encoder model to embed image data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def pe_embed_image(\n                    inference_request: PerceptionEncoderImageEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the Perception Encoder PE model.\n\n                    Args:\n                        inference_request (PerceptionEncoderImageEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        PerceptionEncoderEmbeddingResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /perception_encoder/embed_image\")\n                    pe_model_id = load_pe_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        pe_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(pe_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/perception_encoder/embed_text\",\n                    response_model=PerceptionEncoderEmbeddingResponse,\n                    summary=\"Perception Encoder Text Embeddings\",\n                    description=\"Run the Meta Perception Encoder model to embed text data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def pe_embed_text(\n                    inference_request: PerceptionEncoderTextEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds text data using the Meta Perception Encoder model.\n\n                    Args:\n                        inference_request (PerceptionEncoderTextEmbeddingRequest): The request containing the text to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        PerceptionEncoderEmbeddingResponse: The response containing the embedded text.\n                    \"\"\"\n                    logger.debug(f\"Reached /perception_encoder/embed_text\")\n                    pe_model_id = load_pe_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        pe_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(pe_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/perception_encoder/compare\",\n                    response_model=PerceptionEncoderCompareResponse,\n                    summary=\"Perception Encoder Compare\",\n                    description=\"Run the Meta Perception Encoder model to compute similarity scores.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def pe_compare(\n                    inference_request: PerceptionEncoderCompareRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Computes similarity scores using the Meta Perception Encoder model.\n\n                    Args:\n                        inference_request (PerceptionEncoderCompareRequest): The request containing the data to be compared.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        PerceptionEncoderCompareResponse: The response containing the similarity scores.\n                    \"\"\"\n                    logger.debug(f\"Reached /perception_encoder/compare\")\n                    pe_model_id = load_pe_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        pe_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(pe_model_id, actor, n=2)\n                    return response\n\n            if CORE_MODEL_GROUNDINGDINO_ENABLED:\n\n                @app.post(\n                    \"/grounding_dino/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"Grounding DINO inference.\",\n                    description=\"Run the Grounding DINO zero-shot object detection model.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def grounding_dino_infer(\n                    inference_request: GroundingDINOInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the Grounding DINO model.\n\n                    Args:\n                        inference_request GroundingDINOInferenceRequest): The request containing the image on which to run object detection.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ObjectDetectionInferenceResponse: The object detection response.\n                    \"\"\"\n                    logger.debug(f\"Reached /grounding_dino/infer\")\n                    grounding_dino_model_id = load_grounding_dino_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        grounding_dino_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(grounding_dino_model_id, actor)\n                    return response\n\n            if CORE_MODEL_YOLO_WORLD_ENABLED:\n\n                @app.post(\n                    \"/yolo_world/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"YOLO-World inference.\",\n                    description=\"Run the YOLO-World zero-shot object detection model.\",\n                    response_model_exclude_none=True,\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def yolo_world_infer(\n                    inference_request: YOLOWorldInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Runs the YOLO-World zero-shot object detection model.\n\n                    Args:\n                        inference_request (YOLOWorldInferenceRequest): The request containing the image on which to run object detection.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ObjectDetectionInferenceResponse: The object detection response.\n                    \"\"\"\n                    logger.debug(f\"Reached /yolo_world/infer. Loading model\")\n                    yolo_world_model_id = load_yolo_world_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    logger.debug(\"YOLOWorld model loaded. Staring the inference.\")\n                    response = self.model_manager.infer_from_request_sync(\n                        yolo_world_model_id, inference_request\n                    )\n                    logger.debug(\"YOLOWorld prediction available.\")\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(yolo_world_model_id, actor)\n                        logger.debug(\"Usage of YOLOWorld denoted.\")\n                    return response\n\n            if CORE_MODEL_DOCTR_ENABLED:\n\n                @app.post(\n                    \"/doctr/ocr\",\n                    response_model=Union[\n                        OCRInferenceResponse, List[OCRInferenceResponse]\n                    ],\n                    summary=\"DocTR OCR response\",\n                    description=\"Run the DocTR OCR model to retrieve text in an image.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def doctr_retrieve_text(\n                    inference_request: DoctrOCRInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the DocTR model.\n\n                    Args:\n                        inference_request (M.DoctrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        OCRInferenceResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /doctr/ocr\")\n                    doctr_model_id = load_doctr_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        doctr_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(doctr_model_id, actor)\n                    return orjson_response_keeping_parent_id(response)\n\n            if CORE_MODEL_EASYOCR_ENABLED:\n\n                @app.post(\n                    \"/easy_ocr/ocr\",\n                    response_model=Union[\n                        OCRInferenceResponse, List[OCRInferenceResponse]\n                    ],\n                    summary=\"EasyOCR OCR response\",\n                    description=\"Run the EasyOCR model to retrieve text in an image.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def easy_ocr_retrieve_text(\n                    inference_request: EasyOCRInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the EasyOCR model.\n\n                    Args:\n                        inference_request (EasyOCRInferenceRequest): The request containing the image from which to retrieve text.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        OCRInferenceResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /easy_ocr/ocr\")\n                    easy_ocr_model_id = load_easy_ocr_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        easy_ocr_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(easy_ocr_model_id, actor)\n                    return orjson_response_keeping_parent_id(response)\n\n            if CORE_MODEL_SAM_ENABLED:\n\n                @app.post(\n                    \"/sam/embed_image\",\n                    response_model=SamEmbeddingResponse,\n                    summary=\"SAM Image Embeddings\",\n                    description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def sam_embed_image(\n                    inference_request: SamEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam/embed_image\")\n                    sam_model_id = load_sam_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    model_response = self.model_manager.infer_from_request_sync(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response.embeddings,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n                @app.post(\n                    \"/sam/segment_image\",\n                    response_model=SamSegmentationResponse,\n                    summary=\"SAM Image Segmentation\",\n                    description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def sam_segment_image(\n                    inference_request: SamSegmentationRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamSegmentationRequest): The request containing the image to be segmented.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamSegmentationResponse or Response: The response containing the segmented image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam/segment_image\")\n                    sam_model_id = load_sam_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    model_response = self.model_manager.infer_from_request_sync(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n            if CORE_MODEL_SAM2_ENABLED:\n\n                @app.post(\n                    \"/sam2/embed_image\",\n                    response_model=Sam2EmbeddingResponse,\n                    summary=\"SAM2 Image Embeddings\",\n                    description=\"Run the Meta AI Segment Anything 2 Model to embed image data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def sam2_embed_image(\n                    inference_request: Sam2EmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the Meta AI Segment Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                    \"\"\"\n                    logger.debug(f\"Reached /sam2/embed_image\")\n                    sam2_model_id = load_sam2_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    model_response = self.model_manager.infer_from_request_sync(\n                        sam2_model_id, inference_request\n                    )\n                    return model_response\n\n                @app.post(\n                    \"/sam2/segment_image\",\n                    response_model=Sam2SegmentationResponse,\n                    summary=\"SAM2 Image Segmentation\",\n                    description=\"Run the Meta AI Segment Anything 2 Model to generate segmenations for image data.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def sam2_segment_image(\n                    inference_request: Sam2SegmentationRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Generates segmentations for image data using the Meta AI Segment Anything Model (SAM).\n\n                    Args:\n                        inference_request (Sam2SegmentationRequest): The request containing the image to be segmented.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamSegmentationResponse or Response: The response containing the segmented image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam2/segment_image\")\n                    sam2_model_id = load_sam2_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    model_response = self.model_manager.infer_from_request_sync(\n                        sam2_model_id, inference_request\n                    )\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n            if CORE_MODEL_OWLV2_ENABLED:\n\n                @app.post(\n                    \"/owlv2/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"Owlv2 image prompting\",\n                    description=\"Run the google owlv2 model to few-shot object detect\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def owlv2_infer(\n                    inference_request: OwlV2InferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                    \"\"\"\n                    logger.debug(f\"Reached /owlv2/infer\")\n                    owl2_model_id = load_owlv2_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    model_response = self.model_manager.infer_from_request_sync(\n                        owl2_model_id, inference_request\n                    )\n                    return model_response\n\n            if CORE_MODEL_GAZE_ENABLED:\n\n                @app.post(\n                    \"/gaze/gaze_detection\",\n                    response_model=List[GazeDetectionInferenceResponse],\n                    summary=\"Gaze Detection\",\n                    description=\"Run the gaze detection model to detect gaze.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def gaze_detection(\n                    inference_request: GazeDetectionInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Detect gaze using the gaze detection model.\n\n                    Args:\n                        inference_request (M.GazeDetectionRequest): The request containing the image to be detected.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.GazeDetectionResponse: The response containing all the detected faces and the corresponding gazes.\n                    \"\"\"\n                    logger.debug(f\"Reached /gaze/gaze_detection\")\n                    gaze_model_id = load_gaze_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        gaze_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(gaze_model_id, actor)\n                    return response\n\n            if DEPTH_ESTIMATION_ENABLED:\n\n                @app.post(\n                    \"/infer/depth-estimation\",\n                    response_model=DepthEstimationResponse,\n                    summary=\"Depth Estimation\",\n                    description=\"Run the depth estimation model to generate a depth map.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def depth_estimation(\n                    inference_request: DepthEstimationRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Generate a depth map using the depth estimation model.\n\n                    Args:\n                        inference_request (DepthEstimationRequest): The request containing the image to estimate depth for.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        DepthEstimationResponse: The response containing the normalized depth map and optional visualization.\n                    \"\"\"\n                    logger.debug(f\"Reached /infer/depth-estimation\")\n                    depth_model_id = inference_request.model_id\n                    self.model_manager.add_model(\n                        depth_model_id,\n                        inference_request.api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        depth_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(depth_model_id, actor)\n\n                    # Extract data from nested response structure\n                    depth_data = response.response\n                    depth_response = DepthEstimationResponse(\n                        normalized_depth=depth_data[\"normalized_depth\"].tolist(),\n                        image=depth_data[\"image\"].numpy_image.tobytes().hex(),\n                    )\n                    return depth_response\n\n            if CORE_MODEL_TROCR_ENABLED:\n\n                @app.post(\n                    \"/ocr/trocr\",\n                    response_model=OCRInferenceResponse,\n                    summary=\"TrOCR OCR response\",\n                    description=\"Run the TrOCR model to retrieve text in an image.\",\n                )\n                @with_route_exceptions\n                @usage_collector(\"request\")\n                def trocr_retrieve_text(\n                    inference_request: TrOCRInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                    countinference: Optional[bool] = None,\n                    service_secret: Optional[str] = None,\n                ):\n                    \"\"\"\n                    Retrieves text from image data using the TrOCR model.\n\n                    Args:\n                        inference_request (TrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        OCRInferenceResponse: The response containing the retrieved text.\n                    \"\"\"\n                    logger.debug(f\"Reached /trocr/ocr\")\n                    trocr_model_id = load_trocr_model(\n                        inference_request,\n                        api_key=api_key,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    response = self.model_manager.infer_from_request_sync(\n                        trocr_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(trocr_model_id, actor)\n                    return orjson_response_keeping_parent_id(response)\n\n        if not (LAMBDA or GCP_SERVERLESS):\n\n            @app.get(\n                \"/notebook/start\",\n                summary=\"Jupyter Lab Server Start\",\n                description=\"Starts a jupyter lab server for running development code\",\n            )\n            @with_route_exceptions\n            def notebook_start(browserless: bool = False):\n                \"\"\"Starts a jupyter lab server for running development code.\n\n                Args:\n                    inference_request (NotebookStartRequest): The request containing the necessary details for starting a jupyter lab server.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    NotebookStartResponse: The response containing the URL of the jupyter lab server.\n                \"\"\"\n                logger.debug(f\"Reached /notebook/start\")\n                if NOTEBOOK_ENABLED:\n                    start_notebook()\n                    if browserless:\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Jupyter Lab server started at http://localhost:{NOTEBOOK_PORT}?token={NOTEBOOK_PASSWORD}\",\n                        }\n                    else:\n                        sleep(2)\n                        return RedirectResponse(\n                            f\"http://localhost:{NOTEBOOK_PORT}/lab/tree/quickstart.ipynb?token={NOTEBOOK_PASSWORD}\"\n                        )\n                else:\n                    if browserless:\n                        return {\n                            \"success\": False,\n                            \"message\": \"Notebook server is not enabled. Enable notebooks via the NOTEBOOK_ENABLED environment variable.\",\n                        }\n                    else:\n                        return RedirectResponse(f\"/notebook-instructions.html\")\n\n        if ENABLE_BUILDER:\n            from inference.core.interfaces.http.builder.routes import (\n                router as builder_router,\n            )\n\n            # Allow CORS on only the API, but not the builder UI/iframe (where the CSRF is passed)\n            app.add_middleware(\n                PathAwareCORSMiddleware,\n                match_paths=r\"^/build/api.*\",\n                allow_origins=[BUILDER_ORIGIN],\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n                allow_credentials=True,\n            )\n\n            # Attach all routes from builder to the /build prefix\n            app.include_router(builder_router, prefix=\"/build\", tags=[\"builder\"])\n\n        if LEGACY_ROUTE_ENABLED:\n            # Legacy object detection inference path for backwards compatibility\n            @app.get(\n                \"/{dataset_id}/{version_id:str}\",\n                # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse,\n                    KeypointsDetectionInferenceResponse,\n                    ObjectDetectionInferenceResponse,\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                    Any,\n                ],\n                response_model_exclude_none=True,\n            )\n            @app.post(\n                \"/{dataset_id}/{version_id:str}\",\n                # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse,\n                    KeypointsDetectionInferenceResponse,\n                    ObjectDetectionInferenceResponse,\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                    Any,\n                ],\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def legacy_infer_from_request(\n                background_tasks: BackgroundTasks,\n                request: Request,\n                request_body: Annotated[\n                    Optional[Union[bytes, UploadFile]],\n                    Depends(parse_body_content_for_legacy_request_handler),\n                ],\n                dataset_id: str = Path(\n                    description=\"ID of a Roboflow dataset corresponding to the model to use for inference OR workspace ID\"\n                ),\n                version_id: str = Path(\n                    description=\"ID of a Roboflow dataset version corresponding to the model to use for inference OR model ID\"\n                ),\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                confidence: float = Query(\n                    0.4,\n                    description=\"The confidence threshold used to filter out predictions\",\n                ),\n                keypoint_confidence: float = Query(\n                    0.0,\n                    description=\"The confidence threshold used to filter out keypoints that are not visible based on model confidence\",\n                ),\n                format: str = Query(\n                    \"json\",\n                    description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n                ),\n                image: Optional[str] = Query(\n                    None,\n                    description=\"The publically accessible URL of an image to use for inference.\",\n                ),\n                image_type: Optional[str] = Query(\n                    \"base64\",\n                    description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n                ),\n                labels: Optional[bool] = Query(\n                    False,\n                    description=\"If true, labels will be include in any inference visualization.\",\n                ),\n                mask_decode_mode: Optional[str] = Query(\n                    \"accurate\",\n                    description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n                ),\n                tradeoff_factor: Optional[float] = Query(\n                    0.0,\n                    description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n                ),\n                max_detections: int = Query(\n                    300,\n                    description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n                ),\n                overlap: float = Query(\n                    0.3,\n                    description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n                ),\n                stroke: int = Query(\n                    1, description=\"The stroke width used when visualizing predictions\"\n                ),\n                countinference: Optional[bool] = Query(\n                    True,\n                    description=\"If false, does not track inference against usage.\",\n                    include_in_schema=False,\n                ),\n                service_secret: Optional[str] = Query(\n                    None,\n                    description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                    include_in_schema=False,\n                ),\n                disable_preproc_auto_orient: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic image orientation\"\n                ),\n                disable_preproc_contrast: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic contrast adjustment\"\n                ),\n                disable_preproc_grayscale: Optional[bool] = Query(\n                    False,\n                    description=\"If true, disables automatic grayscale conversion\",\n                ),\n                disable_preproc_static_crop: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic static crop\"\n                ),\n                disable_active_learning: Optional[bool] = Query(\n                    default=False,\n                    description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n                ),\n                active_learning_target_dataset: Optional[str] = Query(\n                    default=None,\n                    description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n                ),\n                source: Optional[str] = Query(\n                    \"external\",\n                    description=\"The source of the inference request\",\n                ),\n                source_info: Optional[str] = Query(\n                    \"external\",\n                    description=\"The detailed source information of the inference request\",\n                ),\n                disable_model_monitoring: Optional[bool] = Query(\n                    False,\n                    description=\"If true, disables model monitoring for this request\",\n                    include_in_schema=False,\n                ),\n            ):\n                \"\"\"\n                Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n                Args:\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference OR workspace ID\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference OR model ID\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    # Other parameters described in the function signature...\n\n                Returns:\n                    Union[InstanceSegmentationInferenceResponse, KeypointsDetectionInferenceRequest, ObjectDetectionInferenceResponse, ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n                \"\"\"\n                logger.debug(\n                    f\"Reached legacy route /:dataset_id/:version_id with {dataset_id}/{version_id}\"\n                )\n                model_id = f\"{dataset_id}/{version_id}\"\n                if confidence &gt;= 1:\n                    confidence /= 100\n                elif confidence &lt; CONFIDENCE_LOWER_BOUND_OOM_PREVENTION:\n                    # allowing lower confidence results in RAM usage explosion\n                    confidence = CONFIDENCE_LOWER_BOUND_OOM_PREVENTION\n\n                if overlap &gt;= 1:\n                    overlap /= 100\n                if image is not None:\n                    request_image = InferenceRequestImage(type=\"url\", value=image)\n                else:\n                    if \"Content-Type\" not in request.headers:\n                        raise ContentTypeMissing(\n                            f\"Request must include a Content-Type header\"\n                        )\n                    if isinstance(request_body, UploadFile):\n                        base64_image_str = request_body.file.read()\n                        base64_image_str = base64.b64encode(base64_image_str)\n                        request_image = InferenceRequestImage(\n                            type=\"base64\", value=base64_image_str.decode(\"ascii\")\n                        )\n                    elif isinstance(request_body, bytes):\n                        request_image = InferenceRequestImage(\n                            type=image_type, value=request_body\n                        )\n                    elif request_body is None:\n                        raise InputImageLoadError(\n                            message=\"Image not found in request body.\",\n                            public_message=\"Image not found in request body.\",\n                        )\n                    else:\n                        raise ContentTypeInvalid(\n                            f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                        )\n\n                if not countinference and service_secret != ROBOFLOW_SERVICE_SECRET:\n                    raise MissingServiceSecretError(\n                        \"Service secret is required to disable inference usage tracking\"\n                    )\n                if LAMBDA:\n                    logger.debug(\"request.scope: %s\", request.scope)\n                    request_model_id = (\n                        request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                            \"lambda\"\n                        ][\"model\"][\"endpoint\"]\n                        .replace(\"--\", \"/\")\n                        .replace(\"rf-\", \"\")\n                        .replace(\"nu-\", \"\")\n                    )\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"actor\"]\n                    if countinference:\n                        trackUsage(request_model_id, actor)\n                    else:\n                        if service_secret != ROBOFLOW_SERVICE_SECRET:\n                            raise MissingServiceSecretError(\n                                \"Service secret is required to disable inference usage tracking\"\n                            )\n                        logger.info(\"Not counting inference for usage\")\n                else:\n                    request_model_id = model_id\n                logger.debug(\n                    f\"State of model registry: {self.model_manager.describe_models()}\"\n                )\n                self.model_manager.add_model(\n                    request_model_id,\n                    api_key,\n                    model_id_alias=model_id,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n                task_type = self.model_manager.get_task_type(model_id, api_key=api_key)\n                inference_request_type = ObjectDetectionInferenceRequest\n                args = dict()\n                if task_type == \"instance-segmentation\":\n                    inference_request_type = InstanceSegmentationInferenceRequest\n                    args = {\n                        \"mask_decode_mode\": mask_decode_mode,\n                        \"tradeoff_factor\": tradeoff_factor,\n                    }\n                elif task_type == \"classification\":\n                    inference_request_type = ClassificationInferenceRequest\n                elif task_type == \"keypoint-detection\":\n                    inference_request_type = KeypointsDetectionInferenceRequest\n                    args = {\"keypoint_confidence\": keypoint_confidence}\n                inference_request = inference_request_type(\n                    api_key=api_key,\n                    model_id=model_id,\n                    image=request_image,\n                    confidence=confidence,\n                    iou_threshold=overlap,\n                    max_detections=max_detections,\n                    visualization_labels=labels,\n                    visualization_stroke_width=stroke,\n                    visualize_predictions=(\n                        format == \"image\" or format == \"image_and_json\"\n                    ),\n                    disable_preproc_auto_orient=disable_preproc_auto_orient,\n                    disable_preproc_contrast=disable_preproc_contrast,\n                    disable_preproc_grayscale=disable_preproc_grayscale,\n                    disable_preproc_static_crop=disable_preproc_static_crop,\n                    disable_active_learning=disable_active_learning,\n                    active_learning_target_dataset=active_learning_target_dataset,\n                    source=source,\n                    source_info=source_info,\n                    usage_billable=countinference,\n                    disable_model_monitoring=disable_model_monitoring,\n                    **args,\n                )\n                inference_response = self.model_manager.infer_from_request_sync(\n                    inference_request.model_id,\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n                logger.debug(\"Response ready.\")\n                if format == \"image\":\n                    return Response(\n                        content=inference_response.visualization,\n                        media_type=\"image/jpeg\",\n                    )\n                else:\n                    return orjson_response(inference_response)\n\n        if not (LAMBDA or GCP_SERVERLESS):\n            # Legacy clear cache endpoint for backwards compatibility\n            @app.get(\"/clear_cache\", response_model=str)\n            def legacy_clear_cache():\n                \"\"\"\n                Clears the model cache.\n\n                This endpoint provides a way to clear the cache of loaded models.\n\n                Returns:\n                    str: A string indicating that the cache has been cleared.\n                \"\"\"\n                logger.debug(f\"Reached /clear_cache\")\n                model_clear()\n                return \"Cache Cleared\"\n\n            # Legacy add model endpoint for backwards compatibility\n            @app.get(\"/start/{dataset_id}/{version_id}\")\n            def model_add_legacy(\n                dataset_id: str,\n                version_id: str,\n                api_key: str = None,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Starts a model inference session.\n\n                This endpoint initializes and starts an inference session for the specified model version.\n\n                Args:\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                    api_key (str, optional): Roboflow API Key for artifact retrieval.\n                    countinference (Optional[bool]): Whether to count inference or not.\n                    service_secret (Optional[str]): The service secret for the request.\n\n                Returns:\n                    JSONResponse: A response object containing the status and a success message.\n                \"\"\"\n                logger.debug(\n                    f\"Reached /start/{dataset_id}/{version_id} with {dataset_id}/{version_id}\"\n                )\n                model_id = f\"{dataset_id}/{version_id}\"\n                self.model_manager.add_model(\n                    model_id,\n                    api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n                return JSONResponse(\n                    {\n                        \"status\": 200,\n                        \"message\": \"inference session started from local memory.\",\n                    }\n                )\n\n        if not ENABLE_DASHBOARD:\n\n            @app.get(\"/dashboard.html\")\n            @app.head(\"/dashboard.html\")\n            async def dashboard_guard():\n                return Response(status_code=404)\n\n        @app.exception_handler(InputImageLoadError)\n        async def unicorn_exception_handler(request: Request, exc: InputImageLoadError):\n            return JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": f\"Could not load input image. Cause: {exc.get_public_error_details()}\"\n                },\n            )\n\n        app.mount(\n            \"/\",\n            StaticFiles(directory=\"./inference/landing/out\", html=True),\n            name=\"root\",\n        )\n\n    def run(self):\n        uvicorn.run(self.app, host=\"127.0.0.1\", port=8080)\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.HttpInterface.__init__","title":"<code>__init__(model_manager, root_path=None)</code>","text":"<p>Initializes the HttpInterface with given model manager and model registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> required <code>root_path</code> <code>Optional[str]</code> <p>The root path for the FastAPI application.</p> <code>None</code> Description <p>Deploy Roboflow trained models to nearly any compute environment!</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def __init__(\n    self,\n    model_manager: ModelManager,\n    root_path: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the HttpInterface with given model manager and model registry.\n\n    Args:\n        model_manager (ModelManager): The manager for handling different models.\n        root_path (Optional[str]): The root path for the FastAPI application.\n\n    Description:\n        Deploy Roboflow trained models to nearly any compute environment!\n    \"\"\"\n\n    description = \"Roboflow inference server\"\n\n    app = FastAPI(\n        title=\"Roboflow Inference Server\",\n        description=description,\n        version=__version__,\n        terms_of_service=\"https://roboflow.com/terms\",\n        contact={\n            \"name\": \"Roboflow Inc.\",\n            \"url\": \"https://roboflow.com/contact\",\n            \"email\": \"help@roboflow.com\",\n        },\n        license_info={\n            \"name\": \"Apache 2.0\",\n            \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n        },\n        root_path=root_path,\n    )\n    # Ensure in-memory logging is initialized as early as possible for all runtimes\n    try:\n        from inference.core.logging.memory_handler import setup_memory_logging\n\n        setup_memory_logging()\n    except Exception:\n        pass\n\n    app.mount(\n        \"/static\",\n        StaticFiles(directory=\"./inference/landing/out/static\", html=True),\n        name=\"static\",\n    )\n    app.mount(\n        \"/_next/static\",\n        StaticFiles(directory=\"./inference/landing/out/_next/static\", html=True),\n        name=\"_next_static\",\n    )\n\n    @app.on_event(\"shutdown\")\n    async def on_shutdown():\n        logger.info(\"Shutting down %s\", description)\n        await usage_collector.async_push_usage_payloads()\n\n    InferenceInstrumentator(app, model_manager=model_manager, endpoint=\"/metrics\")\n    if LAMBDA:\n        app.add_middleware(LambdaMiddleware)\n    if GCP_SERVERLESS:\n        app.add_middleware(GCPServerlessMiddleware)\n\n    if len(ALLOW_ORIGINS) &gt; 0:\n        # Add CORS Middleware (but not for /build**, which is controlled separately)\n        app.add_middleware(\n            PathAwareCORSMiddleware,\n            match_paths=r\"^(?!/build).*\",\n            allow_origins=ALLOW_ORIGINS,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            expose_headers=[PROCESSING_TIME_HEADER],\n        )\n\n    # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n    if PROFILE:\n        app.add_middleware(\n            CProfileMiddleware,\n            enable=True,\n            server_app=app,\n            filename=\"/profile/output.pstats\",\n            strip_dirs=False,\n            sort_by=\"cumulative\",\n        )\n    if API_LOGGING_ENABLED:\n        app.add_middleware(\n            asgi_correlation_id.CorrelationIdMiddleware,\n            header_name=CORRELATION_ID_HEADER,\n            update_request_header=True,\n            generator=lambda: uuid4().hex,\n            validator=lambda a: True,\n            transformer=lambda a: a,\n        )\n    else:\n        app.add_middleware(asgi_correlation_id.CorrelationIdMiddleware)\n\n    if METRICS_ENABLED:\n\n        @app.middleware(\"http\")\n        async def count_errors(request: Request, call_next):\n            \"\"\"Middleware to count errors.\n\n            Args:\n                request (Request): The incoming request.\n                call_next (Callable): The next middleware or endpoint to call.\n\n            Returns:\n                Response: The response from the next middleware or endpoint.\n            \"\"\"\n            response = await call_next(request)\n            if self.model_manager.pingback and response.status_code &gt;= 400:\n                self.model_manager.num_errors += 1\n            return response\n\n    if not (LAMBDA or GCP_SERVERLESS):\n\n        @app.get(\"/device/stats\")\n        def device_stats():\n            not_configured_error_message = {\n                \"error\": \"Device statistics endpoint is not enabled.\",\n                \"hint\": \"Mount the Docker socket and point its location when running the docker \"\n                \"container to collect device stats \"\n                \"(i.e. `docker run ... -v /var/run/docker.sock:/var/run/docker.sock \"\n                \"-e DOCKER_SOCKET_PATH=/var/run/docker.sock ...`).\",\n            }\n            if not DOCKER_SOCKET_PATH:\n                return JSONResponse(\n                    status_code=404,\n                    content=not_configured_error_message,\n                )\n            if not is_docker_socket_mounted(docker_socket_path=DOCKER_SOCKET_PATH):\n                return JSONResponse(\n                    status_code=500,\n                    content=not_configured_error_message,\n                )\n            container_stats = get_container_stats(\n                docker_socket_path=DOCKER_SOCKET_PATH\n            )\n            return JSONResponse(status_code=200, content=container_stats)\n\n    cached_api_keys = dict()\n\n    if GCP_SERVERLESS:\n\n        @app.middleware(\"http\")\n        async def check_authorization_serverless(request: Request, call_next):\n            # exclusions\n            skip_check = (\n                request.method not in [\"GET\", \"POST\"]\n                or request.url.path\n                in [\n                    \"/\",\n                    \"/docs\",\n                    \"/info\",\n                    \"/healthz\",  # health check endpoint for liveness probe\n                    \"/readiness\",\n                    \"/metrics\",\n                    \"/openapi.json\",  # needed for /docs and /redoc\n                    \"/model/registry\",  # dont auth this route, usually not used on serverlerless, but queue based serverless uses it internally (not accessible from outside)\n                ]\n                or request.url.path.startswith(\"/static/\")\n                or request.url.path.startswith(\"/_next/\")\n            )\n\n            # for these routes we only want to auth if dynamic python modules are provided\n            if request.url.path in [\n                \"/workflows/blocks/describe\",\n                \"/workflows/definition/schema\",\n            ]:\n                if request.method == \"GET\":\n                    skip_check = True\n\n                elif (\n                    get_content_type(request) == \"application/json\"\n                    and int(request.headers.get(\"content-length\", 0)) &gt; 0\n                ):\n                    json_params = await request.json()\n                    dynamic_blocks_definitions = json_params.get(\n                        \"dynamic_blocks_definitions\", None\n                    )\n                    if not dynamic_blocks_definitions:\n                        skip_check = True\n\n            if skip_check:\n                return await call_next(request)\n\n            def _unauthorized_response(msg):\n                return JSONResponse(\n                    status_code=401,\n                    content={\n                        \"status\": 401,\n                        \"message\": msg,\n                    },\n                )\n\n            req_params = request.query_params\n            json_params = dict()\n            api_key = req_params.get(\"api_key\", None)\n            if (\n                api_key is None\n                and get_content_type(request) == \"application/json\"\n                and int(request.headers.get(\"content-length\", 0)) &gt; 0\n            ):\n                # have to try catch here, because some legacy endpoints that abuse Content-Type header but dont actually receive json\n                try:\n                    json_params = await request.json()\n                except Exception:\n                    pass\n            api_key = json_params.get(\"api_key\", api_key)\n\n            if api_key is None:\n                return _unauthorized_response(\"Unauthorized api_key\")\n\n            if cached_api_keys.get(api_key, 0) &lt; time.time():\n                try:\n                    await get_roboflow_workspace_async(api_key=api_key)\n                    cached_api_keys[api_key] = (\n                        time.time() + 3600\n                    )  # expired after 1 hour\n                except (RoboflowAPINotAuthorizedError, WorkspaceLoadError):\n                    return _unauthorized_response(\"Unauthorized api_key\")\n\n            return await call_next(request)\n\n    if DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n\n        @app.middleware(\"http\")\n        async def check_authorization(request: Request, call_next):\n            # exclusions\n            skip_check = (\n                request.method not in [\"GET\", \"POST\"]\n                or request.url.path\n                in [\n                    \"/\",\n                    \"/docs\",\n                    \"/redoc\",\n                    \"/info\",\n                    \"/healthz\",  # health check endpoint for liveness probe\n                    \"/readiness\",\n                    \"/metrics\",\n                    \"/openapi.json\",  # needed for /docs and /redoc\n                ]\n                or request.url.path.startswith(\"/static/\")\n                or request.url.path.startswith(\"/_next/\")\n            )\n            if skip_check:\n                return await call_next(request)\n\n            def _unauthorized_response(msg):\n                return JSONResponse(\n                    status_code=401,\n                    content={\n                        \"status\": 401,\n                        \"message\": msg,\n                    },\n                )\n\n            # check api_key\n            req_params = request.query_params\n            json_params = dict()\n            api_key = req_params.get(\"api_key\", None)\n            if (\n                api_key is None\n                and get_content_type(request) == \"application/json\"\n                and int(request.headers.get(\"content-length\", 0)) &gt; 0\n            ):\n                # have to try catch here, because some legacy endpoints that abuse Content-Type header but dont actually receive json\n                try:\n                    json_params = await request.json()\n                except Exception:\n                    pass\n            api_key = json_params.get(\"api_key\", api_key)\n\n            if api_key is None:\n                return _unauthorized_response(\"Unauthorized api_key\")\n\n            if cached_api_keys.get(api_key, 0) &lt; time.time():\n                try:\n                    # TODO: make this request async!\n                    if api_key is None:\n                        workspace_url = None\n                    else:\n                        workspace_url = await get_roboflow_workspace_async(\n                            api_key=api_key\n                        )\n\n                    if workspace_url != DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n                        return _unauthorized_response(\"Unauthorized api_key\")\n\n                    cached_api_keys[api_key] = (\n                        time.time() + 3600\n                    )  # expired after 1 hour\n                except (RoboflowAPINotAuthorizedError, WorkspaceLoadError):\n                    return _unauthorized_response(\"Unauthorized api_key\")\n\n            return await call_next(request)\n\n    self.app = app\n    self.model_manager = model_manager\n    self.stream_manager_client: Optional[StreamManagerClient] = None\n\n    if ENABLE_STREAM_API:\n        operations_timeout = os.getenv(\"STREAM_MANAGER_OPERATIONS_TIMEOUT\")\n        if operations_timeout is not None:\n            operations_timeout = float(operations_timeout)\n        self.stream_manager_client = StreamManagerClient.init(\n            host=os.getenv(\"STREAM_MANAGER_HOST\", \"127.0.0.1\"),\n            port=int(os.getenv(\"STREAM_MANAGER_PORT\", \"7070\")),\n            operations_timeout=operations_timeout,\n        )\n\n    def process_inference_request(\n        inference_request: InferenceRequest,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n        **kwargs,\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes an inference request by calling the appropriate model.\n\n        Args:\n            inference_request (InferenceRequest): The request containing model ID and other inference details.\n            countinference (Optional[bool]): Whether to count inference for usage.\n            service_secret (Optional[str]): The service secret.\n\n        Returns:\n            InferenceResponse: The response containing the inference results.\n        \"\"\"\n        de_aliased_model_id = resolve_roboflow_model_alias(\n            model_id=inference_request.model_id\n        )\n        self.model_manager.add_model(\n            de_aliased_model_id,\n            inference_request.api_key,\n            countinference=countinference,\n            service_secret=service_secret,\n        )\n        resp = self.model_manager.infer_from_request_sync(\n            de_aliased_model_id, inference_request, **kwargs\n        )\n        return orjson_response(resp)\n\n    def process_workflow_inference_request(\n        workflow_request: WorkflowInferenceRequest,\n        workflow_specification: dict,\n        background_tasks: Optional[BackgroundTasks],\n        profiler: WorkflowsProfiler,\n    ) -&gt; WorkflowInferenceResponse:\n\n        workflow_init_parameters = {\n            \"workflows_core.model_manager\": model_manager,\n            \"workflows_core.api_key\": workflow_request.api_key,\n            \"workflows_core.background_tasks\": background_tasks,\n        }\n        execution_engine = ExecutionEngine.init(\n            workflow_definition=workflow_specification,\n            init_parameters=workflow_init_parameters,\n            max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n            prevent_local_images_loading=True,\n            profiler=profiler,\n            workflow_id=workflow_request.workflow_id,\n        )\n        is_preview = False\n        if hasattr(workflow_request, \"is_preview\"):\n            is_preview = workflow_request.is_preview\n        workflow_results = execution_engine.run(\n            runtime_parameters=workflow_request.inputs,\n            serialize_results=True,\n            _is_preview=is_preview,\n        )\n        with profiler.profile_execution_phase(\n            name=\"workflow_results_filtering\",\n            categories=[\"inference_package_operation\"],\n        ):\n            outputs = filter_out_unwanted_workflow_outputs(\n                workflow_results=workflow_results,\n                excluded_fields=workflow_request.excluded_fields,\n            )\n        profiler_trace = profiler.export_trace()\n        response = WorkflowInferenceResponse(\n            outputs=outputs,\n            profiler_trace=profiler_trace,\n        )\n        return orjson_response(response=response)\n\n    def load_core_model(\n        inference_request: InferenceRequest,\n        api_key: Optional[str] = None,\n        core_model: str = None,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n        Args:\n            inference_request (InferenceRequest): The request containing version and other details.\n            api_key (Optional[str]): The API key for the request.\n            core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n            countinference (Optional[bool]): Whether to count inference or not.\n            service_secret (Optional[str]): The service secret for the request.\n\n        Returns:\n            str: The core model ID.\n        \"\"\"\n        if api_key:\n            inference_request.api_key = api_key\n        version_id_field = f\"{core_model}_version_id\"\n        core_model_id = (\n            f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n        )\n        self.model_manager.add_model(\n            core_model_id,\n            inference_request.api_key,\n            endpoint_type=ModelEndpointType.CORE_MODEL,\n            countinference=countinference,\n            service_secret=service_secret,\n        )\n        return core_model_id\n\n    load_clip_model = partial(load_core_model, core_model=\"clip\")\n    \"\"\"Loads the CLIP model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The CLIP model ID.\n    \"\"\"\n\n    load_pe_model = partial(load_core_model, core_model=\"perception_encoder\")\n    \"\"\"Loads the Perception Encoder model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The Perception Encoder model ID.\n    \"\"\"\n\n    load_sam_model = partial(load_core_model, core_model=\"sam\")\n    \"\"\"Loads the SAM model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The SAM model ID.\n    \"\"\"\n    load_sam2_model = partial(load_core_model, core_model=\"sam2\")\n    \"\"\"Loads the SAM2 model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The SAM2 model ID.\n    \"\"\"\n\n    load_gaze_model = partial(load_core_model, core_model=\"gaze\")\n    \"\"\"Loads the GAZE model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The GAZE model ID.\n    \"\"\"\n\n    load_doctr_model = partial(load_core_model, core_model=\"doctr\")\n    \"\"\"Loads the DocTR model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The DocTR model ID.\n    \"\"\"\n\n    load_easy_ocr_model = partial(load_core_model, core_model=\"easy_ocr\")\n    \"\"\"Loads the EasyOCR model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The EasyOCR model ID.\n    \"\"\"\n\n    load_paligemma_model = partial(load_core_model, core_model=\"paligemma\")\n\n    load_grounding_dino_model = partial(\n        load_core_model, core_model=\"grounding_dino\"\n    )\n    \"\"\"Loads the Grounding DINO model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The Grounding DINO model ID.\n    \"\"\"\n\n    load_yolo_world_model = partial(load_core_model, core_model=\"yolo_world\")\n    load_owlv2_model = partial(load_core_model, core_model=\"owlv2\")\n    \"\"\"Loads the YOLO World model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The YOLO World model ID.\n    \"\"\"\n\n    load_trocr_model = partial(load_core_model, core_model=\"trocr\")\n    \"\"\"Loads the TrOCR model into the model manager.\n\n    Args:\n    Same as `load_core_model`.\n\n    Returns:\n    The TrOCR model ID.\n    \"\"\"\n\n    @app.get(\n        \"/info\",\n        response_model=ServerVersionInfo,\n        summary=\"Info\",\n        description=\"Get the server name and version number\",\n    )\n    def root():\n        \"\"\"Endpoint to get the server name and version number.\n\n        Returns:\n            ServerVersionInfo: The server version information.\n        \"\"\"\n        return ServerVersionInfo(\n            name=\"Roboflow Inference Server\",\n            version=__version__,\n            uuid=GLOBAL_INFERENCE_SERVER_ID,\n        )\n\n    @app.get(\n        \"/logs\",\n        summary=\"Get Recent Logs\",\n        description=\"Get recent application logs for debugging\",\n    )\n    def get_logs(\n        limit: Optional[int] = Query(\n            100, description=\"Maximum number of log entries to return\"\n        ),\n        level: Optional[str] = Query(\n            None,\n            description=\"Filter by log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\",\n        ),\n        since: Optional[str] = Query(\n            None, description=\"Return logs since this ISO timestamp\"\n        ),\n    ):\n        \"\"\"Get recent application logs from memory.\n\n        Only available when ENABLE_IN_MEMORY_LOGS environment variable is set to 'true'.\n\n        Args:\n            limit: Maximum number of log entries (default 100)\n            level: Filter by log level\n            since: ISO timestamp to filter logs since\n\n        Returns:\n            List of log entries with timestamp, level, logger, and message\n        \"\"\"\n        # Check if in-memory logging is enabled\n        from inference.core.logging.memory_handler import (\n            get_recent_logs,\n            is_memory_logging_enabled,\n        )\n\n        if not is_memory_logging_enabled():\n            raise HTTPException(\n                status_code=404, detail=\"Logs endpoint not available\"\n            )\n\n        try:\n            logs = get_recent_logs(limit=limit or 100, level=level, since=since)\n            return {\"logs\": logs, \"total_count\": len(logs)}\n        except (ImportError, ModuleNotFoundError):\n            raise HTTPException(\n                status_code=500, detail=\"Logging system not properly initialized\"\n            )\n\n    if not LAMBDA and GET_MODEL_REGISTRY_ENABLED:\n\n        @app.get(\n            \"/model/registry\",\n            response_model=ModelsDescriptions,\n            summary=\"Get model keys\",\n            description=\"Get the ID of each loaded model\",\n        )\n        def registry():\n            \"\"\"Get the ID of each loaded model in the registry.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/registry\")\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n    # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n    if not (LAMBDA or GCP_SERVERLESS):\n\n        @app.post(\n            \"/model/add\",\n            response_model=ModelsDescriptions,\n            summary=\"Load a model\",\n            description=\"Load the model with the given model ID\",\n        )\n        @with_route_exceptions\n        def model_add(\n            request: AddModelRequest,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ):\n            \"\"\"Load the model with the given model ID into the model manager.\n\n            Args:\n                request (AddModelRequest): The request containing the model ID and optional API key.\n                countinference (Optional[bool]): Whether to count inference or not.\n                service_secret (Optional[str]): The service secret for the request.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/add\")\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=request.model_id\n            )\n            logger.info(f\"Loading model: {de_aliased_model_id}\")\n            self.model_manager.add_model(\n                de_aliased_model_id,\n                request.api_key,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/remove\",\n            response_model=ModelsDescriptions,\n            summary=\"Remove a model\",\n            description=\"Remove the model with the given model ID\",\n        )\n        @with_route_exceptions\n        def model_remove(request: ClearModelRequest):\n            \"\"\"Remove the model with the given model ID from the model manager.\n\n            Args:\n                request (ClearModelRequest): The request containing the model ID to be removed.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/remove\")\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=request.model_id\n            )\n            self.model_manager.remove(de_aliased_model_id)\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/clear\",\n            response_model=ModelsDescriptions,\n            summary=\"Remove all models\",\n            description=\"Remove all loaded models\",\n        )\n        @with_route_exceptions\n        def model_clear():\n            \"\"\"Remove all loaded models from the model manager.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/clear\")\n            self.model_manager.clear()\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n    # these NEW endpoints need authentication protection\n    if not LAMBDA and not GCP_SERVERLESS:\n\n        @app.post(\n            \"/infer/object_detection\",\n            response_model=Union[\n                ObjectDetectionInferenceResponse,\n                List[ObjectDetectionInferenceResponse],\n                StubResponse,\n            ],\n            summary=\"Object detection infer\",\n            description=\"Run inference with the specified object detection model\",\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def infer_object_detection(\n            inference_request: ObjectDetectionInferenceRequest,\n            background_tasks: BackgroundTasks,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ):\n            \"\"\"Run inference with the specified object detection model.\n\n            Args:\n                inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/object_detection\")\n            return process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n        @app.post(\n            \"/infer/instance_segmentation\",\n            response_model=Union[\n                InstanceSegmentationInferenceResponse, StubResponse\n            ],\n            summary=\"Instance segmentation infer\",\n            description=\"Run inference with the specified instance segmentation model\",\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def infer_instance_segmentation(\n            inference_request: InstanceSegmentationInferenceRequest,\n            background_tasks: BackgroundTasks,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ):\n            \"\"\"Run inference with the specified instance segmentation model.\n\n            Args:\n                inference_request (InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                InstanceSegmentationInferenceResponse: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/instance_segmentation\")\n            return process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n        @app.post(\n            \"/infer/classification\",\n            response_model=Union[\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n            ],\n            summary=\"Classification infer\",\n            description=\"Run inference with the specified classification model\",\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def infer_classification(\n            inference_request: ClassificationInferenceRequest,\n            background_tasks: BackgroundTasks,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ):\n            \"\"\"Run inference with the specified classification model.\n\n            Args:\n                inference_request (ClassificationInferenceRequest): The request containing the necessary details for classification.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/classification\")\n            return process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n        @app.post(\n            \"/infer/keypoints_detection\",\n            response_model=Union[KeypointsDetectionInferenceResponse, StubResponse],\n            summary=\"Keypoints detection infer\",\n            description=\"Run inference with the specified keypoints detection model\",\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def infer_keypoints(\n            inference_request: KeypointsDetectionInferenceRequest,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ):\n            \"\"\"Run inference with the specified keypoints detection model.\n\n            Args:\n                inference_request (KeypointsDetectionInferenceRequest): The request containing the necessary details for keypoints detection.\n\n            Returns:\n                Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/keypoints_detection\")\n            return process_inference_request(\n                inference_request,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n        if LMM_ENABLED or MOONDREAM2_ENABLED:\n\n            @app.post(\n                \"/infer/lmm\",\n                response_model=Union[\n                    LMMInferenceResponse,\n                    List[LMMInferenceResponse],\n                    StubResponse,\n                ],\n                summary=\"Large multi-modal model infer\",\n                description=\"Run inference with the specified large multi-modal model\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def infer_lmm(\n                inference_request: LMMInferenceRequest,\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"Run inference with the specified object detection model.\n\n                Args:\n                    inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/lmm\")\n                return process_inference_request(\n                    inference_request,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n    if not DISABLE_WORKFLOW_ENDPOINTS:\n\n        @app.post(\n            \"/{workspace_name}/workflows/{workflow_id}/describe_interface\",\n            response_model=DescribeInterfaceResponse,\n            summary=\"Endpoint to describe interface of predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - describes workflow inputs and outputs\",\n        )\n        @with_route_exceptions\n        def describe_predefined_workflow_interface(\n            workspace_name: str,\n            workflow_id: str,\n            workflow_request: PredefinedWorkflowDescribeInterfaceRequest,\n        ) -&gt; DescribeInterfaceResponse:\n            workflow_specification = get_workflow_specification(\n                api_key=workflow_request.api_key,\n                workspace_id=workspace_name,\n                workflow_id=workflow_id,\n                use_cache=workflow_request.use_cache,\n            )\n            return handle_describe_workflows_interface(\n                definition=workflow_specification,\n            )\n\n        @app.post(\n            \"/workflows/describe_interface\",\n            response_model=DescribeInterfaceResponse,\n            summary=\"Endpoint to describe interface of workflow given in request\",\n            description=\"Parses workflow definition and retrieves describes inputs and outputs\",\n        )\n        @with_route_exceptions\n        def describe_workflow_interface(\n            workflow_request: WorkflowSpecificationDescribeInterfaceRequest,\n        ) -&gt; DescribeInterfaceResponse:\n            return handle_describe_workflows_interface(\n                definition=workflow_request.specification,\n            )\n\n        @app.post(\n            \"/{workspace_name}/workflows/{workflow_id}\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"Endpoint to run predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body\",\n        )\n        @app.post(\n            \"/infer/workflows/{workspace_name}/{workflow_id}\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"[LEGACY] Endpoint to run predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024\",\n            deprecated=True,\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def infer_from_predefined_workflow(\n            workspace_name: str,\n            workflow_id: str,\n            workflow_request: PredefinedWorkflowInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ) -&gt; WorkflowInferenceResponse:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                profiler = BaseWorkflowsProfiler.init(\n                    max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                )\n            else:\n                profiler = NullWorkflowsProfiler.init()\n            with profiler.profile_execution_phase(\n                name=\"workflow_definition_fetching\",\n                categories=[\"inference_package_operation\"],\n            ):\n                workflow_specification = get_workflow_specification(\n                    api_key=workflow_request.api_key,\n                    workspace_id=workspace_name,\n                    workflow_id=workflow_id,\n                    use_cache=workflow_request.use_cache,\n                )\n            if not workflow_request.workflow_id:\n                workflow_request.workflow_id = workflow_id\n            if not workflow_specification.get(\"id\"):\n                logger.warning(\n                    \"Internal workflow ID missing in specification for '%s'\",\n                    workflow_id,\n                )\n            return process_workflow_inference_request(\n                workflow_request=workflow_request,\n                workflow_specification=workflow_specification,\n                background_tasks=(\n                    background_tasks if not (LAMBDA or GCP_SERVERLESS) else None\n                ),\n                profiler=profiler,\n            )\n\n        @app.post(\n            \"/workflows/run\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"Endpoint to run workflow specification provided in payload\",\n            description=\"Parses and executes workflow specification, injecting runtime parameters from request body.\",\n        )\n        @app.post(\n            \"/infer/workflows\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"[LEGACY] Endpoint to run workflow specification provided in payload\",\n            description=\"Parses and executes workflow specification, injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024.\",\n            deprecated=True,\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def infer_from_workflow(\n            workflow_request: WorkflowSpecificationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ) -&gt; WorkflowInferenceResponse:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                profiler = BaseWorkflowsProfiler.init(\n                    max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                )\n            else:\n                profiler = NullWorkflowsProfiler.init()\n            return process_workflow_inference_request(\n                workflow_request=workflow_request,\n                workflow_specification=workflow_request.specification,\n                background_tasks=(\n                    background_tasks if not (LAMBDA or GCP_SERVERLESS) else None\n                ),\n                profiler=profiler,\n            )\n\n        @app.get(\n            \"/workflows/execution_engine/versions\",\n            response_model=ExecutionEngineVersions,\n            summary=\"Returns available Execution Engine versions sorted from oldest to newest\",\n            description=\"Returns available Execution Engine versions sorted from oldest to newest\",\n        )\n        @with_route_exceptions\n        def get_execution_engine_versions() -&gt; ExecutionEngineVersions:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            versions = get_available_versions()\n            return ExecutionEngineVersions(versions=versions)\n\n        @app.get(\n            \"/workflows/blocks/describe\",\n            response_model=WorkflowsBlocksDescription,\n            summary=\"[LEGACY] Endpoint to get definition of workflows blocks that are accessible\",\n            description=\"Endpoint provides detailed information about workflows building blocks that are \"\n            \"accessible in the inference server. This information could be used to programmatically \"\n            \"build / display workflows.\",\n            deprecated=True,\n        )\n        @with_route_exceptions\n        def describe_workflows_blocks(\n            request: Request,\n        ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n            result = handle_describe_workflows_blocks_request()\n            return gzip_response_if_requested(request=request, response=result)\n\n        @app.post(\n            \"/workflows/blocks/describe\",\n            response_model=WorkflowsBlocksDescription,\n            summary=\"[EXPERIMENTAL] Endpoint to get definition of workflows blocks that are accessible\",\n            description=\"Endpoint provides detailed information about workflows building blocks that are \"\n            \"accessible in the inference server. This information could be used to programmatically \"\n            \"build / display workflows. Additionally - in request body one can specify list of \"\n            \"dynamic blocks definitions which will be transformed into blocks and used to generate \"\n            \"schemas and definitions of connections\",\n        )\n        @with_route_exceptions\n        def describe_workflows_blocks(\n            request: Request,\n            request_payload: Optional[DescribeBlocksRequest] = None,\n        ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            dynamic_blocks_definitions = None\n            requested_execution_engine_version = None\n            api_key = None\n            if request_payload is not None:\n                dynamic_blocks_definitions = (\n                    request_payload.dynamic_blocks_definitions\n                )\n                requested_execution_engine_version = (\n                    request_payload.execution_engine_version\n                )\n                api_key = request_payload.api_key or request.query_params.get(\n                    \"api_key\", None\n                )\n            result = handle_describe_workflows_blocks_request(\n                dynamic_blocks_definitions=dynamic_blocks_definitions,\n                requested_execution_engine_version=requested_execution_engine_version,\n                api_key=api_key,\n            )\n            return gzip_response_if_requested(request=request, response=result)\n\n        @app.get(\n            \"/workflows/definition/schema\",\n            response_model=WorkflowsBlocksSchemaDescription,\n            summary=\"Endpoint to fetch the workflows block schema\",\n            description=\"Endpoint to fetch the schema of all available blocks. This information can be \"\n            \"used to validate workflow definitions and suggest syntax in the JSON editor.\",\n        )\n        @with_route_exceptions\n        def get_workflow_schema(\n            request: Request,\n        ) -&gt; WorkflowsBlocksSchemaDescription:\n            result = get_workflow_schema_description()\n            return gzip_response_if_requested(request, response=result)\n\n        @app.post(\n            \"/workflows/blocks/dynamic_outputs\",\n            response_model=List[OutputDefinition],\n            summary=\"[EXPERIMENTAL] Endpoint to get definition of dynamic output for workflow step\",\n            description=\"Endpoint to be used when step outputs can be discovered only after \"\n            \"filling manifest with data.\",\n        )\n        @with_route_exceptions\n        def get_dynamic_block_outputs(\n            step_manifest: Dict[str, Any],\n        ) -&gt; List[OutputDefinition]:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            # Potentially TODO: dynamic blocks do not support dynamic outputs, but if it changes\n            # we need to provide dynamic blocks manifests here\n            dummy_workflow_definition = {\n                \"version\": \"1.0\",\n                \"inputs\": [],\n                \"steps\": [step_manifest],\n                \"outputs\": [],\n            }\n            available_blocks = load_workflow_blocks()\n            parsed_definition = parse_workflow_definition(\n                raw_workflow_definition=dummy_workflow_definition,\n                available_blocks=available_blocks,\n            )\n            parsed_manifest = parsed_definition.steps[0]\n            return parsed_manifest.get_actual_outputs()\n\n        @app.post(\n            \"/workflows/validate\",\n            response_model=WorkflowValidationStatus,\n            summary=\"[EXPERIMENTAL] Endpoint to validate\",\n            description=\"Endpoint provides a way to check validity of JSON workflow definition.\",\n        )\n        @with_route_exceptions\n        def validate_workflow(\n            specification: dict,\n        ) -&gt; WorkflowValidationStatus:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            step_execution_mode = StepExecutionMode(WORKFLOWS_STEP_EXECUTION_MODE)\n            workflow_init_parameters = {\n                \"workflows_core.model_manager\": model_manager,\n                \"workflows_core.api_key\": None,\n                \"workflows_core.background_tasks\": None,\n                \"workflows_core.step_execution_mode\": step_execution_mode,\n            }\n            _ = ExecutionEngine.init(\n                workflow_definition=specification,\n                init_parameters=workflow_init_parameters,\n                max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                prevent_local_images_loading=True,\n            )\n            return WorkflowValidationStatus(status=\"ok\")\n\n    if WEBRTC_WORKER_ENABLED:\n\n        @app.post(\n            \"/initialise_webrtc_worker\",\n            response_model=InitializeWebRTCResponse,\n            summary=\"[EXPERIMENTAL] Establishes WebRTC peer connection and processes video stream in spawned process or modal function\",\n            description=\"[EXPERIMENTAL] Establishes WebRTC peer connection and processes video stream in spawned process or modal function\",\n        )\n        @with_route_exceptions_async\n        async def initialise_webrtc_worker(\n            request: WebRTCWorkerRequest,\n        ) -&gt; InitializeWebRTCResponse:\n            logger.debug(\"Received initialise_webrtc_worker request\")\n            worker_result: WebRTCWorkerResult = await start_worker(\n                webrtc_request=request,\n            )\n            if worker_result.exception_type is not None:\n                if worker_result.exception_type == \"WorkflowSyntaxError\":\n                    raise WorkflowSyntaxError(\n                        public_message=worker_result.error_message,\n                        context=worker_result.error_context,\n                        inner_error=worker_result.inner_error,\n                    )\n                expected_exceptions = {\n                    \"Exception\": Exception,\n                    \"KeyError\": KeyError,\n                    \"MissingApiKeyError\": MissingApiKeyError,\n                    \"NotImplementedError\": NotImplementedError,\n                    \"RoboflowAPINotAuthorizedError\": RoboflowAPINotAuthorizedError,\n                    \"RoboflowAPINotNotFoundError\": RoboflowAPINotNotFoundError,\n                    \"ValidationError\": ValidationError,\n                }\n                exc = expected_exceptions.get(\n                    worker_result.exception_type, Exception\n                )(worker_result.error_message)\n                logger.error(\n                    f\"Initialise webrtc worker failed with %s: %s\",\n                    worker_result.exception_type,\n                    worker_result.error_message,\n                )\n                raise exc\n            logger.debug(\"Returning initialise_webrtc_worker response\")\n            return InitializeWebRTCResponse(\n                context=CommandContext(),\n                status=OperationStatus.SUCCESS,\n                sdp=worker_result.answer.sdp,\n                type=worker_result.answer.type,\n            )\n\n    if ENABLE_STREAM_API:\n\n        @app.get(\n            \"/inference_pipelines/list\",\n            response_model=ListPipelinesResponse,\n            summary=\"[EXPERIMENTAL] List active InferencePipelines\",\n            description=\"[EXPERIMENTAL] Listing all active InferencePipelines processing videos\",\n        )\n        @with_route_exceptions_async\n        async def list_pipelines(_: Request) -&gt; ListPipelinesResponse:\n            return await self.stream_manager_client.list_pipelines()\n\n        @app.get(\n            \"/inference_pipelines/{pipeline_id}/status\",\n            response_model=InferencePipelineStatusResponse,\n            summary=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n            description=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n        )\n        @with_route_exceptions_async\n        async def get_status(pipeline_id: str) -&gt; InferencePipelineStatusResponse:\n            return await self.stream_manager_client.get_status(\n                pipeline_id=pipeline_id\n            )\n\n        @app.post(\n            \"/inference_pipelines/initialise\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n            description=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n        )\n        @with_route_exceptions_async\n        async def initialise(request: InitialisePipelinePayload) -&gt; CommandResponse:\n            return await self.stream_manager_client.initialise_pipeline(\n                initialisation_request=request\n            )\n\n        @app.post(\n            \"/inference_pipelines/initialise_webrtc\",\n            response_model=InitializeWebRTCPipelineResponse,\n            summary=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n            description=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n        )\n        @with_route_exceptions_async\n        async def initialise_webrtc_inference_pipeline(\n            request: InitialiseWebRTCPipelinePayload,\n        ) -&gt; CommandResponse:\n            logger.debug(\"Received initialise webrtc inference pipeline request\")\n            resp = await self.stream_manager_client.initialise_webrtc_pipeline(\n                initialisation_request=request\n            )\n            logger.debug(\"Returning initialise webrtc inference pipeline response\")\n            return resp\n\n        @app.post(\n            \"/inference_pipelines/{pipeline_id}/pause\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n            description=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n        )\n        @with_route_exceptions_async\n        async def pause(pipeline_id: str) -&gt; CommandResponse:\n            return await self.stream_manager_client.pause_pipeline(\n                pipeline_id=pipeline_id\n            )\n\n        @app.post(\n            \"/inference_pipelines/{pipeline_id}/resume\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n            description=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n        )\n        @with_route_exceptions_async\n        async def resume(pipeline_id: str) -&gt; CommandResponse:\n            return await self.stream_manager_client.resume_pipeline(\n                pipeline_id=pipeline_id\n            )\n\n        @app.post(\n            \"/inference_pipelines/{pipeline_id}/terminate\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n            description=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n        )\n        @with_route_exceptions_async\n        async def terminate(pipeline_id: str) -&gt; CommandResponse:\n            return await self.stream_manager_client.terminate_pipeline(\n                pipeline_id=pipeline_id\n            )\n\n        @app.get(\n            \"/inference_pipelines/{pipeline_id}/consume\",\n            response_model=ConsumePipelineResponse,\n            summary=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n            description=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n        )\n        @with_route_exceptions_async\n        async def consume(\n            pipeline_id: str,\n            request: Optional[ConsumeResultsPayload] = None,\n        ) -&gt; ConsumePipelineResponse:\n            if request is None:\n                request = ConsumeResultsPayload()\n            return await self.stream_manager_client.consume_pipeline_result(\n                pipeline_id=pipeline_id,\n                excluded_fields=request.excluded_fields,\n            )\n\n    # Enable preloading models at startup\n    if (\n        (PRELOAD_MODELS or DEDICATED_DEPLOYMENT_WORKSPACE_URL)\n        and API_KEY\n        and not (LAMBDA or GCP_SERVERLESS)\n    ):\n\n        class ModelInitState:\n            \"\"\"Class to track model initialization state.\"\"\"\n\n            def __init__(self):\n                self.is_ready = False\n                self.lock = Lock()  # For thread-safe updates\n                self.initialization_errors = []  # Track errors per model\n\n        model_init_state = ModelInitState()\n\n        def initialize_models(state: ModelInitState):\n            \"\"\"Perform asynchronous initialization tasks to load models.\"\"\"\n            # Limit the number of concurrent tasks to prevent resource exhaustion\n\n            def load_model(model_id):\n                logger.debug(f\"load_model({model_id}) - starting\", flush=True)\n                try:\n                    # TODO: how to add timeout here? Probably best to timeout model loading?\n                    model_add(\n                        AddModelRequest(\n                            model_id=model_id,\n                            model_type=None,\n                            api_key=API_KEY,\n                        )\n                    )\n                    logger.info(f\"Model {model_id} loaded successfully.\")\n                except Exception as e:\n                    error_msg = f\"Error loading model {model_id}: {e}\"\n                    logger.error(error_msg)\n                    with state.lock:\n                        state.initialization_errors.append((model_id, str(e)))\n                logger.debug(f\"load_model({model_id}) - finished\", flush=True)\n\n            if PRELOAD_MODELS:\n                # Create tasks for each model to be loaded\n                model_loading_executor = ThreadPoolExecutor(max_workers=2)\n                loaded_futures: List[Tuple[str, Future]] = []\n                for model_id in PRELOAD_MODELS:\n                    future = model_loading_executor.submit(\n                        load_model, model_id=model_id\n                    )\n                    loaded_futures.append((model_id, future))\n\n                for model_id, future in loaded_futures:\n                    try:\n                        future.result(timeout=300)\n                    except (\n                        TimeoutError,\n                        CancelledError,\n                        concurrent.futures.TimeoutError,\n                    ):\n                        state.initialization_errors.append(\n                            (\n                                model_id,\n                                \"Could not finalise model loading before timeout\",\n                            )\n                        )\n                        future.cancel()\n\n            # Update the readiness state in a thread-safe manner\n            with state.lock:\n                state.is_ready = True\n\n        @app.on_event(\"startup\")\n        def startup_model_init():\n            \"\"\"Initialize the models on startup.\"\"\"\n            startup_thread = Thread(\n                target=initialize_models, args=(model_init_state,), daemon=True\n            )\n            startup_thread.start()\n            logger.info(\"Model initialization started in the background.\")\n\n        @app.get(\"/readiness\", status_code=200)\n        def readiness(\n            state: ModelInitState = Depends(lambda: model_init_state),\n        ):\n            \"\"\"Readiness endpoint for Kubernetes readiness probe.\"\"\"\n            with state.lock:\n                if state.is_ready:\n                    return {\"status\": \"ready\"}\n                else:\n                    return JSONResponse(\n                        content={\"status\": \"not ready\"}, status_code=503\n                    )\n\n        @app.get(\"/healthz\", status_code=200)\n        def healthz():\n            \"\"\"Health endpoint for Kubernetes liveness probe.\"\"\"\n            return {\"status\": \"healthy\"}\n\n    if CORE_MODELS_ENABLED:\n        if CORE_MODEL_CLIP_ENABLED:\n\n            @app.post(\n                \"/clip/embed_image\",\n                response_model=ClipEmbeddingResponse,\n                summary=\"CLIP Image Embeddings\",\n                description=\"Run the Open AI CLIP model to embed image data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def clip_embed_image(\n                inference_request: ClipImageEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipEmbeddingResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /clip/embed_image\")\n                clip_model_id = load_clip_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/embed_text\",\n                response_model=ClipEmbeddingResponse,\n                summary=\"CLIP Text Embeddings\",\n                description=\"Run the Open AI CLIP model to embed text data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def clip_embed_text(\n                inference_request: ClipTextEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds text data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipEmbeddingResponse: The response containing the embedded text.\n                \"\"\"\n                logger.debug(f\"Reached /clip/embed_text\")\n                clip_model_id = load_clip_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/compare\",\n                response_model=ClipCompareResponse,\n                summary=\"CLIP Compare\",\n                description=\"Run the Open AI CLIP model to compute similarity scores.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def clip_compare(\n                inference_request: ClipCompareRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Computes similarity scores using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipCompareRequest): The request containing the data to be compared.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipCompareResponse: The response containing the similarity scores.\n                \"\"\"\n                logger.debug(f\"Reached /clip/compare\")\n                clip_model_id = load_clip_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor, n=2)\n                return response\n\n        if CORE_MODEL_PE_ENABLED:\n\n            @app.post(\n                \"/perception_encoder/embed_image\",\n                response_model=PerceptionEncoderEmbeddingResponse,\n                summary=\"PE Image Embeddings\",\n                description=\"Run the Meta Perception Encoder model to embed image data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def pe_embed_image(\n                inference_request: PerceptionEncoderImageEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the Perception Encoder PE model.\n\n                Args:\n                    inference_request (PerceptionEncoderImageEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    PerceptionEncoderEmbeddingResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /perception_encoder/embed_image\")\n                pe_model_id = load_pe_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    pe_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(pe_model_id, actor)\n                return response\n\n            @app.post(\n                \"/perception_encoder/embed_text\",\n                response_model=PerceptionEncoderEmbeddingResponse,\n                summary=\"Perception Encoder Text Embeddings\",\n                description=\"Run the Meta Perception Encoder model to embed text data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def pe_embed_text(\n                inference_request: PerceptionEncoderTextEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds text data using the Meta Perception Encoder model.\n\n                Args:\n                    inference_request (PerceptionEncoderTextEmbeddingRequest): The request containing the text to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    PerceptionEncoderEmbeddingResponse: The response containing the embedded text.\n                \"\"\"\n                logger.debug(f\"Reached /perception_encoder/embed_text\")\n                pe_model_id = load_pe_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    pe_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(pe_model_id, actor)\n                return response\n\n            @app.post(\n                \"/perception_encoder/compare\",\n                response_model=PerceptionEncoderCompareResponse,\n                summary=\"Perception Encoder Compare\",\n                description=\"Run the Meta Perception Encoder model to compute similarity scores.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def pe_compare(\n                inference_request: PerceptionEncoderCompareRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Computes similarity scores using the Meta Perception Encoder model.\n\n                Args:\n                    inference_request (PerceptionEncoderCompareRequest): The request containing the data to be compared.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    PerceptionEncoderCompareResponse: The response containing the similarity scores.\n                \"\"\"\n                logger.debug(f\"Reached /perception_encoder/compare\")\n                pe_model_id = load_pe_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    pe_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(pe_model_id, actor, n=2)\n                return response\n\n        if CORE_MODEL_GROUNDINGDINO_ENABLED:\n\n            @app.post(\n                \"/grounding_dino/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"Grounding DINO inference.\",\n                description=\"Run the Grounding DINO zero-shot object detection model.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def grounding_dino_infer(\n                inference_request: GroundingDINOInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the Grounding DINO model.\n\n                Args:\n                    inference_request GroundingDINOInferenceRequest): The request containing the image on which to run object detection.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ObjectDetectionInferenceResponse: The object detection response.\n                \"\"\"\n                logger.debug(f\"Reached /grounding_dino/infer\")\n                grounding_dino_model_id = load_grounding_dino_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    grounding_dino_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(grounding_dino_model_id, actor)\n                return response\n\n        if CORE_MODEL_YOLO_WORLD_ENABLED:\n\n            @app.post(\n                \"/yolo_world/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"YOLO-World inference.\",\n                description=\"Run the YOLO-World zero-shot object detection model.\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def yolo_world_infer(\n                inference_request: YOLOWorldInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Runs the YOLO-World zero-shot object detection model.\n\n                Args:\n                    inference_request (YOLOWorldInferenceRequest): The request containing the image on which to run object detection.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ObjectDetectionInferenceResponse: The object detection response.\n                \"\"\"\n                logger.debug(f\"Reached /yolo_world/infer. Loading model\")\n                yolo_world_model_id = load_yolo_world_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                logger.debug(\"YOLOWorld model loaded. Staring the inference.\")\n                response = self.model_manager.infer_from_request_sync(\n                    yolo_world_model_id, inference_request\n                )\n                logger.debug(\"YOLOWorld prediction available.\")\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(yolo_world_model_id, actor)\n                    logger.debug(\"Usage of YOLOWorld denoted.\")\n                return response\n\n        if CORE_MODEL_DOCTR_ENABLED:\n\n            @app.post(\n                \"/doctr/ocr\",\n                response_model=Union[\n                    OCRInferenceResponse, List[OCRInferenceResponse]\n                ],\n                summary=\"DocTR OCR response\",\n                description=\"Run the DocTR OCR model to retrieve text in an image.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def doctr_retrieve_text(\n                inference_request: DoctrOCRInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the DocTR model.\n\n                Args:\n                    inference_request (M.DoctrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    OCRInferenceResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /doctr/ocr\")\n                doctr_model_id = load_doctr_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    doctr_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(doctr_model_id, actor)\n                return orjson_response_keeping_parent_id(response)\n\n        if CORE_MODEL_EASYOCR_ENABLED:\n\n            @app.post(\n                \"/easy_ocr/ocr\",\n                response_model=Union[\n                    OCRInferenceResponse, List[OCRInferenceResponse]\n                ],\n                summary=\"EasyOCR OCR response\",\n                description=\"Run the EasyOCR model to retrieve text in an image.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def easy_ocr_retrieve_text(\n                inference_request: EasyOCRInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the EasyOCR model.\n\n                Args:\n                    inference_request (EasyOCRInferenceRequest): The request containing the image from which to retrieve text.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    OCRInferenceResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /easy_ocr/ocr\")\n                easy_ocr_model_id = load_easy_ocr_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    easy_ocr_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(easy_ocr_model_id, actor)\n                return orjson_response_keeping_parent_id(response)\n\n        if CORE_MODEL_SAM_ENABLED:\n\n            @app.post(\n                \"/sam/embed_image\",\n                response_model=SamEmbeddingResponse,\n                summary=\"SAM Image Embeddings\",\n                description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def sam_embed_image(\n                inference_request: SamEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /sam/embed_image\")\n                sam_model_id = load_sam_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                model_response = self.model_manager.infer_from_request_sync(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response.embeddings,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n            @app.post(\n                \"/sam/segment_image\",\n                response_model=SamSegmentationResponse,\n                summary=\"SAM Image Segmentation\",\n                description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def sam_segment_image(\n                inference_request: SamSegmentationRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamSegmentationRequest): The request containing the image to be segmented.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamSegmentationResponse or Response: The response containing the segmented image.\n                \"\"\"\n                logger.debug(f\"Reached /sam/segment_image\")\n                sam_model_id = load_sam_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                model_response = self.model_manager.infer_from_request_sync(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n        if CORE_MODEL_SAM2_ENABLED:\n\n            @app.post(\n                \"/sam2/embed_image\",\n                response_model=Sam2EmbeddingResponse,\n                summary=\"SAM2 Image Embeddings\",\n                description=\"Run the Meta AI Segment Anything 2 Model to embed image data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def sam2_embed_image(\n                inference_request: Sam2EmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the Meta AI Segment Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                \"\"\"\n                logger.debug(f\"Reached /sam2/embed_image\")\n                sam2_model_id = load_sam2_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                model_response = self.model_manager.infer_from_request_sync(\n                    sam2_model_id, inference_request\n                )\n                return model_response\n\n            @app.post(\n                \"/sam2/segment_image\",\n                response_model=Sam2SegmentationResponse,\n                summary=\"SAM2 Image Segmentation\",\n                description=\"Run the Meta AI Segment Anything 2 Model to generate segmenations for image data.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def sam2_segment_image(\n                inference_request: Sam2SegmentationRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Generates segmentations for image data using the Meta AI Segment Anything Model (SAM).\n\n                Args:\n                    inference_request (Sam2SegmentationRequest): The request containing the image to be segmented.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamSegmentationResponse or Response: The response containing the segmented image.\n                \"\"\"\n                logger.debug(f\"Reached /sam2/segment_image\")\n                sam2_model_id = load_sam2_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                model_response = self.model_manager.infer_from_request_sync(\n                    sam2_model_id, inference_request\n                )\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n        if CORE_MODEL_OWLV2_ENABLED:\n\n            @app.post(\n                \"/owlv2/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"Owlv2 image prompting\",\n                description=\"Run the google owlv2 model to few-shot object detect\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def owlv2_infer(\n                inference_request: OwlV2InferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                \"\"\"\n                logger.debug(f\"Reached /owlv2/infer\")\n                owl2_model_id = load_owlv2_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                model_response = self.model_manager.infer_from_request_sync(\n                    owl2_model_id, inference_request\n                )\n                return model_response\n\n        if CORE_MODEL_GAZE_ENABLED:\n\n            @app.post(\n                \"/gaze/gaze_detection\",\n                response_model=List[GazeDetectionInferenceResponse],\n                summary=\"Gaze Detection\",\n                description=\"Run the gaze detection model to detect gaze.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def gaze_detection(\n                inference_request: GazeDetectionInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Detect gaze using the gaze detection model.\n\n                Args:\n                    inference_request (M.GazeDetectionRequest): The request containing the image to be detected.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.GazeDetectionResponse: The response containing all the detected faces and the corresponding gazes.\n                \"\"\"\n                logger.debug(f\"Reached /gaze/gaze_detection\")\n                gaze_model_id = load_gaze_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    gaze_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(gaze_model_id, actor)\n                return response\n\n        if DEPTH_ESTIMATION_ENABLED:\n\n            @app.post(\n                \"/infer/depth-estimation\",\n                response_model=DepthEstimationResponse,\n                summary=\"Depth Estimation\",\n                description=\"Run the depth estimation model to generate a depth map.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def depth_estimation(\n                inference_request: DepthEstimationRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Generate a depth map using the depth estimation model.\n\n                Args:\n                    inference_request (DepthEstimationRequest): The request containing the image to estimate depth for.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    DepthEstimationResponse: The response containing the normalized depth map and optional visualization.\n                \"\"\"\n                logger.debug(f\"Reached /infer/depth-estimation\")\n                depth_model_id = inference_request.model_id\n                self.model_manager.add_model(\n                    depth_model_id,\n                    inference_request.api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    depth_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(depth_model_id, actor)\n\n                # Extract data from nested response structure\n                depth_data = response.response\n                depth_response = DepthEstimationResponse(\n                    normalized_depth=depth_data[\"normalized_depth\"].tolist(),\n                    image=depth_data[\"image\"].numpy_image.tobytes().hex(),\n                )\n                return depth_response\n\n        if CORE_MODEL_TROCR_ENABLED:\n\n            @app.post(\n                \"/ocr/trocr\",\n                response_model=OCRInferenceResponse,\n                summary=\"TrOCR OCR response\",\n                description=\"Run the TrOCR model to retrieve text in an image.\",\n            )\n            @with_route_exceptions\n            @usage_collector(\"request\")\n            def trocr_retrieve_text(\n                inference_request: TrOCRInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                countinference: Optional[bool] = None,\n                service_secret: Optional[str] = None,\n            ):\n                \"\"\"\n                Retrieves text from image data using the TrOCR model.\n\n                Args:\n                    inference_request (TrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    OCRInferenceResponse: The response containing the retrieved text.\n                \"\"\"\n                logger.debug(f\"Reached /trocr/ocr\")\n                trocr_model_id = load_trocr_model(\n                    inference_request,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                response = self.model_manager.infer_from_request_sync(\n                    trocr_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(trocr_model_id, actor)\n                return orjson_response_keeping_parent_id(response)\n\n    if not (LAMBDA or GCP_SERVERLESS):\n\n        @app.get(\n            \"/notebook/start\",\n            summary=\"Jupyter Lab Server Start\",\n            description=\"Starts a jupyter lab server for running development code\",\n        )\n        @with_route_exceptions\n        def notebook_start(browserless: bool = False):\n            \"\"\"Starts a jupyter lab server for running development code.\n\n            Args:\n                inference_request (NotebookStartRequest): The request containing the necessary details for starting a jupyter lab server.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                NotebookStartResponse: The response containing the URL of the jupyter lab server.\n            \"\"\"\n            logger.debug(f\"Reached /notebook/start\")\n            if NOTEBOOK_ENABLED:\n                start_notebook()\n                if browserless:\n                    return {\n                        \"success\": True,\n                        \"message\": f\"Jupyter Lab server started at http://localhost:{NOTEBOOK_PORT}?token={NOTEBOOK_PASSWORD}\",\n                    }\n                else:\n                    sleep(2)\n                    return RedirectResponse(\n                        f\"http://localhost:{NOTEBOOK_PORT}/lab/tree/quickstart.ipynb?token={NOTEBOOK_PASSWORD}\"\n                    )\n            else:\n                if browserless:\n                    return {\n                        \"success\": False,\n                        \"message\": \"Notebook server is not enabled. Enable notebooks via the NOTEBOOK_ENABLED environment variable.\",\n                    }\n                else:\n                    return RedirectResponse(f\"/notebook-instructions.html\")\n\n    if ENABLE_BUILDER:\n        from inference.core.interfaces.http.builder.routes import (\n            router as builder_router,\n        )\n\n        # Allow CORS on only the API, but not the builder UI/iframe (where the CSRF is passed)\n        app.add_middleware(\n            PathAwareCORSMiddleware,\n            match_paths=r\"^/build/api.*\",\n            allow_origins=[BUILDER_ORIGIN],\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_credentials=True,\n        )\n\n        # Attach all routes from builder to the /build prefix\n        app.include_router(builder_router, prefix=\"/build\", tags=[\"builder\"])\n\n    if LEGACY_ROUTE_ENABLED:\n        # Legacy object detection inference path for backwards compatibility\n        @app.get(\n            \"/{dataset_id}/{version_id:str}\",\n            # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n            response_model=Union[\n                InstanceSegmentationInferenceResponse,\n                KeypointsDetectionInferenceResponse,\n                ObjectDetectionInferenceResponse,\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n                Any,\n            ],\n            response_model_exclude_none=True,\n        )\n        @app.post(\n            \"/{dataset_id}/{version_id:str}\",\n            # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n            response_model=Union[\n                InstanceSegmentationInferenceResponse,\n                KeypointsDetectionInferenceResponse,\n                ObjectDetectionInferenceResponse,\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n                Any,\n            ],\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        @usage_collector(\"request\")\n        def legacy_infer_from_request(\n            background_tasks: BackgroundTasks,\n            request: Request,\n            request_body: Annotated[\n                Optional[Union[bytes, UploadFile]],\n                Depends(parse_body_content_for_legacy_request_handler),\n            ],\n            dataset_id: str = Path(\n                description=\"ID of a Roboflow dataset corresponding to the model to use for inference OR workspace ID\"\n            ),\n            version_id: str = Path(\n                description=\"ID of a Roboflow dataset version corresponding to the model to use for inference OR model ID\"\n            ),\n            api_key: Optional[str] = Query(\n                None,\n                description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n            ),\n            confidence: float = Query(\n                0.4,\n                description=\"The confidence threshold used to filter out predictions\",\n            ),\n            keypoint_confidence: float = Query(\n                0.0,\n                description=\"The confidence threshold used to filter out keypoints that are not visible based on model confidence\",\n            ),\n            format: str = Query(\n                \"json\",\n                description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n            ),\n            image: Optional[str] = Query(\n                None,\n                description=\"The publically accessible URL of an image to use for inference.\",\n            ),\n            image_type: Optional[str] = Query(\n                \"base64\",\n                description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n            ),\n            labels: Optional[bool] = Query(\n                False,\n                description=\"If true, labels will be include in any inference visualization.\",\n            ),\n            mask_decode_mode: Optional[str] = Query(\n                \"accurate\",\n                description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n            ),\n            tradeoff_factor: Optional[float] = Query(\n                0.0,\n                description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n            ),\n            max_detections: int = Query(\n                300,\n                description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n            ),\n            overlap: float = Query(\n                0.3,\n                description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n            ),\n            stroke: int = Query(\n                1, description=\"The stroke width used when visualizing predictions\"\n            ),\n            countinference: Optional[bool] = Query(\n                True,\n                description=\"If false, does not track inference against usage.\",\n                include_in_schema=False,\n            ),\n            service_secret: Optional[str] = Query(\n                None,\n                description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                include_in_schema=False,\n            ),\n            disable_preproc_auto_orient: Optional[bool] = Query(\n                False, description=\"If true, disables automatic image orientation\"\n            ),\n            disable_preproc_contrast: Optional[bool] = Query(\n                False, description=\"If true, disables automatic contrast adjustment\"\n            ),\n            disable_preproc_grayscale: Optional[bool] = Query(\n                False,\n                description=\"If true, disables automatic grayscale conversion\",\n            ),\n            disable_preproc_static_crop: Optional[bool] = Query(\n                False, description=\"If true, disables automatic static crop\"\n            ),\n            disable_active_learning: Optional[bool] = Query(\n                default=False,\n                description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n            ),\n            active_learning_target_dataset: Optional[str] = Query(\n                default=None,\n                description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n            ),\n            source: Optional[str] = Query(\n                \"external\",\n                description=\"The source of the inference request\",\n            ),\n            source_info: Optional[str] = Query(\n                \"external\",\n                description=\"The detailed source information of the inference request\",\n            ),\n            disable_model_monitoring: Optional[bool] = Query(\n                False,\n                description=\"If true, disables model monitoring for this request\",\n                include_in_schema=False,\n            ),\n        ):\n            \"\"\"\n            Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n            Args:\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference OR workspace ID\n                version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference OR model ID\n                api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                # Other parameters described in the function signature...\n\n            Returns:\n                Union[InstanceSegmentationInferenceResponse, KeypointsDetectionInferenceRequest, ObjectDetectionInferenceResponse, ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n            \"\"\"\n            logger.debug(\n                f\"Reached legacy route /:dataset_id/:version_id with {dataset_id}/{version_id}\"\n            )\n            model_id = f\"{dataset_id}/{version_id}\"\n            if confidence &gt;= 1:\n                confidence /= 100\n            elif confidence &lt; CONFIDENCE_LOWER_BOUND_OOM_PREVENTION:\n                # allowing lower confidence results in RAM usage explosion\n                confidence = CONFIDENCE_LOWER_BOUND_OOM_PREVENTION\n\n            if overlap &gt;= 1:\n                overlap /= 100\n            if image is not None:\n                request_image = InferenceRequestImage(type=\"url\", value=image)\n            else:\n                if \"Content-Type\" not in request.headers:\n                    raise ContentTypeMissing(\n                        f\"Request must include a Content-Type header\"\n                    )\n                if isinstance(request_body, UploadFile):\n                    base64_image_str = request_body.file.read()\n                    base64_image_str = base64.b64encode(base64_image_str)\n                    request_image = InferenceRequestImage(\n                        type=\"base64\", value=base64_image_str.decode(\"ascii\")\n                    )\n                elif isinstance(request_body, bytes):\n                    request_image = InferenceRequestImage(\n                        type=image_type, value=request_body\n                    )\n                elif request_body is None:\n                    raise InputImageLoadError(\n                        message=\"Image not found in request body.\",\n                        public_message=\"Image not found in request body.\",\n                    )\n                else:\n                    raise ContentTypeInvalid(\n                        f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                    )\n\n            if not countinference and service_secret != ROBOFLOW_SERVICE_SECRET:\n                raise MissingServiceSecretError(\n                    \"Service secret is required to disable inference usage tracking\"\n                )\n            if LAMBDA:\n                logger.debug(\"request.scope: %s\", request.scope)\n                request_model_id = (\n                    request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"model\"][\"endpoint\"]\n                    .replace(\"--\", \"/\")\n                    .replace(\"rf-\", \"\")\n                    .replace(\"nu-\", \"\")\n                )\n                actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                    \"lambda\"\n                ][\"actor\"]\n                if countinference:\n                    trackUsage(request_model_id, actor)\n                else:\n                    if service_secret != ROBOFLOW_SERVICE_SECRET:\n                        raise MissingServiceSecretError(\n                            \"Service secret is required to disable inference usage tracking\"\n                        )\n                    logger.info(\"Not counting inference for usage\")\n            else:\n                request_model_id = model_id\n            logger.debug(\n                f\"State of model registry: {self.model_manager.describe_models()}\"\n            )\n            self.model_manager.add_model(\n                request_model_id,\n                api_key,\n                model_id_alias=model_id,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n            task_type = self.model_manager.get_task_type(model_id, api_key=api_key)\n            inference_request_type = ObjectDetectionInferenceRequest\n            args = dict()\n            if task_type == \"instance-segmentation\":\n                inference_request_type = InstanceSegmentationInferenceRequest\n                args = {\n                    \"mask_decode_mode\": mask_decode_mode,\n                    \"tradeoff_factor\": tradeoff_factor,\n                }\n            elif task_type == \"classification\":\n                inference_request_type = ClassificationInferenceRequest\n            elif task_type == \"keypoint-detection\":\n                inference_request_type = KeypointsDetectionInferenceRequest\n                args = {\"keypoint_confidence\": keypoint_confidence}\n            inference_request = inference_request_type(\n                api_key=api_key,\n                model_id=model_id,\n                image=request_image,\n                confidence=confidence,\n                iou_threshold=overlap,\n                max_detections=max_detections,\n                visualization_labels=labels,\n                visualization_stroke_width=stroke,\n                visualize_predictions=(\n                    format == \"image\" or format == \"image_and_json\"\n                ),\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n                disable_active_learning=disable_active_learning,\n                active_learning_target_dataset=active_learning_target_dataset,\n                source=source,\n                source_info=source_info,\n                usage_billable=countinference,\n                disable_model_monitoring=disable_model_monitoring,\n                **args,\n            )\n            inference_response = self.model_manager.infer_from_request_sync(\n                inference_request.model_id,\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n            logger.debug(\"Response ready.\")\n            if format == \"image\":\n                return Response(\n                    content=inference_response.visualization,\n                    media_type=\"image/jpeg\",\n                )\n            else:\n                return orjson_response(inference_response)\n\n    if not (LAMBDA or GCP_SERVERLESS):\n        # Legacy clear cache endpoint for backwards compatibility\n        @app.get(\"/clear_cache\", response_model=str)\n        def legacy_clear_cache():\n            \"\"\"\n            Clears the model cache.\n\n            This endpoint provides a way to clear the cache of loaded models.\n\n            Returns:\n                str: A string indicating that the cache has been cleared.\n            \"\"\"\n            logger.debug(f\"Reached /clear_cache\")\n            model_clear()\n            return \"Cache Cleared\"\n\n        # Legacy add model endpoint for backwards compatibility\n        @app.get(\"/start/{dataset_id}/{version_id}\")\n        def model_add_legacy(\n            dataset_id: str,\n            version_id: str,\n            api_key: str = None,\n            countinference: Optional[bool] = None,\n            service_secret: Optional[str] = None,\n        ):\n            \"\"\"\n            Starts a model inference session.\n\n            This endpoint initializes and starts an inference session for the specified model version.\n\n            Args:\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                api_key (str, optional): Roboflow API Key for artifact retrieval.\n                countinference (Optional[bool]): Whether to count inference or not.\n                service_secret (Optional[str]): The service secret for the request.\n\n            Returns:\n                JSONResponse: A response object containing the status and a success message.\n            \"\"\"\n            logger.debug(\n                f\"Reached /start/{dataset_id}/{version_id} with {dataset_id}/{version_id}\"\n            )\n            model_id = f\"{dataset_id}/{version_id}\"\n            self.model_manager.add_model(\n                model_id,\n                api_key,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n            return JSONResponse(\n                {\n                    \"status\": 200,\n                    \"message\": \"inference session started from local memory.\",\n                }\n            )\n\n    if not ENABLE_DASHBOARD:\n\n        @app.get(\"/dashboard.html\")\n        @app.head(\"/dashboard.html\")\n        async def dashboard_guard():\n            return Response(status_code=404)\n\n    @app.exception_handler(InputImageLoadError)\n    async def unicorn_exception_handler(request: Request, exc: InputImageLoadError):\n        return JSONResponse(\n            status_code=400,\n            content={\n                \"message\": f\"Could not load input image. Cause: {exc.get_public_error_details()}\"\n            },\n        )\n\n    app.mount(\n        \"/\",\n        StaticFiles(directory=\"./inference/landing/out\", html=True),\n        name=\"root\",\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.load_gaze_model","title":"<code>load_gaze_model(inference_request, api_key=None)</code>","text":"<p>Loads the gaze detection model.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>GazeDetectionInferenceRequest</code> <p>The inference request.</p> required <code>api_key</code> <code>Optional[str], default None</code> <p>The Roboflow API key.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The model ID.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def load_gaze_model(\n    inference_request: GazeDetectionInferenceRequest, api_key: Optional[str] = None\n) -&gt; str:\n    \"\"\"Loads the gaze detection model.\n\n    Args:\n        inference_request (GazeDetectionInferenceRequest): The inference request.\n        api_key (Optional[str], default None): The Roboflow API key.\n\n    Returns:\n        str: The model ID.\n    \"\"\"\n    return inference_request.model_id\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/","title":"Routes","text":""},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.builder_browse","title":"<code>builder_browse()</code>  <code>async</code>","text":"<p>Loads the main builder UI (editor.html). Injects the CSRF token and BUILDER_ORIGIN so the client can parse them on page load.</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.get(\n    \"\",\n    summary=\"Workflow Builder List\",\n    description=\"Loads the list of Workflows available for editing\",\n)\n@with_route_exceptions_async\nasync def builder_browse():\n    \"\"\"\n    Loads the main builder UI (editor.html).\n    Injects the CSRF token and BUILDER_ORIGIN\n    so the client can parse them on page load.\n    \"\"\"\n    base_path = Path(__file__).parent\n    file_path = base_path / \"editor.html\"\n    content = file_path.read_text(encoding=\"utf-8\")\n    content = content.replace(\"{{BUILDER_ORIGIN}}\", BUILDER_ORIGIN)\n    content = content.replace(\"{{CSRF}}\", csrf)\n\n    return HTMLResponse(content)\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.builder_edit","title":"<code>builder_edit(workflow_id)</code>  <code>async</code>","text":"<p>Loads a specific workflow for editing.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>str</code> <p>The ID of the workflow to be edited.</p> required Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.get(\n    \"/edit/{workflow_id}\",\n    summary=\"Workflow Builder\",\n    description=\"Loads a specific workflow for editing\",\n)\n@with_route_exceptions_async\nasync def builder_edit(workflow_id: str):\n    \"\"\"\n    Loads a specific workflow for editing.\n\n    Args:\n        workflow_id (str): The ID of the workflow to be edited.\n    \"\"\"\n    base_path = Path(__file__).parent\n    file_path = base_path / \"editor.html\"\n    content = file_path.read_text(encoding=\"utf-8\")\n    content = content.replace(\"{{BUILDER_ORIGIN}}\", BUILDER_ORIGIN)\n    content = content.replace(\"{{CSRF}}\", csrf)\n\n    return HTMLResponse(content)\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.builder_maybe_redirect","title":"<code>builder_maybe_redirect(workflow_id)</code>  <code>async</code>","text":"<p>If the workflow_id.json file exists, redirect to /build/edit/{workflow_id}. Otherwise, redirect back to /build.</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.get(\"/{workflow_id}\", include_in_schema=False)\n@with_route_exceptions_async\nasync def builder_maybe_redirect(workflow_id: str):\n    \"\"\"\n    If the workflow_id.json file exists, redirect to /build/edit/{workflow_id}.\n    Otherwise, redirect back to /build.\n    \"\"\"\n    if not re.match(r\"^[\\w\\-]+$\", workflow_id):\n        return RedirectResponse(url=\"/build\", status_code=302)\n\n    workflow_hash = sha256(workflow_id.encode()).hexdigest()\n    file_path = workflow_local_dir / f\"{workflow_hash}.json\"\n    if file_path.exists():\n        return RedirectResponse(url=f\"/build/edit/{workflow_id}\", status_code=302)\n    else:\n        return RedirectResponse(url=\"/build\", status_code=302)\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.builder_redirect","title":"<code>builder_redirect()</code>  <code>async</code>","text":"<p>If user hits /build/ with trailing slash, redirect to /build</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.get(\"/\", include_in_schema=False)\nasync def builder_redirect():\n    \"\"\"\n    If user hits /build/ with trailing slash, redirect to /build\n    \"\"\"\n    return RedirectResponse(url=\"/build\", status_code=302)\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.create_or_overwrite_workflow","title":"<code>create_or_overwrite_workflow(workflow_id, request_body=Body(...))</code>  <code>async</code>","text":"<p>Create or overwrite a workflow's JSON file on disk. Protected by CSRF token check.</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.post(\"/api/{workflow_id}\", dependencies=[Depends(verify_csrf_token)])\n@with_route_exceptions_async\nasync def create_or_overwrite_workflow(\n    workflow_id: str, request_body: dict = Body(...)\n):\n    \"\"\"\n    Create or overwrite a workflow's JSON file on disk.\n    Protected by CSRF token check.\n    \"\"\"\n    if not re.match(r\"^[\\w\\-]+$\", workflow_id):\n        return JSONResponse({\"error\": \"invalid id\"}, status_code=HTTP_400_BAD_REQUEST)\n\n    workflow_local_dir.mkdir(parents=True, exist_ok=True)\n\n    # If the body claims a different ID, treat that as a \"rename\".\n    if request_body.get(\"id\") and request_body.get(\"id\") != workflow_id:\n        old_id: str = request_body[\"id\"]\n        if not re.match(r\"^[\\w\\-]+$\", old_id):\n            return JSONResponse(\n                {\"error\": \"invalid id\"}, status_code=HTTP_400_BAD_REQUEST\n            )\n\n        old_workflow_hash = sha256(old_id.encode()).hexdigest()\n        old_file_path = workflow_local_dir / f\"{old_workflow_hash}.json\"\n        if old_file_path.exists():\n            try:\n                old_file_path.unlink()\n            except Exception as e:\n                logger.error(f\"Error deleting {old_id} from {old_file_path}: {e}\")\n                return JSONResponse({\"error\": \"unable to delete file\"}, status_code=500)\n\n    request_body[\"id\"] = workflow_id\n\n    workflow_hash = sha256(workflow_id.encode()).hexdigest()\n    file_path = workflow_local_dir / f\"{workflow_hash}.json\"\n    try:\n        with file_path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(request_body, f, indent=2)\n    except Exception as e:\n        logger.error(f\"Error writing JSON for {workflow_id} to {file_path}: {e}\")\n        return JSONResponse({\"error\": \"unable to write file\"}, status_code=500)\n\n    return JSONResponse(\n        {\"message\": f\"Workflow '{workflow_id}' created/updated successfully.\"},\n        status_code=HTTP_201_CREATED,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.delete_workflow","title":"<code>delete_workflow(workflow_id)</code>  <code>async</code>","text":"<p>Delete a workflow's JSON file from disk. Protected by CSRF token check.</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.delete(\"/api/{workflow_id}\", dependencies=[Depends(verify_csrf_token)])\n@with_route_exceptions_async\nasync def delete_workflow(workflow_id: str):\n    \"\"\"\n    Delete a workflow's JSON file from disk.\n    Protected by CSRF token check.\n    \"\"\"\n    if not re.match(r\"^[\\w\\-]+$\", workflow_id):\n        return JSONResponse({\"error\": \"invalid id\"}, status_code=HTTP_400_BAD_REQUEST)\n\n    workflow_hash = sha256(workflow_id.encode()).hexdigest()\n    file_path = workflow_local_dir / f\"{workflow_hash}.json\"\n    if not file_path.exists():\n        return JSONResponse({\"error\": \"not found\"}, status_code=HTTP_404_NOT_FOUND)\n\n    try:\n        file_path.unlink()\n    except Exception as e:\n        logger.error(f\"Error deleting {workflow_id} from {file_path}: {e}\")\n        return JSONResponse({\"error\": \"unable to delete file\"}, status_code=500)\n\n    return JSONResponse(\n        {\"message\": f\"Workflow '{workflow_id}' deleted successfully.\"}, status_code=200\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.get_all_workflows","title":"<code>get_all_workflows()</code>  <code>async</code>","text":"<p>Returns JSON info about all .json files in {MODEL_CACHE_DIR}/workflow/local. Protected by CSRF token check.</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.get(\"/api\", dependencies=[Depends(verify_csrf_token)])\n@with_route_exceptions_async\nasync def get_all_workflows():\n    \"\"\"\n    Returns JSON info about all .json files in {MODEL_CACHE_DIR}/workflow/local.\n    Protected by CSRF token check.\n    \"\"\"\n    data = {}\n    for json_file in workflow_local_dir.glob(\"*.json\"):\n        stat_info = json_file.stat()\n        try:\n            with json_file.open(\"r\", encoding=\"utf-8\") as f:\n                config_contents: Dict[str, Any] = json.load(f)\n        except json.JSONDecodeError as e:\n            logger.error(f\"Error decoding JSON from {json_file}: {e}\")\n            continue\n\n        data[config_contents.get(\"id\", json_file.stem)] = {\n            \"createTime\": {\"_seconds\": int(stat_info.st_ctime)},\n            \"updateTime\": {\"_seconds\": int(stat_info.st_mtime)},\n            \"config\": config_contents,\n        }\n\n    return Response(\n        content=json.dumps({\"data\": data}, indent=4),\n        media_type=\"application/json\",\n        status_code=200,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/builder/routes/#inference.core.interfaces.http.builder.routes.get_workflow","title":"<code>get_workflow(workflow_id)</code>  <code>async</code>","text":"<p>Return JSON for workflow_id.json, or 404 if missing.</p> Source code in <code>inference/core/interfaces/http/builder/routes.py</code> <pre><code>@router.get(\"/api/{workflow_id}\", dependencies=[Depends(verify_csrf_token)])\n@with_route_exceptions_async\nasync def get_workflow(workflow_id: str):\n    \"\"\"\n    Return JSON for workflow_id.json, or 404 if missing.\n    \"\"\"\n    if not re.match(r\"^[\\w\\-]+$\", workflow_id):\n        return JSONResponse({\"error\": \"invalid id\"}, status_code=HTTP_400_BAD_REQUEST)\n\n    workflow_hash = sha256(workflow_id.encode()).hexdigest()\n    file_path = workflow_local_dir / f\"{workflow_hash}.json\"\n    if not file_path.exists():\n        return JSONResponse({\"error\": \"not found\"}, status_code=HTTP_404_NOT_FOUND)\n\n    stat_info = file_path.stat()\n    try:\n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            config_contents = json.load(f)\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error reading JSON for {workflow_id} from '{file_path}': {e}\")\n        return JSONResponse({\"error\": \"invalid JSON\"}, status_code=500)\n\n    return Response(\n        content=json.dumps(\n            {\n                \"data\": {\n                    \"createTime\": int(stat_info.st_ctime),\n                    \"updateTime\": int(stat_info.st_mtime),\n                    \"config\": config_contents,\n                }\n            },\n            indent=4,\n        ),\n        media_type=\"application/json\",\n        status_code=200,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/middlewares/cors/","title":"Cors","text":""},{"location":"reference/inference/core/interfaces/http/middlewares/cors/#inference.core.interfaces.http.middlewares.cors.PathAwareCORSMiddleware","title":"<code>PathAwareCORSMiddleware</code>","text":"<p>               Bases: <code>CORSMiddleware</code></p> <p>Extends Starlette's CORSMiddleware to allow specifying a regex of paths that this middleware should apply to. If 'match_paths' is given, only requests matching that regex will have CORS headers applied.</p> Source code in <code>inference/core/interfaces/http/middlewares/cors.py</code> <pre><code>class PathAwareCORSMiddleware(StarletteCORSMiddleware):\n    \"\"\"\n    Extends Starlette's CORSMiddleware to allow specifying a regex of paths that\n    this middleware should apply to.\n    If 'match_paths' is given, only requests matching that regex will have CORS\n    headers applied.\n    \"\"\"\n\n    def __init__(\n        self,\n        app: ASGIApp,\n        match_paths: str | None = None,\n        allow_origins: typing.Sequence[str] = (),\n        allow_methods: typing.Sequence[str] = (\"GET\",),\n        allow_headers: typing.Sequence[str] = (),\n        allow_credentials: bool = False,\n        allow_origin_regex: str | None = None,\n        expose_headers: typing.Sequence[str] = (),\n        max_age: int = 600,\n    ) -&gt; None:\n        super().__init__(\n            app=app,\n            allow_origins=allow_origins,\n            allow_methods=allow_methods,\n            allow_headers=allow_headers,\n            allow_credentials=allow_credentials,\n            allow_origin_regex=allow_origin_regex,\n            expose_headers=expose_headers,\n            max_age=max_age,\n        )\n        self.match_paths_regex = re.compile(match_paths) if match_paths else None\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -&gt; None:\n        \"\"\"\n        Only apply the CORS logic if the path matches self.match_paths_regex\n        (when provided). Otherwise, just call the wrapped 'app'.\n        \"\"\"\n        # If it's not an HTTP request, skip the CORS processing:\n        if scope[\"type\"] != \"http\":\n            await self.app(scope, receive, send)\n            return\n\n        # If match_paths was supplied, check if the current path matches\n        if self.match_paths_regex is not None:\n            path = scope.get(\"path\", \"\")\n            if not self.match_paths_regex.match(path):\n                # If it does NOT match, just run the app without CORS\n                await self.app(scope, receive, send)\n                return\n\n        # If we got here, apply the normal Starlette CORSMiddleware behavior\n        await super().__call__(scope, receive, send)\n</code></pre>"},{"location":"reference/inference/core/interfaces/http/middlewares/cors/#inference.core.interfaces.http.middlewares.cors.PathAwareCORSMiddleware.__call__","title":"<code>__call__(scope, receive, send)</code>  <code>async</code>","text":"<p>Only apply the CORS logic if the path matches self.match_paths_regex (when provided). Otherwise, just call the wrapped 'app'.</p> Source code in <code>inference/core/interfaces/http/middlewares/cors.py</code> <pre><code>async def __call__(self, scope: Scope, receive: Receive, send: Send) -&gt; None:\n    \"\"\"\n    Only apply the CORS logic if the path matches self.match_paths_regex\n    (when provided). Otherwise, just call the wrapped 'app'.\n    \"\"\"\n    # If it's not an HTTP request, skip the CORS processing:\n    if scope[\"type\"] != \"http\":\n        await self.app(scope, receive, send)\n        return\n\n    # If match_paths was supplied, check if the current path matches\n    if self.match_paths_regex is not None:\n        path = scope.get(\"path\", \"\")\n        if not self.match_paths_regex.match(path):\n            # If it does NOT match, just run the app without CORS\n            await self.app(scope, receive, send)\n            return\n\n    # If we got here, apply the normal Starlette CORSMiddleware behavior\n    await super().__call__(scope, receive, send)\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/","title":"Sinks","text":""},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink","title":"<code>UDPSink</code>","text":"Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>class UDPSink:\n    @classmethod\n    def init(cls, ip_address: str, port: int) -&gt; \"UDPSink\":\n        \"\"\"\n        Creates `InferencePipeline` predictions sink capable of sending model predictions over network\n        using UDP socket.\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n        Args:\n            ip_address (str): IP address to send predictions\n            port (int): Port to send predictions\n\n        Returns: Initialised object of `UDPSink` class.\n        \"\"\"\n        udp_socket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n        return cls(\n            ip_address=ip_address,\n            port=port,\n            udp_socket=udp_socket,\n        )\n\n    def __init__(self, ip_address: str, port: int, udp_socket: socket.socket):\n        self._ip_address = ip_address\n        self._port = port\n        self._socket = udp_socket\n\n    def send_predictions(\n        self,\n        predictions: Union[dict, List[Optional[dict]]],\n        video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    ) -&gt; None:\n        \"\"\"\n        Method to send predictions via UDP socket. Useful in combination with `InferencePipeline` as\n        a sink for predictions.\n\n        Args:\n            predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n                processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n                should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n            video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n                by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n                to `predictions` list). Order is expected to match with `predictions`\n\n        Returns: None\n        Side effects: Sends serialised `predictions` and `video_frame` metadata via the UDP socket as\n            JSON string. It adds key named \"inference_metadata\" into `predictions` dict (mutating its\n            state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message\n            emission time in datetime iso format.\n\n        Example:\n            ```python\n            import cv2\n            from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\n            from inference.core.interfaces.stream.sinks import UDPSink\n\n            udp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\n            pipeline = InferencePipeline.init(\n                 model_id=\"your-model/3\",\n                 video_reference=\"./some_file.mp4\",\n                 on_prediction=udp_sink.send_predictions,\n            )\n            pipeline.start()\n            pipeline.join()\n            ```\n            `UDPSink` used in this way will emit predictions to receiver automatically.\n        \"\"\"\n        video_frame = wrap_in_list(element=video_frame)\n        predictions = wrap_in_list(element=predictions)\n        for single_frame, frame_predictions in zip(video_frame, predictions):\n            if single_frame is None:\n                continue\n            inference_metadata = {\n                \"source_id\": single_frame.source_id,\n                \"frame_id\": single_frame.frame_id,\n                \"frame_decoding_time\": single_frame.frame_timestamp.isoformat(),\n                \"emission_time\": datetime.now().isoformat(),\n            }\n            frame_predictions[\"inference_metadata\"] = inference_metadata\n            serialised_predictions = json.dumps(frame_predictions).encode(\"utf-8\")\n            self._socket.sendto(\n                serialised_predictions,\n                (\n                    self._ip_address,\n                    self._port,\n                ),\n            )\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink.init","title":"<code>init(ip_address, port)</code>  <code>classmethod</code>","text":"<p>Creates <code>InferencePipeline</code> predictions sink capable of sending model predictions over network using UDP socket.</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects. Args:     ip_address (str): IP address to send predictions     port (int): Port to send predictions</p> <p>Returns: Initialised object of <code>UDPSink</code> class.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>@classmethod\ndef init(cls, ip_address: str, port: int) -&gt; \"UDPSink\":\n    \"\"\"\n    Creates `InferencePipeline` predictions sink capable of sending model predictions over network\n    using UDP socket.\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n    Args:\n        ip_address (str): IP address to send predictions\n        port (int): Port to send predictions\n\n    Returns: Initialised object of `UDPSink` class.\n    \"\"\"\n    udp_socket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n    return cls(\n        ip_address=ip_address,\n        port=port,\n        udp_socket=udp_socket,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink.send_predictions","title":"<code>send_predictions(predictions, video_frame)</code>","text":"<p>Method to send predictions via UDP socket. Useful in combination with <code>InferencePipeline</code> as a sink for predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <p>Side effects: Sends serialised <code>predictions</code> and <code>video_frame</code> metadata via the UDP socket as     JSON string. It adds key named \"inference_metadata\" into <code>predictions</code> dict (mutating its     state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message     emission time in datetime iso format.</p> Example <p><pre><code>import cv2\nfrom inference.core.interfaces.stream.inference_pipeline import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import UDPSink\n\nudp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\npipeline = InferencePipeline.init(\n     model_id=\"your-model/3\",\n     video_reference=\"./some_file.mp4\",\n     on_prediction=udp_sink.send_predictions,\n)\npipeline.start()\npipeline.join()\n</code></pre> <code>UDPSink</code> used in this way will emit predictions to receiver automatically.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def send_predictions(\n    self,\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    \"\"\"\n    Method to send predictions via UDP socket. Useful in combination with `InferencePipeline` as\n    a sink for predictions.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n\n    Returns: None\n    Side effects: Sends serialised `predictions` and `video_frame` metadata via the UDP socket as\n        JSON string. It adds key named \"inference_metadata\" into `predictions` dict (mutating its\n        state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message\n        emission time in datetime iso format.\n\n    Example:\n        ```python\n        import cv2\n        from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\n        from inference.core.interfaces.stream.sinks import UDPSink\n\n        udp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\n        pipeline = InferencePipeline.init(\n             model_id=\"your-model/3\",\n             video_reference=\"./some_file.mp4\",\n             on_prediction=udp_sink.send_predictions,\n        )\n        pipeline.start()\n        pipeline.join()\n        ```\n        `UDPSink` used in this way will emit predictions to receiver automatically.\n    \"\"\"\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    for single_frame, frame_predictions in zip(video_frame, predictions):\n        if single_frame is None:\n            continue\n        inference_metadata = {\n            \"source_id\": single_frame.source_id,\n            \"frame_id\": single_frame.frame_id,\n            \"frame_decoding_time\": single_frame.frame_timestamp.isoformat(),\n            \"emission_time\": datetime.now().isoformat(),\n        }\n        frame_predictions[\"inference_metadata\"] = inference_metadata\n        serialised_predictions = json.dumps(frame_predictions).encode(\"utf-8\")\n        self._socket.sendto(\n            serialised_predictions,\n            (\n                self._ip_address,\n                self._port,\n            ),\n        )\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink","title":"<code>VideoFileSink</code>","text":"Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>class VideoFileSink:\n    @classmethod\n    def init(\n        cls,\n        video_file_name: str,\n        annotator: Optional[Union[BaseAnnotator, List[BaseAnnotator]]] = None,\n        display_size: Optional[Tuple[int, int]] = (1280, 720),\n        fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n        display_statistics: bool = False,\n        output_fps: int = 25,\n        quiet: bool = False,\n        video_frame_size: Tuple[int, int] = (1280, 720),\n    ) -&gt; \"VideoFileSink\":\n        \"\"\"\n        Creates `InferencePipeline` predictions sink capable of saving model predictions into video file.\n        It works both for pipelines with single input video and multiple ones.\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n        Args:\n            video_file_name (str): name of the video file to save predictions\n            annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator\n                or list of such instances. If nothing is passed chain of `sv.BoxAnnotator()` and `sv.LabelAnnotator()` is used.\n            display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should\n                be set to the same value as `display_size` for InferencePipeline with single video source, otherwise\n                it represents the size of single visualisation tile (whole tiles mosaic will be scaled to\n                `video_frame_size`)\n            fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n            display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n                if enabled, throughput will only be presented if `fps_monitor` is not None\n            output_fps (int): desired FPS of output file\n            quiet (bool): Flag to decide whether to log progress\n            video_frame_size (Tuple[int, int]): The size of frame in target video file.\n\n        Attributes:\n            on_prediction (Callable[[dict, VideoFrame], None]): callable to be used as a sink for predictions\n\n        Returns: Initialized object of `VideoFileSink` class.\n\n        Example:\n            ```python\n            import cv2\n            from inference import InferencePipeline\n            from inference.core.interfaces.stream.sinks import VideoFileSink\n\n            video_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\n            pipeline = InferencePipeline.init(\n                model_id=\"your-model/3\",\n                video_reference=\"./some_file.mp4\",\n                on_prediction=video_sink.on_prediction,\n            )\n            pipeline.start()\n            pipeline.join()\n            video_sink.release()\n            ```\n\n            `VideoFileSink` used in this way will save predictions to video file automatically.\n        \"\"\"\n        return cls(\n            video_file_name=video_file_name,\n            annotator=annotator,\n            display_size=display_size,\n            fps_monitor=fps_monitor,\n            display_statistics=display_statistics,\n            output_fps=output_fps,\n            quiet=quiet,\n            video_frame_size=video_frame_size,\n        )\n\n    def __init__(\n        self,\n        video_file_name: str,\n        annotator: Union[BaseAnnotator, List[BaseAnnotator]],\n        display_size: Optional[Tuple[int, int]],\n        fps_monitor: Optional[sv.FPSMonitor],\n        display_statistics: bool,\n        output_fps: int,\n        quiet: bool,\n        video_frame_size: Tuple[int, int],\n    ):\n        self._video_file_name = video_file_name\n        self._annotator = annotator\n        self._display_size = display_size\n        self._fps_monitor = fps_monitor\n        self._display_statistics = display_statistics\n        self._output_fps = output_fps\n        self._quiet = quiet\n        self._frame_idx = 0\n        self._video_frame_size = video_frame_size\n        self._video_writer: Optional[cv2.VideoWriter] = None\n        self.on_prediction = partial(\n            render_boxes,\n            annotator=self._annotator,\n            display_size=self._display_size,\n            fps_monitor=self._fps_monitor,\n            display_statistics=self._display_statistics,\n            on_frame_rendered=self._save_predictions,\n        )\n\n    def release(self) -&gt; None:\n        \"\"\"\n        Releases VideoWriter object.\n        \"\"\"\n        if self._video_writer is not None and self._video_writer.isOpened():\n            self._video_writer.release()\n\n    def _save_predictions(\n        self,\n        frame: Union[ImageWithSourceID, List[ImageWithSourceID]],\n    ) -&gt; None:\n        if self._video_writer is None:\n            self._initialise_sink()\n        if issubclass(type(frame), list):\n            frame = create_tiles(images=[i[1] for i in frame])\n        else:\n            frame = frame[1]\n        if (frame.shape[1], frame.shape[0]) != self._video_frame_size:\n            frame = letterbox_image(image=frame, desired_size=self._video_frame_size)\n        self._video_writer.write(frame)\n        if not self._quiet:\n            print(f\"Writing frame {self._frame_idx}\", end=\"\\r\")\n        self._frame_idx += 1\n\n    def _initialise_sink(self) -&gt; None:\n        self._video_writer = cv2.VideoWriter(\n            self._video_file_name,\n            cv2.VideoWriter_fourcc(*\"MJPG\"),\n            self._output_fps,\n            self._video_frame_size,\n        )\n\n    def __enter__(self) -&gt; \"VideoFileSink\":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        self.release()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink.init","title":"<code>init(video_file_name, annotator=None, display_size=(1280, 720), fps_monitor=DEFAULT_FPS_MONITOR, display_statistics=False, output_fps=25, quiet=False, video_frame_size=(1280, 720))</code>  <code>classmethod</code>","text":"<p>Creates <code>InferencePipeline</code> predictions sink capable of saving model predictions into video file. It works both for pipelines with single input video and multiple ones.</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects. Args:     video_file_name (str): name of the video file to save predictions     annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator         or list of such instances. If nothing is passed chain of <code>sv.BoxAnnotator()</code> and <code>sv.LabelAnnotator()</code> is used.     display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should         be set to the same value as <code>display_size</code> for InferencePipeline with single video source, otherwise         it represents the size of single visualisation tile (whole tiles mosaic will be scaled to         <code>video_frame_size</code>)     fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput     display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,         if enabled, throughput will only be presented if <code>fps_monitor</code> is not None     output_fps (int): desired FPS of output file     quiet (bool): Flag to decide whether to log progress     video_frame_size (Tuple[int, int]): The size of frame in target video file.</p> <p>Attributes:</p> Name Type Description <code>on_prediction</code> <code>Callable[[dict, VideoFrame], None]</code> <p>callable to be used as a sink for predictions</p> <p>Returns: Initialized object of <code>VideoFileSink</code> class.</p> Example <pre><code>import cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import VideoFileSink\n\nvideo_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\npipeline = InferencePipeline.init(\n    model_id=\"your-model/3\",\n    video_reference=\"./some_file.mp4\",\n    on_prediction=video_sink.on_prediction,\n)\npipeline.start()\npipeline.join()\nvideo_sink.release()\n</code></pre> <p><code>VideoFileSink</code> used in this way will save predictions to video file automatically.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_file_name: str,\n    annotator: Optional[Union[BaseAnnotator, List[BaseAnnotator]]] = None,\n    display_size: Optional[Tuple[int, int]] = (1280, 720),\n    fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n    display_statistics: bool = False,\n    output_fps: int = 25,\n    quiet: bool = False,\n    video_frame_size: Tuple[int, int] = (1280, 720),\n) -&gt; \"VideoFileSink\":\n    \"\"\"\n    Creates `InferencePipeline` predictions sink capable of saving model predictions into video file.\n    It works both for pipelines with single input video and multiple ones.\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n    Args:\n        video_file_name (str): name of the video file to save predictions\n        annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator\n            or list of such instances. If nothing is passed chain of `sv.BoxAnnotator()` and `sv.LabelAnnotator()` is used.\n        display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should\n            be set to the same value as `display_size` for InferencePipeline with single video source, otherwise\n            it represents the size of single visualisation tile (whole tiles mosaic will be scaled to\n            `video_frame_size`)\n        fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n        display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n            if enabled, throughput will only be presented if `fps_monitor` is not None\n        output_fps (int): desired FPS of output file\n        quiet (bool): Flag to decide whether to log progress\n        video_frame_size (Tuple[int, int]): The size of frame in target video file.\n\n    Attributes:\n        on_prediction (Callable[[dict, VideoFrame], None]): callable to be used as a sink for predictions\n\n    Returns: Initialized object of `VideoFileSink` class.\n\n    Example:\n        ```python\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import VideoFileSink\n\n        video_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\n        pipeline = InferencePipeline.init(\n            model_id=\"your-model/3\",\n            video_reference=\"./some_file.mp4\",\n            on_prediction=video_sink.on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        video_sink.release()\n        ```\n\n        `VideoFileSink` used in this way will save predictions to video file automatically.\n    \"\"\"\n    return cls(\n        video_file_name=video_file_name,\n        annotator=annotator,\n        display_size=display_size,\n        fps_monitor=fps_monitor,\n        display_statistics=display_statistics,\n        output_fps=output_fps,\n        quiet=quiet,\n        video_frame_size=video_frame_size,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink.release","title":"<code>release()</code>","text":"<p>Releases VideoWriter object.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def release(self) -&gt; None:\n    \"\"\"\n    Releases VideoWriter object.\n    \"\"\"\n    if self._video_writer is not None and self._video_writer.isOpened():\n        self._video_writer.release()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.active_learning_sink","title":"<code>active_learning_sink(predictions, video_frame, active_learning_middleware, model_type, disable_preproc_auto_orient=False)</code>","text":"<p>Function to serve as Active Learning sink for InferencePipeline.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <code>active_learning_middleware</code> <code>ActiveLearningMiddleware</code> <p>instance of middleware to register data.</p> required <code>model_type</code> <code>str</code> <p>Type of Roboflow model in use</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>Flag to denote how image is preprocessed which is important in Active Learning.</p> <code>False</code> <p>Returns: None Side effects: Can register data and predictions in Roboflow backend if that's the evaluation of sampling engine.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def active_learning_sink(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    active_learning_middleware: ActiveLearningMiddleware,\n    model_type: str,\n    disable_preproc_auto_orient: bool = False,\n) -&gt; None:\n    \"\"\"\n    Function to serve as Active Learning sink for InferencePipeline.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n        active_learning_middleware (ActiveLearningMiddleware): instance of middleware to register data.\n        model_type (str): Type of Roboflow model in use\n        disable_preproc_auto_orient (bool): Flag to denote how image is preprocessed which is important in\n            Active Learning.\n\n    Returns: None\n    Side effects: Can register data and predictions in Roboflow backend if that's the evaluation of sampling engine.\n    \"\"\"\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    images = [f.image for f in video_frame if f is not None]\n    predictions = [p for p in predictions if p is not None]\n    active_learning_middleware.register_batch(\n        inference_inputs=images,\n        predictions=predictions,\n        prediction_type=model_type,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n    )\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.multi_sink","title":"<code>multi_sink(predictions, video_frame, sinks)</code>","text":"<p>Helper util useful to combine multiple sinks together, while using <code>InferencePipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>video_frame</code> <code>VideoFrame</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code></p> required <code>predictions</code> <code>dict</code> <p>Roboflow object detection predictions with Bounding Boxes</p> required <code>sinks</code> <code>List[Callable[[VideoFrame, dict], None]]</code> <p>list of sinks to be used. Each will be executed one-by-one in the order pointed in input list, all errors will be caught and reported via logger, without re-raising.</p> required <p>Returns: None Side effects: Uses all sinks in context if (video_frame, predictions) input.</p> Example <pre><code>from functools import partial\nimport cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import UDPSink, render_boxes\n\nudp_sink = UDPSink(ip_address=\"127.0.0.1\", port=9090)\non_prediction = partial(multi_sink, sinks=[udp_sink.send_predictions, render_boxes])\n\npipeline = InferencePipeline.init(\n    model_id=\"your-model/3\",\n    video_reference=\"./some_file.mp4\",\n    on_prediction=on_prediction,\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>As a result, predictions will both be sent via UDP socket and displayed in the screen.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def multi_sink(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    sinks: List[SinkHandler],\n) -&gt; None:\n    \"\"\"\n    Helper util useful to combine multiple sinks together, while using `InferencePipeline`.\n\n    Args:\n        video_frame (VideoFrame): frame of video with its basic metadata emitted by `VideoSource`\n        predictions (dict): Roboflow object detection predictions with Bounding Boxes\n        sinks (List[Callable[[VideoFrame, dict], None]]): list of sinks to be used. Each will be executed\n            one-by-one in the order pointed in input list, all errors will be caught and reported via logger,\n            without re-raising.\n\n    Returns: None\n    Side effects: Uses all sinks in context if (video_frame, predictions) input.\n\n    Example:\n        ```python\n        from functools import partial\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import UDPSink, render_boxes\n\n        udp_sink = UDPSink(ip_address=\"127.0.0.1\", port=9090)\n        on_prediction = partial(multi_sink, sinks=[udp_sink.send_predictions, render_boxes])\n\n        pipeline = InferencePipeline.init(\n            model_id=\"your-model/3\",\n            video_reference=\"./some_file.mp4\",\n            on_prediction=on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        ```\n\n        As a result, predictions will both be sent via UDP socket and displayed in the screen.\n    \"\"\"\n    for sink in sinks:\n        try:\n            sink(predictions, video_frame)\n        except Exception as error:\n            logger.error(\n                f\"Could not send prediction and/or frame to sink due to error: {error}.\"\n            )\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.render_boxes","title":"<code>render_boxes(predictions, video_frame, annotator=None, display_size=(1280, 720), fps_monitor=DEFAULT_FPS_MONITOR, display_statistics=False, on_frame_rendered=display_image)</code>","text":"<p>Helper tool to render object detection predictions on top of video frame. It is designed to be used with <code>InferencePipeline</code>, as sink for predictions. By default, it uses standard <code>sv.BoxAnnotator()</code> chained with <code>sv.LabelAnnotator()</code> to draw bounding boxes and resizes prediction to 1280x720 (keeping aspect ratio and adding black padding). One may configure default behaviour, for instance to display latency and throughput statistics. In batch mode it will display tiles of frames and overlay predictions.</p> <p>This sink is only partially compatible with stubs and classification models (it will not fail, although predictions will not be displayed).</p> <p>Since version <code>0.9.18</code>, when multi-source InferencePipeline was introduced - it support batch input, without changes to old functionality when single (predictions, video_frame) is used.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <code>annotator</code> <code>Union[BaseAnnotator, List[BaseAnnotator]]</code> <p>instance of class inheriting from supervision BaseAnnotator or list of such instances. If nothing is passed chain of <code>sv.BoxAnnotator()</code> and <code>sv.LabelAnnotator()</code> is used.</p> <code>None</code> <code>display_size</code> <code>Tuple[int, int]</code> <p>tuple in format (width, height) to resize visualisation output</p> <code>(1280, 720)</code> <code>fps_monitor</code> <code>Optional[FPSMonitor]</code> <p>FPS monitor used to monitor throughput</p> <code>DEFAULT_FPS_MONITOR</code> <code>display_statistics</code> <code>bool</code> <p>Flag to decide if throughput and latency can be displayed in the result image, if enabled, throughput will only be presented if <code>fps_monitor</code> is not None</p> <code>False</code> <code>on_frame_rendered</code> <code>Callable[[Union[ImageWithSourceID, List[ImageWithSourceID]]], None]</code> <p>callback to be called once frame is rendered - by default, function will display OpenCV window. It expects optional integer identifier with np.ndarray or list of those elements. Identifier is supposed to refer to either source_id (for sequential input) or position in the batch (from 0 to batch_size-1).</p> <code>display_image</code> <p>Side effects: on_frame_rendered() is called against the tuple (stream_id, np.ndarray) produced from video     frame and predictions.</p> Example <pre><code>from functools import partial\nimport cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\noutput_size = (640, 480)\nvideo_sink = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 25.0, output_size)\non_prediction = partial(\n    render_boxes,\n    display_size=output_size,\n    on_frame_rendered=lambda frame_data: video_sink.write(frame_data[1])\n)\n\npipeline = InferencePipeline.init(\n     model_id=\"your-model/3\",\n     video_reference=\"./some_file.mp4\",\n     on_prediction=on_prediction,\n)\npipeline.start()\npipeline.join()\nvideo_sink.release()\n</code></pre> <p>In this example, <code>render_boxes()</code> is used as a sink for <code>InferencePipeline</code> predictions - making frames with predictions displayed to be saved into video file. Please note that this is oversimplified example of usage which will not be robust against multiple streams - better implementation available in <code>VideoFileSink</code> class.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def render_boxes(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    annotator: Union[BaseAnnotator, List[BaseAnnotator]] = None,\n    display_size: Optional[Tuple[int, int]] = (1280, 720),\n    fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n    display_statistics: bool = False,\n    on_frame_rendered: Callable[\n        [Union[ImageWithSourceID, List[ImageWithSourceID]]], None\n    ] = display_image,\n) -&gt; None:\n    \"\"\"\n    Helper tool to render object detection predictions on top of video frame. It is designed\n    to be used with `InferencePipeline`, as sink for predictions. By default, it uses\n    standard `sv.BoxAnnotator()` chained with `sv.LabelAnnotator()`\n    to draw bounding boxes and resizes prediction to 1280x720 (keeping aspect ratio and adding black padding).\n    One may configure default behaviour, for instance to display latency and throughput statistics.\n    In batch mode it will display tiles of frames and overlay predictions.\n\n    This sink is only partially compatible with stubs and classification models (it will not fail,\n    although predictions will not be displayed).\n\n    Since version `0.9.18`, when multi-source InferencePipeline was introduced - it support batch input, without\n    changes to old functionality when single (predictions, video_frame) is used.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n        annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator\n            or list of such instances. If nothing is passed chain of `sv.BoxAnnotator()` and `sv.LabelAnnotator()` is used.\n        display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output\n        fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n        display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n            if enabled, throughput will only be presented if `fps_monitor` is not None\n        on_frame_rendered (Callable[[Union[ImageWithSourceID, List[ImageWithSourceID]]], None]): callback to be\n            called once frame is rendered - by default, function will display OpenCV window. It expects optional integer\n            identifier with np.ndarray or list of those elements. Identifier is supposed to refer to either source_id\n            (for sequential input) or position in the batch (from 0 to batch_size-1).\n\n    Returns: None\n    Side effects: on_frame_rendered() is called against the tuple (stream_id, np.ndarray) produced from video\n        frame and predictions.\n\n    Example:\n        ```python\n        from functools import partial\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import render_boxes\n\n        output_size = (640, 480)\n        video_sink = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 25.0, output_size)\n        on_prediction = partial(\n            render_boxes,\n            display_size=output_size,\n            on_frame_rendered=lambda frame_data: video_sink.write(frame_data[1])\n        )\n\n        pipeline = InferencePipeline.init(\n             model_id=\"your-model/3\",\n             video_reference=\"./some_file.mp4\",\n             on_prediction=on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        video_sink.release()\n        ```\n\n        In this example, `render_boxes()` is used as a sink for `InferencePipeline` predictions - making frames with\n        predictions displayed to be saved into video file. Please note that this is oversimplified example of usage\n        which will not be robust against multiple streams - better implementation available in `VideoFileSink` class.\n    \"\"\"\n    sequential_input_provided = False\n    if not isinstance(video_frame, list):\n        sequential_input_provided = True\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    if annotator is None:\n        annotator = [\n            DEFAULT_BBOX_ANNOTATOR,\n            DEFAULT_LABEL_ANNOTATOR,\n        ]\n    fps_value = None\n    if fps_monitor is not None:\n        ticks = sum(f is not None for f in video_frame)\n        for _ in range(ticks):\n            fps_monitor.tick()\n        if hasattr(fps_monitor, \"fps\"):\n            fps_value = fps_monitor.fps\n        else:\n            fps_value = fps_monitor()\n    images: List[ImageWithSourceID] = []\n    annotators = annotator if isinstance(annotator, list) else [annotator]\n    for idx, (single_frame, frame_prediction) in enumerate(\n        zip(video_frame, predictions)\n    ):\n        image = _handle_frame_rendering(\n            frame=single_frame,\n            prediction=frame_prediction,\n            annotators=annotators,\n            display_size=display_size,\n            display_statistics=display_statistics,\n            fps_value=fps_value,\n        )\n        images.append((idx, image))\n    if sequential_input_provided:\n        on_frame_rendered((video_frame[0].source_id, images[0][1]))\n    else:\n        on_frame_rendered(images)\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/stream/","title":"Stream","text":""},{"location":"reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream","title":"<code>Stream</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Roboflow defined stream interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>json_response</code> <code>bool</code> <p>Flag to toggle JSON response format.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model</code> <code>str | Callable</code> <p>The model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> <code>use_bytetrack</code> <code>bool</code> <p>Flag to use bytetrack,</p> <p>Methods:</p> Name Description <code>init_infer</code> <p>Initialize the inference with a test frame.</p> <code>preprocess_thread</code> <p>Preprocess incoming frames for inference.</p> <code>inference_request_thread</code> <p>Manage the inference requests.</p> <code>run_thread</code> <p>Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>class Stream(BaseInterface):\n    \"\"\"Roboflow defined stream interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        json_response (bool): Flag to toggle JSON response format.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model (str|Callable): The model to be used.\n        stream_id (str): The ID of the stream to be used.\n        use_bytetrack (bool): Flag to use bytetrack,\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        enforce_fps: bool = ENFORCE_FPS,\n        iou_threshold: float = IOU_THRESHOLD,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model: Union[str, Callable] = MODEL_ID,\n        source: Union[int, str] = STREAM_ID,\n        use_bytetrack: bool = ENABLE_BYTE_TRACK,\n        use_main_thread: bool = False,\n        output_channel_order: str = \"RGB\",\n        on_prediction: Callable = None,\n        on_start: Callable = None,\n        on_stop: Callable = None,\n    ):\n        \"\"\"Initialize the stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        logger.info(\"Initializing server\")\n\n        self.frame_count = 0\n        self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n        self.use_bytetrack = use_bytetrack\n\n        if source == \"webcam\":\n            stream_id = 0\n        else:\n            stream_id = source\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n\n        self.active_learning_middleware = NullActiveLearningMiddleware()\n        if isinstance(model, str):\n            self.model = get_model(model, self.api_key)\n            if ACTIVE_LEARNING_ENABLED:\n                self.active_learning_middleware = (\n                    ThreadingActiveLearningMiddleware.init(\n                        api_key=self.api_key,\n                        model_id=self.model_id,\n                        cache=cache,\n                    )\n                )\n            self.task_type = get_model_type(\n                model_id=self.model_id, api_key=self.api_key\n            )[0]\n        else:\n            self.model = model\n            self.task_type = \"unknown\"\n\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.use_main_thread = use_main_thread\n        self.output_channel_order = output_channel_order\n\n        self.inference_request_type = (\n            inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n        )\n\n        self.webcam_stream = WebcamStream(\n            stream_id=self.stream_id, enforce_fps=enforce_fps\n        )\n        logger.info(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.on_start_callbacks = []\n        self.on_stop_callbacks = [\n            lambda: self.active_learning_middleware.stop_registration_thread()\n        ]\n        self.on_prediction_callbacks = []\n\n        if on_prediction:\n            self.on_prediction_callbacks.append(on_prediction)\n\n        if on_start:\n            self.on_start_callbacks.append(on_start)\n\n        if on_stop:\n            self.on_stop_callbacks.append(on_stop)\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame = None\n        self.frame_cv = None\n        self.frame_id = None\n        logger.info(\"Server initialized with settings:\")\n        logger.info(f\"Stream ID: {self.stream_id}\")\n        logger.info(f\"Model ID: {self.model_id}\")\n        logger.info(f\"Enforce FPS: {enforce_fps}\")\n        logger.info(f\"Confidence: {self.confidence}\")\n        logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n        logger.info(f\"Max Candidates: {self.max_candidates}\")\n        logger.info(f\"Max Detections: {self.max_detections}\")\n\n        self.run_thread()\n\n    def on_start(self, callback):\n        self.on_start_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_start_callbacks.remove(callback)\n        return unsubscribe\n\n    def on_stop(self, callback):\n        self.on_stop_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_stop_callbacks.remove(callback)\n        return unsubscribe\n\n    def on_prediction(self, callback):\n        self.on_prediction_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_prediction_callbacks.remove(callback)\n        return unsubscribe\n\n    def init_infer(self):\n        \"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        self.model.infer(\n            frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n        )\n        self.active_learning_middleware.start_registration_thread()\n\n    def preprocess_thread(self):\n        \"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame_cv, frame_id = webcam_stream.read_opencv()\n                    if frame_id &gt; 0 and frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        self.frame = cv2.cvtColor(self.frame_cv, cv2.COLOR_BGR2RGB)\n                        self.preproc_result = self.model.preprocess(self.frame_cv)\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            logger.exception(e)\n\n    def inference_request_thread(self):\n        \"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        to registered callbacks.\n        \"\"\"\n        last_print = time.perf_counter()\n        print_ind = 0\n        while True:\n            if self.webcam_stream.stopped is True or self.stop:\n                while len(self.on_stop_callbacks) &gt; 0:\n                    # run each onStop callback only once from this thread\n                    cb = self.on_stop_callbacks.pop()\n                    cb()\n                break\n            if self.queue_control:\n                while len(self.on_start_callbacks) &gt; 0:\n                    # run each onStart callback only once from this thread\n                    cb = self.on_start_callbacks.pop()\n                    cb()\n\n                self.queue_control = False\n                frame_id = self.frame_id\n                inference_input = np.copy(self.frame_cv)\n                start = time.perf_counter()\n                predictions = self.model.predict(\n                    self.img_in,\n                )\n                predictions = self.model.postprocess(\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )[0]\n\n                self.active_learning_middleware.register(\n                    inference_input=inference_input,\n                    prediction=predictions.dict(by_alias=True, exclude_none=True),\n                    prediction_type=self.task_type,\n                )\n                if self.use_bytetrack:\n                    if hasattr(sv.Detections, \"from_inference\"):\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True, exclude_none=True)\n                        )\n                    else:\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True, exclude_none=True)\n                        )\n                    detections = self.byte_tracker.update_with_detections(detections)\n\n                    if detections.tracker_id is None:\n                        detections.tracker_id = np.array([], dtype=int)\n\n                    for pred, detect in zip(predictions.predictions, detections):\n                        pred.tracker_id = int(detect[4])\n                predictions.frame_id = frame_id\n                predictions = predictions.dict(by_alias=True, exclude_none=True)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                for cb in self.on_prediction_callbacks:\n                    if self.output_channel_order == \"BGR\":\n                        cb(predictions, self.frame_cv)\n                    else:\n                        cb(predictions, np.asarray(self.frame))\n\n                current = time.perf_counter()\n                self.webcam_stream.max_fps = 1 / (current - start)\n                logger.debug(f\"FPS: {self.webcam_stream.max_fps:.2f}\")\n\n                if time.perf_counter() - last_print &gt; 1:\n                    print_ind = (print_ind + 1) % 4\n                    last_print = time.perf_counter()\n\n    def run_thread(self):\n        \"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        preprocess_thread.start()\n\n        if self.use_main_thread:\n            self.inference_request_thread()\n        else:\n            # start a thread that looks for the predictions\n            # and call the callbacks\n            inference_request_thread = threading.Thread(\n                target=self.inference_request_thread\n            )\n            inference_request_thread.start()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.__init__","title":"<code>__init__(api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, enforce_fps=ENFORCE_FPS, iou_threshold=IOU_THRESHOLD, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model=MODEL_ID, source=STREAM_ID, use_bytetrack=ENABLE_BYTE_TRACK, use_main_thread=False, output_channel_order='RGB', on_prediction=None, on_start=None, on_stop=None)</code>","text":"<p>Initialize the stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    enforce_fps: bool = ENFORCE_FPS,\n    iou_threshold: float = IOU_THRESHOLD,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model: Union[str, Callable] = MODEL_ID,\n    source: Union[int, str] = STREAM_ID,\n    use_bytetrack: bool = ENABLE_BYTE_TRACK,\n    use_main_thread: bool = False,\n    output_channel_order: str = \"RGB\",\n    on_prediction: Callable = None,\n    on_start: Callable = None,\n    on_stop: Callable = None,\n):\n    \"\"\"Initialize the stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    logger.info(\"Initializing server\")\n\n    self.frame_count = 0\n    self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n    self.use_bytetrack = use_bytetrack\n\n    if source == \"webcam\":\n        stream_id = 0\n    else:\n        stream_id = source\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n\n    self.active_learning_middleware = NullActiveLearningMiddleware()\n    if isinstance(model, str):\n        self.model = get_model(model, self.api_key)\n        if ACTIVE_LEARNING_ENABLED:\n            self.active_learning_middleware = (\n                ThreadingActiveLearningMiddleware.init(\n                    api_key=self.api_key,\n                    model_id=self.model_id,\n                    cache=cache,\n                )\n            )\n        self.task_type = get_model_type(\n            model_id=self.model_id, api_key=self.api_key\n        )[0]\n    else:\n        self.model = model\n        self.task_type = \"unknown\"\n\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.use_main_thread = use_main_thread\n    self.output_channel_order = output_channel_order\n\n    self.inference_request_type = (\n        inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n    )\n\n    self.webcam_stream = WebcamStream(\n        stream_id=self.stream_id, enforce_fps=enforce_fps\n    )\n    logger.info(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.on_start_callbacks = []\n    self.on_stop_callbacks = [\n        lambda: self.active_learning_middleware.stop_registration_thread()\n    ]\n    self.on_prediction_callbacks = []\n\n    if on_prediction:\n        self.on_prediction_callbacks.append(on_prediction)\n\n    if on_start:\n        self.on_start_callbacks.append(on_start)\n\n    if on_stop:\n        self.on_stop_callbacks.append(on_stop)\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame = None\n    self.frame_cv = None\n    self.frame_id = None\n    logger.info(\"Server initialized with settings:\")\n    logger.info(f\"Stream ID: {self.stream_id}\")\n    logger.info(f\"Model ID: {self.model_id}\")\n    logger.info(f\"Enforce FPS: {enforce_fps}\")\n    logger.info(f\"Confidence: {self.confidence}\")\n    logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n    logger.info(f\"Max Candidates: {self.max_candidates}\")\n    logger.info(f\"Max Detections: {self.max_detections}\")\n\n    self.run_thread()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results to registered callbacks.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def inference_request_thread(self):\n    \"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    to registered callbacks.\n    \"\"\"\n    last_print = time.perf_counter()\n    print_ind = 0\n    while True:\n        if self.webcam_stream.stopped is True or self.stop:\n            while len(self.on_stop_callbacks) &gt; 0:\n                # run each onStop callback only once from this thread\n                cb = self.on_stop_callbacks.pop()\n                cb()\n            break\n        if self.queue_control:\n            while len(self.on_start_callbacks) &gt; 0:\n                # run each onStart callback only once from this thread\n                cb = self.on_start_callbacks.pop()\n                cb()\n\n            self.queue_control = False\n            frame_id = self.frame_id\n            inference_input = np.copy(self.frame_cv)\n            start = time.perf_counter()\n            predictions = self.model.predict(\n                self.img_in,\n            )\n            predictions = self.model.postprocess(\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )[0]\n\n            self.active_learning_middleware.register(\n                inference_input=inference_input,\n                prediction=predictions.dict(by_alias=True, exclude_none=True),\n                prediction_type=self.task_type,\n            )\n            if self.use_bytetrack:\n                if hasattr(sv.Detections, \"from_inference\"):\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True, exclude_none=True)\n                    )\n                else:\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True, exclude_none=True)\n                    )\n                detections = self.byte_tracker.update_with_detections(detections)\n\n                if detections.tracker_id is None:\n                    detections.tracker_id = np.array([], dtype=int)\n\n                for pred, detect in zip(predictions.predictions, detections):\n                    pred.tracker_id = int(detect[4])\n            predictions.frame_id = frame_id\n            predictions = predictions.dict(by_alias=True, exclude_none=True)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            for cb in self.on_prediction_callbacks:\n                if self.output_channel_order == \"BGR\":\n                    cb(predictions, self.frame_cv)\n                else:\n                    cb(predictions, np.asarray(self.frame))\n\n            current = time.perf_counter()\n            self.webcam_stream.max_fps = 1 / (current - start)\n            logger.debug(f\"FPS: {self.webcam_stream.max_fps:.2f}\")\n\n            if time.perf_counter() - last_print &gt; 1:\n                print_ind = (print_ind + 1) % 4\n                last_print = time.perf_counter()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def init_infer(self):\n    \"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    self.model.infer(\n        frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n    )\n    self.active_learning_middleware.start_registration_thread()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def preprocess_thread(self):\n    \"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame_cv, frame_id = webcam_stream.read_opencv()\n                if frame_id &gt; 0 and frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    self.frame = cv2.cvtColor(self.frame_cv, cv2.COLOR_BGR2RGB)\n                    self.preproc_result = self.model.preprocess(self.frame_cv)\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        logger.exception(e)\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def run_thread(self):\n    \"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    preprocess_thread.start()\n\n    if self.use_main_thread:\n        self.inference_request_thread()\n    else:\n        # start a thread that looks for the predictions\n        # and call the callbacks\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n        inference_request_thread.start()\n</code></pre>"},{"location":"reference/inference/core/interfaces/stream/watchdog/","title":"Watchdog","text":"<p>This module contains component intended to use in combination with <code>InferencePipeline</code> to ensure observability. Please consider them internal details of implementation.</p>"},{"location":"reference/inference/core/interfaces/stream/watchdog/#inference.core.interfaces.stream.watchdog.BasePipelineWatchDog","title":"<code>BasePipelineWatchDog</code>","text":"<p>               Bases: <code>PipelineWatchDog</code></p> <p>Implementation to be used from single inference thread, as it keeps state assumed to represent status of consecutive stage of prediction process in latency monitor.</p> Source code in <code>inference/core/interfaces/stream/watchdog.py</code> <pre><code>class BasePipelineWatchDog(PipelineWatchDog):\n    \"\"\"\n    Implementation to be used from single inference thread, as it keeps\n    state assumed to represent status of consecutive stage of prediction process\n    in latency monitor.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._video_sources: Optional[List[VideoSource]] = None\n        self._inference_throughput_monitor = sv.FPSMonitor()\n        self._latency_monitors: Dict[Optional[int], LatencyMonitor] = {}\n        self._stream_updates = deque(maxlen=MAX_UPDATES_CONTEXT)\n\n    def register_video_sources(self, video_sources: List[VideoSource]) -&gt; None:\n        self._video_sources = video_sources\n        for source in video_sources:\n            self._latency_monitors[source.source_id] = LatencyMonitor(\n                source_id=source.source_id\n            )\n\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        if status_update.severity.value &lt;= UpdateSeverity.DEBUG.value:\n            return None\n        self._stream_updates.append(status_update)\n\n    def on_model_inference_started(self, frames: List[VideoFrame]) -&gt; None:\n        for frame in frames:\n            self._latency_monitors[frame.source_id].register_inference_start(\n                frame_timestamp=frame.frame_timestamp,\n                frame_id=frame.frame_id,\n            )\n\n    def on_model_prediction_ready(self, frames: List[VideoFrame]) -&gt; None:\n        for frame in frames:\n            self._latency_monitors[frame.source_id].register_prediction_ready(\n                frame_timestamp=frame.frame_timestamp,\n                frame_id=frame.frame_id,\n            )\n            self._inference_throughput_monitor.tick()\n\n    def get_report(self) -&gt; PipelineStateReport:\n        sources_metadata = []\n        if self._video_sources is not None:\n            sources_metadata = [s.describe_source() for s in self._video_sources]\n        latency_reports = [\n            monitor.summarise_reports() for monitor in self._latency_monitors.values()\n        ]\n        if hasattr(self._inference_throughput_monitor, \"fps\"):\n            _inference_throughput_fps = self._inference_throughput_monitor.fps\n        else:\n            _inference_throughput_fps = self._inference_throughput_monitor()\n        return PipelineStateReport(\n            video_source_status_updates=list(self._stream_updates),\n            latency_reports=latency_reports,\n            inference_throughput=_inference_throughput_fps,\n            sources_metadata=sources_metadata,\n        )\n</code></pre>"},{"location":"reference/inference/core/interfaces/udp/udp_stream/","title":"Udp stream","text":""},{"location":"reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream","title":"<code>UdpStream</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Roboflow defined UDP interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>ip_broadcast_addr</code> <code>str</code> <p>The IP address to broadcast to.</p> <code>ip_broadcast_port</code> <code>int</code> <p>The port to broadcast on.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model_id</code> <code>str</code> <p>The ID of the model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> <code>use_bytetrack</code> <code>bool</code> <p>Flag to use bytetrack,</p> <p>Methods:</p> Name Description <code>init_infer</code> <p>Initialize the inference with a test frame.</p> <code>preprocess_thread</code> <p>Preprocess incoming frames for inference.</p> <code>inference_request_thread</code> <p>Manage the inference requests.</p> <code>run_thread</code> <p>Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>class UdpStream(BaseInterface):\n    \"\"\"Roboflow defined UDP interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        ip_broadcast_addr (str): The IP address to broadcast to.\n        ip_broadcast_port (int): The port to broadcast on.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model_id (str): The ID of the model to be used.\n        stream_id (str): The ID of the stream to be used.\n        use_bytetrack (bool): Flag to use bytetrack,\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        enforce_fps: bool = ENFORCE_FPS,\n        ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n        ip_broadcast_port: int = IP_BROADCAST_PORT,\n        iou_threshold: float = IOU_THRESHOLD,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model_id: str = MODEL_ID,\n        stream_id: Union[int, str] = STREAM_ID,\n        use_bytetrack: bool = ENABLE_BYTE_TRACK,\n    ):\n        \"\"\"Initialize the UDP stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        logger.info(\"Initializing server\")\n\n        self.frame_count = 0\n        self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n        self.use_bytetrack = use_bytetrack\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model_id\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n        if not self.api_key:\n            raise ValueError(\n                f\"API key is missing. Either pass it explicitly to constructor, or use one of env variables: \"\n                f\"{API_KEY_ENV_NAMES}. Visit \"\n                f\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to generate \"\n                f\"the key.\"\n            )\n\n        self.model = get_model(self.model_id, self.api_key)\n        self.task_type = get_model_type(model_id=self.model_id, api_key=self.api_key)[0]\n        self.active_learning_middleware = NullActiveLearningMiddleware()\n        if ACTIVE_LEARNING_ENABLED:\n            self.active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n                api_key=self.api_key,\n                model_id=self.model_id,\n                cache=cache,\n            )\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.ip_broadcast_addr = ip_broadcast_addr\n        self.ip_broadcast_port = ip_broadcast_port\n\n        self.inference_request_type = (\n            inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n        )\n\n        self.UDPServerSocket = socket.socket(\n            family=socket.AF_INET, type=socket.SOCK_DGRAM\n        )\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n\n        self.webcam_stream = WebcamStream(\n            stream_id=self.stream_id, enforce_fps=enforce_fps\n        )\n        logger.info(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame_cv = None\n        self.frame_id = None\n        logger.info(\"Server initialized with settings:\")\n        logger.info(f\"Stream ID: {self.stream_id}\")\n        logger.info(f\"Model ID: {self.model_id}\")\n        logger.info(f\"Confidence: {self.confidence}\")\n        logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n        logger.info(f\"Max Candidates: {self.max_candidates}\")\n        logger.info(f\"Max Detections: {self.max_detections}\")\n\n    def init_infer(self):\n        \"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        self.model.infer(\n            frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n        )\n        self.active_learning_middleware.start_registration_thread()\n\n    def preprocess_thread(self):\n        \"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame_cv, frame_id = webcam_stream.read_opencv()\n                    if frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        self.preproc_result = self.model.preprocess(self.frame_cv)\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            logger.error(e)\n\n    def inference_request_thread(self):\n        \"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        as a UDP broadcast.\n        \"\"\"\n        last_print = time.perf_counter()\n        print_ind = 0\n        print_chars = [\"|\", \"/\", \"-\", \"\\\\\"]\n        while True:\n            if self.stop:\n                break\n            if self.queue_control:\n                self.queue_control = False\n                frame_id = self.frame_id\n                inference_input = np.copy(self.frame_cv)\n                predictions = self.model.predict(\n                    self.img_in,\n                )\n                predictions = self.model.postprocess(\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )[0]\n                self.active_learning_middleware.register(\n                    inference_input=inference_input,\n                    prediction=predictions.dict(by_alias=True, exclude_none=True),\n                    prediction_type=self.task_type,\n                )\n                if self.use_bytetrack:\n                    if hasattr(sv.Detections, \"from_inference\"):\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True), self.model.class_names\n                        )\n                    else:\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True), self.model.class_names\n                        )\n                    detections = self.byte_tracker.update_with_detections(detections)\n                    for pred, detect in zip(predictions.predictions, detections):\n                        pred.tracker_id = int(detect[4])\n                predictions.frame_id = frame_id\n                predictions = predictions.json(exclude_none=True, by_alias=True)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                bytesToSend = predictions.encode(\"utf-8\")\n                self.UDPServerSocket.sendto(\n                    bytesToSend,\n                    (\n                        self.ip_broadcast_addr,\n                        self.ip_broadcast_port,\n                    ),\n                )\n                if time.perf_counter() - last_print &gt; 1:\n                    print(f\"Streaming {print_chars[print_ind]}\", end=\"\\r\")\n                    print_ind = (print_ind + 1) % 4\n                    last_print = time.perf_counter()\n\n    def run_thread(self):\n        \"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n\n        preprocess_thread.start()\n        inference_request_thread.start()\n\n        while True:\n            try:\n                time.sleep(10)\n            except KeyboardInterrupt:\n                logger.info(\"Stopping server...\")\n                self.stop = True\n                self.active_learning_middleware.stop_registration_thread()\n                time.sleep(3)\n                sys.exit(0)\n</code></pre>"},{"location":"reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.__init__","title":"<code>__init__(api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, enforce_fps=ENFORCE_FPS, ip_broadcast_addr=IP_BROADCAST_ADDR, ip_broadcast_port=IP_BROADCAST_PORT, iou_threshold=IOU_THRESHOLD, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model_id=MODEL_ID, stream_id=STREAM_ID, use_bytetrack=ENABLE_BYTE_TRACK)</code>","text":"<p>Initialize the UDP stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    enforce_fps: bool = ENFORCE_FPS,\n    ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n    ip_broadcast_port: int = IP_BROADCAST_PORT,\n    iou_threshold: float = IOU_THRESHOLD,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model_id: str = MODEL_ID,\n    stream_id: Union[int, str] = STREAM_ID,\n    use_bytetrack: bool = ENABLE_BYTE_TRACK,\n):\n    \"\"\"Initialize the UDP stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    logger.info(\"Initializing server\")\n\n    self.frame_count = 0\n    self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n    self.use_bytetrack = use_bytetrack\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model_id\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n    if not self.api_key:\n        raise ValueError(\n            f\"API key is missing. Either pass it explicitly to constructor, or use one of env variables: \"\n            f\"{API_KEY_ENV_NAMES}. Visit \"\n            f\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to generate \"\n            f\"the key.\"\n        )\n\n    self.model = get_model(self.model_id, self.api_key)\n    self.task_type = get_model_type(model_id=self.model_id, api_key=self.api_key)[0]\n    self.active_learning_middleware = NullActiveLearningMiddleware()\n    if ACTIVE_LEARNING_ENABLED:\n        self.active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n            api_key=self.api_key,\n            model_id=self.model_id,\n            cache=cache,\n        )\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.ip_broadcast_addr = ip_broadcast_addr\n    self.ip_broadcast_port = ip_broadcast_port\n\n    self.inference_request_type = (\n        inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n    )\n\n    self.UDPServerSocket = socket.socket(\n        family=socket.AF_INET, type=socket.SOCK_DGRAM\n    )\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n\n    self.webcam_stream = WebcamStream(\n        stream_id=self.stream_id, enforce_fps=enforce_fps\n    )\n    logger.info(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame_cv = None\n    self.frame_id = None\n    logger.info(\"Server initialized with settings:\")\n    logger.info(f\"Stream ID: {self.stream_id}\")\n    logger.info(f\"Model ID: {self.model_id}\")\n    logger.info(f\"Confidence: {self.confidence}\")\n    logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n    logger.info(f\"Max Candidates: {self.max_candidates}\")\n    logger.info(f\"Max Detections: {self.max_detections}\")\n</code></pre>"},{"location":"reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results as a UDP broadcast.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def inference_request_thread(self):\n    \"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    as a UDP broadcast.\n    \"\"\"\n    last_print = time.perf_counter()\n    print_ind = 0\n    print_chars = [\"|\", \"/\", \"-\", \"\\\\\"]\n    while True:\n        if self.stop:\n            break\n        if self.queue_control:\n            self.queue_control = False\n            frame_id = self.frame_id\n            inference_input = np.copy(self.frame_cv)\n            predictions = self.model.predict(\n                self.img_in,\n            )\n            predictions = self.model.postprocess(\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )[0]\n            self.active_learning_middleware.register(\n                inference_input=inference_input,\n                prediction=predictions.dict(by_alias=True, exclude_none=True),\n                prediction_type=self.task_type,\n            )\n            if self.use_bytetrack:\n                if hasattr(sv.Detections, \"from_inference\"):\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True), self.model.class_names\n                    )\n                else:\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True), self.model.class_names\n                    )\n                detections = self.byte_tracker.update_with_detections(detections)\n                for pred, detect in zip(predictions.predictions, detections):\n                    pred.tracker_id = int(detect[4])\n            predictions.frame_id = frame_id\n            predictions = predictions.json(exclude_none=True, by_alias=True)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            bytesToSend = predictions.encode(\"utf-8\")\n            self.UDPServerSocket.sendto(\n                bytesToSend,\n                (\n                    self.ip_broadcast_addr,\n                    self.ip_broadcast_port,\n                ),\n            )\n            if time.perf_counter() - last_print &gt; 1:\n                print(f\"Streaming {print_chars[print_ind]}\", end=\"\\r\")\n                print_ind = (print_ind + 1) % 4\n                last_print = time.perf_counter()\n</code></pre>"},{"location":"reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def init_infer(self):\n    \"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    self.model.infer(\n        frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n    )\n    self.active_learning_middleware.start_registration_thread()\n</code></pre>"},{"location":"reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def preprocess_thread(self):\n    \"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame_cv, frame_id = webcam_stream.read_opencv()\n                if frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    self.preproc_result = self.model.preprocess(self.frame_cv)\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        logger.error(e)\n</code></pre>"},{"location":"reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def run_thread(self):\n    \"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    inference_request_thread = threading.Thread(\n        target=self.inference_request_thread\n    )\n\n    preprocess_thread.start()\n    inference_request_thread.start()\n\n    while True:\n        try:\n            time.sleep(10)\n        except KeyboardInterrupt:\n            logger.info(\"Stopping server...\")\n            self.stop = True\n            self.active_learning_middleware.stop_registration_thread()\n            time.sleep(3)\n            sys.exit(0)\n</code></pre>"},{"location":"reference/inference/core/logging/memory_handler/","title":"Memory handler","text":"<p>In-memory logging handler for dashboard log viewing.</p> <p>This module provides a custom logging handler that stores log records in memory for retrieval via the /logs API endpoint. It's designed to be used when ENABLE_IN_MEMORY_LOGS environment variable is set to 'true'.</p>"},{"location":"reference/inference/core/logging/memory_handler/#inference.core.logging.memory_handler.MemoryLogHandler","title":"<code>MemoryLogHandler</code>","text":"<p>               Bases: <code>Handler</code></p> <p>Custom log handler that stores log records in memory for dashboard access</p> Source code in <code>inference/core/logging/memory_handler.py</code> <pre><code>class MemoryLogHandler(logging.Handler):\n    \"\"\"Custom log handler that stores log records in memory for dashboard access\"\"\"\n\n    def emit(self, record):\n        try:\n            # Format the log entry for JSON serialization\n            log_entry = {\n                \"timestamp\": datetime.fromtimestamp(record.created).isoformat(),\n                \"level\": record.levelname,\n                \"logger\": record.name,\n                \"message\": self.format(record),\n                \"module\": record.module or \"\",\n                \"line\": record.lineno,\n            }\n\n            with _log_lock:\n                _log_entries.append(log_entry)\n        except Exception:\n            # Silently handle any errors in logging to prevent recursion\n            pass\n</code></pre>"},{"location":"reference/inference/core/logging/memory_handler/#inference.core.logging.memory_handler.get_recent_logs","title":"<code>get_recent_logs(limit=100, level=None, since=None)</code>","text":"<p>Get recent log entries from memory</p> Source code in <code>inference/core/logging/memory_handler.py</code> <pre><code>def get_recent_logs(\n    limit: int = 100, level: str = None, since: str = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get recent log entries from memory\"\"\"\n    with _log_lock:\n        logs = list(_log_entries)\n\n    # Filter by log level if specified\n    if level:\n        level_upper = level.upper()\n        logs = [log for log in logs if log[\"level\"] == level_upper]\n\n    # Filter by timestamp if specified\n    if since:\n        try:\n            since_dt = datetime.fromisoformat(since.replace(\"Z\", \"+00:00\"))\n            logs = [\n                log\n                for log in logs\n                if datetime.fromisoformat(log[\"timestamp\"]) &gt; since_dt\n            ]\n        except ValueError:\n            pass  # Invalid since timestamp, ignore filter\n\n    # Limit results\n    return logs[-limit:] if limit else logs\n</code></pre>"},{"location":"reference/inference/core/logging/memory_handler/#inference.core.logging.memory_handler.setup_memory_logging","title":"<code>setup_memory_logging()</code>","text":"<p>Set up memory logging handler for the current logger hierarchy</p> Source code in <code>inference/core/logging/memory_handler.py</code> <pre><code>def setup_memory_logging() -&gt; None:\n    \"\"\"Set up memory logging handler for the current logger hierarchy\"\"\"\n    if not is_memory_logging_enabled():\n        return\n    logger.info(\"Setting up memory logging\")\n    memory_handler = MemoryLogHandler()\n    memory_handler.setLevel(logging.DEBUG)  # Capture all levels\n    memory_formatter = logging.Formatter(\n        \"%(asctime)s %(levelname)s %(name)s: %(message)s\"\n    )\n    memory_handler.setFormatter(memory_formatter)\n\n    # Add to root logger to capture all logs immediately\n    root_logger = logging.getLogger()\n    if memory_handler not in root_logger.handlers:\n        root_logger.addHandler(memory_handler)\n\n    # Specifically add to uvicorn.access logger to ensure access logs are captured now\n    access_logger = logging.getLogger(\"uvicorn.access\")\n    if memory_handler not in access_logger.handlers:\n        access_logger.addHandler(memory_handler)\n\n    # Also patch uvicorn's default LOGGING_CONFIG so when uvicorn applies dictConfig,\n    # our in-memory handler remains attached\n    global _uvicorn_config_patched\n    if not _uvicorn_config_patched:\n        try:\n            from uvicorn.config import LOGGING_CONFIG as UVICORN_LOGGING_CONFIG\n\n            # Modify in-place (safe: uvicorn makes a deep copy later)\n            log_config = UVICORN_LOGGING_CONFIG\n\n            log_config.setdefault(\"formatters\", {})\n            if \"default\" not in log_config[\"formatters\"]:\n                log_config[\"formatters\"][\"default\"] = {\n                    \"()\": \"uvicorn.logging.DefaultFormatter\",\n                    \"fmt\": \"%(levelprefix)s %(message)s\",\n                    \"use_colors\": None,\n                }\n\n            log_config.setdefault(\"handlers\", {})[\"inmemory\"] = {\n                \"class\": \"inference.core.logging.memory_handler.MemoryLogHandler\",\n                \"level\": \"DEBUG\",\n                \"formatter\": \"default\",\n            }\n\n            log_config.setdefault(\"loggers\", {})\n            log_config[\"loggers\"].setdefault(\n                \"uvicorn.access\",\n                {\n                    \"handlers\": [\"default\"],\n                    \"level\": \"INFO\",\n                    \"propagate\": False,\n                },\n            )\n            if \"inmemory\" not in log_config[\"loggers\"][\"uvicorn.access\"][\"handlers\"]:\n                log_config[\"loggers\"][\"uvicorn.access\"][\"handlers\"].append(\"inmemory\")\n\n            log_config[\"loggers\"].setdefault(\n                \"uvicorn\", {\"handlers\": [\"default\"], \"level\": \"INFO\"}\n            )\n            log_config[\"loggers\"].setdefault(\"uvicorn.error\", {\"level\": \"INFO\"})\n\n            root_cfg = log_config.setdefault(\n                \"root\", {\"handlers\": [\"default\"], \"level\": \"INFO\"}\n            )\n            if \"inmemory\" not in root_cfg.get(\"handlers\", []):\n                root_cfg.setdefault(\"handlers\", []).append(\"inmemory\")\n\n            _uvicorn_config_patched = True\n            logger.info(\"Patched uvicorn LOGGING_CONFIG to include MemoryLogHandler\")\n        except Exception:\n            # Avoid hard failure if uvicorn is not available\n            pass\n\n    return memory_handler\n</code></pre>"},{"location":"reference/inference/core/managers/base/","title":"Base","text":""},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager","title":"<code>ModelManager</code>","text":"<p>Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>class ModelManager:\n    \"\"\"Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.\"\"\"\n\n    def __init__(self, model_registry: ModelRegistry, models: Optional[dict] = None):\n        self.model_registry = model_registry\n        self._models: Dict[str, Model] = models if models is not None else {}\n        self.pingback = None\n        self._state_lock = Lock()\n        self._models_state_locks: Dict[str, Lock] = {}\n\n    def init_pingback(self):\n        \"\"\"Initializes pingback mechanism.\"\"\"\n        self.num_errors = 0  # in the device\n        self.uuid = ROBOFLOW_SERVER_UUID\n        if METRICS_ENABLED:\n            self.pingback = PingbackInfo(self)\n            self.pingback.start()\n\n    def add_model(\n        self,\n        model_id: str,\n        api_key: str,\n        model_id_alias: Optional[str] = None,\n        endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Adds a new model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n            endpoint_type (ModelEndpointType, optional): The endpoint type to use for the model.\n        \"\"\"\n        if MODELS_CACHE_AUTH_ENABLED:\n            if not _check_if_api_key_has_access_to_model(\n                api_key=api_key,\n                model_id=model_id,\n                endpoint_type=endpoint_type,\n                countinference=countinference,\n                service_secret=service_secret,\n            ):\n                raise RoboflowAPINotAuthorizedError(\n                    f\"API key {api_key} does not have access to model {model_id}\"\n                )\n\n        logger.debug(\n            f\"ModelManager - Adding model with model_id={model_id}, model_id_alias={model_id_alias}\"\n        )\n        resolved_identifier = model_id if model_id_alias is None else model_id_alias\n        model_lock = self._get_lock_for_a_model(model_id=resolved_identifier)\n        with acquire_with_timeout(lock=model_lock) as acquired:\n            if not acquired:\n                # if failed to acquire - then in use, no need to purge lock\n                raise ModelManagerLockAcquisitionError(\n                    f\"Could not acquire lock for model with id={resolved_identifier}.\"\n                )\n            if resolved_identifier in self._models:\n                logger.debug(\n                    f\"ModelManager - model with model_id={resolved_identifier} is already loaded.\"\n                )\n                return\n            try:\n                logger.debug(\"ModelManager - model initialisation...\")\n                model_class = self.model_registry.get_model(\n                    resolved_identifier,\n                    api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n                model = model_class(\n                    model_id=model_id,\n                    api_key=api_key,\n                    countinference=countinference,\n                    service_secret=service_secret,\n                )\n\n                # Pass countinference and service_secret to download_model_artifacts_from_roboflow_api if available\n                if (\n                    hasattr(model, \"download_model_artifacts_from_roboflow_api\")\n                    and INTERNAL_WEIGHTS_URL_SUFFIX == \"serverless\"\n                ):\n                    # Only pass these parameters if INTERNAL_WEIGHTS_URL_SUFFIX is \"serverless\"\n                    if (\n                        hasattr(model, \"cache_model_artefacts\")\n                        and not model.has_model_metadata\n                    ):\n                        # Override the download_model_artifacts_from_roboflow_api method with parameters\n                        original_method = (\n                            model.download_model_artifacts_from_roboflow_api\n                        )\n                        model.download_model_artifacts_from_roboflow_api = (\n                            lambda: original_method(\n                                countinference=countinference,\n                                service_secret=service_secret,\n                            )\n                        )\n\n                logger.debug(\"ModelManager - model successfully loaded.\")\n                self._models[resolved_identifier] = model\n            except Exception as error:\n                self._dispose_model_lock(model_id=resolved_identifier)\n                raise error\n\n    def check_for_model(self, model_id: str) -&gt; None:\n        \"\"\"Checks whether the model with the given ID is in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Raises:\n            InferenceModelNotFound: If the model is not found in the manager.\n        \"\"\"\n        if model_id not in self:\n            raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Runs inference on the specified model with the given request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - inference from request started for model_id={model_id}.\"\n        )\n        enable_model_monitoring = not getattr(\n            request, \"disable_model_monitoring\", False\n        )\n        if METRICS_ENABLED and self.pingback and enable_model_monitoring:\n            logger.debug(\"ModelManager - setting pingback fallback api key...\")\n            self.pingback.fallback_api_key = request.api_key\n        try:\n            rtn_val = await self.model_infer(\n                model_id=model_id, request=request, **kwargs\n            )\n            logger.debug(\n                f\"ModelManager - inference from request finished for model_id={model_id}.\"\n            )\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n                try:\n                    logger.debug(\n                        f\"ModelManager - caching inference request started for model_id={model_id}\"\n                    )\n                    cache.zadd(\n                        f\"models\",\n                        value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                    if (\n                        hasattr(request, \"image\")\n                        and hasattr(request.image, \"type\")\n                        and request.image.type == \"numpy\"\n                    ):\n                        request.image.value = str(request.image.value)\n                    cache.zadd(\n                        f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                        value=to_cachable_inference_item(request, rtn_val),\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                    logger.debug(\n                        f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                    )\n                except Exception as cache_error:\n                    logger.warning(\n                        f\"Failed to cache inference data for model {model_id}: {cache_error}\"\n                    )\n            return rtn_val\n        except Exception as e:\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n                try:\n                    cache.zadd(\n                        f\"models\",\n                        value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                    cache.zadd(\n                        f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                        value={\n                            \"request\": jsonable_encoder(\n                                request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                            ),\n                            \"error\": str(e),\n                        },\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                except Exception as cache_error:\n                    logger.warning(\n                        f\"Failed to cache error data for model {model_id}: {cache_error}\"\n                    )\n            raise\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Runs inference on the specified model with the given request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - inference from request started for model_id={model_id}.\"\n        )\n        enable_model_monitoring = not getattr(\n            request, \"disable_model_monitoring\", False\n        )\n        if METRICS_ENABLED and self.pingback and enable_model_monitoring:\n            logger.debug(\"ModelManager - setting pingback fallback api key...\")\n            self.pingback.fallback_api_key = request.api_key\n        try:\n            rtn_val = self.model_infer_sync(\n                model_id=model_id, request=request, **kwargs\n            )\n            logger.debug(\n                f\"ModelManager - inference from request finished for model_id={model_id}.\"\n            )\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n                try:\n                    logger.debug(\n                        f\"ModelManager - caching inference request started for model_id={model_id}\"\n                    )\n                    cache.zadd(\n                        f\"models\",\n                        value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                    if (\n                        hasattr(request, \"image\")\n                        and hasattr(request.image, \"type\")\n                        and request.image.type == \"numpy\"\n                    ):\n                        request.image.value = str(request.image.value)\n                    cache.zadd(\n                        f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                        value=to_cachable_inference_item(request, rtn_val),\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                    logger.debug(\n                        f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                    )\n                except Exception as cache_error:\n                    logger.warning(\n                        f\"Failed to cache inference data for model {model_id}: {cache_error}\"\n                    )\n            return rtn_val\n        except Exception as e:\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n                try:\n                    cache.zadd(\n                        f\"models\",\n                        value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                    cache.zadd(\n                        f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                        value={\n                            \"request\": jsonable_encoder(\n                                request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                            ),\n                            \"error\": str(e),\n                        },\n                        score=finish_time,\n                        expire=METRICS_INTERVAL * 2,\n                    )\n                except Exception as cache_error:\n                    logger.warning(\n                        f\"Failed to cache error data for model {model_id}: {cache_error}\"\n                    )\n            raise\n\n    async def model_infer(self, model_id: str, request: InferenceRequest, **kwargs):\n        model = self._get_model_reference(model_id=model_id)\n        return model.infer_from_request(request)\n\n    def model_infer_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n        model = self._get_model_reference(model_id=model_id)\n        return model.infer_from_request(request)\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Creates a response object from the model's predictions.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (List[List[float]]): The model's predictions.\n\n        Returns:\n            InferenceResponse: The created response object.\n        \"\"\"\n        model = self._get_model_reference(model_id=model_id)\n        return model.make_response(predictions, *args, **kwargs)\n\n    def postprocess(\n        self,\n        model_id: str,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        *args,\n        **kwargs,\n    ) -&gt; List[List[float]]:\n        \"\"\"Processes the model's predictions after inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (np.ndarray): The model's predictions.\n\n        Returns:\n            List[List[float]]: The post-processed predictions.\n        \"\"\"\n        model = self._get_model_reference(model_id=model_id)\n        return model.postprocess(\n            predictions, preprocess_return_metadata, *args, **kwargs\n        )\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"Runs prediction on the specified model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            np.ndarray: The predictions from the model.\n        \"\"\"\n        model = self._get_model_reference(model_id=model_id)\n        model.metrics[\"num_inferences\"] += 1\n        tic = time.perf_counter()\n        res = model.predict(*args, **kwargs)\n        toc = time.perf_counter()\n        model.metrics[\"avg_inference_time\"] += toc - tic\n        return res\n\n    def preprocess(\n        self, model_id: str, request: InferenceRequest\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        \"\"\"Preprocesses the request before inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n        \"\"\"\n        model = self._get_model_reference(model_id=model_id)\n        return model.preprocess(**request.dict())\n\n    def get_class_names(self, model_id):\n        \"\"\"Retrieves the class names for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            List[str]: The class names of the model.\n        \"\"\"\n        model = self._get_model_reference(model_id=model_id)\n        return model.class_names\n\n    def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n        \"\"\"Retrieves the task type for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type of the model.\n        \"\"\"\n        model = self._get_model_reference(model_id=model_id)\n        return model.task_type\n\n    def remove(self, model_id: str, delete_from_disk: bool = True) -&gt; None:\n        \"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n        \"\"\"\n        try:\n            logger.debug(f\"Removing model {model_id} from base model manager\")\n            model_lock = self._get_lock_for_a_model(model_id=model_id)\n            with acquire_with_timeout(lock=model_lock) as acquired:\n                if not acquired:\n                    raise ModelManagerLockAcquisitionError(\n                        f\"Could not acquire lock for model with id={model_id}.\"\n                    )\n                if model_id not in self._models:\n                    return None\n                self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)\n                del self._models[model_id]\n                self._dispose_model_lock(model_id=model_id)\n        except InferenceModelNotFound:\n            logger.warning(\n                f\"Attempted to remove model with id {model_id}, but it is not loaded. Skipping...\"\n            )\n\n    def clear(self) -&gt; None:\n        \"\"\"Removes all models from the manager.\"\"\"\n        model_ids = list(self.keys())\n        for model_id in model_ids:\n            self.remove(model_id)\n\n    def _get_model_reference(self, model_id: str) -&gt; Model:\n        try:\n            return self._models[model_id]\n        except KeyError as error:\n            raise InferenceModelNotFound(\n                f\"Model with id {model_id} not loaded.\"\n            ) from error\n\n    def __contains__(self, model_id: str) -&gt; bool:\n        \"\"\"Checks if the model is contained in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: Whether the model is in the manager.\n        \"\"\"\n        return model_id in self._models\n\n    def __getitem__(self, key: str) -&gt; Model:\n        \"\"\"Retrieve a model from the manager by key.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model corresponding to the key.\n        \"\"\"\n        return self._get_model_reference(model_id=key)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Retrieve the number of models in the manager.\n\n        Returns:\n            int: The number of models in the manager.\n        \"\"\"\n        return len(self._models)\n\n    def keys(self):\n        \"\"\"Retrieve the keys (model identifiers) from the manager.\n\n        Returns:\n            List[str]: The keys of the models in the manager.\n        \"\"\"\n        return self._models.keys()\n\n    def models(self) -&gt; Dict[str, Model]:\n        \"\"\"Retrieve the models dictionary from the manager.\n\n        Returns:\n            Dict[str, Model]: The keys of the models in the manager.\n        \"\"\"\n        return self._models\n\n    def describe_models(self) -&gt; List[ModelDescription]:\n        return [\n            ModelDescription(\n                model_id=model_id,\n                task_type=model.task_type,\n                batch_size=getattr(model, \"batch_size\", None),\n                input_width=getattr(model, \"img_size_w\", None),\n                input_height=getattr(model, \"img_size_h\", None),\n            )\n            for model_id, model in self._models.items()\n        ]\n\n    def _get_lock_for_a_model(self, model_id: str) -&gt; Lock:\n        with acquire_with_timeout(lock=self._state_lock) as acquired:\n            if not acquired:\n                raise ModelManagerLockAcquisitionError(\n                    \"Could not acquire lock on Model Manager state to retrieve model lock.\"\n                )\n            if model_id not in self._models_state_locks:\n                self._models_state_locks[model_id] = Lock()\n            return self._models_state_locks[model_id]\n\n    def _dispose_model_lock(self, model_id: str) -&gt; None:\n        with acquire_with_timeout(lock=self._state_lock) as acquired:\n            if not acquired:\n                raise ModelManagerLockAcquisitionError(\n                    \"Could not acquire lock on Model Manager state to dispose model lock.\"\n                )\n            if model_id not in self._models_state_locks:\n                return None\n            del self._models_state_locks[model_id]\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if the model is contained in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the model is in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __contains__(self, model_id: str) -&gt; bool:\n    \"\"\"Checks if the model is contained in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: Whether the model is in the manager.\n    \"\"\"\n    return model_id in self._models\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve a model from the manager by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model corresponding to the key.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n    \"\"\"Retrieve a model from the manager by key.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model corresponding to the key.\n    \"\"\"\n    return self._get_model_reference(model_id=key)\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__len__","title":"<code>__len__()</code>","text":"<p>Retrieve the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Retrieve the number of models in the manager.\n\n    Returns:\n        int: The number of models in the manager.\n    \"\"\"\n    return len(self._models)\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None, endpoint_type=ModelEndpointType.ORT, countinference=None, service_secret=None)</code>","text":"<p>Adds a new model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required <code>endpoint_type</code> <code>ModelEndpointType</code> <p>The endpoint type to use for the model.</p> <code>ORT</code> Source code in <code>inference/core/managers/base.py</code> <pre><code>def add_model(\n    self,\n    model_id: str,\n    api_key: str,\n    model_id_alias: Optional[str] = None,\n    endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n    countinference: Optional[bool] = None,\n    service_secret: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Adds a new model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n        endpoint_type (ModelEndpointType, optional): The endpoint type to use for the model.\n    \"\"\"\n    if MODELS_CACHE_AUTH_ENABLED:\n        if not _check_if_api_key_has_access_to_model(\n            api_key=api_key,\n            model_id=model_id,\n            endpoint_type=endpoint_type,\n            countinference=countinference,\n            service_secret=service_secret,\n        ):\n            raise RoboflowAPINotAuthorizedError(\n                f\"API key {api_key} does not have access to model {model_id}\"\n            )\n\n    logger.debug(\n        f\"ModelManager - Adding model with model_id={model_id}, model_id_alias={model_id_alias}\"\n    )\n    resolved_identifier = model_id if model_id_alias is None else model_id_alias\n    model_lock = self._get_lock_for_a_model(model_id=resolved_identifier)\n    with acquire_with_timeout(lock=model_lock) as acquired:\n        if not acquired:\n            # if failed to acquire - then in use, no need to purge lock\n            raise ModelManagerLockAcquisitionError(\n                f\"Could not acquire lock for model with id={resolved_identifier}.\"\n            )\n        if resolved_identifier in self._models:\n            logger.debug(\n                f\"ModelManager - model with model_id={resolved_identifier} is already loaded.\"\n            )\n            return\n        try:\n            logger.debug(\"ModelManager - model initialisation...\")\n            model_class = self.model_registry.get_model(\n                resolved_identifier,\n                api_key,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n            model = model_class(\n                model_id=model_id,\n                api_key=api_key,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n\n            # Pass countinference and service_secret to download_model_artifacts_from_roboflow_api if available\n            if (\n                hasattr(model, \"download_model_artifacts_from_roboflow_api\")\n                and INTERNAL_WEIGHTS_URL_SUFFIX == \"serverless\"\n            ):\n                # Only pass these parameters if INTERNAL_WEIGHTS_URL_SUFFIX is \"serverless\"\n                if (\n                    hasattr(model, \"cache_model_artefacts\")\n                    and not model.has_model_metadata\n                ):\n                    # Override the download_model_artifacts_from_roboflow_api method with parameters\n                    original_method = (\n                        model.download_model_artifacts_from_roboflow_api\n                    )\n                    model.download_model_artifacts_from_roboflow_api = (\n                        lambda: original_method(\n                            countinference=countinference,\n                            service_secret=service_secret,\n                        )\n                    )\n\n            logger.debug(\"ModelManager - model successfully loaded.\")\n            self._models[resolved_identifier] = model\n        except Exception as error:\n            self._dispose_model_lock(model_id=resolved_identifier)\n            raise error\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.check_for_model","title":"<code>check_for_model(model_id)</code>","text":"<p>Checks whether the model with the given ID is in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Raises:</p> Type Description <code>InferenceModelNotFound</code> <p>If the model is not found in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def check_for_model(self, model_id: str) -&gt; None:\n    \"\"\"Checks whether the model with the given ID is in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Raises:\n        InferenceModelNotFound: If the model is not found in the manager.\n    \"\"\"\n    if model_id not in self:\n        raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.clear","title":"<code>clear()</code>","text":"<p>Removes all models from the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Removes all models from the manager.\"\"\"\n    model_ids = list(self.keys())\n    for model_id in model_ids:\n        self.remove(model_id)\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Retrieves the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List[str]: The class names of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_class_names(self, model_id):\n    \"\"\"Retrieves the class names for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        List[str]: The class names of the model.\n    \"\"\"\n    model = self._get_model_reference(model_id=model_id)\n    return model.class_names\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.get_task_type","title":"<code>get_task_type(model_id, api_key=None)</code>","text":"<p>Retrieves the task type for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n    \"\"\"Retrieves the task type for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type of the model.\n    \"\"\"\n    model = self._get_model_reference(model_id=model_id)\n    return model.task_type\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Runs inference on the specified model with the given request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Runs inference on the specified model with the given request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - inference from request started for model_id={model_id}.\"\n    )\n    enable_model_monitoring = not getattr(\n        request, \"disable_model_monitoring\", False\n    )\n    if METRICS_ENABLED and self.pingback and enable_model_monitoring:\n        logger.debug(\"ModelManager - setting pingback fallback api key...\")\n        self.pingback.fallback_api_key = request.api_key\n    try:\n        rtn_val = await self.model_infer(\n            model_id=model_id, request=request, **kwargs\n        )\n        logger.debug(\n            f\"ModelManager - inference from request finished for model_id={model_id}.\"\n        )\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n            try:\n                logger.debug(\n                    f\"ModelManager - caching inference request started for model_id={model_id}\"\n                )\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                if (\n                    hasattr(request, \"image\")\n                    and hasattr(request.image, \"type\")\n                    and request.image.type == \"numpy\"\n                ):\n                    request.image.value = str(request.image.value)\n                cache.zadd(\n                    f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value=to_cachable_inference_item(request, rtn_val),\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                logger.debug(\n                    f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                )\n            except Exception as cache_error:\n                logger.warning(\n                    f\"Failed to cache inference data for model {model_id}: {cache_error}\"\n                )\n        return rtn_val\n    except Exception as e:\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n            try:\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                cache.zadd(\n                    f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value={\n                        \"request\": jsonable_encoder(\n                            request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                        ),\n                        \"error\": str(e),\n                    },\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n            except Exception as cache_error:\n                logger.warning(\n                    f\"Failed to cache error data for model {model_id}: {cache_error}\"\n                )\n        raise\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Runs inference on the specified model with the given request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Runs inference on the specified model with the given request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - inference from request started for model_id={model_id}.\"\n    )\n    enable_model_monitoring = not getattr(\n        request, \"disable_model_monitoring\", False\n    )\n    if METRICS_ENABLED and self.pingback and enable_model_monitoring:\n        logger.debug(\"ModelManager - setting pingback fallback api key...\")\n        self.pingback.fallback_api_key = request.api_key\n    try:\n        rtn_val = self.model_infer_sync(\n            model_id=model_id, request=request, **kwargs\n        )\n        logger.debug(\n            f\"ModelManager - inference from request finished for model_id={model_id}.\"\n        )\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n            try:\n                logger.debug(\n                    f\"ModelManager - caching inference request started for model_id={model_id}\"\n                )\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                if (\n                    hasattr(request, \"image\")\n                    and hasattr(request.image, \"type\")\n                    and request.image.type == \"numpy\"\n                ):\n                    request.image.value = str(request.image.value)\n                cache.zadd(\n                    f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value=to_cachable_inference_item(request, rtn_val),\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                logger.debug(\n                    f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                )\n            except Exception as cache_error:\n                logger.warning(\n                    f\"Failed to cache inference data for model {model_id}: {cache_error}\"\n                )\n        return rtn_val\n    except Exception as e:\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE and enable_model_monitoring:\n            try:\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                cache.zadd(\n                    f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value={\n                        \"request\": jsonable_encoder(\n                            request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                        ),\n                        \"error\": str(e),\n                    },\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n            except Exception as cache_error:\n                logger.warning(\n                    f\"Failed to cache error data for model {model_id}: {cache_error}\"\n                )\n        raise\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.init_pingback","title":"<code>init_pingback()</code>","text":"<p>Initializes pingback mechanism.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def init_pingback(self):\n    \"\"\"Initializes pingback mechanism.\"\"\"\n    self.num_errors = 0  # in the device\n    self.uuid = ROBOFLOW_SERVER_UUID\n    if METRICS_ENABLED:\n        self.pingback = PingbackInfo(self)\n        self.pingback.start()\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.keys","title":"<code>keys()</code>","text":"<p>Retrieve the keys (model identifiers) from the manager.</p> <p>Returns:</p> Type Description <p>List[str]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def keys(self):\n    \"\"\"Retrieve the keys (model identifiers) from the manager.\n\n    Returns:\n        List[str]: The keys of the models in the manager.\n    \"\"\"\n    return self._models.keys()\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.make_response","title":"<code>make_response(model_id, predictions, *args, **kwargs)</code>","text":"<p>Creates a response object from the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>List[List[float]]</code> <p>The model's predictions.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The created response object.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def make_response(\n    self, model_id: str, predictions: List[List[float]], *args, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Creates a response object from the model's predictions.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (List[List[float]]): The model's predictions.\n\n    Returns:\n        InferenceResponse: The created response object.\n    \"\"\"\n    model = self._get_model_reference(model_id=model_id)\n    return model.make_response(predictions, *args, **kwargs)\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.models","title":"<code>models()</code>","text":"<p>Retrieve the models dictionary from the manager.</p> <p>Returns:</p> Type Description <code>Dict[str, Model]</code> <p>Dict[str, Model]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def models(self) -&gt; Dict[str, Model]:\n    \"\"\"Retrieve the models dictionary from the manager.\n\n    Returns:\n        Dict[str, Model]: The keys of the models in the manager.\n    \"\"\"\n    return self._models\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.postprocess","title":"<code>postprocess(model_id, predictions, preprocess_return_metadata, *args, **kwargs)</code>","text":"<p>Processes the model's predictions after inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>ndarray</code> <p>The model's predictions.</p> required <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: The post-processed predictions.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def postprocess(\n    self,\n    model_id: str,\n    predictions: Tuple[np.ndarray, ...],\n    preprocess_return_metadata: PreprocessReturnMetadata,\n    *args,\n    **kwargs,\n) -&gt; List[List[float]]:\n    \"\"\"Processes the model's predictions after inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (np.ndarray): The model's predictions.\n\n    Returns:\n        List[List[float]]: The post-processed predictions.\n    \"\"\"\n    model = self._get_model_reference(model_id=model_id)\n    return model.postprocess(\n        predictions, preprocess_return_metadata, *args, **kwargs\n    )\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.predict","title":"<code>predict(model_id, *args, **kwargs)</code>","text":"<p>Runs prediction on the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>np.ndarray: The predictions from the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"Runs prediction on the specified model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        np.ndarray: The predictions from the model.\n    \"\"\"\n    model = self._get_model_reference(model_id=model_id)\n    model.metrics[\"num_inferences\"] += 1\n    tic = time.perf_counter()\n    res = model.predict(*args, **kwargs)\n    toc = time.perf_counter()\n    model.metrics[\"avg_inference_time\"] += toc - tic\n    return res\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Preprocesses the request before inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, PreprocessReturnMetadata]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def preprocess(\n    self, model_id: str, request: InferenceRequest\n) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n    \"\"\"Preprocesses the request before inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n    \"\"\"\n    model = self._get_model_reference(model_id=model_id)\n    return model.preprocess(**request.dict())\n</code></pre>"},{"location":"reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.remove","title":"<code>remove(model_id, delete_from_disk=True)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def remove(self, model_id: str, delete_from_disk: bool = True) -&gt; None:\n    \"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n    \"\"\"\n    try:\n        logger.debug(f\"Removing model {model_id} from base model manager\")\n        model_lock = self._get_lock_for_a_model(model_id=model_id)\n        with acquire_with_timeout(lock=model_lock) as acquired:\n            if not acquired:\n                raise ModelManagerLockAcquisitionError(\n                    f\"Could not acquire lock for model with id={model_id}.\"\n                )\n            if model_id not in self._models:\n                return None\n            self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)\n            del self._models[model_id]\n            self._dispose_model_lock(model_id=model_id)\n    except InferenceModelNotFound:\n        logger.warning(\n            f\"Attempted to remove model with id {model_id}, but it is not loaded. Skipping...\"\n        )\n</code></pre>"},{"location":"reference/inference/core/managers/metrics/","title":"Metrics","text":""},{"location":"reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_container_stats","title":"<code>get_container_stats(docker_socket_path)</code>","text":"<p>Gets the container stats. Returns:     dict: A dictionary containing the container stats.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_container_stats(docker_socket_path: str) -&gt; dict:\n    \"\"\"\n    Gets the container stats.\n    Returns:\n        dict: A dictionary containing the container stats.\n    \"\"\"\n\n    try:\n        container_id = socket.gethostname()\n        connection = http.client.HTTPConnection(\"localhost\")\n        connection.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        connection.sock.connect(docker_socket_path)\n        connection.request(\n            \"GET\",\n            f\"/containers/{container_id}/stats?stream=false\",\n            headers={\"Host\": \"localhost\"},\n        )\n        response = connection.getresponse()\n        data = response.read()\n        connection.close()\n        if response.status != 200:\n            raise Exception(data.decode())\n        stats = json.loads(data.decode())\n        return {\"stats\": stats}\n    except Exception as e:\n        logger.exception(e)\n        raise Exception(\"An error occurred while fetching container stats.\")\n</code></pre>"},{"location":"reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_model_metrics","title":"<code>get_model_metrics(inference_server_id, model_id, min=-1, max=float('inf'))</code>","text":"<p>Gets the metrics for a given model between a specified time range.</p> <p>Parameters:</p> Name Type Description Default <code>device_id</code> <code>str</code> <p>The identifier of the device.</p> required <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>start</code> <code>float</code> <p>The starting timestamp of the time range. Defaults to -1.</p> required <code>stop</code> <code>float</code> <p>The ending timestamp of the time range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the metrics of the model:   - num_inferences (int): The number of inferences made.   - avg_inference_time (float): The average inference time.   - num_errors (int): The number of errors occurred.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_model_metrics(\n    inference_server_id: str, model_id: str, min: float = -1, max: float = float(\"inf\")\n) -&gt; dict:\n    \"\"\"\n    Gets the metrics for a given model between a specified time range.\n\n    Args:\n        device_id (str): The identifier of the device.\n        model_id (str): The identifier of the model.\n        start (float, optional): The starting timestamp of the time range. Defaults to -1.\n        stop (float, optional): The ending timestamp of the time range. Defaults to float(\"inf\").\n\n    Returns:\n        dict: A dictionary containing the metrics of the model:\n              - num_inferences (int): The number of inferences made.\n              - avg_inference_time (float): The average inference time.\n              - num_errors (int): The number of errors occurred.\n    \"\"\"\n    now = time.time()\n    inferences_with_times = cache.zrangebyscore(\n        f\"inference:{inference_server_id}:{model_id}\", min=min, max=max, withscores=True\n    )\n    num_inferences = len(inferences_with_times)\n    inference_times = []\n    for inference, t in inferences_with_times:\n        response = inference[\"response\"]\n        if isinstance(response, list):\n            times = [r[\"time\"] for r in response if \"time\" in r]\n            inference_times.extend(times)\n        else:\n            if \"time\" in response:\n                inference_times.append(response[\"time\"])\n    avg_inference_time = (\n        sum(inference_times) / len(inference_times) if len(inference_times) &gt; 0 else 0\n    )\n    errors_with_times = cache.zrangebyscore(\n        f\"error:{inference_server_id}:{model_id}\", min=min, max=max, withscores=True\n    )\n    num_errors = len(errors_with_times)\n    return {\n        \"num_inferences\": num_inferences,\n        \"avg_inference_time\": avg_inference_time,\n        \"num_errors\": num_errors,\n    }\n</code></pre>"},{"location":"reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Collects system information such as platform, architecture, hostname, IP address, MAC address, and processor details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing detailed system information.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_system_info() -&gt; dict:\n    \"\"\"Collects system information such as platform, architecture, hostname, IP address, MAC address, and processor details.\n\n    Returns:\n        dict: A dictionary containing detailed system information.\n    \"\"\"\n    info = {}\n    try:\n        info[\"platform\"] = platform.system()\n        info[\"platform_release\"] = platform.release()\n        info[\"platform_version\"] = platform.version()\n        info[\"architecture\"] = platform.machine()\n        info[\"hostname\"] = socket.gethostname()\n        info[\"ip_address\"] = socket.gethostbyname(socket.gethostname())\n        info[\"mac_address\"] = \":\".join(re.findall(\"..\", \"%012x\" % uuid.getnode()))\n        info[\"processor\"] = platform.processor()\n    except Exception as e:\n        logger.exception(e)\n    finally:\n        return info\n</code></pre>"},{"location":"reference/inference/core/managers/pingback/","title":"Pingback","text":""},{"location":"reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo","title":"<code>PingbackInfo</code>","text":"<p>Class responsible for managing pingback information for Roboflow.</p> <p>This class initializes a scheduler to periodically post data to Roboflow, containing information about the models, container, and device.</p> <p>Attributes:</p> Name Type Description <code>scheduler</code> <code>BackgroundScheduler</code> <p>A scheduler for running jobs in the background.</p> <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> <code>process_startup_time</code> <code>str</code> <p>Unix timestamp indicating when the process started.</p> <code>METRICS_URL</code> <code>str</code> <p>URL to send the pingback data to.</p> <code>system_info</code> <code>dict</code> <p>Information about the system.</p> <code>window_start_timestamp</code> <code>str</code> <p>Unix timestamp indicating the start of the current window.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>class PingbackInfo:\n    \"\"\"Class responsible for managing pingback information for Roboflow.\n\n    This class initializes a scheduler to periodically post data to Roboflow, containing information about the models,\n    container, and device.\n\n    Attributes:\n        scheduler (BackgroundScheduler): A scheduler for running jobs in the background.\n        model_manager (ModelManager): Reference to the model manager object.\n        process_startup_time (str): Unix timestamp indicating when the process started.\n        METRICS_URL (str): URL to send the pingback data to.\n        system_info (dict): Information about the system.\n        window_start_timestamp (str): Unix timestamp indicating the start of the current window.\n    \"\"\"\n\n    def __init__(self, manager):\n        \"\"\"Initializes PingbackInfo with the given manager.\n\n        Args:\n            manager (ModelManager): Reference to the model manager object.\n        \"\"\"\n        try:\n            self.scheduler = BackgroundScheduler(\n                job_defaults={\"coalesce\": True, \"max_instances\": 1}\n            )\n            self.model_manager = manager\n            self.process_startup_time = str(int(time.time()))\n            logger.debug(\n                \"UUID: \" + self.model_manager.uuid\n            )  # To correlate with UI container view\n            self.window_start_timestamp = str(int(time.time()))\n            context = {\n                \"api_key\": API_KEY,\n                \"timestamp\": str(int(time.time())),\n                \"device_id\": GLOBAL_DEVICE_ID,\n                \"inference_server_id\": GLOBAL_INFERENCE_SERVER_ID,\n                \"inference_server_version\": __version__,\n                \"tags\": TAGS,\n            }\n            self.environment_info = context | get_system_info()\n\n            # we will set this from model manager when a new api key is used\n            # to use in case there is no global ENV api key configured\n            self.fallback_api_key = None\n\n        except Exception as e:\n            logger.debug(\n                \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n                + str(e)\n            )\n\n    def start(self):\n        \"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n        If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n        \"\"\"\n        if METRICS_ENABLED == False:\n            logger.warning(\n                \"Metrics reporting to Roboflow is disabled; not sending back stats to Roboflow.\"\n            )\n            return\n        try:\n            self.scheduler.add_job(\n                self.post_data,\n                \"interval\",\n                seconds=METRICS_INTERVAL,\n                args=[self.model_manager],\n                replace_existing=True,\n            )\n            self.scheduler.start()\n        except Exception as e:\n            logger.debug(e)\n\n    def stop(self):\n        \"\"\"Stops the scheduler.\"\"\"\n        self.scheduler.shutdown()\n\n    def post_data(self, model_manager):\n        \"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n        Args:\n            model_manager (ModelManager): Reference to the model manager object.\n\n        The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n        \"\"\"\n        all_data = self.environment_info.copy()\n        all_data[\"inference_results\"] = []\n\n        # use fallback api key if env didn't have one\n        if self.fallback_api_key and not all_data.get(\"api_key\"):\n            all_data[\"api_key\"] = self.fallback_api_key\n\n        try:\n            now = time.time()\n            start = now - METRICS_INTERVAL\n            for model_id in model_manager.models():\n                results = get_inference_results_for_model(\n                    GLOBAL_INFERENCE_SERVER_ID, model_id, min=start, max=now\n                )\n                all_data[\"inference_results\"] = all_data[\"inference_results\"] + results\n            res = requests.post(wrap_url(METRICS_URL), json=all_data, timeout=10)\n            try:\n                api_key_safe_raise_for_status(response=res)\n                logger.debug(\n                    \"Sent metrics to Roboflow {} at {}.\".format(\n                        METRICS_URL, str(all_data)\n                    )\n                )\n            except Exception as e:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable.\"\n                )\n\n        except Exception as e:\n            try:\n                logger.exception(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}. Data was: {all_data}\"\n                )\n\n            except Exception as e2:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}.\"\n                )\n</code></pre>"},{"location":"reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.__init__","title":"<code>__init__(manager)</code>","text":"<p>Initializes PingbackInfo with the given manager.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def __init__(self, manager):\n    \"\"\"Initializes PingbackInfo with the given manager.\n\n    Args:\n        manager (ModelManager): Reference to the model manager object.\n    \"\"\"\n    try:\n        self.scheduler = BackgroundScheduler(\n            job_defaults={\"coalesce\": True, \"max_instances\": 1}\n        )\n        self.model_manager = manager\n        self.process_startup_time = str(int(time.time()))\n        logger.debug(\n            \"UUID: \" + self.model_manager.uuid\n        )  # To correlate with UI container view\n        self.window_start_timestamp = str(int(time.time()))\n        context = {\n            \"api_key\": API_KEY,\n            \"timestamp\": str(int(time.time())),\n            \"device_id\": GLOBAL_DEVICE_ID,\n            \"inference_server_id\": GLOBAL_INFERENCE_SERVER_ID,\n            \"inference_server_version\": __version__,\n            \"tags\": TAGS,\n        }\n        self.environment_info = context | get_system_info()\n\n        # we will set this from model manager when a new api key is used\n        # to use in case there is no global ENV api key configured\n        self.fallback_api_key = None\n\n    except Exception as e:\n        logger.debug(\n            \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n            + str(e)\n        )\n</code></pre>"},{"location":"reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.post_data","title":"<code>post_data(model_manager)</code>","text":"<p>Posts data to Roboflow about the models, container, device, and other relevant metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required <p>The data is collected and reset for the next window, and a POST request is made to the pingback URL.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def post_data(self, model_manager):\n    \"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n    Args:\n        model_manager (ModelManager): Reference to the model manager object.\n\n    The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n    \"\"\"\n    all_data = self.environment_info.copy()\n    all_data[\"inference_results\"] = []\n\n    # use fallback api key if env didn't have one\n    if self.fallback_api_key and not all_data.get(\"api_key\"):\n        all_data[\"api_key\"] = self.fallback_api_key\n\n    try:\n        now = time.time()\n        start = now - METRICS_INTERVAL\n        for model_id in model_manager.models():\n            results = get_inference_results_for_model(\n                GLOBAL_INFERENCE_SERVER_ID, model_id, min=start, max=now\n            )\n            all_data[\"inference_results\"] = all_data[\"inference_results\"] + results\n        res = requests.post(wrap_url(METRICS_URL), json=all_data, timeout=10)\n        try:\n            api_key_safe_raise_for_status(response=res)\n            logger.debug(\n                \"Sent metrics to Roboflow {} at {}.\".format(\n                    METRICS_URL, str(all_data)\n                )\n            )\n        except Exception as e:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable.\"\n            )\n\n    except Exception as e:\n        try:\n            logger.exception(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}. Data was: {all_data}\"\n            )\n\n        except Exception as e2:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}.\"\n            )\n</code></pre>"},{"location":"reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.start","title":"<code>start()</code>","text":"<p>Starts the scheduler to periodically post data to Roboflow.</p> <p>If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def start(self):\n    \"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n    If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n    \"\"\"\n    if METRICS_ENABLED == False:\n        logger.warning(\n            \"Metrics reporting to Roboflow is disabled; not sending back stats to Roboflow.\"\n        )\n        return\n    try:\n        self.scheduler.add_job(\n            self.post_data,\n            \"interval\",\n            seconds=METRICS_INTERVAL,\n            args=[self.model_manager],\n            replace_existing=True,\n        )\n        self.scheduler.start()\n    except Exception as e:\n        logger.debug(e)\n</code></pre>"},{"location":"reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.stop","title":"<code>stop()</code>","text":"<p>Stops the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the scheduler.\"\"\"\n    self.scheduler.shutdown()\n</code></pre>"},{"location":"reference/inference/core/managers/prometheus/","title":"Prometheus","text":""},{"location":"reference/inference/core/managers/prometheus/#inference.core.managers.prometheus.InferenceInstrumentator","title":"<code>InferenceInstrumentator</code>","text":"<p>Class responsible for managing the Prometheus metrics for the inference server.</p> <p>This class inititalizes the Prometheus Instrumentator and exposes the metrics endpoint.</p> Source code in <code>inference/core/managers/prometheus.py</code> <pre><code>class InferenceInstrumentator:\n    \"\"\"\n    Class responsible for managing the Prometheus metrics for the inference server.\n\n    This class inititalizes the Prometheus Instrumentator and exposes the metrics endpoint.\n\n    \"\"\"\n\n    def __init__(self, app, model_manager, endpoint: str = \"/metrics\"):\n        self.instrumentator = Instrumentator()\n        self.instrumentator.instrument(app).expose(app, endpoint)\n        self.collector = CustomCollector(model_manager)\n        REGISTRY.register(self.collector)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/","title":"Base","text":""},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator","title":"<code>ModelManagerDecorator</code>","text":"<p>               Bases: <code>ModelManager</code></p> <p>Basic decorator, it acts like a <code>ModelManager</code> and contains a <code>ModelManager</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Instance of a ModelManager.</p> required <p>Methods:</p> Name Description <code>add_model</code> <p>Adds a model to the manager.</p> <code>infer</code> <p>Processes a complete inference request.</p> <code>infer_only</code> <p>Performs only the inference part of a request.</p> <code>preprocess</code> <p>Processes the preprocessing part of a request.</p> <code>get_task_type</code> <p>Gets the task type associated with a model.</p> <code>get_class_names</code> <p>Gets the class names for a given model.</p> <code>remove</code> <p>Removes a model from the manager.</p> <code>__len__</code> <p>Returns the number of models in the manager.</p> <code>__getitem__</code> <p>Retrieves a model by its ID.</p> <code>__contains__</code> <p>Checks if a model exists in the manager.</p> <code>keys</code> <p>Returns the keys (model IDs) from the manager.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>class ModelManagerDecorator(ModelManager):\n    \"\"\"Basic decorator, it acts like a `ModelManager` and contains a `ModelManager`.\n\n    Args:\n        model_manager (ModelManager): Instance of a ModelManager.\n\n    Methods:\n        add_model: Adds a model to the manager.\n        infer: Processes a complete inference request.\n        infer_only: Performs only the inference part of a request.\n        preprocess: Processes the preprocessing part of a request.\n        get_task_type: Gets the task type associated with a model.\n        get_class_names: Gets the class names for a given model.\n        remove: Removes a model from the manager.\n        __len__: Returns the number of models in the manager.\n        __getitem__: Retrieves a model by its ID.\n        __contains__: Checks if a model exists in the manager.\n        keys: Returns the keys (model IDs) from the manager.\n    \"\"\"\n\n    @property\n    def _models(self):\n        raise ValueError(\"Should only be accessing self.model_manager._models\")\n\n    @property\n    def model_registry(self):\n        raise ValueError(\"Should only be accessing self.model_manager.model_registry\")\n\n    def __init__(self, model_manager: ModelManager):\n        \"\"\"Initializes the decorator with an instance of a ModelManager.\"\"\"\n        self.model_manager = model_manager\n\n    def init_pingback(self):\n        self.model_manager.init_pingback()\n\n    @property\n    def pingback(self):\n        return self.model_manager.pingback\n\n    def add_model(\n        self,\n        model_id: str,\n        api_key: str,\n        model_id_alias: Optional[str] = None,\n        endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n    ):\n        \"\"\"Adds a model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n            endpoint_type (ModelEndpointType, optional): The endpoint type to use for the model.\n        \"\"\"\n        if model_id in self:\n            return\n        self.model_manager.add_model(\n            model_id,\n            api_key,\n            model_id_alias=model_id_alias,\n            endpoint_type=endpoint_type,\n            countinference=countinference,\n            service_secret=service_secret,\n        )\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        return await self.model_manager.infer_from_request(model_id, request, **kwargs)\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        return self.model_manager.infer_from_request_sync(model_id, request, **kwargs)\n\n    def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n        \"\"\"Performs only the inference part of a request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request: The request to process.\n            img_in: Input image.\n            img_dims: Image dimensions.\n            batch_size (int, optional): Batch size.\n\n        Returns:\n            Response from the inference-only operation.\n        \"\"\"\n        return self.model_manager.infer_only(\n            model_id, request, img_in, img_dims, batch_size\n        )\n\n    def preprocess(self, model_id: str, request: InferenceRequest):\n        \"\"\"Processes the preprocessing part of a request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n        \"\"\"\n        return self.model_manager.preprocess(model_id, request)\n\n    def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n        \"\"\"Gets the task type associated with a model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type.\n        \"\"\"\n        if api_key is None:\n            api_key = API_KEY\n        return self.model_manager.get_task_type(model_id, api_key=api_key)\n\n    def get_class_names(self, model_id):\n        \"\"\"Gets the class names for a given model.\n\n        Args:\n            model_id: The identifier of the model.\n\n        Returns:\n            List of class names.\n        \"\"\"\n        return self.model_manager.get_class_names(model_id)\n\n    def remove(self, model_id: str, delete_from_disk: bool = True) -&gt; Model:\n        \"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            Model: The removed model.\n        \"\"\"\n        return self.model_manager.remove(model_id, delete_from_disk=delete_from_disk)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of models in the manager.\n\n        Returns:\n            int: Number of models.\n        \"\"\"\n        return len(self.model_manager)\n\n    def __getitem__(self, key: str) -&gt; Model:\n        \"\"\"Retrieves a model by its ID.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model instance.\n        \"\"\"\n        return self.model_manager[key]\n\n    def __contains__(self, model_id: str):\n        \"\"\"Checks if a model exists in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: True if the model exists, False otherwise.\n        \"\"\"\n        return model_id in self.model_manager\n\n    def keys(self):\n        \"\"\"Returns the keys (model IDs) from the manager.\n\n        Returns:\n            List of keys (model IDs).\n        \"\"\"\n        return self.model_manager.keys()\n\n    def models(self):\n        return self.model_manager.models()\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        return self.model_manager.predict(model_id, *args, **kwargs)\n\n    def postprocess(\n        self,\n        model_id: str,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        *args,\n        **kwargs\n    ) -&gt; List[List[float]]:\n        return self.model_manager.postprocess(\n            model_id, predictions, preprocess_return_metadata, *args, **kwargs\n        )\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n        return self.model_manager.make_response(model_id, predictions, *args, **kwargs)\n\n    @property\n    def num_errors(self):\n        return self.model_manager.num_errors\n\n    @num_errors.setter\n    def num_errors(self, value):\n        self.model_manager.num_errors = value\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if a model exists in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model exists, False otherwise.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __contains__(self, model_id: str):\n    \"\"\"Checks if a model exists in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: True if the model exists, False otherwise.\n    \"\"\"\n    return model_id in self.model_manager\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieves a model by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model instance.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n    \"\"\"Retrieves a model by its ID.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model instance.\n    \"\"\"\n    return self.model_manager[key]\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__init__","title":"<code>__init__(model_manager)</code>","text":"<p>Initializes the decorator with an instance of a ModelManager.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __init__(self, model_manager: ModelManager):\n    \"\"\"Initializes the decorator with an instance of a ModelManager.\"\"\"\n    self.model_manager = model_manager\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of models.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of models in the manager.\n\n    Returns:\n        int: Number of models.\n    \"\"\"\n    return len(self.model_manager)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None, endpoint_type=ModelEndpointType.ORT, countinference=None, service_secret=None)</code>","text":"<p>Adds a model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required <code>endpoint_type</code> <code>ModelEndpointType</code> <p>The endpoint type to use for the model.</p> <code>ORT</code> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def add_model(\n    self,\n    model_id: str,\n    api_key: str,\n    model_id_alias: Optional[str] = None,\n    endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n    countinference: Optional[bool] = None,\n    service_secret: Optional[str] = None,\n):\n    \"\"\"Adds a model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n        endpoint_type (ModelEndpointType, optional): The endpoint type to use for the model.\n    \"\"\"\n    if model_id in self:\n        return\n    self.model_manager.add_model(\n        model_id,\n        api_key,\n        model_id_alias=model_id_alias,\n        endpoint_type=endpoint_type,\n        countinference=countinference,\n        service_secret=service_secret,\n    )\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Gets the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List of class names.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def get_class_names(self, model_id):\n    \"\"\"Gets the class names for a given model.\n\n    Args:\n        model_id: The identifier of the model.\n\n    Returns:\n        List of class names.\n    \"\"\"\n    return self.model_manager.get_class_names(model_id)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.get_task_type","title":"<code>get_task_type(model_id, api_key=None)</code>","text":"<p>Gets the task type associated with a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n    \"\"\"Gets the task type associated with a model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type.\n    \"\"\"\n    if api_key is None:\n        api_key = API_KEY\n    return self.model_manager.get_task_type(model_id, api_key=api_key)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    return await self.model_manager.infer_from_request(model_id, request, **kwargs)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Processes a complete inference request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    return self.model_manager.infer_from_request_sync(model_id, request, **kwargs)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_only","title":"<code>infer_only(model_id, request, img_in, img_dims, batch_size=None)</code>","text":"<p>Performs only the inference part of a request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <p>The request to process.</p> required <code>img_in</code> <p>Input image.</p> required <code>img_dims</code> <p>Image dimensions.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the inference-only operation.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n    \"\"\"Performs only the inference part of a request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request: The request to process.\n        img_in: Input image.\n        img_dims: Image dimensions.\n        batch_size (int, optional): Batch size.\n\n    Returns:\n        Response from the inference-only operation.\n    \"\"\"\n    return self.model_manager.infer_only(\n        model_id, request, img_in, img_dims, batch_size\n    )\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.keys","title":"<code>keys()</code>","text":"<p>Returns the keys (model IDs) from the manager.</p> <p>Returns:</p> Type Description <p>List of keys (model IDs).</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def keys(self):\n    \"\"\"Returns the keys (model IDs) from the manager.\n\n    Returns:\n        List of keys (model IDs).\n    \"\"\"\n    return self.model_manager.keys()\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Processes the preprocessing part of a request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def preprocess(self, model_id: str, request: InferenceRequest):\n    \"\"\"Processes the preprocessing part of a request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n    \"\"\"\n    return self.model_manager.preprocess(model_id, request)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.remove","title":"<code>remove(model_id, delete_from_disk=True)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The removed model.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def remove(self, model_id: str, delete_from_disk: bool = True) -&gt; Model:\n    \"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        Model: The removed model.\n    \"\"\"\n    return self.model_manager.remove(model_id, delete_from_disk=delete_from_disk)\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/locked_load/","title":"Locked load","text":""},{"location":"reference/inference/core/managers/decorators/locked_load/#inference.core.managers.decorators.locked_load.LockedLoadModelManagerDecorator","title":"<code>LockedLoadModelManagerDecorator</code>","text":"<p>               Bases: <code>ModelManagerDecorator</code></p> <p>Must acquire lock to load model</p> Source code in <code>inference/core/managers/decorators/locked_load.py</code> <pre><code>class LockedLoadModelManagerDecorator(ModelManagerDecorator):\n    \"\"\"Must acquire lock to load model\"\"\"\n\n    def add_model(\n        self,\n        model_id: str,\n        api_key: str,\n        model_id_alias=None,\n        endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n    ):\n        with cache.lock(lock_str(model_id), expire=180.0):\n            return super().add_model(\n                model_id,\n                api_key,\n                model_id_alias=model_id_alias,\n                endpoint_type=endpoint_type,\n                countinference=countinference,\n                service_secret=service_secret,\n            )\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/logger/","title":"Logger","text":""},{"location":"reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger","title":"<code>WithLogger</code>","text":"<p>               Bases: <code>ModelManagerDecorator</code></p> <p>Logger Decorator, it logs what's going on inside the manager.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>class WithLogger(ModelManagerDecorator):\n    \"\"\"Logger Decorator, it logs what's going on inside the manager.\"\"\"\n\n    def add_model(\n        self,\n        model_id: str,\n        api_key: str,\n        model_id_alias: Optional[str] = None,\n        endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n    ):\n        \"\"\"Adds a model to the manager and logs the action.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n\n        Returns:\n            The result of the add_model method from the superclass.\n        \"\"\"\n        logger.info(f\"\ud83e\udd16 {model_id} added.\")\n        return super().add_model(\n            model_id,\n            api_key,\n            model_id_alias=model_id_alias,\n            endpoint_type=endpoint_type,\n            countinference=countinference,\n            service_secret=service_secret,\n        )\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request and logs both the request and response.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n        res = await super().infer_from_request(model_id, request, **kwargs)\n        logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n        return res\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request and logs both the request and response.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n        res = super().infer_from_request_sync(model_id, request, **kwargs)\n        logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n        return res\n\n    def remove(self, model_id: str, delete_from_disk: bool = True) -&gt; Model:\n        \"\"\"Removes a model from the manager and logs the action.\n\n        Args:\n            model_id (str): The identifier of the model to remove.\n\n        Returns:\n            Model: The removed model.\n        \"\"\"\n        res = super().remove(model_id)\n        logger.info(f\"\u274c removed {model_id}, delete_from_disk={delete_from_disk}\")\n        return res\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None, endpoint_type=ModelEndpointType.ORT, countinference=None, service_secret=None)</code>","text":"<p>Adds a model to the manager and logs the action.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required <p>Returns:</p> Type Description <p>The result of the add_model method from the superclass.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def add_model(\n    self,\n    model_id: str,\n    api_key: str,\n    model_id_alias: Optional[str] = None,\n    endpoint_type: ModelEndpointType = ModelEndpointType.ORT,\n    countinference: Optional[bool] = None,\n    service_secret: Optional[str] = None,\n):\n    \"\"\"Adds a model to the manager and logs the action.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n\n    Returns:\n        The result of the add_model method from the superclass.\n    \"\"\"\n    logger.info(f\"\ud83e\udd16 {model_id} added.\")\n    return super().add_model(\n        model_id,\n        api_key,\n        model_id_alias=model_id_alias,\n        endpoint_type=endpoint_type,\n        countinference=countinference,\n        service_secret=service_secret,\n    )\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request and logs both the request and response.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request and logs both the request and response.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n    res = await super().infer_from_request(model_id, request, **kwargs)\n    logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n    return res\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Processes a complete inference request and logs both the request and response.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request and logs both the request and response.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n    res = super().infer_from_request_sync(model_id, request, **kwargs)\n    logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n    return res\n</code></pre>"},{"location":"reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.remove","title":"<code>remove(model_id, delete_from_disk=True)</code>","text":"<p>Removes a model from the manager and logs the action.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to remove.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The removed model.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def remove(self, model_id: str, delete_from_disk: bool = True) -&gt; Model:\n    \"\"\"Removes a model from the manager and logs the action.\n\n    Args:\n        model_id (str): The identifier of the model to remove.\n\n    Returns:\n        Model: The removed model.\n    \"\"\"\n    res = super().remove(model_id)\n    logger.info(f\"\u274c removed {model_id}, delete_from_disk={delete_from_disk}\")\n    return res\n</code></pre>"},{"location":"reference/inference/core/models/base/","title":"Base","text":""},{"location":"reference/inference/core/models/base/#inference.core.models.base.BaseInference","title":"<code>BaseInference</code>","text":"<p>General inference class.</p> <p>This class provides a basic interface for inference tasks.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>class BaseInference:\n    \"\"\"General inference class.\n\n    This class provides a basic interface for inference tasks.\n    \"\"\"\n\n    @usage_collector(\"model\")\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Runs inference on given data.\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        preproc_image, returned_metadata = self.preprocess(image, **kwargs)\n        logger.debug(\n            f\"Preprocessed input shape: {getattr(preproc_image, 'shape', None)}\"\n        )\n        predicted_arrays = self.predict(preproc_image, **kwargs)\n        postprocessed = self.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\n        return postprocessed\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        raise NotImplementedError\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        raise NotImplementedError\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        raise NotImplementedError\n\n    def infer_from_request(\n        self, request: InferenceRequest\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n        \"\"\"Runs inference on a request\n\n        Args:\n            request (InferenceRequest): The request object.\n\n        Returns:\n            Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def make_response(\n        self, *args, **kwargs\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n        \"\"\"Constructs an object detection response.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.BaseInference.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Runs inference on given data. - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>@usage_collector(\"model\")\ndef infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Runs inference on given data.\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    preproc_image, returned_metadata = self.preprocess(image, **kwargs)\n    logger.debug(\n        f\"Preprocessed input shape: {getattr(preproc_image, 'shape', None)}\"\n    )\n    predicted_arrays = self.predict(preproc_image, **kwargs)\n    postprocessed = self.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\n    return postprocessed\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.BaseInference.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Runs inference on a request</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InferenceRequest</code> <p>The request object.</p> required <p>Returns:</p> Type Description <code>Union[InferenceResponse, List[InferenceResponse]]</code> <p>Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer_from_request(\n    self, request: InferenceRequest\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n    \"\"\"Runs inference on a request\n\n    Args:\n        request (InferenceRequest): The request object.\n\n    Returns:\n        Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.BaseInference.make_response","title":"<code>make_response(*args, **kwargs)</code>","text":"<p>Constructs an object detection response.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def make_response(\n    self, *args, **kwargs\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n    \"\"\"Constructs an object detection response.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>BaseInference</code></p> <p>Base Inference Model (Inherits from BaseInference to define the needed methods)</p> <p>This class provides the foundational methods for inference and logging, and can be extended by specific models.</p> <p>Methods:</p> Name Description <code>log</code> <p>Print the given message.</p> <code>clear_cache</code> <p>Clears any cache if necessary.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>class Model(BaseInference):\n    \"\"\"Base Inference Model (Inherits from BaseInference to define the needed methods)\n\n    This class provides the foundational methods for inference and logging, and can be extended by specific models.\n\n    Methods:\n        log(m): Print the given message.\n        clear_cache(): Clears any cache if necessary.\n    \"\"\"\n\n    def log(self, m):\n        \"\"\"Prints the given message.\n\n        Args:\n            m (str): The message to print.\n        \"\"\"\n        print(m)\n\n    def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n        \"\"\"Clears any cache if necessary. This method should be implemented in derived classes as needed.\n\n        Args:\n            delete_from_disk (bool, optional): Whether to delete cached files from disk. Defaults to True.\n        \"\"\"\n        pass\n\n    def infer_from_request(\n        self,\n        request: InferenceRequest,\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n        \"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        The function can handle both single and multiple image inference requests. Optionally, it also provides\n        a visualization of the predictions if requested.\n\n        Args:\n            request (InferenceRequest): The request object containing details for inference, such as the image or\n                images to process, any classes to filter by, and whether or not to visualize the predictions.\n\n        Returns:\n            Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains\n            multiple images, or a single response object if the request contains one image. Each response object\n            contains details about the segmented instances, the time taken for inference, and optionally, a visualization.\n\n        Examples:\n            &gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n            &gt;&gt;&gt; response = infer_from_request(request)\n            &gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n            0.125\n            &gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n\n        Notes:\n            - The processing time for each response is included within the response itself.\n            - If `visualize_predictions` is set to True in the request, a visualization of the prediction\n              is also included in the response.\n        \"\"\"\n        t1 = perf_counter()\n        responses = self.infer(**request.dict(), return_image_dims=False)\n        for response in responses:\n            response.time = perf_counter() - t1\n            if request.id:\n                response.inference_id = request.id\n\n        if hasattr(request, \"visualize_predictions\") and request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if not isinstance(request.image, list) and len(responses) &gt; 0:\n            responses = responses[0]\n\n        return responses\n\n    def make_response(\n        self, *args, **kwargs\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n        \"\"\"Makes an inference response from the given arguments.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            InferenceResponse: The inference response.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".make_response\")\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.Model.clear_cache","title":"<code>clear_cache(delete_from_disk=True)</code>","text":"<p>Clears any cache if necessary. This method should be implemented in derived classes as needed.</p> <p>Parameters:</p> Name Type Description Default <code>delete_from_disk</code> <code>bool</code> <p>Whether to delete cached files from disk. Defaults to True.</p> <code>True</code> Source code in <code>inference/core/models/base.py</code> <pre><code>def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n    \"\"\"Clears any cache if necessary. This method should be implemented in derived classes as needed.\n\n    Args:\n        delete_from_disk (bool, optional): Whether to delete cached files from disk. Defaults to True.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.Model.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses. The function can handle both single and multiple image inference requests. Optionally, it also provides a visualization of the predictions if requested.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InferenceRequest</code> <p>The request object containing details for inference, such as the image or images to process, any classes to filter by, and whether or not to visualize the predictions.</p> required <p>Returns:</p> Type Description <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains</p> <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>multiple images, or a single response object if the request contains one image. Each response object</p> <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>contains details about the segmented instances, the time taken for inference, and optionally, a visualization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n&gt;&gt;&gt; response = infer_from_request(request)\n&gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n0.125\n&gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n</code></pre> Notes <ul> <li>The processing time for each response is included within the response itself.</li> <li>If <code>visualize_predictions</code> is set to True in the request, a visualization of the prediction   is also included in the response.</li> </ul> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer_from_request(\n    self,\n    request: InferenceRequest,\n) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n    \"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    The function can handle both single and multiple image inference requests. Optionally, it also provides\n    a visualization of the predictions if requested.\n\n    Args:\n        request (InferenceRequest): The request object containing details for inference, such as the image or\n            images to process, any classes to filter by, and whether or not to visualize the predictions.\n\n    Returns:\n        Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains\n        multiple images, or a single response object if the request contains one image. Each response object\n        contains details about the segmented instances, the time taken for inference, and optionally, a visualization.\n\n    Examples:\n        &gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n        &gt;&gt;&gt; response = infer_from_request(request)\n        &gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n        0.125\n        &gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n\n    Notes:\n        - The processing time for each response is included within the response itself.\n        - If `visualize_predictions` is set to True in the request, a visualization of the prediction\n          is also included in the response.\n    \"\"\"\n    t1 = perf_counter()\n    responses = self.infer(**request.dict(), return_image_dims=False)\n    for response in responses:\n        response.time = perf_counter() - t1\n        if request.id:\n            response.inference_id = request.id\n\n    if hasattr(request, \"visualize_predictions\") and request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if not isinstance(request.image, list) and len(responses) &gt; 0:\n        responses = responses[0]\n\n    return responses\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.Model.log","title":"<code>log(m)</code>","text":"<p>Prints the given message.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>str</code> <p>The message to print.</p> required Source code in <code>inference/core/models/base.py</code> <pre><code>def log(self, m):\n    \"\"\"Prints the given message.\n\n    Args:\n        m (str): The message to print.\n    \"\"\"\n    print(m)\n</code></pre>"},{"location":"reference/inference/core/models/base/#inference.core.models.base.Model.make_response","title":"<code>make_response(*args, **kwargs)</code>","text":"<p>Makes an inference response from the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>Union[InferenceResponse, List[InferenceResponse]]</code> <p>The inference response.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def make_response(\n    self, *args, **kwargs\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n    \"\"\"Makes an inference response from the given arguments.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        InferenceResponse: The inference response.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".make_response\")\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/","title":"Classification base","text":""},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel","title":"<code>ClassificationBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Base class for ONNX models for Roboflow classification inference.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>Whether the classification is multi-class or not.</p> <p>Methods:</p> Name Description <code>get_infer_bucket_file_list</code> <p>Get the list of required files for inference.</p> <code>softmax</code> <p>Compute softmax values for a given set of scores.</p> <code>infer</code> <p>ClassificationInferenceRequest) -&gt; Union[List[Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]], Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]]: Perform inference on a given request and return the response.</p> <code>draw_predictions</code> <p>Draw prediction visuals on an image.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>class ClassificationBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n    \"\"\"Base class for ONNX models for Roboflow classification inference.\n\n    Attributes:\n        multiclass (bool): Whether the classification is multi-class or not.\n\n    Methods:\n        get_infer_bucket_file_list() -&gt; list: Get the list of required files for inference.\n        softmax(x): Compute softmax values for a given set of scores.\n        infer(request: ClassificationInferenceRequest) -&gt; Union[List[Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]], Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]]: Perform inference on a given request and return the response.\n        draw_predictions(inference_request, inference_response): Draw prediction visuals on an image.\n    \"\"\"\n\n    task_type = \"classification\"\n\n    preprocess_means = [0.5, 0.5, 0.5]\n    preprocess_stds = [0.5, 0.5, 0.5]\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the model, setting whether it is multiclass or not.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    def draw_predictions(self, inference_request, inference_response):\n        \"\"\"Draw prediction visuals on an image.\n\n        This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.\n\n        Args:\n            inference_request: The request object containing the image and parameters.\n            inference_response: The response object containing the predictions and other details.\n\n        Returns:\n            bytes: The bytes of the visualized image in JPEG format.\n        \"\"\"\n        image = load_image_rgb(inference_request.image)\n        image = Image.fromarray(image)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        if isinstance(inference_response.predictions, list):\n            prediction = inference_response.predictions[0]\n            color = self.colors.get(prediction.class_name, \"#4892EA\")\n            draw.rectangle(\n                [0, 0, image.size[1], image.size[0]],\n                outline=color,\n                width=inference_request.visualization_stroke_width,\n            )\n            text = f\"{prediction.class_id} - {prediction.class_name} {prediction.confidence:.2f}\"\n            text_size = font.getbbox(text)\n\n            # set button size + 10px margins\n            button_size = (text_size[2] + 20, text_size[3] + 20)\n            button_img = Image.new(\"RGBA\", button_size, color)\n            # put text on button with 10px margins\n            button_draw = ImageDraw.Draw(button_img)\n            button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n            # put button on source image in position (0, 0)\n            image.paste(button_img, (0, 0))\n        else:\n            if len(inference_response.predictions) &gt; 0:\n                box_color = \"#4892EA\"\n                draw.rectangle(\n                    [0, 0, image.size[1], image.size[0]],\n                    outline=box_color,\n                    width=inference_request.visualization_stroke_width,\n                )\n            row = 0\n            predictions = [\n                (cls_name, pred)\n                for cls_name, pred in inference_response.predictions.items()\n            ]\n            predictions = sorted(\n                predictions, key=lambda x: x[1].confidence, reverse=True\n            )\n            for i, (cls_name, pred) in enumerate(predictions):\n                color = self.colors.get(cls_name, \"#4892EA\")\n                text = f\"{cls_name} {pred.confidence:.2f}\"\n                text_size = font.getbbox(text)\n\n                # set button size + 10px margins\n                button_size = (text_size[2] + 20, text_size[3] + 20)\n                button_img = Image.new(\"RGBA\", button_size, color)\n                # put text on button with 10px margins\n                button_draw = ImageDraw.Draw(button_img)\n                button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n                # put button on source image in position (0, 0)\n                image.paste(button_img, (0, row))\n                row += button_size[1]\n\n        buffered = BytesIO()\n        image = image.convert(\"RGB\")\n        image.save(buffered, format=\"JPEG\")\n        return buffered.getvalue()\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"environment.json\"].\n        \"\"\"\n        return [\"environment.json\"]\n\n    def infer(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        return_image_dims: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Perform inference on the provided image(s) and return the predictions.\n\n        Args:\n            image (Any): The image or list of images to be processed.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            return_image_dims (bool, optional): If set to True, the function will also return the dimensions of the image. Defaults to False.\n            **kwargs: Additional parameters to customize the inference process.\n\n        Returns:\n            Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:\n            If `return_image_dims` is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.\n            If `return_image_dims` is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.\n            If `return_image_dims` is False and a list of images is provided, only the list of prediction arrays is returned.\n            If `return_image_dims` is False and a single image is provided, only the prediction array is returned.\n\n        Notes:\n            - The input image(s) will be preprocessed (normalized and reshaped) before inference.\n            - This function uses an ONNX session to perform inference on the input image(s).\n        \"\"\"\n        return super().infer(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        return_image_dims=False,\n        **kwargs,\n    ) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n        predictions = predictions[0]\n        return self.make_response(\n            predictions, preprocess_return_metadata[\"img_dims\"], **kwargs\n        )\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )\n        return (predictions,)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        if isinstance(image, list):\n            imgs_with_dims = [\n                self.preproc_image(\n                    i,\n                    disable_preproc_auto_orient=kwargs.get(\n                        \"disable_preproc_auto_orient\", False\n                    ),\n                    disable_preproc_contrast=kwargs.get(\n                        \"disable_preproc_contrast\", False\n                    ),\n                    disable_preproc_grayscale=kwargs.get(\n                        \"disable_preproc_grayscale\", False\n                    ),\n                    disable_preproc_static_crop=kwargs.get(\n                        \"disable_preproc_static_crop\", False\n                    ),\n                )\n                for i in image\n            ]\n            imgs, img_dims = zip(*imgs_with_dims)\n            if isinstance(imgs[0], np.ndarray):\n                img_in = np.concatenate(imgs, axis=0)\n            elif USE_PYTORCH_FOR_PREPROCESSING:\n                img_in = torch.cat(imgs, dim=0)\n            else:\n                raise ValueError(\n                    f\"Received a list of images of unknown type, {type(imgs[0])}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n        else:\n            img_in, img_dims = self.preproc_image(\n                image,\n                disable_preproc_auto_orient=kwargs.get(\n                    \"disable_preproc_auto_orient\", False\n                ),\n                disable_preproc_contrast=kwargs.get(\"disable_preproc_contrast\", False),\n                disable_preproc_grayscale=kwargs.get(\n                    \"disable_preproc_grayscale\", False\n                ),\n                disable_preproc_static_crop=kwargs.get(\n                    \"disable_preproc_static_crop\", False\n                ),\n            )\n            img_dims = [img_dims]\n\n        img_in /= 255.0\n\n        mean = self.preprocess_means\n        std = self.preprocess_stds\n        if isinstance(img_in, np.ndarray):\n            img_in = img_in.astype(np.float32)\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            img_in = img_in.float()\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(img_in)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[0]) / std[0]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[2]) / std[2]\n        return img_in, PreprocessReturnMetadata({\"img_dims\": img_dims})\n\n    def infer_from_request(\n        self,\n        request: ClassificationInferenceRequest,\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n        \"\"\"\n        Handle an inference request to produce an appropriate response.\n\n        Args:\n            request (ClassificationInferenceRequest): The request object encapsulating the image(s) and relevant parameters.\n\n        Returns:\n            Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.\n\n        Notes:\n            - Starts a timer at the beginning to calculate inference time.\n            - Processes the image(s) through the `infer` method.\n            - Generates the appropriate response object(s) using `make_response`.\n            - Calculates and sets the time taken for inference.\n            - If visualization is requested, the predictions are drawn on the image.\n        \"\"\"\n        t1 = perf_counter()\n        responses = self.infer(**request.dict(), return_image_dims=True)\n        for response in responses:\n            response.time = perf_counter() - t1\n            response.inference_id = getattr(request, \"id\", None)\n\n        if request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if not isinstance(request.image, list):\n            responses = responses[0]\n\n        return responses\n\n    def make_response(\n        self,\n        predictions,\n        img_dims,\n        confidence: float = 0.5,\n        **kwargs,\n    ) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n        \"\"\"\n        Create response objects for the given predictions and image dimensions.\n\n        Args:\n            predictions (list): List of prediction arrays from the inference process.\n            img_dims (list): List of tuples indicating the dimensions (width, height) of each image.\n            confidence (float, optional): Confidence threshold for filtering predictions. Defaults to 0.5.\n            **kwargs: Additional parameters to influence the response creation process.\n\n        Returns:\n            Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.\n\n        Notes:\n            - If the model is multiclass, a `MultiLabelClassificationInferenceResponse` is generated for each image.\n            - If the model is not multiclass, a `ClassificationInferenceResponse` is generated for each image.\n            - Predictions below the confidence threshold are filtered out.\n        \"\"\"\n        responses = []\n        confidence_threshold = float(confidence)\n        for ind, prediction in enumerate(predictions):\n            if self.multiclass:\n                preds = prediction[0]\n                results = dict()\n                predicted_classes = []\n                for i, o in enumerate(preds):\n                    cls_name = self.class_names[i]\n                    score = float(o)\n                    results[cls_name] = {\"confidence\": score, \"class_id\": i}\n                    if score &gt; confidence_threshold:\n                        predicted_classes.append(cls_name)\n                response = MultiLabelClassificationInferenceResponse(\n                    image=InferenceResponseImage(\n                        width=img_dims[ind][0], height=img_dims[ind][1]\n                    ),\n                    predicted_classes=predicted_classes,\n                    predictions=results,\n                )\n            else:\n                preds = prediction[0]\n                preds = self.softmax(preds)\n                results = []\n                for i, cls_name in enumerate(self.class_names):\n                    score = float(preds[i])\n                    if score &lt; confidence_threshold:\n                        continue\n                    pred = {\n                        \"class_id\": i,\n                        \"class\": cls_name,\n                        \"confidence\": round(score, 4),\n                    }\n                    results.append(pred)\n                results = sorted(results, key=lambda x: x[\"confidence\"], reverse=True)\n\n                response = ClassificationInferenceResponse(\n                    image=InferenceResponseImage(\n                        width=img_dims[ind][1], height=img_dims[ind][0]\n                    ),\n                    predictions=results,\n                    top=results[0][\"class\"] if results else \"\",\n                    confidence=results[0][\"confidence\"] if results else 0.0,\n                )\n            responses.append(response)\n\n        return responses\n\n    @staticmethod\n    def softmax(x):\n        \"\"\"Compute softmax values for each set of scores in x.\n\n        Args:\n            x (np.array): The input array containing the scores.\n\n        Returns:\n            np.array: The softmax values for each set of scores.\n        \"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    def get_model_output_shape(self) -&gt; Tuple[int, int, int]:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        test_image, _ = self.preprocess(test_image)\n        output = np.array(self.predict(test_image))\n        return output.shape\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = output_shape[3]\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the model, setting whether it is multiclass or not.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize the model, setting whether it is multiclass or not.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw prediction visuals on an image.</p> <p>This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <p>The request object containing the image and parameters.</p> required <code>inference_response</code> <p>The response object containing the predictions and other details.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <p>The bytes of the visualized image in JPEG format.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def draw_predictions(self, inference_request, inference_response):\n    \"\"\"Draw prediction visuals on an image.\n\n    This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.\n\n    Args:\n        inference_request: The request object containing the image and parameters.\n        inference_response: The response object containing the predictions and other details.\n\n    Returns:\n        bytes: The bytes of the visualized image in JPEG format.\n    \"\"\"\n    image = load_image_rgb(inference_request.image)\n    image = Image.fromarray(image)\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    if isinstance(inference_response.predictions, list):\n        prediction = inference_response.predictions[0]\n        color = self.colors.get(prediction.class_name, \"#4892EA\")\n        draw.rectangle(\n            [0, 0, image.size[1], image.size[0]],\n            outline=color,\n            width=inference_request.visualization_stroke_width,\n        )\n        text = f\"{prediction.class_id} - {prediction.class_name} {prediction.confidence:.2f}\"\n        text_size = font.getbbox(text)\n\n        # set button size + 10px margins\n        button_size = (text_size[2] + 20, text_size[3] + 20)\n        button_img = Image.new(\"RGBA\", button_size, color)\n        # put text on button with 10px margins\n        button_draw = ImageDraw.Draw(button_img)\n        button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n        # put button on source image in position (0, 0)\n        image.paste(button_img, (0, 0))\n    else:\n        if len(inference_response.predictions) &gt; 0:\n            box_color = \"#4892EA\"\n            draw.rectangle(\n                [0, 0, image.size[1], image.size[0]],\n                outline=box_color,\n                width=inference_request.visualization_stroke_width,\n            )\n        row = 0\n        predictions = [\n            (cls_name, pred)\n            for cls_name, pred in inference_response.predictions.items()\n        ]\n        predictions = sorted(\n            predictions, key=lambda x: x[1].confidence, reverse=True\n        )\n        for i, (cls_name, pred) in enumerate(predictions):\n            color = self.colors.get(cls_name, \"#4892EA\")\n            text = f\"{cls_name} {pred.confidence:.2f}\"\n            text_size = font.getbbox(text)\n\n            # set button size + 10px margins\n            button_size = (text_size[2] + 20, text_size[3] + 20)\n            button_img = Image.new(\"RGBA\", button_size, color)\n            # put text on button with 10px margins\n            button_draw = ImageDraw.Draw(button_img)\n            button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n            # put button on source image in position (0, 0)\n            image.paste(button_img, (0, row))\n            row += button_size[1]\n\n    buffered = BytesIO()\n    image = image.convert(\"RGB\")\n    image.save(buffered, format=\"JPEG\")\n    return buffered.getvalue()\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"environment.json\"].</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"environment.json\"].\n    \"\"\"\n    return [\"environment.json\"]\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, return_image_dims=False, **kwargs)</code>","text":"<p>Perform inference on the provided image(s) and return the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be processed. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>return_image_dims</code> <code>bool</code> <p>If set to True, the function will also return the dimensions of the image. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to customize the inference process.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:</p> <p>If <code>return_image_dims</code> is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.</p> <p>If <code>return_image_dims</code> is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.</p> <p>If <code>return_image_dims</code> is False and a list of images is provided, only the list of prediction arrays is returned.</p> <p>If <code>return_image_dims</code> is False and a single image is provided, only the prediction array is returned.</p> Notes <ul> <li>The input image(s) will be preprocessed (normalized and reshaped) before inference.</li> <li>This function uses an ONNX session to perform inference on the input image(s).</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    return_image_dims: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Perform inference on the provided image(s) and return the predictions.\n\n    Args:\n        image (Any): The image or list of images to be processed.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        return_image_dims (bool, optional): If set to True, the function will also return the dimensions of the image. Defaults to False.\n        **kwargs: Additional parameters to customize the inference process.\n\n    Returns:\n        Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:\n        If `return_image_dims` is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.\n        If `return_image_dims` is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.\n        If `return_image_dims` is False and a list of images is provided, only the list of prediction arrays is returned.\n        If `return_image_dims` is False and a single image is provided, only the prediction array is returned.\n\n    Notes:\n        - The input image(s) will be preprocessed (normalized and reshaped) before inference.\n        - This function uses an ONNX session to perform inference on the input image(s).\n    \"\"\"\n    return super().infer(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Handle an inference request to produce an appropriate response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClassificationInferenceRequest</code> <p>The request object encapsulating the image(s) and relevant parameters.</p> required <p>Returns:</p> Type Description <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.</p> Notes <ul> <li>Starts a timer at the beginning to calculate inference time.</li> <li>Processes the image(s) through the <code>infer</code> method.</li> <li>Generates the appropriate response object(s) using <code>make_response</code>.</li> <li>Calculates and sets the time taken for inference.</li> <li>If visualization is requested, the predictions are drawn on the image.</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def infer_from_request(\n    self,\n    request: ClassificationInferenceRequest,\n) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n    \"\"\"\n    Handle an inference request to produce an appropriate response.\n\n    Args:\n        request (ClassificationInferenceRequest): The request object encapsulating the image(s) and relevant parameters.\n\n    Returns:\n        Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.\n\n    Notes:\n        - Starts a timer at the beginning to calculate inference time.\n        - Processes the image(s) through the `infer` method.\n        - Generates the appropriate response object(s) using `make_response`.\n        - Calculates and sets the time taken for inference.\n        - If visualization is requested, the predictions are drawn on the image.\n    \"\"\"\n    t1 = perf_counter()\n    responses = self.infer(**request.dict(), return_image_dims=True)\n    for response in responses:\n        response.time = perf_counter() - t1\n        response.inference_id = getattr(request, \"id\", None)\n\n    if request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if not isinstance(request.image, list):\n        responses = responses[0]\n\n    return responses\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, confidence=0.5, **kwargs)</code>","text":"<p>Create response objects for the given predictions and image dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>List of prediction arrays from the inference process.</p> required <code>img_dims</code> <code>list</code> <p>List of tuples indicating the dimensions (width, height) of each image.</p> required <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering predictions. Defaults to 0.5.</p> <code>0.5</code> <code>**kwargs</code> <p>Additional parameters to influence the response creation process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]</code> <p>Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.</p> Notes <ul> <li>If the model is multiclass, a <code>MultiLabelClassificationInferenceResponse</code> is generated for each image.</li> <li>If the model is not multiclass, a <code>ClassificationInferenceResponse</code> is generated for each image.</li> <li>Predictions below the confidence threshold are filtered out.</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def make_response(\n    self,\n    predictions,\n    img_dims,\n    confidence: float = 0.5,\n    **kwargs,\n) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n    \"\"\"\n    Create response objects for the given predictions and image dimensions.\n\n    Args:\n        predictions (list): List of prediction arrays from the inference process.\n        img_dims (list): List of tuples indicating the dimensions (width, height) of each image.\n        confidence (float, optional): Confidence threshold for filtering predictions. Defaults to 0.5.\n        **kwargs: Additional parameters to influence the response creation process.\n\n    Returns:\n        Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.\n\n    Notes:\n        - If the model is multiclass, a `MultiLabelClassificationInferenceResponse` is generated for each image.\n        - If the model is not multiclass, a `ClassificationInferenceResponse` is generated for each image.\n        - Predictions below the confidence threshold are filtered out.\n    \"\"\"\n    responses = []\n    confidence_threshold = float(confidence)\n    for ind, prediction in enumerate(predictions):\n        if self.multiclass:\n            preds = prediction[0]\n            results = dict()\n            predicted_classes = []\n            for i, o in enumerate(preds):\n                cls_name = self.class_names[i]\n                score = float(o)\n                results[cls_name] = {\"confidence\": score, \"class_id\": i}\n                if score &gt; confidence_threshold:\n                    predicted_classes.append(cls_name)\n            response = MultiLabelClassificationInferenceResponse(\n                image=InferenceResponseImage(\n                    width=img_dims[ind][0], height=img_dims[ind][1]\n                ),\n                predicted_classes=predicted_classes,\n                predictions=results,\n            )\n        else:\n            preds = prediction[0]\n            preds = self.softmax(preds)\n            results = []\n            for i, cls_name in enumerate(self.class_names):\n                score = float(preds[i])\n                if score &lt; confidence_threshold:\n                    continue\n                pred = {\n                    \"class_id\": i,\n                    \"class\": cls_name,\n                    \"confidence\": round(score, 4),\n                }\n                results.append(pred)\n            results = sorted(results, key=lambda x: x[\"confidence\"], reverse=True)\n\n            response = ClassificationInferenceResponse(\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n                predictions=results,\n                top=results[0][\"class\"] if results else \"\",\n                confidence=results[0][\"confidence\"] if results else 0.0,\n            )\n        responses.append(response)\n\n    return responses\n</code></pre>"},{"location":"reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.softmax","title":"<code>softmax(x)</code>  <code>staticmethod</code>","text":"<p>Compute softmax values for each set of scores in x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>The input array containing the scores.</p> required <p>Returns:</p> Type Description <p>np.array: The softmax values for each set of scores.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>@staticmethod\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\n\n    Args:\n        x (np.array): The input array containing the scores.\n\n    Returns:\n        np.array: The softmax values for each set of scores.\n    \"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n</code></pre>"},{"location":"reference/inference/core/models/instance_segmentation_base/","title":"Instance segmentation base","text":""},{"location":"reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel","title":"<code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Instance Segmentation model.</p> <p>This class implements an instance segmentation specific inference method for ONNX models provided by Roboflow.</p> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>class InstanceSegmentationBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Instance Segmentation model.\n\n    This class implements an instance segmentation specific inference method\n    for ONNX models provided by Roboflow.\n    \"\"\"\n\n    task_type = \"instance-segmentation\"\n    num_masks = 32\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = False,\n        confidence: float = DEFAULT_CONFIDENCE,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        mask_decode_mode: str = DEFAULT_MASK_DECODE_MODE,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        tradeoff_factor: float = DEFAULT_TRADEOFF_FACTOR,\n        **kwargs,\n    ) -&gt; Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]:\n        \"\"\"\n        Process an image or list of images for instance segmentation.\n\n        Args:\n            image (Any): An image or a list of images for processing.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for predictions. Defaults to 0.4.\n            iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.3.\n            mask_decode_mode (str, optional): Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".\n            max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the processed images. Defaults to False.\n            tradeoff_factor (float, optional): Tradeoff factor used when `mask_decode_mode` is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            **kwargs: Additional parameters to customize the inference process.\n\n        Returns:\n            Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.\n\n        Raises:\n            InvalidMaskDecodeArgument: If an invalid `mask_decode_mode` is provided or if the `tradeoff_factor` is outside the allowed range.\n\n        Notes:\n            - Processes input images and normalizes them.\n            - Makes predictions using the ONNX runtime.\n            - Applies non-maximum suppression to the predictions.\n            - Decodes the masks according to the specified mode.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            iou_threshold=iou_threshold,\n            mask_decode_mode=mask_decode_mode,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            tradeoff_factor=tradeoff_factor,\n            **kwargs,\n        )\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Union[\n        InstanceSegmentationInferenceResponse,\n        List[InstanceSegmentationInferenceResponse],\n    ]:\n        predictions, protos = predictions\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=kwargs[\"confidence\"],\n            iou_thresh=kwargs[\"iou_threshold\"],\n            class_agnostic=kwargs[\"class_agnostic_nms\"],\n            max_detections=kwargs[\"max_detections\"],\n            max_candidate_detections=kwargs[\"max_candidates\"],\n            num_masks=self.num_masks,\n        )\n        infer_shape = (self.img_size_h, self.img_size_w)\n        masks = []\n        mask_decode_mode = kwargs[\"mask_decode_mode\"]\n        tradeoff_factor = kwargs[\"tradeoff_factor\"]\n        img_in_shape = preprocess_return_metadata[\"im_shape\"]\n\n        predictions = [np.array(p) for p in predictions]\n\n        for pred, proto, img_dim in zip(\n            predictions, protos, preprocess_return_metadata[\"img_dims\"]\n        ):\n            if pred.size == 0:\n                masks.append([])\n                continue\n            if mask_decode_mode == \"accurate\":\n                batch_masks = process_mask_accurate(\n                    proto, pred[:, 7:], pred[:, :4], img_in_shape[2:]\n                )\n                output_mask_shape = img_in_shape[2:]\n            elif mask_decode_mode == \"tradeoff\":\n                if not 0 &lt;= tradeoff_factor &lt;= 1:\n                    raise InvalidMaskDecodeArgument(\n                        f\"Invalid tradeoff_factor: {tradeoff_factor}. Must be in [0.0, 1.0]\"\n                    )\n                batch_masks = process_mask_tradeoff(\n                    proto,\n                    pred[:, 7:],\n                    pred[:, :4],\n                    img_in_shape[2:],\n                    tradeoff_factor,\n                )\n                output_mask_shape = batch_masks.shape[1:]\n            elif mask_decode_mode == \"fast\":\n                batch_masks = process_mask_fast(\n                    proto, pred[:, 7:], pred[:, :4], img_in_shape[2:]\n                )\n                output_mask_shape = batch_masks.shape[1:]\n            else:\n                raise InvalidMaskDecodeArgument(\n                    f\"Invalid mask_decode_mode: {mask_decode_mode}. Must be one of ['accurate', 'fast', 'tradeoff']\"\n                )\n            polys = masks2poly(batch_masks)\n            pred[:, :4] = post_process_bboxes(\n                [pred[:, :4]],\n                infer_shape,\n                [img_dim],\n                self.preproc,\n                resize_method=self.resize_method,\n                disable_preproc_static_crop=preprocess_return_metadata[\n                    \"disable_preproc_static_crop\"\n                ],\n            )[0]\n            polys = post_process_polygons(\n                img_dim,\n                polys,\n                output_mask_shape,\n                self.preproc,\n                resize_method=self.resize_method,\n            )\n            masks.append(polys)\n        return self.make_response(\n            predictions, masks, preprocess_return_metadata[\"img_dims\"], **kwargs\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=kwargs.get(\"disable_preproc_auto_orient\"),\n            disable_preproc_contrast=kwargs.get(\"disable_preproc_contrast\"),\n            disable_preproc_grayscale=kwargs.get(\"disable_preproc_grayscale\"),\n            disable_preproc_static_crop=kwargs.get(\"disable_preproc_static_crop\"),\n        )\n\n        img_in /= 255.0\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"im_shape\": img_in.shape,\n                \"disable_preproc_static_crop\": kwargs.get(\n                    \"disable_preproc_static_crop\"\n                ),\n            }\n        )\n\n    def make_response(\n        self,\n        predictions: List[List[List[float]]],\n        masks: List[List[List[float]]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: List[str] = [],\n        **kwargs,\n    ) -&gt; Union[\n        InstanceSegmentationInferenceResponse,\n        List[InstanceSegmentationInferenceResponse],\n    ]:\n        \"\"\"\n        Create instance segmentation inference response objects for the provided predictions and masks.\n\n        Args:\n            predictions (List[List[List[float]]]): List of prediction data, one for each image.\n            masks (List[List[List[float]]]): List of masks corresponding to the predictions.\n            img_dims (List[Tuple[int, int]]): List of image dimensions corresponding to the processed images.\n            class_filter (List[str], optional): List of class names to filter predictions by. Defaults to an empty list (no filtering).\n\n        Returns:\n            Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.\n\n        Notes:\n            - For each image, constructs an `InstanceSegmentationInferenceResponse` object.\n            - Each response contains a list of `InstanceSegmentationPrediction` objects.\n        \"\"\"\n        responses = []\n        for ind, (batch_predictions, batch_masks) in enumerate(zip(predictions, masks)):\n            predictions = []\n            for pred, mask in zip(batch_predictions, batch_masks):\n                if class_filter and not self.class_names[int(pred[6])] in class_filter:\n                    # TODO: logger.debug\n                    continue\n                # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                predictions.append(\n                    InstanceSegmentationPrediction(\n                        **{\n                            \"x\": pred[0] + (pred[2] - pred[0]) / 2,\n                            \"y\": pred[1] + (pred[3] - pred[1]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                        }\n                    )\n                )\n            response = InstanceSegmentationInferenceResponse(\n                predictions=predictions,\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            responses.append(response)\n        return responses\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Runs inference on the ONNX model.\n\n        Args:\n            img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError(\"predict must be implemented by a subclass\")\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            output_shape[2], masks=self.num_masks\n        )\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, class_agnostic_nms=False, confidence=DEFAULT_CONFIDENCE, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, iou_threshold=DEFAULT_IOU_THRESH, mask_decode_mode=DEFAULT_MASK_DECODE_MODE, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, tradeoff_factor=DEFAULT_TRADEOFF_FACTOR, **kwargs)</code>","text":"<p>Process an image or list of images for instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>An image or a list of images for processing. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to use class-agnostic non-maximum suppression. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.4.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression. Defaults to 0.3.</p> <code>DEFAULT_IOU_THRESH</code> <code>mask_decode_mode</code> <code>str</code> <p>Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".</p> <code>DEFAULT_MASK_DECODE_MODE</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections after non-maximum suppression. Defaults to 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the processed images. Defaults to False.</p> <code>False</code> <code>tradeoff_factor</code> <code>float</code> <p>Tradeoff factor used when <code>mask_decode_mode</code> is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.</p> <code>DEFAULT_TRADEOFF_FACTOR</code> <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to customize the inference process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]</code> <p>Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.</p> <p>Raises:</p> Type Description <code>InvalidMaskDecodeArgument</code> <p>If an invalid <code>mask_decode_mode</code> is provided or if the <code>tradeoff_factor</code> is outside the allowed range.</p> Notes <ul> <li>Processes input images and normalizes them.</li> <li>Makes predictions using the ONNX runtime.</li> <li>Applies non-maximum suppression to the predictions.</li> <li>Decodes the masks according to the specified mode.</li> </ul> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = False,\n    confidence: float = DEFAULT_CONFIDENCE,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    mask_decode_mode: str = DEFAULT_MASK_DECODE_MODE,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    tradeoff_factor: float = DEFAULT_TRADEOFF_FACTOR,\n    **kwargs,\n) -&gt; Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]:\n    \"\"\"\n    Process an image or list of images for instance segmentation.\n\n    Args:\n        image (Any): An image or a list of images for processing.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for predictions. Defaults to 0.4.\n        iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.3.\n        mask_decode_mode (str, optional): Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".\n        max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the processed images. Defaults to False.\n        tradeoff_factor (float, optional): Tradeoff factor used when `mask_decode_mode` is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        **kwargs: Additional parameters to customize the inference process.\n\n    Returns:\n        Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.\n\n    Raises:\n        InvalidMaskDecodeArgument: If an invalid `mask_decode_mode` is provided or if the `tradeoff_factor` is outside the allowed range.\n\n    Notes:\n        - Processes input images and normalizes them.\n        - Makes predictions using the ONNX runtime.\n        - Applies non-maximum suppression to the predictions.\n        - Decodes the masks according to the specified mode.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        iou_threshold=iou_threshold,\n        mask_decode_mode=mask_decode_mode,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        tradeoff_factor=tradeoff_factor,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, masks, img_dims, class_filter=[], **kwargs)</code>","text":"<p>Create instance segmentation inference response objects for the provided predictions and masks.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>List of prediction data, one for each image.</p> required <code>masks</code> <code>List[List[List[float]]]</code> <p>List of masks corresponding to the predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>List of image dimensions corresponding to the processed images.</p> required <code>class_filter</code> <code>List[str]</code> <p>List of class names to filter predictions by. Defaults to an empty list (no filtering).</p> <code>[]</code> <p>Returns:</p> Type Description <code>Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]</code> <p>Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.</p> Notes <ul> <li>For each image, constructs an <code>InstanceSegmentationInferenceResponse</code> object.</li> <li>Each response contains a list of <code>InstanceSegmentationPrediction</code> objects.</li> </ul> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[List[float]]],\n    masks: List[List[List[float]]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: List[str] = [],\n    **kwargs,\n) -&gt; Union[\n    InstanceSegmentationInferenceResponse,\n    List[InstanceSegmentationInferenceResponse],\n]:\n    \"\"\"\n    Create instance segmentation inference response objects for the provided predictions and masks.\n\n    Args:\n        predictions (List[List[List[float]]]): List of prediction data, one for each image.\n        masks (List[List[List[float]]]): List of masks corresponding to the predictions.\n        img_dims (List[Tuple[int, int]]): List of image dimensions corresponding to the processed images.\n        class_filter (List[str], optional): List of class names to filter predictions by. Defaults to an empty list (no filtering).\n\n    Returns:\n        Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.\n\n    Notes:\n        - For each image, constructs an `InstanceSegmentationInferenceResponse` object.\n        - Each response contains a list of `InstanceSegmentationPrediction` objects.\n    \"\"\"\n    responses = []\n    for ind, (batch_predictions, batch_masks) in enumerate(zip(predictions, masks)):\n        predictions = []\n        for pred, mask in zip(batch_predictions, batch_masks):\n            if class_filter and not self.class_names[int(pred[6])] in class_filter:\n                # TODO: logger.debug\n                continue\n            # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n            predictions.append(\n                InstanceSegmentationPrediction(\n                    **{\n                        \"x\": pred[0] + (pred[2] - pred[0]) / 2,\n                        \"y\": pred[1] + (pred[3] - pred[1]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n            )\n        response = InstanceSegmentationInferenceResponse(\n            predictions=predictions,\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        responses.append(response)\n    return responses\n</code></pre>"},{"location":"reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Runs inference on the ONNX model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>The preprocessed image(s) to run inference on.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Runs inference on the ONNX model.\n\n    Args:\n        img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError(\"predict must be implemented by a subclass\")\n</code></pre>"},{"location":"reference/inference/core/models/keypoints_detection_base/","title":"Keypoints detection base","text":""},{"location":"reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel","title":"<code>KeypointsDetectionBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model. This class implements an object detection specific infer method.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>class KeypointsDetectionBaseOnnxRoboflowInferenceModel(\n    ObjectDetectionBaseOnnxRoboflowInferenceModel\n):\n    \"\"\"Roboflow ONNX Object detection model. This class implements an object detection specific infer method.\"\"\"\n\n    task_type = \"keypoint-detection\"\n\n    def __init__(self, model_id: str, *args, **kwargs):\n        super().__init__(model_id, *args, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n        Returns:\n            list: A list of filenames specific to ONNX models.\n        \"\"\"\n        return [\"environment.json\", \"class_names.txt\", \"keypoints_metadata.json\"]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[KeypointsDetectionInferenceResponse]:\n        \"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n            max_candidates (int): Maximum number of candidate detections. Default is 3000.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[KeypointsDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        number_of_classes = len(self.get_class_names)\n        num_masks = predictions.shape[2] - 5 - number_of_classes\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            num_masks=num_masks,\n        )\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions=predictions,\n            infer_shape=infer_shape,\n            img_dims=img_dims,\n            preproc=self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        predictions = post_process_keypoints(\n            predictions=predictions,\n            keypoints_start_index=-num_masks,\n            infer_shape=infer_shape,\n            img_dims=img_dims,\n            preproc=self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[KeypointsDetectionInferenceResponse]:\n        \"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.\n        \"\"\"\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n        keypoint_confidence_threshold = 0.0\n        if \"request\" in kwargs:\n            keypoint_confidence_threshold = kwargs[\"request\"].keypoint_confidence\n        responses = [\n            KeypointsDetectionInferenceResponse(\n                predictions=[\n                    KeypointsPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                            \"keypoints\": model_keypoints_to_response(\n                                keypoints_metadata=self.keypoints_metadata,\n                                keypoints=pred[7:],\n                                predicted_object_class_id=int(pred[6]),\n                                keypoint_confidence_threshold=keypoint_confidence_threshold,\n                            ),\n                        }\n                    )\n                    for pred in batch_predictions\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n\n    def keypoints_count(self) -&gt; int:\n        raise NotImplementedError\n\n    def validate_model_classes(self) -&gt; None:\n        num_keypoints = self.keypoints_count()\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            len_prediction=output_shape[2], keypoints=num_keypoints\n        )\n        if num_classes != self.num_classes:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Returns the list of files to be downloaded from the inference bucket for ONNX model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of filenames specific to ONNX models.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n    Returns:\n        list: A list of filenames specific to ONNX models.\n    \"\"\"\n    return [\"environment.json\", \"class_names.txt\", \"keypoints_metadata.json\"]\n</code></pre>"},{"location":"reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[KeypointsDetectionInferenceResponse]</code> <p>List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[KeypointsDetectionInferenceResponse]:\n    \"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.\n    \"\"\"\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n    keypoint_confidence_threshold = 0.0\n    if \"request\" in kwargs:\n        keypoint_confidence_threshold = kwargs[\"request\"].keypoint_confidence\n    responses = [\n        KeypointsDetectionInferenceResponse(\n            predictions=[\n                KeypointsPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                        \"keypoints\": model_keypoints_to_response(\n                            keypoints_metadata=self.keypoints_metadata,\n                            keypoints=pred[7:],\n                            predicted_object_class_id=int(pred[6]),\n                            keypoint_confidence_threshold=keypoint_confidence_threshold,\n                        ),\n                    }\n                )\n                for pred in batch_predictions\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to apply class-agnostic non-max suppression. Default is False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression. Default is 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Default is 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[KeypointsDetectionInferenceResponse]</code> <p>List[KeypointsDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[KeypointsDetectionInferenceResponse]:\n    \"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n        max_candidates (int): Maximum number of candidate detections. Default is 3000.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[KeypointsDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    number_of_classes = len(self.get_class_names)\n    num_masks = predictions.shape[2] - 5 - number_of_classes\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=confidence,\n        iou_thresh=iou_threshold,\n        class_agnostic=class_agnostic_nms,\n        max_detections=max_detections,\n        max_candidate_detections=max_candidates,\n        num_masks=num_masks,\n    )\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions=predictions,\n        infer_shape=infer_shape,\n        img_dims=img_dims,\n        preproc=self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    predictions = post_process_keypoints(\n        predictions=predictions,\n        keypoints_start_index=-num_masks,\n        infer_shape=infer_shape,\n        img_dims=img_dims,\n        preproc=self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"reference/inference/core/models/object_detection_base/","title":"Object detection base","text":""},{"location":"reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel","title":"<code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model. This class implements an object detection specific infer method.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>class ObjectDetectionBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model. This class implements an object detection specific infer method.\"\"\"\n\n    task_type = \"object-detection\"\n    box_format = \"xywh\"\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        fix_batch_size: bool = False,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Runs object detection inference on one or multiple images and returns the detections.\n\n        Args:\n            image (Any): The input image or a list of images to process.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for predictions. Defaults to 0.4.\n            iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.3.\n            fix_batch_size (bool, optional): If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.\n            max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the processed images along with the predictions. Defaults to False.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If `return_image_dims` is True, it will return a tuple with predictions and image dimensions.\n\n        Raises:\n            ValueError: If batching is not enabled for the model and more than one image is passed for processing.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            iou_threshold=iou_threshold,\n            fix_batch_size=fix_batch_size,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n        \"\"\"\n\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n\n        predictions = predictions[\n            : len(img_dims)\n        ]  # If the batch size was fixed we have empty preds at the end\n        responses = [\n            ObjectDetectionInferenceResponse(\n                predictions=[\n                    ObjectDetectionPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                        }\n                    )\n                    for pred in batch_predictions\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n            max_candidates (int): Maximum number of candidate detections. Default is 3000.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            box_format=self.box_format,\n        )\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions,\n            infer_shape,\n            img_dims,\n            self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def preprocess(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        fix_batch_size: bool = False,\n        **kwargs,\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        \"\"\"Preprocesses an object detection inference request.\n\n        Args:\n            request (ObjectDetectionInferenceRequest): The request object containing images.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.\n        \"\"\"\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        img_in /= 255.0\n\n        if self.batching_enabled:\n            batch_padding = 0\n            if FIX_BATCH_SIZE or fix_batch_size:\n                if MAX_BATCH_SIZE == float(\"inf\"):\n                    logger.warning(\n                        \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                    )\n                    batch_padding = 0\n                else:\n                    batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n            if batch_padding &lt; 0:\n                raise ValueError(\n                    f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                    f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                    f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                    f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n                )\n            width_remainder = img_in.shape[2] % 32\n            height_remainder = img_in.shape[3] % 32\n            if width_remainder &gt; 0:\n                width_padding = 32 - width_remainder\n            else:\n                width_padding = 0\n            if height_remainder &gt; 0:\n                height_padding = 32 - height_remainder\n            else:\n                height_padding = 0\n\n            if isinstance(img_in, np.ndarray):\n                img_in = np.pad(\n                    img_in,\n                    (\n                        (0, batch_padding),\n                        (0, 0),\n                        (0, width_padding),\n                        (0, height_padding),\n                    ),\n                    \"constant\",\n                )\n            elif USE_PYTORCH_FOR_PREPROCESSING:\n                img_in = torch.nn.functional.pad(\n                    img_in,\n                    (\n                        0,\n                        height_padding,  # height padding\n                        0,\n                        width_padding,  # width padding\n                        0,\n                        0,  # channels\n                        0,\n                        batch_padding,\n                    ),  # batch\n                    mode=\"constant\",\n                    value=0,\n                )\n            else:\n                raise ValueError(\n                    f\"Received an image of unknown type, {type(img_in)}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"disable_preproc_static_crop\": disable_preproc_static_crop,\n            }\n        )\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Runs inference on the ONNX model.\n\n        Args:\n            img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n        Returns:\n            Tuple[np.ndarray]: The ONNX model predictions.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError(\"predict must be implemented by a subclass\")\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            output_shape[2], masks=0\n        )\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, iou_threshold=DEFAULT_IOU_THRESH, fix_batch_size=False, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Runs object detection inference on one or multiple images and returns the detections.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The input image or a list of images to process. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to use class-agnostic non-maximum suppression. Defaults to False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.4.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression. Defaults to 0.3.</p> <code>DEFAULT_IOU_THRESH</code> <code>fix_batch_size</code> <code>bool</code> <p>If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.</p> <code>False</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections after non-maximum suppression. Defaults to 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the processed images along with the predictions. Defaults to False.</p> <code>False</code> <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>*args</code> <p>Variable length argument list.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If <code>return_image_dims</code> is True, it will return a tuple with predictions and image dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batching is not enabled for the model and more than one image is passed for processing.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    fix_batch_size: bool = False,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Runs object detection inference on one or multiple images and returns the detections.\n\n    Args:\n        image (Any): The input image or a list of images to process.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for predictions. Defaults to 0.4.\n        iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.3.\n        fix_batch_size (bool, optional): If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.\n        max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the processed images along with the predictions. Defaults to False.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If `return_image_dims` is True, it will return a tuple with predictions and image dimensions.\n\n    Raises:\n        ValueError: If batching is not enabled for the model and more than one image is passed for processing.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        iou_threshold=iou_threshold,\n        fix_batch_size=fix_batch_size,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n    \"\"\"\n\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n\n    predictions = predictions[\n        : len(img_dims)\n    ]  # If the batch size was fixed we have empty preds at the end\n    responses = [\n        ObjectDetectionInferenceResponse(\n            predictions=[\n                ObjectDetectionPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n                for pred in batch_predictions\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to apply class-agnostic non-max suppression. Default is False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression. Default is 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Default is 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray, ...],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n        max_candidates (int): Maximum number of candidate detections. Default is 3000.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=confidence,\n        iou_thresh=iou_threshold,\n        class_agnostic=class_agnostic_nms,\n        max_detections=max_detections,\n        max_candidate_detections=max_candidates,\n        box_format=self.box_format,\n    )\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions,\n        infer_shape,\n        img_dims,\n        self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Runs inference on the ONNX model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>The preprocessed image(s) to run inference on.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: The ONNX model predictions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Runs inference on the ONNX model.\n\n    Args:\n        img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n    Returns:\n        Tuple[np.ndarray]: The ONNX model predictions.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError(\"predict must be implemented by a subclass\")\n</code></pre>"},{"location":"reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.preprocess","title":"<code>preprocess(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, fix_batch_size=False, **kwargs)</code>","text":"<p>Preprocesses an object detection inference request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ObjectDetectionInferenceRequest</code> <p>The request object containing images.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, PreprocessReturnMetadata]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def preprocess(\n    self,\n    image: Any,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    fix_batch_size: bool = False,\n    **kwargs,\n) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n    \"\"\"Preprocesses an object detection inference request.\n\n    Args:\n        request (ObjectDetectionInferenceRequest): The request object containing images.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.\n    \"\"\"\n    img_in, img_dims = self.load_image(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    img_in /= 255.0\n\n    if self.batching_enabled:\n        batch_padding = 0\n        if FIX_BATCH_SIZE or fix_batch_size:\n            if MAX_BATCH_SIZE == float(\"inf\"):\n                logger.warning(\n                    \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                )\n                batch_padding = 0\n            else:\n                batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n        if batch_padding &lt; 0:\n            raise ValueError(\n                f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n            )\n        width_remainder = img_in.shape[2] % 32\n        height_remainder = img_in.shape[3] % 32\n        if width_remainder &gt; 0:\n            width_padding = 32 - width_remainder\n        else:\n            width_padding = 0\n        if height_remainder &gt; 0:\n            height_padding = 32 - height_remainder\n        else:\n            height_padding = 0\n\n        if isinstance(img_in, np.ndarray):\n            img_in = np.pad(\n                img_in,\n                (\n                    (0, batch_padding),\n                    (0, 0),\n                    (0, width_padding),\n                    (0, height_padding),\n                ),\n                \"constant\",\n            )\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            img_in = torch.nn.functional.pad(\n                img_in,\n                (\n                    0,\n                    height_padding,  # height padding\n                    0,\n                    width_padding,  # width padding\n                    0,\n                    0,  # channels\n                    0,\n                    batch_padding,\n                ),  # batch\n                mode=\"constant\",\n                value=0,\n            )\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(img_in)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n    return img_in, PreprocessReturnMetadata(\n        {\n            \"img_dims\": img_dims,\n            \"disable_preproc_static_crop\": disable_preproc_static_crop,\n        }\n    )\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/","title":"Roboflow","text":""},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowCoreModel","title":"<code>OnnxRoboflowCoreModel</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>Roboflow Inference Model that operates using an ONNX model file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class OnnxRoboflowCoreModel(RoboflowCoreModel):\n    \"\"\"Roboflow Inference Model that operates using an ONNX model file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel","title":"<code>OnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> <p>Roboflow Inference Model that operates using an ONNX model file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class OnnxRoboflowInferenceModel(RoboflowInferenceModel):\n    \"\"\"Roboflow Inference Model that operates using an ONNX model file.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        onnxruntime_execution_providers: List[\n            str\n        ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initializes the OnnxRoboflowInferenceModel instance.\n\n        Args:\n            model_id (str): The identifier for the specific ONNX model.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(model_id, *args, **kwargs)\n        if self.load_weights or not self.has_model_metadata:\n            self.onnxruntime_execution_providers = onnxruntime_execution_providers\n            expanded_execution_providers = []\n            for ep in self.onnxruntime_execution_providers:\n                if ep == \"TensorrtExecutionProvider\":\n                    ep = (\n                        \"TensorrtExecutionProvider\",\n                        {\n                            \"trt_engine_cache_enable\": True,\n                            \"trt_engine_cache_path\": os.path.join(\n                                TENSORRT_CACHE_PATH, self.endpoint\n                            ),\n                            \"trt_fp16_enable\": True,\n                        },\n                    )\n                expanded_execution_providers.append(ep)\n            self.onnxruntime_execution_providers = expanded_execution_providers\n\n        self.image_loader_threadpool = ThreadPoolExecutor(max_workers=None)\n        self._session_lock = Lock()\n        try:\n            self.initialize_model(**kwargs)\n            self.validate_model()\n        except ModelArtefactError as e:\n            logger.error(f\"Unable to validate model artifacts, clearing cache: {e}\")\n            if DISK_CACHE_CLEANUP:\n                self.clear_cache(delete_from_disk=True)\n            else:\n                logger.error(\"NOT deleting model from cache, inspect model artifacts\")\n            raise ModelArtefactError from e\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Runs inference on given data.\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        input_elements = len(image) if isinstance(image, list) else 1\n        max_batch_size = MAX_BATCH_SIZE if self.batching_enabled else self.batch_size\n        if (input_elements == 1) or (max_batch_size == float(\"inf\")):\n            return super().infer(image, **kwargs)\n        logger.debug(\n            f\"Inference will be executed in batches, as there is {input_elements} input elements and \"\n            f\"maximum batch size for a model is set to: {max_batch_size}\"\n        )\n        inference_results = []\n        for batch_input in create_batches(sequence=image, batch_size=max_batch_size):\n            batch_inference_results = super().infer(batch_input, **kwargs)\n            inference_results.append(batch_inference_results)\n        return self.merge_inference_results(inference_results=inference_results)\n\n    def merge_inference_results(self, inference_results: List[Any]) -&gt; Any:\n        return list(itertools.chain(*inference_results))\n\n    def validate_model(self) -&gt; None:\n        if MODEL_VALIDATION_DISABLED:\n            logger.debug(\"Model validation disabled.\")\n            return None\n        logger.debug(f\"Starting model validation for {self.endpoint}\")\n        validate_model_error_count = cache.get(\n            self.endpoint + \"_validate_model_error_count\"\n        )\n        if validate_model_error_count is None:\n            validate_model_error_count = 0\n        if validate_model_error_count &gt; 3:\n            raise ModelArtefactError(\n                \"Model validation failed multiple times, ignoring this model.\"\n            )\n        if not self.load_weights:\n            return\n        try:\n            assert self.onnx_session is not None\n        except AssertionError as e:\n            cache.set(\n                self.endpoint + \"_validate_model_error_count\",\n                validate_model_error_count + 1,\n                expire=60,\n            )\n            raise ModelArtefactError(\n                \"ONNX session not initialized. Check that the model weights are available.\"\n            ) from e\n        try:\n            self.run_test_inference()\n        except Exception as e:\n            cache.set(\n                self.endpoint + \"_validate_model_error_count\",\n                validate_model_error_count + 1,\n                expire=60,\n            )\n            raise ModelArtefactError(f\"Unable to run test inference. Cause: {e}\") from e\n        try:\n            self.validate_model_classes()\n        except Exception as e:\n            cache.set(\n                self.endpoint + \"_validate_model_error_count\",\n                validate_model_error_count + 1,\n                expire=60,\n            )\n            raise ModelArtefactError(\n                f\"Unable to validate model classes. Cause: {e}\"\n            ) from e\n        logger.debug(f\"Model validation finished for {self.endpoint}\")\n        cache.set(self.endpoint + \"_validate_model_error_count\", 0, expire=3600)\n\n    def run_test_inference(self) -&gt; None:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        logger.debug(f\"Running test inference. Image size: {test_image.shape}\")\n        result = self.infer(test_image, usage_inference_test_run=True)\n        logger.debug(f\"Test inference finished.\")\n        return result\n\n    def get_model_output_shape(self) -&gt; Tuple[int, int, int]:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        logger.debug(f\"Getting model output shape. Image size: {test_image.shape}\")\n        test_image, _ = self.preprocess(test_image)\n        output = self.predict(test_image)[0]\n        logger.debug(f\"Model output shape test finished.\")\n        return output.shape\n\n    def validate_model_classes(self) -&gt; None:\n        pass\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n        Returns:\n            list: A list of filenames specific to ONNX models.\n        \"\"\"\n        return [\"environment.json\", \"class_names.txt\"]\n\n    def initialize_model(self, **kwargs) -&gt; None:\n        \"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n        logger.debug(\"Getting model artefacts\")\n        self.get_model_artifacts(**kwargs)\n        logger.debug(\"Creating inference session\")\n        if self.load_weights or not self.has_model_metadata:\n            t1_session = perf_counter()\n            # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n            providers = self.onnxruntime_execution_providers\n\n            if not self.load_weights:\n                providers = [\"OpenVINOExecutionProvider\", \"CPUExecutionProvider\"]\n            try:\n                session_options = onnxruntime.SessionOptions()\n                session_options.log_severity_level = 3\n                # TensorRT does better graph optimization for its EP than onnx\n                if has_trt(providers):\n                    session_options.graph_optimization_level = (\n                        onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n                    )\n                self.onnx_session = onnxruntime.InferenceSession(\n                    self.cache_file(self.weights_file),\n                    providers=providers,\n                    sess_options=session_options,\n                )\n            except Exception as e:\n                self.clear_cache()\n                raise ModelArtefactError(\n                    f\"Unable to load ONNX session. Cause: {e}\"\n                ) from e\n            logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n            if REQUIRED_ONNX_PROVIDERS:\n                available_providers = onnxruntime.get_available_providers()\n                for provider in REQUIRED_ONNX_PROVIDERS:\n                    if provider not in available_providers:\n                        raise OnnxProviderNotAvailable(\n                            f\"Required ONNX Execution Provider {provider} is not availble. \"\n                            \"Check that you are using the correct docker image on a supported device. \"\n                            \"Export list of available providers as ONNXRUNTIME_EXECUTION_PROVIDERS environmental variable, \"\n                            \"consult documentation for more details.\"\n                        )\n\n            inputs = self.onnx_session.get_inputs()[0]\n            input_shape = inputs.shape\n            self.batch_size = input_shape[0]\n            self.img_size_h = input_shape[2]\n            self.img_size_w = input_shape[3]\n            self.input_name = inputs.name\n            if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n                if \"resize\" in self.preproc:\n                    self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                    self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n                else:\n                    self.img_size_h = 640\n                    self.img_size_w = 640\n\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n\n            model_metadata = {\n                \"batch_size\": self.batch_size,\n                \"img_size_h\": self.img_size_h,\n                \"img_size_w\": self.img_size_w,\n            }\n            logger.debug(f\"Writing model metadata to memcache\")\n            self.write_model_metadata_to_memcache(model_metadata)\n            if not self.load_weights:  # had to load weights to get metadata\n                del self.onnx_session\n        else:\n            if not self.has_model_metadata:\n                raise ValueError(\n                    \"This should be unreachable, should get weights if we don't have model metadata\"\n                )\n            logger.debug(f\"Loading model metadata from memcache\")\n            metadata = self.model_metadata_from_memcache()\n            self.batch_size = metadata[\"batch_size\"]\n            self.img_size_h = metadata[\"img_size_h\"]\n            self.img_size_w = metadata[\"img_size_w\"]\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n\n        logger.debug(\"Model initialisation finished.\")\n\n    def load_image(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[Tuple[int, int], ...]]:\n        if isinstance(image, list) and len(image) &gt; 1:\n            preproc_image = partial(\n                self.preproc_image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n            )\n            imgs_with_dims = self.image_loader_threadpool.map(preproc_image, image)\n            imgs, img_dims = zip(*imgs_with_dims)\n            if isinstance(imgs[0], np.ndarray):\n                img_in = np.concatenate(imgs, axis=0)\n            elif USE_PYTORCH_FOR_PREPROCESSING:\n                img_in = torch.cat(imgs, dim=0)\n            else:\n                raise ValueError(\n                    f\"Received a list of images of unknown type, {type(imgs[0])}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n        else:\n            if isinstance(image, list):\n                image = image[0]\n            img_in, img_dims = self.preproc_image(\n                image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n            )\n            img_dims = (img_dims,)\n        return img_in, img_dims\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Returns the file containing the ONNX model weights.\n\n        Returns:\n            str: The file path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Returns the file containing the ONNX model weights.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path to the weights file.</p>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.__init__","title":"<code>__init__(model_id, onnxruntime_execution_providers=get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS), *args, **kwargs)</code>","text":"<p>Initializes the OnnxRoboflowInferenceModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier for the specific ONNX model.</p> required <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    onnxruntime_execution_providers: List[\n        str\n    ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n    *args,\n    **kwargs,\n):\n    \"\"\"Initializes the OnnxRoboflowInferenceModel instance.\n\n    Args:\n        model_id (str): The identifier for the specific ONNX model.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(model_id, *args, **kwargs)\n    if self.load_weights or not self.has_model_metadata:\n        self.onnxruntime_execution_providers = onnxruntime_execution_providers\n        expanded_execution_providers = []\n        for ep in self.onnxruntime_execution_providers:\n            if ep == \"TensorrtExecutionProvider\":\n                ep = (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": os.path.join(\n                            TENSORRT_CACHE_PATH, self.endpoint\n                        ),\n                        \"trt_fp16_enable\": True,\n                    },\n                )\n            expanded_execution_providers.append(ep)\n        self.onnxruntime_execution_providers = expanded_execution_providers\n\n    self.image_loader_threadpool = ThreadPoolExecutor(max_workers=None)\n    self._session_lock = Lock()\n    try:\n        self.initialize_model(**kwargs)\n        self.validate_model()\n    except ModelArtefactError as e:\n        logger.error(f\"Unable to validate model artifacts, clearing cache: {e}\")\n        if DISK_CACHE_CLEANUP:\n            self.clear_cache(delete_from_disk=True)\n        else:\n            logger.error(\"NOT deleting model from cache, inspect model artifacts\")\n        raise ModelArtefactError from e\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Returns the list of files to be downloaded from the inference bucket for ONNX model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of filenames specific to ONNX models.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n    Returns:\n        list: A list of filenames specific to ONNX models.\n    \"\"\"\n    return [\"environment.json\", \"class_names.txt\"]\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Runs inference on given data. - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Runs inference on given data.\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    input_elements = len(image) if isinstance(image, list) else 1\n    max_batch_size = MAX_BATCH_SIZE if self.batching_enabled else self.batch_size\n    if (input_elements == 1) or (max_batch_size == float(\"inf\")):\n        return super().infer(image, **kwargs)\n    logger.debug(\n        f\"Inference will be executed in batches, as there is {input_elements} input elements and \"\n        f\"maximum batch size for a model is set to: {max_batch_size}\"\n    )\n    inference_results = []\n    for batch_input in create_batches(sequence=image, batch_size=max_batch_size):\n        batch_inference_results = super().infer(batch_input, **kwargs)\n        inference_results.append(batch_inference_results)\n    return self.merge_inference_results(inference_results=inference_results)\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.initialize_model","title":"<code>initialize_model(**kwargs)</code>","text":"<p>Initializes the ONNX model, setting up the inference session and other necessary properties.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def initialize_model(self, **kwargs) -&gt; None:\n    \"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n    logger.debug(\"Getting model artefacts\")\n    self.get_model_artifacts(**kwargs)\n    logger.debug(\"Creating inference session\")\n    if self.load_weights or not self.has_model_metadata:\n        t1_session = perf_counter()\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        providers = self.onnxruntime_execution_providers\n\n        if not self.load_weights:\n            providers = [\"OpenVINOExecutionProvider\", \"CPUExecutionProvider\"]\n        try:\n            session_options = onnxruntime.SessionOptions()\n            session_options.log_severity_level = 3\n            # TensorRT does better graph optimization for its EP than onnx\n            if has_trt(providers):\n                session_options.graph_optimization_level = (\n                    onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n                )\n            self.onnx_session = onnxruntime.InferenceSession(\n                self.cache_file(self.weights_file),\n                providers=providers,\n                sess_options=session_options,\n            )\n        except Exception as e:\n            self.clear_cache()\n            raise ModelArtefactError(\n                f\"Unable to load ONNX session. Cause: {e}\"\n            ) from e\n        logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. \"\n                        \"Check that you are using the correct docker image on a supported device. \"\n                        \"Export list of available providers as ONNXRUNTIME_EXECUTION_PROVIDERS environmental variable, \"\n                        \"consult documentation for more details.\"\n                    )\n\n        inputs = self.onnx_session.get_inputs()[0]\n        input_shape = inputs.shape\n        self.batch_size = input_shape[0]\n        self.img_size_h = input_shape[2]\n        self.img_size_w = input_shape[3]\n        self.input_name = inputs.name\n        if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n            if \"resize\" in self.preproc:\n                self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n            else:\n                self.img_size_h = 640\n                self.img_size_w = 640\n\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n\n        model_metadata = {\n            \"batch_size\": self.batch_size,\n            \"img_size_h\": self.img_size_h,\n            \"img_size_w\": self.img_size_w,\n        }\n        logger.debug(f\"Writing model metadata to memcache\")\n        self.write_model_metadata_to_memcache(model_metadata)\n        if not self.load_weights:  # had to load weights to get metadata\n            del self.onnx_session\n    else:\n        if not self.has_model_metadata:\n            raise ValueError(\n                \"This should be unreachable, should get weights if we don't have model metadata\"\n            )\n        logger.debug(f\"Loading model metadata from memcache\")\n        metadata = self.model_metadata_from_memcache()\n        self.batch_size = metadata[\"batch_size\"]\n        self.img_size_h = metadata[\"img_size_h\"]\n        self.img_size_w = metadata[\"img_size_w\"]\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n\n    logger.debug(\"Model initialisation finished.\")\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel","title":"<code>RoboflowCoreModel</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> <p>Base Roboflow inference model (Inherits from CvModel since all Roboflow models are CV models currently).</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class RoboflowCoreModel(RoboflowInferenceModel):\n    \"\"\"Base Roboflow inference model (Inherits from CvModel since all Roboflow models are CV models currently).\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        api_key=None,\n        **kwargs,\n    ):\n        \"\"\"Initializes the RoboflowCoreModel instance.\n\n        Args:\n            model_id (str): The identifier for the specific model.\n            api_key ([type], optional): The API key for authentication. Defaults to None.\n        \"\"\"\n        super().__init__(model_id, api_key=api_key, **kwargs)\n        self.download_weights()\n\n    def download_weights(self) -&gt; None:\n        \"\"\"Downloads the model weights from the configured source.\n\n        This method includes handling for AWS access keys and error handling.\n        \"\"\"\n        if MODELS_CACHE_AUTH_ENABLED:\n            if not _check_if_api_key_has_access_to_model(\n                api_key=self.api_key,\n                model_id=self.endpoint,\n                endpoint_type=ModelEndpointType.CORE_MODEL,\n            ):\n                raise RoboflowAPINotAuthorizedError(\n                    f\"API key {self.api_key} does not have access to model {self.endpoint}\"\n                )\n        infer_bucket_files = self.get_infer_bucket_file_list()\n        if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n            logger.debug(\"Model artifacts already downloaded, loading from cache\")\n            return None\n        if is_model_artefacts_bucket_available():\n            self.download_model_artefacts_from_s3()\n            return None\n        self.download_model_from_roboflow_api()\n\n    def download_model_from_roboflow_api(self) -&gt; None:\n        api_data = get_roboflow_model_data(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.CORE_MODEL,\n            device_id=self.device_id,\n        )\n        if \"weights\" not in api_data:\n            raise ModelArtefactError(\n                f\"`weights` key not available in Roboflow API response while downloading model weights.\"\n            )\n        for weights_url_key in api_data[\"weights\"]:\n            weights_url = api_data[\"weights\"][weights_url_key]\n            t1 = perf_counter()\n            model_weights_response = get_from_url(weights_url, json_response=False)\n            filename = weights_url.split(\"?\")[0].split(\"/\")[-1]\n            save_bytes_in_cache(\n                content=model_weights_response.content,\n                file=filename,\n                model_id=self.endpoint,\n            )\n            if perf_counter() - t1 &gt; 120:\n                logger.debug(\n                    \"Weights download took longer than 120 seconds, refreshing API request\"\n                )\n                api_data = get_roboflow_model_data(\n                    api_key=self.api_key,\n                    model_id=self.endpoint,\n                    endpoint_type=ModelEndpointType.CORE_MODEL,\n                    device_id=self.device_id,\n                )\n\n    def get_device_id(self) -&gt; str:\n        \"\"\"Returns the device ID associated with this model.\n\n        Returns:\n            str: The device ID.\n        \"\"\"\n        return self.device_id\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Abstract method to get the list of files to be downloaded from the inference bucket.\n\n        Raises:\n            NotImplementedError: This method must be implemented in subclasses.\n\n        Returns:\n            List[str]: A list of filenames.\n        \"\"\"\n        raise NotImplementedError(\n            \"get_infer_bucket_file_list not implemented for RoboflowCoreModel\"\n        )\n\n    def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"Abstract method to preprocess an image.\n\n        Raises:\n            NotImplementedError: This method must be implemented in subclasses.\n\n        Returns:\n            Image.Image: The preprocessed PIL image.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".preprocess_image\")\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Abstract property representing the file containing the model weights. For core models, all model artifacts are handled through get_infer_bucket_file_list method.\"\"\"\n        return None\n\n    @property\n    def model_artifact_bucket(self):\n        return CORE_MODEL_BUCKET\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Abstract property representing the file containing the model weights. For core models, all model artifacts are handled through get_infer_bucket_file_list method.</p>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.__init__","title":"<code>__init__(model_id, api_key=None, **kwargs)</code>","text":"<p>Initializes the RoboflowCoreModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier for the specific model.</p> required <code>api_key</code> <code>[type]</code> <p>The API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    api_key=None,\n    **kwargs,\n):\n    \"\"\"Initializes the RoboflowCoreModel instance.\n\n    Args:\n        model_id (str): The identifier for the specific model.\n        api_key ([type], optional): The API key for authentication. Defaults to None.\n    \"\"\"\n    super().__init__(model_id, api_key=api_key, **kwargs)\n    self.download_weights()\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.download_weights","title":"<code>download_weights()</code>","text":"<p>Downloads the model weights from the configured source.</p> <p>This method includes handling for AWS access keys and error handling.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def download_weights(self) -&gt; None:\n    \"\"\"Downloads the model weights from the configured source.\n\n    This method includes handling for AWS access keys and error handling.\n    \"\"\"\n    if MODELS_CACHE_AUTH_ENABLED:\n        if not _check_if_api_key_has_access_to_model(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.CORE_MODEL,\n        ):\n            raise RoboflowAPINotAuthorizedError(\n                f\"API key {self.api_key} does not have access to model {self.endpoint}\"\n            )\n    infer_bucket_files = self.get_infer_bucket_file_list()\n    if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n        logger.debug(\"Model artifacts already downloaded, loading from cache\")\n        return None\n    if is_model_artefacts_bucket_available():\n        self.download_model_artefacts_from_s3()\n        return None\n    self.download_model_from_roboflow_api()\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.get_device_id","title":"<code>get_device_id()</code>","text":"<p>Returns the device ID associated with this model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The device ID.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_device_id(self) -&gt; str:\n    \"\"\"Returns the device ID associated with this model.\n\n    Returns:\n        str: The device ID.\n    \"\"\"\n    return self.device_id\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Abstract method to get the list of files to be downloaded from the inference bucket.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in subclasses.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of filenames.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Abstract method to get the list of files to be downloaded from the inference bucket.\n\n    Raises:\n        NotImplementedError: This method must be implemented in subclasses.\n\n    Returns:\n        List[str]: A list of filenames.\n    \"\"\"\n    raise NotImplementedError(\n        \"get_infer_bucket_file_list not implemented for RoboflowCoreModel\"\n    )\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Abstract method to preprocess an image.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in subclasses.</p> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The preprocessed PIL image.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n    \"\"\"Abstract method to preprocess an image.\n\n    Raises:\n        NotImplementedError: This method must be implemented in subclasses.\n\n    Returns:\n        Image.Image: The preprocessed PIL image.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".preprocess_image\")\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel","title":"<code>RoboflowInferenceModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>Base Roboflow inference model.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class RoboflowInferenceModel(Model):\n    \"\"\"Base Roboflow inference model.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        cache_dir_root=MODEL_CACHE_DIR,\n        api_key=None,\n        load_weights=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the RoboflowInferenceModel object.\n\n        Args:\n            model_id (str): The unique identifier for the model.\n            cache_dir_root (str, optional): The root directory for the cache. Defaults to MODEL_CACHE_DIR.\n            api_key (str, optional): API key for authentication. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.load_weights = load_weights\n        self.metrics = {\"num_inferences\": 0, \"avg_inference_time\": 0.0}\n        self.api_key = api_key if api_key else API_KEY\n        model_id = resolve_roboflow_model_alias(model_id=model_id)\n        self.dataset_id, self.version_id = get_model_id_chunks(model_id=model_id)\n        self.endpoint = model_id\n        self.device_id = GLOBAL_DEVICE_ID\n        self.cache_dir = os.path.join(cache_dir_root, self.endpoint)\n        self.keypoints_metadata: Optional[dict] = None\n        initialise_cache(model_id=self.endpoint)\n\n    def cache_file(self, f: str) -&gt; str:\n        \"\"\"Get the cache file path for a given file.\n\n        Args:\n            f (str): Filename.\n\n        Returns:\n            str: Full path to the cached file.\n        \"\"\"\n        return get_cache_file_path(file=f, model_id=self.endpoint)\n\n    def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n        \"\"\"Clear the cache directory.\n\n        Args:\n            delete_from_disk (bool, optional): Whether to delete cached files from disk. Defaults to True.\n        \"\"\"\n        clear_cache(model_id=self.endpoint, delete_from_disk=delete_from_disk)\n\n    def draw_predictions(\n        self,\n        inference_request: InferenceRequest,\n        inference_response: InferenceResponse,\n    ) -&gt; bytes:\n        \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n        Args:\n            inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n            inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n        Returns:\n            str: A base64 encoded image string\n        \"\"\"\n        return draw_detection_predictions(\n            inference_request=inference_request,\n            inference_response=inference_response,\n            colors=self.colors,\n        )\n\n    @property\n    def get_class_names(self):\n        return self.class_names\n\n    def get_device_id(self) -&gt; str:\n        \"\"\"\n        Get the device identifier on which the model is deployed.\n\n        Returns:\n            str: Device identifier.\n        \"\"\"\n        return self.device_id\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Get a list of inference bucket files.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n\n        Returns:\n            List[str]: A list of inference bucket files.\n        \"\"\"\n        raise NotImplementedError(\n            self.__class__.__name__ + \".get_infer_bucket_file_list\"\n        )\n\n    @property\n    def cache_key(self):\n        return f\"metadata:{self.endpoint}\"\n\n    @staticmethod\n    def model_metadata_from_memcache_endpoint(endpoint):\n        model_metadata = cache.get(f\"metadata:{endpoint}\")\n        return model_metadata\n\n    def model_metadata_from_memcache(self):\n        model_metadata = cache.get(self.cache_key)\n        return model_metadata\n\n    def write_model_metadata_to_memcache(self, metadata):\n        cache.set(\n            self.cache_key, metadata, expire=MODEL_METADATA_CACHE_EXPIRATION_TIMEOUT\n        )\n\n    @property\n    def has_model_metadata(self):\n        return self.model_metadata_from_memcache() is not None\n\n    def get_model_artifacts(\n        self,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Fetch or load the model artifacts.\n\n        Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.\n        \"\"\"\n        if MODELS_CACHE_AUTH_ENABLED:\n            if not _check_if_api_key_has_access_to_model(\n                api_key=self.api_key,\n                model_id=self.endpoint,\n                endpoint_type=ModelEndpointType.ORT,\n                countinference=countinference,\n                service_secret=service_secret,\n            ):\n                raise RoboflowAPINotAuthorizedError(\n                    f\"API key {self.api_key} does not have access to model {self.endpoint}\"\n                )\n        self.cache_model_artefacts(\n            countinference=countinference,\n            service_secret=service_secret,\n            **kwargs,\n        )\n        self.load_model_artifacts_from_cache()\n\n    def cache_model_artefacts(\n        self,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        infer_bucket_files = self.get_all_required_infer_bucket_file()\n\n        if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n            return None\n        if is_model_artefacts_bucket_available():\n            self.download_model_artefacts_from_s3()\n            return None\n        self.download_model_artifacts_from_roboflow_api(\n            countinference=countinference,\n            service_secret=service_secret,\n            **kwargs,\n        )\n\n    def get_all_required_infer_bucket_file(self) -&gt; List[str]:\n        infer_bucket_files = self.get_infer_bucket_file_list()\n        infer_bucket_files.append(self.weights_file)\n        logger.debug(f\"List of files required to load model: {infer_bucket_files}\")\n        return [f for f in infer_bucket_files if f is not None]\n\n    def download_model_artefacts_from_s3(self) -&gt; None:\n        try:\n            logger.debug(\"Downloading model artifacts from S3\")\n            infer_bucket_files = self.get_all_required_infer_bucket_file()\n            cache_directory = get_cache_dir()\n            s3_keys = [f\"{self.endpoint}/{file}\" for file in infer_bucket_files]\n            download_s3_files_to_directory(\n                bucket=self.model_artifact_bucket,\n                keys=s3_keys,\n                target_dir=cache_directory,\n                s3_client=S3_CLIENT,\n            )\n        except Exception as error:\n            raise ModelArtefactError(\n                f\"Could not obtain model artefacts from S3 with keys {s3_keys}. Cause: {error}\"\n            ) from error\n\n    @property\n    def model_artifact_bucket(self):\n        return INFER_BUCKET\n\n    def download_model_artifacts_from_roboflow_api(\n        self,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n        **kwargs,\n    ) -&gt; None:\n        logger.debug(\"Downloading model artifacts from Roboflow API\")\n\n        # Use the same lock file pattern as in clear_cache\n        lock_dir = MODEL_CACHE_DIR + \"/_file_locks\"  # Dedicated lock directory\n        os.makedirs(lock_dir, exist_ok=True)  # Ensure lock directory exists.\n        lock_file = os.path.join(lock_dir, f\"{os.path.basename(self.cache_dir)}.lock\")\n        try:\n            lock = FileLock(lock_file, timeout=120)  # 120 second timeout for downloads\n            with lock:\n                if self.version_id is not None:\n                    api_data = get_roboflow_model_data(\n                        api_key=self.api_key,\n                        model_id=self.endpoint,\n                        endpoint_type=ModelEndpointType.ORT,\n                        device_id=self.device_id,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    if \"ort\" not in api_data.keys():\n                        raise ModelArtefactError(\n                            \"Could not find `ort` key in roboflow API model description response.\"\n                        )\n                    api_data = api_data[\"ort\"]\n                    if \"classes\" in api_data:\n                        save_text_lines_in_cache(\n                            content=api_data[\"classes\"],\n                            file=\"class_names.txt\",\n                            model_id=self.endpoint,\n                        )\n                    if \"model\" not in api_data:\n                        raise ModelArtefactError(\n                            \"Could not find `model` key in roboflow API model description response.\"\n                        )\n                    if \"environment\" not in api_data:\n                        raise ModelArtefactError(\n                            \"Could not find `environment` key in roboflow API model description response.\"\n                        )\n                    environment = get_from_url(\n                        api_data[\"environment\"], verify_content_length=True\n                    )\n                    model_weights_response = get_from_url(\n                        api_data[\"model\"],\n                        json_response=False,\n                        verify_content_length=True,\n                    )\n                else:\n                    api_data = get_roboflow_instant_model_data(\n                        api_key=self.api_key,\n                        model_id=self.endpoint,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    if (\n                        \"modelFiles\" not in api_data\n                        or \"ort\" not in api_data[\"modelFiles\"]\n                        or \"model\" not in api_data[\"modelFiles\"][\"ort\"]\n                    ):\n                        raise ModelArtefactError(\n                            \"Could not find `modelFiles` key or `modelFiles`.`ort` or `modelFiles`.`ort`.`model` key in roboflow API model description response.\"\n                        )\n                    if \"environment\" not in api_data:\n                        raise ModelArtefactError(\n                            \"Could not find `environment` key in roboflow API model description response.\"\n                        )\n                    model_weights_response = get_from_url(\n                        api_data[\"modelFiles\"][\"ort\"][\"model\"],\n                        json_response=False,\n                        verify_content_length=True,\n                    )\n                    environment = api_data[\"environment\"]\n                    if \"classes\" in api_data:\n                        save_text_lines_in_cache(\n                            content=api_data[\"classes\"],\n                            file=\"class_names.txt\",\n                            model_id=self.endpoint,\n                        )\n\n                save_bytes_in_cache(\n                    content=model_weights_response.content,\n                    file=self.weights_file,\n                    model_id=self.endpoint,\n                )\n                if \"colors\" in api_data:\n                    environment[\"COLORS\"] = api_data[\"colors\"]\n                save_json_in_cache(\n                    content=environment,\n                    file=\"environment.json\",\n                    model_id=self.endpoint,\n                )\n                if \"keypoints_metadata\" in api_data:\n                    # TODO: make sure backend provides that\n                    save_json_in_cache(\n                        content=api_data[\"keypoints_metadata\"],\n                        file=\"keypoints_metadata.json\",\n                        model_id=self.endpoint,\n                    )\n        except Exception as e:\n            logger.error(f\"Error downloading model artifacts: {e}\")\n            raise\n        finally:\n            try:\n                if os.path.exists(lock_file):\n                    os.unlink(lock_file)  # Clean up lock file\n            except OSError:\n                pass  # Best effort cleanup\n\n    def load_model_artifacts_from_cache(self) -&gt; None:\n        logger.debug(\"Model artifacts already downloaded, loading model from cache\")\n        infer_bucket_files = self.get_all_required_infer_bucket_file()\n        if \"environment.json\" in infer_bucket_files:\n            self.environment = load_json_from_cache(\n                file=\"environment.json\",\n                model_id=self.endpoint,\n                object_pairs_hook=OrderedDict,\n            )\n        if \"class_names.txt\" in infer_bucket_files:\n            self.class_names = load_text_file_from_cache(\n                file=\"class_names.txt\",\n                model_id=self.endpoint,\n                split_lines=True,\n                strip_white_chars=True,\n            )\n        else:\n            self.class_names = get_class_names_from_environment_file(\n                environment=self.environment\n            )\n        self.colors = get_color_mapping_from_environment(\n            environment=self.environment,\n            class_names=self.class_names,\n        )\n        if \"keypoints_metadata.json\" in infer_bucket_files:\n            self.keypoints_metadata = parse_keypoints_metadata(\n                load_json_from_cache(\n                    file=\"keypoints_metadata.json\",\n                    model_id=self.endpoint,\n                    object_pairs_hook=OrderedDict,\n                )\n            )\n        self.num_classes = len(self.class_names)\n        if \"PREPROCESSING\" not in self.environment:\n            raise ModelArtefactError(\n                \"Could not find `PREPROCESSING` key in environment file.\"\n            )\n        if issubclass(type(self.environment[\"PREPROCESSING\"]), dict):\n            self.preproc = self.environment[\"PREPROCESSING\"]\n        else:\n            self.preproc = json.loads(self.environment[\"PREPROCESSING\"])\n        if self.preproc.get(\"resize\"):\n            self.resize_method = self.preproc[\"resize\"].get(\"format\", \"Stretch to\")\n            if self.resize_method in [\n                \"Fit (reflect edges) in\",\n                \"Fit within\",\n                \"Fill (with center crop) in\",\n            ]:\n                fallback_resize_method = \"Fit (black edges) in\"\n                logger.warning(\n                    \"Unsupported resize method '%s', defaulting to '%s' - this may result in degraded model performance.\",\n                    self.resize_method,\n                    fallback_resize_method,\n                )\n                self.resize_method = fallback_resize_method\n            if self.resize_method not in [\n                \"Stretch to\",\n                \"Fit (black edges) in\",\n                \"Fit (grey edges) in\",\n                \"Fit (white edges) in\",\n            ]:\n                logger.error(\n                    \"Unsupported resize method '%s', defaulting to 'Stretch to' - this may result in degraded model performance.\",\n                    self.resize_method,\n                )\n                self.resize_method = \"Stretch to\"\n        else:\n            logger.error(\n                \"Unknown resize method, defaulting to 'Stretch to' - this may result in degraded model performance.\"\n            )\n            self.resize_method = \"Stretch to\"\n        logger.debug(f\"Resize method is '{self.resize_method}'\")\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    def initialize_model(self, **kwargs) -&gt; None:\n        \"\"\"Initialize the model.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".initialize_model\")\n\n    def preproc_image(\n        self,\n        image: Union[Any, InferenceRequestImage],\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        \"\"\"\n        Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n        Args:\n            image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n        \"\"\"\n        np_image, is_bgr = load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient\n            or \"auto-orient\" not in self.preproc.keys()\n            or DISABLE_PREPROC_AUTO_ORIENT,\n        )\n        preprocessed_image, img_dims = self.preprocess_image(\n            np_image,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        if USE_PYTORCH_FOR_PREPROCESSING:\n            preprocessed_image = torch.from_numpy(\n                np.ascontiguousarray(preprocessed_image)\n            )\n            if torch.cuda.is_available():\n                preprocessed_image = preprocessed_image.cuda()\n            preprocessed_image = (\n                preprocessed_image.permute(2, 0, 1).unsqueeze(0).contiguous().float()\n            )\n        if self.resize_method == \"Stretch to\":\n            if isinstance(preprocessed_image, np.ndarray):\n                preprocessed_image = preprocessed_image.astype(np.float32)\n                resized = cv2.resize(\n                    preprocessed_image,\n                    (self.img_size_w, self.img_size_h),\n                )\n            elif USE_PYTORCH_FOR_PREPROCESSING:\n                resized = torch.nn.functional.interpolate(\n                    preprocessed_image,\n                    size=(self.img_size_h, self.img_size_w),\n                    mode=\"bilinear\",\n                )\n            else:\n                raise ValueError(\n                    f\"Received an image of unknown type, {type(preprocessed_image)}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n\n        elif self.resize_method == \"Fit (black edges) in\":\n            resized = letterbox_image(\n                preprocessed_image, (self.img_size_w, self.img_size_h)\n            )\n        elif self.resize_method == \"Fit (white edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(255, 255, 255),\n            )\n        elif self.resize_method == \"Fit (grey edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(114, 114, 114),\n            )\n\n        if is_bgr:\n            if isinstance(resized, np.ndarray):\n                resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n            else:\n                resized = resized[:, [2, 1, 0], :, :]\n\n        if isinstance(resized, np.ndarray):\n            img_in = np.transpose(resized, (2, 0, 1))\n            img_in = img_in.astype(np.float32)\n            img_in = np.expand_dims(img_in, axis=0)\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            img_in = resized.float()\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(resized)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n        return img_in, img_dims\n\n    def preprocess_image(\n        self,\n        image: np.ndarray,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        \"\"\"\n        Preprocesses the given image using specified preprocessing steps.\n\n        Args:\n            image (Image.Image): The PIL image to preprocess.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Image.Image: The preprocessed PIL image.\n        \"\"\"\n        return prepare(\n            image,\n            self.preproc,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Abstract property representing the file containing the model weights.\n\n        Raises:\n            NotImplementedError: This property must be implemented in subclasses.\n\n        Returns:\n            str: The file path to the weights file.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".weights_file\")\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Abstract property representing the file containing the model weights.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This property must be implemented in subclasses.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path to the weights file.</p>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.__init__","title":"<code>__init__(model_id, cache_dir_root=MODEL_CACHE_DIR, api_key=None, load_weights=True, **kwargs)</code>","text":"<p>Initialize the RoboflowInferenceModel object.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The unique identifier for the model.</p> required <code>cache_dir_root</code> <code>str</code> <p>The root directory for the cache. Defaults to MODEL_CACHE_DIR.</p> <code>MODEL_CACHE_DIR</code> <code>api_key</code> <code>str</code> <p>API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    cache_dir_root=MODEL_CACHE_DIR,\n    api_key=None,\n    load_weights=True,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the RoboflowInferenceModel object.\n\n    Args:\n        model_id (str): The unique identifier for the model.\n        cache_dir_root (str, optional): The root directory for the cache. Defaults to MODEL_CACHE_DIR.\n        api_key (str, optional): API key for authentication. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.load_weights = load_weights\n    self.metrics = {\"num_inferences\": 0, \"avg_inference_time\": 0.0}\n    self.api_key = api_key if api_key else API_KEY\n    model_id = resolve_roboflow_model_alias(model_id=model_id)\n    self.dataset_id, self.version_id = get_model_id_chunks(model_id=model_id)\n    self.endpoint = model_id\n    self.device_id = GLOBAL_DEVICE_ID\n    self.cache_dir = os.path.join(cache_dir_root, self.endpoint)\n    self.keypoints_metadata: Optional[dict] = None\n    initialise_cache(model_id=self.endpoint)\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.cache_file","title":"<code>cache_file(f)</code>","text":"<p>Get the cache file path for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>Filename.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Full path to the cached file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def cache_file(self, f: str) -&gt; str:\n    \"\"\"Get the cache file path for a given file.\n\n    Args:\n        f (str): Filename.\n\n    Returns:\n        str: Full path to the cached file.\n    \"\"\"\n    return get_cache_file_path(file=f, model_id=self.endpoint)\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.clear_cache","title":"<code>clear_cache(delete_from_disk=True)</code>","text":"<p>Clear the cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>delete_from_disk</code> <code>bool</code> <p>Whether to delete cached files from disk. Defaults to True.</p> <code>True</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n    \"\"\"Clear the cache directory.\n\n    Args:\n        delete_from_disk (bool, optional): Whether to delete cached files from disk. Defaults to True.\n    \"\"\"\n    clear_cache(model_id=self.endpoint, delete_from_disk=delete_from_disk)\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw predictions from an inference response onto the original image provided by an inference request</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>ObjectDetectionInferenceRequest</code> <p>The inference request containing the image on which to draw predictions</p> required <code>inference_response</code> <code>ObjectDetectionInferenceResponse</code> <p>The inference response containing predictions to be drawn</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>bytes</code> <p>A base64 encoded image string</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def draw_predictions(\n    self,\n    inference_request: InferenceRequest,\n    inference_response: InferenceResponse,\n) -&gt; bytes:\n    \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n    Args:\n        inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n        inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n    Returns:\n        str: A base64 encoded image string\n    \"\"\"\n    return draw_detection_predictions(\n        inference_request=inference_request,\n        inference_response=inference_response,\n        colors=self.colors,\n    )\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_device_id","title":"<code>get_device_id()</code>","text":"<p>Get the device identifier on which the model is deployed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Device identifier.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_device_id(self) -&gt; str:\n    \"\"\"\n    Get the device identifier on which the model is deployed.\n\n    Returns:\n        str: Device identifier.\n    \"\"\"\n    return self.device_id\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get a list of inference bucket files.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of inference bucket files.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Get a list of inference bucket files.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n\n    Returns:\n        List[str]: A list of inference bucket files.\n    \"\"\"\n    raise NotImplementedError(\n        self.__class__.__name__ + \".get_infer_bucket_file_list\"\n    )\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_model_artifacts","title":"<code>get_model_artifacts(countinference=None, service_secret=None, **kwargs)</code>","text":"<p>Fetch or load the model artifacts.</p> <p>Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_model_artifacts(\n    self,\n    countinference: Optional[bool] = None,\n    service_secret: Optional[str] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Fetch or load the model artifacts.\n\n    Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.\n    \"\"\"\n    if MODELS_CACHE_AUTH_ENABLED:\n        if not _check_if_api_key_has_access_to_model(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.ORT,\n            countinference=countinference,\n            service_secret=service_secret,\n        ):\n            raise RoboflowAPINotAuthorizedError(\n                f\"API key {self.api_key} does not have access to model {self.endpoint}\"\n            )\n    self.cache_model_artefacts(\n        countinference=countinference,\n        service_secret=service_secret,\n        **kwargs,\n    )\n    self.load_model_artifacts_from_cache()\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.initialize_model","title":"<code>initialize_model(**kwargs)</code>","text":"<p>Initialize the model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def initialize_model(self, **kwargs) -&gt; None:\n    \"\"\"Initialize the model.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".initialize_model\")\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.preproc_image","title":"<code>preproc_image(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Any, InferenceRequestImage]</code> <p>An object containing information necessary to load the image for inference.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preproc_image(\n    self,\n    image: Union[Any, InferenceRequestImage],\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n    Args:\n        image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n    \"\"\"\n    np_image, is_bgr = load_image(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient\n        or \"auto-orient\" not in self.preproc.keys()\n        or DISABLE_PREPROC_AUTO_ORIENT,\n    )\n    preprocessed_image, img_dims = self.preprocess_image(\n        np_image,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    if USE_PYTORCH_FOR_PREPROCESSING:\n        preprocessed_image = torch.from_numpy(\n            np.ascontiguousarray(preprocessed_image)\n        )\n        if torch.cuda.is_available():\n            preprocessed_image = preprocessed_image.cuda()\n        preprocessed_image = (\n            preprocessed_image.permute(2, 0, 1).unsqueeze(0).contiguous().float()\n        )\n    if self.resize_method == \"Stretch to\":\n        if isinstance(preprocessed_image, np.ndarray):\n            preprocessed_image = preprocessed_image.astype(np.float32)\n            resized = cv2.resize(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n            )\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            resized = torch.nn.functional.interpolate(\n                preprocessed_image,\n                size=(self.img_size_h, self.img_size_w),\n                mode=\"bilinear\",\n            )\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(preprocessed_image)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n    elif self.resize_method == \"Fit (black edges) in\":\n        resized = letterbox_image(\n            preprocessed_image, (self.img_size_w, self.img_size_h)\n        )\n    elif self.resize_method == \"Fit (white edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(255, 255, 255),\n        )\n    elif self.resize_method == \"Fit (grey edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(114, 114, 114),\n        )\n\n    if is_bgr:\n        if isinstance(resized, np.ndarray):\n            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        else:\n            resized = resized[:, [2, 1, 0], :, :]\n\n    if isinstance(resized, np.ndarray):\n        img_in = np.transpose(resized, (2, 0, 1))\n        img_in = img_in.astype(np.float32)\n        img_in = np.expand_dims(img_in, axis=0)\n    elif USE_PYTORCH_FOR_PREPROCESSING:\n        img_in = resized.float()\n    else:\n        raise ValueError(\n            f\"Received an image of unknown type, {type(resized)}; \"\n            \"This is most likely a bug. Contact Roboflow team through github issues \"\n            \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n        )\n\n    return img_in, img_dims\n</code></pre>"},{"location":"reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.preprocess_image","title":"<code>preprocess_image(image, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses the given image using specified preprocessing steps.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL image to preprocess.</p> required <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Image.Image: The preprocessed PIL image.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preprocess_image(\n    self,\n    image: np.ndarray,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Preprocesses the given image using specified preprocessing steps.\n\n    Args:\n        image (Image.Image): The PIL image to preprocess.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Image.Image: The preprocessed PIL image.\n    \"\"\"\n    return prepare(\n        image,\n        self.preproc,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n</code></pre>"},{"location":"reference/inference/core/models/utils/keypoints/","title":"Keypoints","text":""},{"location":"reference/inference/core/models/utils/keypoints/#inference.core.models.utils.keypoints.superset_keypoints_count","title":"<code>superset_keypoints_count(keypoints_metadata={})</code>","text":"<p>Returns the number of keypoints in the superset.</p> Source code in <code>inference/core/models/utils/keypoints.py</code> <pre><code>def superset_keypoints_count(keypoints_metadata={}) -&gt; int:\n    \"\"\"Returns the number of keypoints in the superset.\"\"\"\n    max_keypoints = 0\n    for keypoints in keypoints_metadata.values():\n        if len(keypoints) &gt; max_keypoints:\n            max_keypoints = len(keypoints)\n    return max_keypoints\n</code></pre>"},{"location":"reference/inference/core/registries/base/","title":"Base","text":""},{"location":"reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry","title":"<code>ModelRegistry</code>","text":"<p>An object which is able to return model classes based on given model IDs and model types.</p> <p>Attributes:</p> Name Type Description <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>class ModelRegistry:\n    \"\"\"An object which is able to return model classes based on given model IDs and model types.\n\n    Attributes:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n\n    def __init__(self, registry_dict) -&gt; None:\n        \"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n        Args:\n            registry_dict (dict): A dictionary mapping model types to model classes.\n        \"\"\"\n        self.registry_dict = registry_dict\n\n    def get_model(\n        self,\n        model_type: str,\n        model_id: str,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Returns the model class based on the given model type.\n\n        Args:\n            model_type (str): The type of the model to be retrieved.\n            model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n        Returns:\n            Model: The model class corresponding to the given model type.\n\n        Raises:\n            ModelNotRecognisedError: If the model_type is not found in the registry_dict.\n        \"\"\"\n        if model_type not in self.registry_dict:\n            raise ModelNotRecognisedError(\n                f\"Could not find model of type: {model_type} in configured registry.\"\n            )\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry.__init__","title":"<code>__init__(registry_dict)</code>","text":"<p>Initializes the ModelRegistry with the given dictionary of registered models.</p> <p>Parameters:</p> Name Type Description Default <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> required Source code in <code>inference/core/registries/base.py</code> <pre><code>def __init__(self, registry_dict) -&gt; None:\n    \"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n    Args:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n    self.registry_dict = registry_dict\n</code></pre>"},{"location":"reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry.get_model","title":"<code>get_model(model_type, model_id, **kwargs)</code>","text":"<p>Returns the model class based on the given model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of the model to be retrieved.</p> required <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved (unused in the current implementation).</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model type.</p> <p>Raises:</p> Type Description <code>ModelNotRecognisedError</code> <p>If the model_type is not found in the registry_dict.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>def get_model(\n    self,\n    model_type: str,\n    model_id: str,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Returns the model class based on the given model type.\n\n    Args:\n        model_type (str): The type of the model to be retrieved.\n        model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n    Returns:\n        Model: The model class corresponding to the given model type.\n\n    Raises:\n        ModelNotRecognisedError: If the model_type is not found in the registry_dict.\n    \"\"\"\n    if model_type not in self.registry_dict:\n        raise ModelNotRecognisedError(\n            f\"Could not find model of type: {model_type} in configured registry.\"\n        )\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"reference/inference/core/registries/roboflow/","title":"Roboflow","text":""},{"location":"reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry","title":"<code>RoboflowModelRegistry</code>","text":"<p>               Bases: <code>ModelRegistry</code></p> <p>A Roboflow-specific model registry which gets the model type using the model id, then returns a model class based on the model type.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>class RoboflowModelRegistry(ModelRegistry):\n    \"\"\"A Roboflow-specific model registry which gets the model type using the model id,\n    then returns a model class based on the model type.\n    \"\"\"\n\n    def get_model(\n        self,\n        model_id: ModelID,\n        api_key: str,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n    ) -&gt; Model:\n        \"\"\"Returns the model class based on the given model id and API key.\n\n        Args:\n            model_id (str): The ID of the model to be retrieved.\n            api_key (str): The API key used to authenticate.\n\n        Returns:\n            Model: The model class corresponding to the given model ID and type.\n\n        Raises:\n            ModelNotRecognisedError: If the model type is not supported or found.\n        \"\"\"\n        model_type = get_model_type(\n            model_id,\n            api_key,\n            countinference=countinference,\n            service_secret=service_secret,\n        )\n        logger.debug(f\"Model type: {model_type}\")\n\n        if model_type not in self.registry_dict:\n            raise ModelNotRecognisedError(\n                f\"Model type not supported, you may want to try a different inference server configuration or endpoint: {model_type}\"\n            )\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry.get_model","title":"<code>get_model(model_id, api_key, countinference=None, service_secret=None)</code>","text":"<p>Returns the model class based on the given model id and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model ID and type.</p> <p>Raises:</p> Type Description <code>ModelNotRecognisedError</code> <p>If the model type is not supported or found.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model(\n    self,\n    model_id: ModelID,\n    api_key: str,\n    countinference: Optional[bool] = None,\n    service_secret: Optional[str] = None,\n) -&gt; Model:\n    \"\"\"Returns the model class based on the given model id and API key.\n\n    Args:\n        model_id (str): The ID of the model to be retrieved.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        Model: The model class corresponding to the given model ID and type.\n\n    Raises:\n        ModelNotRecognisedError: If the model type is not supported or found.\n    \"\"\"\n    model_type = get_model_type(\n        model_id,\n        api_key,\n        countinference=countinference,\n        service_secret=service_secret,\n    )\n    logger.debug(f\"Model type: {model_type}\")\n\n    if model_type not in self.registry_dict:\n        raise ModelNotRecognisedError(\n            f\"Model type not supported, you may want to try a different inference server configuration or endpoint: {model_type}\"\n        )\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.get_model_type","title":"<code>get_model_type(model_id, api_key=None, countinference=None, service_secret=None)</code>","text":"<p>Retrieves the model type based on the given model ID and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[TaskType, ModelType]</code> <p>The project task type and the model type.</p> <p>Raises:</p> Type Description <code>WorkspaceLoadError</code> <p>If the workspace could not be loaded or if the API key is invalid.</p> <code>DatasetLoadError</code> <p>If the dataset could not be loaded due to invalid ID, workspace ID or version ID.</p> <code>MissingDefaultModelError</code> <p>If default model is not configured and API does not provide this info</p> <code>MalformedRoboflowAPIResponseError</code> <p>Roboflow API responds in invalid format.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model_type(\n    model_id: ModelID,\n    api_key: Optional[str] = None,\n    countinference: Optional[bool] = None,\n    service_secret: Optional[str] = None,\n) -&gt; Tuple[TaskType, ModelType]:\n    \"\"\"Retrieves the model type based on the given model ID and API key.\n\n    Args:\n        model_id (str): The ID of the model.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        tuple: The project task type and the model type.\n\n    Raises:\n        WorkspaceLoadError: If the workspace could not be loaded or if the API key is invalid.\n        DatasetLoadError: If the dataset could not be loaded due to invalid ID, workspace ID or version ID.\n        MissingDefaultModelError: If default model is not configured and API does not provide this info\n        MalformedRoboflowAPIResponseError: Roboflow API responds in invalid format.\n    \"\"\"\n    model_id = resolve_roboflow_model_alias(model_id=model_id)\n    dataset_id, version_id = get_model_id_chunks(model_id=model_id)\n\n    if dataset_id in GENERIC_MODELS:\n        logger.debug(f\"Loading generic model: {dataset_id}.\")\n        return GENERIC_MODELS[dataset_id]\n\n    if MODELS_CACHE_AUTH_ENABLED:\n        if not _check_if_api_key_has_access_to_model(\n            api_key=api_key,\n            model_id=model_id,\n            countinference=countinference,\n            service_secret=service_secret,\n        ):\n            raise RoboflowAPINotAuthorizedError(\n                f\"API key {api_key} does not have access to model {model_id}\"\n            )\n\n    cached_metadata = get_model_metadata_from_cache(\n        dataset_id=dataset_id, version_id=version_id\n    )\n\n    if cached_metadata is not None:\n        return cached_metadata[0], cached_metadata[1]\n    if version_id == STUB_VERSION_ID:\n        if api_key is None:\n            raise MissingApiKeyError(\n                \"Stub model version provided but no API key was provided. API key is required to load stub models.\"\n            )\n        workspace_id = get_roboflow_workspace(api_key=api_key)\n        project_task_type = get_roboflow_dataset_type(\n            api_key=api_key, workspace_id=workspace_id, dataset_id=dataset_id\n        )\n        model_type = \"stub\"\n        save_model_metadata_in_cache(\n            dataset_id=dataset_id,\n            version_id=version_id,\n            project_task_type=project_task_type,\n            model_type=model_type,\n        )\n        return project_task_type, model_type\n\n    if version_id is not None:\n        api_data = get_roboflow_model_data(\n            api_key=api_key,\n            model_id=model_id,\n            countinference=countinference,\n            service_secret=service_secret,\n            endpoint_type=ModelEndpointType.ORT,\n            device_id=GLOBAL_DEVICE_ID,\n        ).get(\"ort\")\n        project_task_type = api_data.get(\"type\", \"object-detection\")\n    else:\n        api_data = get_roboflow_instant_model_data(\n            api_key=api_key,\n            model_id=model_id,\n            countinference=countinference,\n            service_secret=service_secret,\n        )\n        project_task_type = api_data.get(\"taskType\", \"object-detection\")\n    if api_data is None:\n        raise ModelArtefactError(\"Error loading model artifacts from Roboflow API.\")\n\n    # some older projects do not have type field - hence defaulting\n    model_type = api_data.get(\"modelType\")\n    if model_type is None or model_type == \"ort\":\n        # some very old model versions do not have modelType reported - and API respond in a generic way -\n        # then we shall attempt using default model for given task type\n        model_type = MODEL_TYPE_DEFAULTS.get(project_task_type)\n    if model_type is None or project_task_type is None:\n        raise ModelArtefactError(\"Error loading model artifacts from Roboflow API.\")\n    save_model_metadata_in_cache(\n        dataset_id=dataset_id,\n        version_id=version_id,\n        project_task_type=project_task_type,\n        model_type=model_type,\n    )\n\n    return project_task_type, model_type\n</code></pre>"},{"location":"reference/inference/core/utils/container/","title":"Container","text":""},{"location":"reference/inference/core/utils/container/#inference.core.utils.container.is_docker_socket_mounted","title":"<code>is_docker_socket_mounted(docker_socket_path)</code>","text":"<p>Check if the given path is a mounted Docker socket.</p> <p>Parameters:</p> Name Type Description Default <code>docker_socket_path</code> <code>str</code> <p>The path to the socket file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the path is a Unix socket, False otherwise.</p> Source code in <code>inference/core/utils/container.py</code> <pre><code>def is_docker_socket_mounted(docker_socket_path: str) -&gt; bool:\n    \"\"\"\n    Check if the given path is a mounted Docker socket.\n\n    Args:\n        docker_socket_path (str): The path to the socket file.\n\n    Returns:\n        bool: True if the path is a Unix socket, False otherwise.\n    \"\"\"\n    if os.path.exists(docker_socket_path):\n        socket_stat = os.stat(docker_socket_path)\n        if stat.S_ISSOCK(socket_stat.st_mode):\n            return True\n    return False\n</code></pre>"},{"location":"reference/inference/core/utils/environment/","title":"Environment","text":""},{"location":"reference/inference/core/utils/environment/#inference.core.utils.environment.safe_env_to_type","title":"<code>safe_env_to_type(variable_name, default_value=None, type_constructor=None)</code>","text":"<p>Converts env variable to specified type, but only if variable is set - otherwise default is returned. If <code>type_constructor</code> is not given - value of type str will be returned.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def safe_env_to_type(\n    variable_name: str,\n    default_value: Optional[T] = None,\n    type_constructor: Optional[Union[Type[T], Callable[[str], T]]] = None,\n) -&gt; Optional[T]:\n    \"\"\"\n    Converts env variable to specified type, but only if variable is set - otherwise default is returned.\n    If `type_constructor` is not given - value of type str will be returned.\n    \"\"\"\n    if variable_name not in os.environ:\n        return default_value\n    variable_value = os.environ[variable_name]\n    if type_constructor is None:\n        return variable_value\n    return type_constructor(variable_value)\n</code></pre>"},{"location":"reference/inference/core/utils/environment/#inference.core.utils.environment.safe_split_value","title":"<code>safe_split_value(value, delimiter=',')</code>","text":"<p>Splits a separated environment variable into a list.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The environment variable value to be split.</p> required <code>delimiter</code> <code>str</code> <p>Delimiter to be used</p> <code>','</code> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list or None: The split values as a list, or None if the input is None.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def safe_split_value(value: Optional[str], delimiter: str = \",\") -&gt; Optional[List[str]]:\n    \"\"\"\n    Splits a separated environment variable into a list.\n\n    Args:\n        value (str): The environment variable value to be split.\n        delimiter(str): Delimiter to be used\n\n    Returns:\n        list or None: The split values as a list, or None if the input is None.\n    \"\"\"\n    if value is None:\n        return None\n    else:\n        return value.split(delimiter)\n</code></pre>"},{"location":"reference/inference/core/utils/environment/#inference.core.utils.environment.str2bool","title":"<code>str2bool(value)</code>","text":"<p>Converts an environment variable to a boolean value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str or bool</code> <p>The environment variable value to be converted.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>The converted boolean value.</p> <p>Raises:</p> Type Description <code>InvalidEnvironmentVariableError</code> <p>If the value is not 'true', 'false', or a boolean.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def str2bool(value: Any) -&gt; bool:\n    \"\"\"\n    Converts an environment variable to a boolean value.\n\n    Args:\n        value (str or bool): The environment variable value to be converted.\n\n    Returns:\n        bool: The converted boolean value.\n\n    Raises:\n        InvalidEnvironmentVariableError: If the value is not 'true', 'false', or a boolean.\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n    if not issubclass(type(value), str):\n        raise InvalidEnvironmentVariableError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n    if value.lower() == \"true\":\n        return True\n    elif value.lower() == \"false\":\n        return False\n    else:\n        raise InvalidEnvironmentVariableError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n</code></pre>"},{"location":"reference/inference/core/utils/file_system/","title":"File system","text":""},{"location":"reference/inference/core/utils/file_system/#inference.core.utils.file_system.AtomicPath","title":"<code>AtomicPath</code>","text":"<p>Context manager for atomic file writes.</p> <p>Ensures that files are either written completely or not at all, preventing partial/corrupted files from power failures or crashes.</p> Usage <p>with AtomicPath(target_path, allow_override=False) as temp_path:     # Write to temp_path     with open(temp_path, 'w') as f:         f.write(data)</p> Source code in <code>inference/core/utils/file_system.py</code> <pre><code>class AtomicPath:\n    \"\"\"Context manager for atomic file writes.\n\n    Ensures that files are either written completely or not at all,\n    preventing partial/corrupted files from power failures or crashes.\n\n    Usage:\n        with AtomicPath(target_path, allow_override=False) as temp_path:\n            # Write to temp_path\n            with open(temp_path, 'w') as f:\n                f.write(data)\n        # File is atomically moved to target_path on successful exit\n    \"\"\"\n\n    def __init__(self, target_path: str, allow_override: bool = False):\n        self.target_path = target_path\n        self.allow_override = allow_override\n        self.temp_path: Optional[str] = None\n        self.temp_file = None\n\n    def __enter__(self) -&gt; str:\n        ensure_write_is_allowed(\n            path=self.target_path, allow_override=self.allow_override\n        )\n        ensure_parent_dir_exists(path=self.target_path)\n\n        dir_name = os.path.dirname(os.path.abspath(self.target_path))\n        base_name = os.path.basename(self.target_path)\n        self.temp_file = tempfile.NamedTemporaryFile(\n            dir=dir_name, prefix=\".tmp_\", suffix=\"_\" + base_name, delete=False\n        )\n        self.temp_path = self.temp_file.name\n        self.temp_file.close()\n        return self.temp_path\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is None:\n            try:\n                if os.name == \"nt\":  # Windows\n                    if os.path.exists(self.target_path):\n                        os.remove(self.target_path)\n                    os.rename(self.temp_path, self.target_path)\n                else:  # POSIX\n                    os.replace(self.temp_path, self.target_path)\n            except Exception:\n                try:\n                    os.unlink(self.temp_path)\n                except OSError:\n                    pass\n                raise\n        else:\n            # Error occurred - clean up temp file\n            try:\n                os.unlink(self.temp_path)\n            except OSError:\n                pass\n        return False  # Don't suppress exceptions\n</code></pre>"},{"location":"reference/inference/core/utils/file_system/#inference.core.utils.file_system.AtomicPath--file-is-atomically-moved-to-target_path-on-successful-exit","title":"File is atomically moved to target_path on successful exit","text":""},{"location":"reference/inference/core/utils/image_utils/","title":"Image utils","text":""},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.attempt_loading_image_from_string","title":"<code>attempt_loading_image_from_string(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Attempt to load an image from a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, bytes, bytearray, _IOBase]</code> <p>The image data in string format.</p> required <code>cv_imread_flags</code> <code>int</code> <p>OpenCV flags used for image reading.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: A tuple of the loaded image in numpy array format and a boolean flag indicating if the image is in BGR format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def attempt_loading_image_from_string(\n    value: Union[str, bytes, bytearray, _IOBase],\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"\n    Attempt to load an image from a string.\n\n    Args:\n        value (Union[str, bytes, bytearray, _IOBase]): The image data in string format.\n        cv_imread_flags (int): OpenCV flags used for image reading.\n\n    Returns:\n        Tuple[np.ndarray, bool]: A tuple of the loaded image in numpy array format and a boolean flag indicating if the image is in BGR format.\n    \"\"\"\n    try:\n        return load_image_base64(value=value, cv_imread_flags=cv_imread_flags), True\n    except:\n        pass\n    try:\n        return (\n            load_image_from_encoded_bytes(value=value, cv_imread_flags=cv_imread_flags),\n            True,\n        )\n    except:\n        pass\n    try:\n        return (\n            load_image_from_buffer(value=value, cv_imread_flags=cv_imread_flags),\n            True,\n        )\n    except:\n        pass\n    try:\n        return load_image_from_numpy_str(value=value), True\n    except InvalidImageTypeDeclared as error:\n        raise error\n    except InvalidNumpyInput as error:\n        raise InputFormatInferenceFailed(\n            message=\"Input image format could not be inferred from string.\",\n            public_message=\"Input image format could not be inferred from string.\",\n        ) from error\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.choose_image_decoding_flags","title":"<code>choose_image_decoding_flags(disable_preproc_auto_orient)</code>","text":"<p>Choose the appropriate OpenCV image decoding flags.</p> <p>Parameters:</p> Name Type Description Default <code>disable_preproc_auto_orient</code> <code>bool</code> <p>Flag to disable preprocessing auto-orientation.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>OpenCV image decoding flags.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def choose_image_decoding_flags(disable_preproc_auto_orient: bool) -&gt; int:\n    \"\"\"Choose the appropriate OpenCV image decoding flags.\n\n    Args:\n        disable_preproc_auto_orient (bool): Flag to disable preprocessing auto-orientation.\n\n    Returns:\n        int: OpenCV image decoding flags.\n    \"\"\"\n    cv_imread_flags = cv2.IMREAD_COLOR\n    if disable_preproc_auto_orient:\n        cv_imread_flags = cv_imread_flags | cv2.IMREAD_IGNORE_ORIENTATION\n    return cv_imread_flags\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.convert_gray_image_to_bgr","title":"<code>convert_gray_image_to_bgr(image)</code>","text":"<p>Convert a grayscale image to BGR format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The grayscale image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The converted BGR image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def convert_gray_image_to_bgr(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Convert a grayscale image to BGR format.\n\n    Args:\n        image (np.ndarray): The grayscale image.\n\n    Returns:\n        np.ndarray: The converted BGR image.\n    \"\"\"\n\n    if len(image.shape) == 2 or image.shape[2] == 1:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    return image\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.encode_image_to_jpeg_bytes","title":"<code>encode_image_to_jpeg_bytes(image, jpeg_quality=90)</code>","text":"<p>Encode a numpy image to JPEG format in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array representing a BGR image.</p> required <code>jpeg_quality</code> <code>int</code> <p>Quality of the JPEG image.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The JPEG encoded image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def encode_image_to_jpeg_bytes(image: np.ndarray, jpeg_quality: int = 90) -&gt; bytes:\n    \"\"\"\n    Encode a numpy image to JPEG format in bytes.\n\n    Args:\n        image (np.ndarray): The numpy array representing a BGR image.\n        jpeg_quality (int): Quality of the JPEG image.\n\n    Returns:\n        bytes: The JPEG encoded image.\n    \"\"\"\n    encoding_param = [int(cv2.IMWRITE_JPEG_QUALITY), jpeg_quality]\n    _, img_encoded = cv2.imencode(\".jpg\", image, encoding_param)\n    return np.array(img_encoded).tobytes()\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.extract_image_payload_and_type","title":"<code>extract_image_payload_and_type(value)</code>","text":"<p>Extract the image payload and type from the given value.</p> <p>This function supports different types of image inputs (e.g., InferenceRequestImage, dict, etc.) and extracts the relevant data and image type for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value which can be an image or information to derive the image.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Optional[ImageType]]</code> <p>Tuple[Any, Optional[ImageType]]: A tuple containing the extracted image data and the corresponding image type.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def extract_image_payload_and_type(value: Any) -&gt; Tuple[Any, Optional[ImageType]]:\n    \"\"\"Extract the image payload and type from the given value.\n\n    This function supports different types of image inputs (e.g., InferenceRequestImage, dict, etc.)\n    and extracts the relevant data and image type for further processing.\n\n    Args:\n        value (Any): The input value which can be an image or information to derive the image.\n\n    Returns:\n        Tuple[Any, Optional[ImageType]]: A tuple containing the extracted image data and the corresponding image type.\n    \"\"\"\n    image_type = None\n    if issubclass(type(value), InferenceRequestImage):\n        image_type = value.type\n        value = value.value\n    elif issubclass(type(value), dict):\n        image_type = value.get(\"type\")\n        value = value.get(\"value\")\n    allowed_payload_types = {e.value for e in ImageType}\n    if image_type is None:\n        return value, image_type\n    if image_type.lower() not in allowed_payload_types:\n        raise InvalidImageTypeDeclared(\n            message=f\"Declared image type: {image_type.lower()} which is not in allowed types: {allowed_payload_types}.\",\n            public_message=\"Image declaration contains not recognised image type.\",\n        )\n    return value, ImageType(image_type.lower())\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image","title":"<code>load_image(value, disable_preproc_auto_orient=False)</code>","text":"<p>Loads an image based on the specified type and value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Image value which could be an instance of InferenceRequestImage, a dict with 'type' and 'value' keys, or inferred based on the value's content.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Image.Image: The loaded PIL image, converted to RGB.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the specified image type is not supported.</p> <code>InvalidNumpyInput</code> <p>If the numpy input method is used and the input data is invalid.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image(\n    value: Any,\n    disable_preproc_auto_orient: bool = False,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"Loads an image based on the specified type and value.\n\n    Args:\n        value (Any): Image value which could be an instance of InferenceRequestImage,\n            a dict with 'type' and 'value' keys, or inferred based on the value's content.\n\n    Returns:\n        Image.Image: The loaded PIL image, converted to RGB.\n\n    Raises:\n        NotImplementedError: If the specified image type is not supported.\n        InvalidNumpyInput: If the numpy input method is used and the input data is invalid.\n    \"\"\"\n    cv_imread_flags = choose_image_decoding_flags(\n        disable_preproc_auto_orient=disable_preproc_auto_orient\n    )\n    value, image_type = extract_image_payload_and_type(value=value)\n    if image_type is not None:\n        np_image, is_bgr = load_image_with_known_type(\n            value=value,\n            image_type=image_type,\n            cv_imread_flags=cv_imread_flags,\n        )\n    else:\n        np_image, is_bgr = load_image_with_inferred_type(\n            value, cv_imread_flags=cv_imread_flags\n        )\n    np_image = convert_gray_image_to_bgr(image=np_image)\n    logger.debug(f\"Loaded inference image. Shape: {getattr(np_image, 'shape', None)}\")\n    return np_image, is_bgr\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_base64","title":"<code>load_image_base64(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a base64 encoded string using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Base64 encoded string representing the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The loaded image as a numpy array.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_base64(\n    value: Union[str, bytes], cv_imread_flags=cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n    \"\"\"Loads an image from a base64 encoded string using OpenCV.\n\n    Args:\n        value (str): Base64 encoded string representing the image.\n\n    Returns:\n        np.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    # New routes accept images via json body (str), legacy routes accept bytes which need to be decoded as strings\n    if not isinstance(value, str):\n        value = value.decode(\"utf-8\")\n    value = BASE64_DATA_TYPE_PATTERN.sub(\"\", value)\n    try:\n        value = pybase64.b64decode(value)\n    except binascii.Error as error:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Malformed base64 input image.\",\n        ) from error\n    if len(value) == 0:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Empty image payload.\",\n        )\n    image_np = np.frombuffer(value, np.uint8)\n    result = cv2.imdecode(image_np, cv_imread_flags)\n    if result is None:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Malformed base64 input image.\",\n        )\n    return result\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_buffer","title":"<code>load_image_from_buffer(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a multipart-encoded input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Multipart-encoded input representing the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_buffer(\n    value: _IOBase,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; np.ndarray:\n    \"\"\"Loads an image from a multipart-encoded input.\n\n    Args:\n        value (Any): Multipart-encoded input representing the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n    \"\"\"\n    value.seek(0)\n    image_np = np.frombuffer(value.read(), np.uint8)\n    result = cv2.imdecode(image_np, cv_imread_flags)\n    if result is None:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from buffer.\",\n            public_message=\"Could not decode bytes into image.\",\n        )\n    return result\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_encoded_bytes","title":"<code>load_image_from_encoded_bytes(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image from encoded bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The byte sequence representing the image.</p> required <code>cv_imread_flags</code> <code>int</code> <p>OpenCV flags used for image reading.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The loaded image as a numpy array.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_encoded_bytes(\n    value: bytes, cv_imread_flags: int = cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n    \"\"\"\n    Load an image from encoded bytes.\n\n    Args:\n        value (bytes): The byte sequence representing the image.\n        cv_imread_flags (int): OpenCV flags used for image reading.\n\n    Returns:\n        np.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    image_np = np.asarray(bytearray(value), dtype=np.uint8)\n    image = cv2.imdecode(image_np, cv_imread_flags)\n    if image is None:\n        raise InputImageLoadError(\n            message=f\"Could not decode bytes as image.\",\n            public_message=\"Data is not image.\",\n        )\n    return image\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_numpy_str","title":"<code>load_image_from_numpy_str(value)</code>","text":"<p>Loads an image from a numpy array string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[bytes, str]</code> <p>Base64 string or byte sequence representing the pickled numpy array of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> <p>Raises:</p> Type Description <code>InvalidNumpyInput</code> <p>If the numpy data is invalid.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_numpy_str(value: Union[bytes, str]) -&gt; np.ndarray:\n    \"\"\"Loads an image from a numpy array string.\n\n    Args:\n        value (Union[bytes, str]): Base64 string or byte sequence representing the pickled numpy array of the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n\n    Raises:\n        InvalidNumpyInput: If the numpy data is invalid.\n    \"\"\"\n    if not ALLOW_NUMPY_INPUT:\n        raise InvalidImageTypeDeclared(\n            message=f\"NumPy image type is not supported in this configuration of `inference`.\",\n            public_message=f\"NumPy image type is not supported in this configuration of `inference`.\",\n        )\n    try:\n        if isinstance(value, str):\n            value = pybase64.b64decode(value)\n        data = pickle.loads(value)\n    except (EOFError, TypeError, pickle.UnpicklingError, binascii.Error) as error:\n        raise InvalidNumpyInput(\n            message=f\"Could not unpickle image data. Cause: {error}\",\n            public_message=\"Could not deserialize pickle payload.\",\n        ) from error\n    validate_numpy_image(data=data)\n    return data\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_url","title":"<code>load_image_from_url(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a given URL.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>URL of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_url(\n    value: str, cv_imread_flags: int = cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n    \"\"\"Loads an image from a given URL.\n\n    Args:\n        value (str): URL of the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n    \"\"\"\n    _ensure_url_input_allowed()\n    try:\n        parsed_url = urllib.parse.urlparse(value)\n    except ValueError as error:\n        message = \"Provided image URL is invalid\"\n        raise InputImageLoadError(\n            message=message,\n            public_message=message,\n        ) from error\n    _ensure_resource_schema_allowed(schema=parsed_url.scheme)\n    domain_extraction_result = tldextract.TLDExtract(suffix_list_urls=())(\n        parsed_url.netloc\n    )  # we get rid of potential ports and parse FQDNs\n    _ensure_resource_fqdn_allowed(fqdn=domain_extraction_result.fqdn)\n    address_parts_concatenated = _concatenate_chunks_of_network_location(\n        extraction_result=domain_extraction_result\n    )  # concatenation of chunks - even if there is no FQDN, but address\n    # it allows white-/black-list verification\n    _ensure_location_matches_destination_whitelist(\n        destination=address_parts_concatenated\n    )\n    _ensure_location_matches_destination_blacklist(\n        destination=address_parts_concatenated\n    )\n    try:\n        response = requests.get(value, stream=True)\n        api_key_safe_raise_for_status(response=response)\n        return load_image_from_encoded_bytes(\n            value=response.content, cv_imread_flags=cv_imread_flags\n        )\n    except (RequestException, ConnectionError) as error:\n        raise InputImageLoadError(\n            message=f\"Could not load image from url: {value}. Details: {error}\",\n            public_message=\"Data pointed by URL could not be decoded into image.\",\n        )\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_with_inferred_type","title":"<code>load_image_with_inferred_type(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image by inferring its type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The image data.</p> required <code>cv_imread_flags</code> <code>int</code> <p>Flags used for OpenCV's imread function.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: Loaded image as a numpy array and a boolean indicating if the image is in BGR format.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the image type could not be inferred.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_with_inferred_type(\n    value: Any,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"Load an image by inferring its type.\n\n    Args:\n        value (Any): The image data.\n        cv_imread_flags (int): Flags used for OpenCV's imread function.\n\n    Returns:\n        Tuple[np.ndarray, bool]: Loaded image as a numpy array and a boolean indicating if the image is in BGR format.\n\n    Raises:\n        NotImplementedError: If the image type could not be inferred.\n    \"\"\"\n    if isinstance(value, (np.ndarray, np.generic)):\n        validate_numpy_image(data=value)\n        return value, True\n    elif isinstance(value, Image.Image):\n        return np.asarray(value.convert(\"RGB\")), False\n    elif isinstance(value, str) and (value.startswith(\"http\")):\n        return load_image_from_url(value=value, cv_imread_flags=cv_imread_flags), True\n    elif (\n        isinstance(value, str)\n        and ALLOW_LOADING_IMAGES_FROM_LOCAL_FILESYSTEM\n        and os.path.isfile(value)\n    ):\n        return cv2.imread(value, cv_imread_flags), True\n    else:\n        return attempt_loading_image_from_string(\n            value=value, cv_imread_flags=cv_imread_flags\n        )\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_with_known_type","title":"<code>load_image_with_known_type(value, image_type, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image using the known image type.</p> <p>Supports various image types (e.g., NUMPY, PILLOW, etc.) and loads them into a numpy array format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The image data.</p> required <code>image_type</code> <code>ImageType</code> <p>The type of the image.</p> required <code>cv_imread_flags</code> <code>int</code> <p>Flags used for OpenCV's imread function.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: A tuple of the loaded image as a numpy array and a boolean indicating if the image is in BGR format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_with_known_type(\n    value: Any,\n    image_type: ImageType,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"Load an image using the known image type.\n\n    Supports various image types (e.g., NUMPY, PILLOW, etc.) and loads them into a numpy array format.\n\n    Args:\n        value (Any): The image data.\n        image_type (ImageType): The type of the image.\n        cv_imread_flags (int): Flags used for OpenCV's imread function.\n\n    Returns:\n        Tuple[np.ndarray, bool]: A tuple of the loaded image as a numpy array and a boolean indicating if the image is in BGR format.\n    \"\"\"\n    if image_type is ImageType.FILE and not ALLOW_LOADING_IMAGES_FROM_LOCAL_FILESYSTEM:\n        raise InputImageLoadError(\n            message=\"Loading images from local filesystem is disabled.\",\n            public_message=\"Loading images from local filesystem is disabled.\",\n        )\n    loader = IMAGE_LOADERS[image_type]\n    is_bgr = True if image_type is not ImageType.PILLOW else False\n    image = loader(value, cv_imread_flags)\n    return image, is_bgr\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.np_image_to_base64","title":"<code>np_image_to_base64(image)</code>","text":"<p>TODO: This function is broken: https://github.com/roboflow/inference/issues/439 Convert a numpy image to a base64 encoded byte string.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The base64 encoded image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>@deprecated(\n    reason=\"Method replaced with inference.core.utils.image_utils.encode_image_to_jpeg_bytes\"\n)\ndef np_image_to_base64(image: np.ndarray) -&gt; bytes:\n    \"\"\"\n    TODO: This function is broken: https://github.com/roboflow/inference/issues/439\n    Convert a numpy image to a base64 encoded byte string.\n\n    Args:\n        image (np.ndarray): The numpy array representing an image.\n\n    Returns:\n        bytes: The base64 encoded image.\n    \"\"\"\n    image = Image.fromarray(image)\n    with BytesIO() as buffer:\n        image = image.convert(\"RGB\")\n        image.save(buffer, format=\"JPEG\")\n        buffer.seek(0)\n        return buffer.getvalue()\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.validate_numpy_image","title":"<code>validate_numpy_image(data)</code>","text":"<p>Validate if the provided data is a valid numpy image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <p>Raises:</p> Type Description <code>InvalidNumpyInput</code> <p>If the provided data is not a valid numpy image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def validate_numpy_image(data: np.ndarray) -&gt; None:\n    \"\"\"\n    Validate if the provided data is a valid numpy image.\n\n    Args:\n        data (np.ndarray): The numpy array representing an image.\n\n    Raises:\n        InvalidNumpyInput: If the provided data is not a valid numpy image.\n    \"\"\"\n    if not issubclass(type(data), np.ndarray):\n        raise InvalidNumpyInput(\n            message=f\"Data provided as input could not be decoded into np.ndarray object.\",\n            public_message=f\"Data provided as input could not be decoded into np.ndarray object.\",\n        )\n    if len(data.shape) != 3 and len(data.shape) != 2:\n        raise InvalidNumpyInput(\n            message=f\"For image given as np.ndarray expected 2 or 3 dimensions, got {len(data.shape)} dimensions.\",\n            public_message=f\"For image given as np.ndarray expected 2 or 3 dimensions.\",\n        )\n    if data.shape[-1] != 3 and data.shape[-1] != 1:\n        raise InvalidNumpyInput(\n            message=f\"For image given as np.ndarray expected 1 or 3 channels, got {data.shape[-1]} channels.\",\n            public_message=\"For image given as np.ndarray expected 1 or 3 channels.\",\n        )\n</code></pre>"},{"location":"reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.xyxy_to_xywh","title":"<code>xyxy_to_xywh(xyxy)</code>","text":"<p>Convert bounding box format from (xmin, ymin, xmax, ymax) to (xcenter, ycenter, width, height).</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>List[int]</code> <p>List containing the coordinates in (xmin, ymin, xmax, ymax) format.</p> required <p>Returns:</p> Type Description <p>List[int]: List containing the converted coordinates in (xcenter, ycenter, width, height) format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def xyxy_to_xywh(xyxy):\n    \"\"\"\n    Convert bounding box format from (xmin, ymin, xmax, ymax) to (xcenter, ycenter, width, height).\n\n    Args:\n        xyxy (List[int]): List containing the coordinates in (xmin, ymin, xmax, ymax) format.\n\n    Returns:\n        List[int]: List containing the converted coordinates in (xcenter, ycenter, width, height) format.\n    \"\"\"\n    x_temp = (xyxy[0] + xyxy[2]) / 2\n    y_temp = (xyxy[1] + xyxy[3]) / 2\n    w_temp = abs(xyxy[0] - xyxy[2])\n    h_temp = abs(xyxy[1] - xyxy[3])\n\n    return [int(x_temp), int(y_temp), int(w_temp), int(h_temp)]\n</code></pre>"},{"location":"reference/inference/core/utils/onnx/","title":"Onnx","text":""},{"location":"reference/inference/core/utils/onnx/#inference.core.utils.onnx.get_onnxruntime_execution_providers","title":"<code>get_onnxruntime_execution_providers(value)</code>","text":"<p>Extracts the ONNX runtime execution providers from the given string.</p> <p>The input string is expected to be a comma-separated list, possibly enclosed within square brackets and containing single quotes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string containing the list of ONNX runtime execution providers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings representing each execution provider.</p> Source code in <code>inference/core/utils/onnx.py</code> <pre><code>def get_onnxruntime_execution_providers(value: str) -&gt; List[str]:\n    \"\"\"Extracts the ONNX runtime execution providers from the given string.\n\n    The input string is expected to be a comma-separated list, possibly enclosed\n    within square brackets and containing single quotes.\n\n    Args:\n        value (str): The string containing the list of ONNX runtime execution providers.\n\n    Returns:\n        List[str]: A list of strings representing each execution provider.\n    \"\"\"\n    if len(value) == 0:\n        return []\n    value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\" \", \"\")\n    return value.split(\",\")\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/","title":"Postprocess","text":""},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.cosine_similarity","title":"<code>cosine_similarity(a, b)</code>","text":"<p>Compute the cosine similarity between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>Vector A.</p> required <code>b</code> <code>ndarray</code> <p>Vector B.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>Union[number, ndarray]</code> <p>Cosine similarity between vectors A and B.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; Union[np.number, np.ndarray]:\n    \"\"\"\n    Compute the cosine similarity between two vectors.\n\n    Args:\n        a (np.ndarray): Vector A.\n        b (np.ndarray): Vector B.\n\n    Returns:\n        float: Cosine similarity between vectors A and B.\n    \"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.crop_mask","title":"<code>crop_mask(masks, boxes)</code>","text":"<p>\"Crop\" predicted masks by zeroing out everything not in the predicted bbox. Vectorized by Chong (thanks Chong).</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def crop_mask(masks: np.ndarray, boxes: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    \"Crop\" predicted masks by zeroing out everything not in the predicted bbox.\n    Vectorized by Chong (thanks Chong).\n\n    Args:\n        - masks should be a size [h, w, n] tensor of masks\n        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n    \"\"\"\n\n    n, h, w = masks.shape\n    x1, y1, x2, y2 = np.split(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)\n    r = np.arange(w, dtype=x1.dtype)[None, None, :]  # rows shape(1,w,1)\n    c = np.arange(h, dtype=x1.dtype)[None, :, None]  # cols shape(h,1,1)\n\n    masks = masks * ((r &gt;= x1) * (r &lt; x2) * (c &gt;= y1) * (c &lt; y2))\n    return masks\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.get_static_crop_dimensions","title":"<code>get_static_crop_dimensions(orig_shape, preproc, disable_preproc_static_crop=False)</code>","text":"<p>Generates a transformation based on preprocessing configuration.</p> <p>Parameters:</p> Name Type Description Default <code>orig_shape</code> <code>tuple</code> <p>The original shape of the object (e.g., image) - (height, width).</p> required <code>preproc</code> <code>dict</code> <p>Preprocessing configuration dictionary, containing information such as static cropping.</p> required <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tuple[int, int], Tuple[int, int]]</code> <p>A tuple containing the shift in the x and y directions, and the updated original shape after cropping.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def get_static_crop_dimensions(\n    orig_shape: Tuple[int, int],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n    \"\"\"\n    Generates a transformation based on preprocessing configuration.\n\n    Args:\n        orig_shape (tuple): The original shape of the object (e.g., image) - (height, width).\n        preproc (dict): Preprocessing configuration dictionary, containing information such as static cropping.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        tuple: A tuple containing the shift in the x and y directions, and the updated original shape after cropping.\n    \"\"\"\n    try:\n        if static_crop_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        ):\n            x_min, y_min, x_max, y_max = standardise_static_crop(\n                static_crop_config=preproc[STATIC_CROP_KEY]\n            )\n        else:\n            x_min, y_min, x_max, y_max = 0, 0, 1, 1\n        crop_shift_x, crop_shift_y = (\n            round(x_min * orig_shape[1]),\n            round(y_min * orig_shape[0]),\n        )\n        cropped_percent_x = x_max - x_min\n        cropped_percent_y = y_max - y_min\n        orig_shape = (\n            round(orig_shape[0] * cropped_percent_y),\n            round(orig_shape[1] * cropped_percent_x),\n        )\n        return (crop_shift_x, crop_shift_y), orig_shape\n    except KeyError as error:\n        raise PostProcessingError(\n            f\"Could not find a proper configuration key {error} in post-processing.\"\n        )\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.mask2multipoly","title":"<code>mask2multipoly(mask)</code>","text":"<p>Find all contours in the mask and return them as a float32 array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contours represented as a float32 array.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def mask2multipoly(mask: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Find all contours in the mask and return them as a float32 array.\n\n    Args:\n        mask (np.ndarray): A binary mask.\n\n    Returns:\n        np.ndarray: Contours represented as a float32 array.\n    \"\"\"\n    contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n    if contours:\n        contours = [c.reshape(-1, 2).astype(\"float32\") for c in contours]\n    else:\n        contours = [np.zeros((0, 2)).astype(\"float32\")]\n    return contours\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.mask2poly","title":"<code>mask2poly(mask)</code>","text":"<p>Find contours in the mask and return them as a float32 array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contours represented as a float32 array.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def mask2poly(mask: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Find contours in the mask and return them as a float32 array.\n\n    Args:\n        mask (np.ndarray): A binary mask.\n\n    Returns:\n        np.ndarray: Contours represented as a float32 array.\n    \"\"\"\n    contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n    if contours:\n        contours = np.array(\n            contours[np.array([len(x) for x in contours]).argmax()]\n        ).reshape(-1, 2)\n    else:\n        contours = np.zeros((0, 2))\n    return contours.astype(\"float32\")\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.masks2multipoly","title":"<code>masks2multipoly(masks)</code>","text":"<p>Converts binary masks to polygonal segments.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[ndarray]</code> <p>A list of segments, where each segment is obtained by converting the corresponding mask.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def masks2multipoly(masks: np.ndarray) -&gt; List[np.ndarray]:\n    \"\"\"Converts binary masks to polygonal segments.\n\n    Args:\n        masks (numpy.ndarray): A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.\n\n    Returns:\n        list: A list of segments, where each segment is obtained by converting the corresponding mask.\n    \"\"\"\n    segments = []\n    # Process per-mask to avoid allocating an entire N x H x W uint8 copy\n    for mask in masks:\n        # Fast-path: bool -&gt; zero-copy uint8 view\n        if mask.dtype == np.bool_:\n            m_uint8 = mask\n            if not m_uint8.flags.c_contiguous:\n                m_uint8 = np.ascontiguousarray(m_uint8)\n            m_uint8 = m_uint8.view(np.uint8)\n        elif mask.dtype == np.uint8:\n            m_uint8 = mask if mask.flags.c_contiguous else np.ascontiguousarray(mask)\n        else:\n            # Fallback: threshold to bool then view as uint8 (may allocate once)\n            m_bool = mask &gt; 0\n            if not m_bool.flags.c_contiguous:\n                m_bool = np.ascontiguousarray(m_bool)\n            m_uint8 = m_bool.view(np.uint8)\n\n        # Quickly skip empty masks\n        if not np.any(m_uint8):\n            segments.append([np.zeros((0, 2), dtype=np.float32)])\n            continue\n\n        segments.append(mask2multipoly(m_uint8))\n    return segments\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.masks2poly","title":"<code>masks2poly(masks)</code>","text":"<p>Converts binary masks to polygonal segments.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[ndarray]</code> <p>A list of segments, where each segment is obtained by converting the corresponding mask.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def masks2poly(masks: np.ndarray) -&gt; List[np.ndarray]:\n    \"\"\"Converts binary masks to polygonal segments.\n\n    Args:\n        masks (numpy.ndarray): A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.\n\n    Returns:\n        list: A list of segments, where each segment is obtained by converting the corresponding mask.\n    \"\"\"\n    segments = []\n    # Process per-mask to avoid allocating an entire N x H x W uint8 copy\n    for mask in masks:\n        # Fast-path: bool -&gt; zero-copy uint8 view\n        if mask.dtype == np.bool_:\n            m_uint8 = mask\n            if not m_uint8.flags.c_contiguous:\n                m_uint8 = np.ascontiguousarray(m_uint8)\n            m_uint8 = m_uint8.view(np.uint8)\n        elif mask.dtype == np.uint8:\n            m_uint8 = mask if mask.flags.c_contiguous else np.ascontiguousarray(mask)\n        else:\n            # Fallback: threshold to bool then view as uint8 (may allocate once)\n            m_bool = mask &gt; 0\n            if not m_bool.flags.c_contiguous:\n                m_bool = np.ascontiguousarray(m_bool)\n            m_uint8 = m_bool.view(np.uint8)\n\n        # Quickly skip empty masks\n        if not np.any(m_uint8):\n            segments.append(np.zeros((0, 2), dtype=np.float32))\n            continue\n\n        segments.append(mask2poly(m_uint8))\n    return segments\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_bboxes","title":"<code>post_process_bboxes(predictions, infer_shape, img_dims, preproc, disable_preproc_static_crop=False, resize_method='Stretch to')</code>","text":"<p>Postprocesses each patch of detections by scaling them to the original image coordinates and by shifting them based on a static crop preproc (if applied).</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>The predictions output from NMS, indices are: batch x prediction x [x1, y1, x2, y2, ...].</p> required <code>infer_shape</code> <code>Tuple[int, int]</code> <p>The shape of the inference image.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>The dimensions of the original image for each batch, indices are: batch x [height, width].</p> required <code>preproc</code> <code>dict</code> <p>Preprocessing configuration dictionary.</p> required <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>resize_method</code> <code>str</code> <p>Resize method for image. Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <p>Returns:</p> Type Description <code>List[List[List[float]]]</code> <p>List[List[List[float]]]: The scaled and shifted predictions, indices are: batch x prediction x [x1, y1, x2, y2, ...].</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_bboxes(\n    predictions: List[List[List[float]]],\n    infer_shape: Tuple[int, int],\n    img_dims: List[Tuple[int, int]],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[List[float]]]:\n    \"\"\"\n    Postprocesses each patch of detections by scaling them to the original image coordinates and by shifting them based on a static crop preproc (if applied).\n\n    Args:\n        predictions (List[List[List[float]]]): The predictions output from NMS, indices are: batch x prediction x [x1, y1, x2, y2, ...].\n        infer_shape (Tuple[int, int]): The shape of the inference image.\n        img_dims (List[Tuple[int, int]]): The dimensions of the original image for each batch, indices are: batch x [height, width].\n        preproc (dict): Preprocessing configuration dictionary.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        resize_method (str, optional): Resize method for image. Defaults to \"Stretch to\".\n\n    Returns:\n        List[List[List[float]]]: The scaled and shifted predictions, indices are: batch x prediction x [x1, y1, x2, y2, ...].\n    \"\"\"\n\n    # Get static crop params\n    scaled_predictions = []\n    # Loop through batches\n    for i, batch_predictions in enumerate(predictions):\n        if len(batch_predictions) == 0:\n            scaled_predictions.append([])\n            continue\n        np_batch_predictions = np.array(batch_predictions)\n        # Get bboxes from predictions (x1,y1,x2,y2)\n        predicted_bboxes = np_batch_predictions[:, :4]\n        (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n            img_dims[i],\n            preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if resize_method == \"Stretch to\":\n            predicted_bboxes = stretch_bboxes(\n                predicted_bboxes=predicted_bboxes,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        elif (\n            resize_method == \"Fit (black edges) in\"\n            or resize_method == \"Fit (white edges) in\"\n            or resize_method == \"Fit (grey edges) in\"\n        ):\n            predicted_bboxes = undo_image_padding_for_predicted_boxes(\n                predicted_bboxes=predicted_bboxes,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        predicted_bboxes = clip_boxes_coordinates(\n            predicted_bboxes=predicted_bboxes,\n            origin_shape=origin_shape,\n        )\n        predicted_bboxes = shift_bboxes(\n            bboxes=predicted_bboxes,\n            shift_x=crop_shift_x,\n            shift_y=crop_shift_y,\n        )\n        np_batch_predictions[:, :4] = predicted_bboxes\n        scaled_predictions.append(np_batch_predictions.tolist())\n    return scaled_predictions\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_keypoints","title":"<code>post_process_keypoints(predictions, keypoints_start_index, infer_shape, img_dims, preproc, disable_preproc_static_crop=False, resize_method='Stretch to')</code>","text":"<p>Scales and shifts keypoints based on the given image shapes and preprocessing method.</p> <p>This function performs polygon scaling and shifting based on the specified resizing method and pre-processing steps. The polygons are transformed according to the ratio and padding between two images.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>predictions from model</p> required <code>keypoints_start_index</code> <code>int</code> <p>offset in the 3rd dimension pointing where in the prediction start keypoints [(x, y, cfg), ...] for each keypoint class</p> required <code>img_dims list of</code> <code>tuple of int</code> <p>Shape of the source image (height, width).</p> required <code>infer_shape</code> <code>tuple of int</code> <p>Shape of the target image (height, width).</p> required <code>preproc</code> <code>object</code> <p>Preprocessing details used for generating the transformation.</p> required <code>resize_method</code> <code>str</code> <p>Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>flag to disable static crop</p> <code>False</code> <p>Returns:     list of list of list: predictions with post-processed keypoints</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_keypoints(\n    predictions: List[List[List[float]]],\n    keypoints_start_index: int,\n    infer_shape: Tuple[int, int],\n    img_dims: List[Tuple[int, int]],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[List[float]]]:\n    \"\"\"Scales and shifts keypoints based on the given image shapes and preprocessing method.\n\n    This function performs polygon scaling and shifting based on the specified resizing method and\n    pre-processing steps. The polygons are transformed according to the ratio and padding between two images.\n\n    Args:\n        predictions: predictions from model\n        keypoints_start_index: offset in the 3rd dimension pointing where in the prediction start keypoints [(x, y, cfg), ...] for each keypoint class\n        img_dims list of (tuple of int): Shape of the source image (height, width).\n        infer_shape (tuple of int): Shape of the target image (height, width).\n        preproc (object): Preprocessing details used for generating the transformation.\n        resize_method (str, optional): Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".\n        disable_preproc_static_crop: flag to disable static crop\n    Returns:\n        list of list of list: predictions with post-processed keypoints\n    \"\"\"\n    # Get static crop params\n    scaled_predictions = []\n    # Loop through batches\n    for i, batch_predictions in enumerate(predictions):\n        if len(batch_predictions) == 0:\n            scaled_predictions.append([])\n            continue\n        np_batch_predictions = np.array(batch_predictions)\n        keypoints = np_batch_predictions[:, keypoints_start_index:]\n        (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n            img_dims[i],\n            preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if resize_method == \"Stretch to\":\n            keypoints = stretch_keypoints(\n                keypoints=keypoints,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        elif (\n            resize_method == \"Fit (black edges) in\"\n            or resize_method == \"Fit (white edges) in\"\n            or resize_method == \"Fit (grey edges) in\"\n        ):\n            keypoints = undo_image_padding_for_predicted_keypoints(\n                keypoints=keypoints,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        keypoints = clip_keypoints_coordinates(\n            keypoints=keypoints, origin_shape=origin_shape\n        )\n        keypoints = shift_keypoints(\n            keypoints=keypoints, shift_x=crop_shift_x, shift_y=crop_shift_y\n        )\n        np_batch_predictions[:, keypoints_start_index:] = keypoints\n        scaled_predictions.append(np_batch_predictions.tolist())\n    return scaled_predictions\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_polygons","title":"<code>post_process_polygons(origin_shape, polys, infer_shape, preproc, resize_method='Stretch to')</code>","text":"<p>Scales and shifts polygons based on the given image shapes and preprocessing method.</p> <p>This function performs polygon scaling and shifting based on the specified resizing method and pre-processing steps. The polygons are transformed according to the ratio and padding between two images.</p> <p>Parameters:</p> Name Type Description Default <code>origin_shape</code> <code>tuple of int</code> <p>Shape of the source image (height, width).</p> required <code>infer_shape</code> <code>tuple of int</code> <p>Shape of the target image (height, width).</p> required <code>polys</code> <code>list of list of tuple</code> <p>List of polygons, where each polygon is represented by a list of (x, y) coordinates.</p> required <code>preproc</code> <code>object</code> <p>Preprocessing details used for generating the transformation.</p> required <code>resize_method</code> <code>str</code> <p>Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <p>Returns:</p> Type Description <code>List[List[Tuple[float, float]]]</code> <p>list of list of tuple: A list of shifted and scaled polygons.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_polygons(\n    origin_shape: Tuple[int, int],\n    polys: List[List[Tuple[float, float]]],\n    infer_shape: Tuple[int, int],\n    preproc: dict,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[Tuple[float, float]]]:\n    \"\"\"Scales and shifts polygons based on the given image shapes and preprocessing method.\n\n    This function performs polygon scaling and shifting based on the specified resizing method and\n    pre-processing steps. The polygons are transformed according to the ratio and padding between two images.\n\n    Args:\n        origin_shape (tuple of int): Shape of the source image (height, width).\n        infer_shape (tuple of int): Shape of the target image (height, width).\n        polys (list of list of tuple): List of polygons, where each polygon is represented by a list of (x, y) coordinates.\n        preproc (object): Preprocessing details used for generating the transformation.\n        resize_method (str, optional): Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".\n\n    Returns:\n        list of list of tuple: A list of shifted and scaled polygons.\n    \"\"\"\n    (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n        origin_shape, preproc\n    )\n    new_polys = []\n    if resize_method == \"Stretch to\":\n        width_ratio = origin_shape[1] / infer_shape[1]\n        height_ratio = origin_shape[0] / infer_shape[0]\n        new_polys = scale_polygons(\n            polygons=polys,\n            x_scale=width_ratio,\n            y_scale=height_ratio,\n        )\n    elif resize_method in {\n        \"Fit (black edges) in\",\n        \"Fit (white edges) in\",\n        \"Fit (grey edges) in\",\n    }:\n        new_polys = undo_image_padding_for_predicted_polygons(\n            polygons=polys,\n            infer_shape=infer_shape,\n            origin_shape=origin_shape,\n        )\n    shifted_polys = []\n    for poly in new_polys:\n        poly = [(p[0] + crop_shift_x, p[1] + crop_shift_y) for p in poly]\n        shifted_polys.append(poly)\n    return shifted_polys\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_accurate","title":"<code>process_mask_accurate(protos, masks_in, bboxes, shape)</code>","text":"<p>Returns masks that are the size of the original image.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_accurate(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Returns masks that are the size of the original image.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n    # Order = 1 -&gt; bilinear\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=0)\n    masks = masks.transpose((1, 2, 0))\n    masks = cv2.resize(masks, (shape[1], shape[0]), cv2.INTER_LINEAR)\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=2)\n    masks = masks.transpose((2, 0, 1))\n    masks = crop_mask(masks, bboxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_fast","title":"<code>process_mask_fast(protos, masks_in, bboxes, shape)</code>","text":"<p>Returns masks in their original size.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_fast(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Returns masks in their original size.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    ih, iw = shape\n    c, mh, mw = protos.shape  # CHW\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n    down_sampled_boxes = scale_bboxes(\n        bboxes=deepcopy(bboxes),\n        scale_x=mw / iw,\n        scale_y=mh / ih,\n    )\n    masks = crop_mask(masks, down_sampled_boxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_tradeoff","title":"<code>process_mask_tradeoff(protos, masks_in, bboxes, shape, tradeoff_factor)</code>","text":"<p>Returns masks that are the size of the original image with a tradeoff factor applied.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <code>tradeoff_factor</code> <code>float</code> <p>Tradeoff factor for resizing masks.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_tradeoff(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n    tradeoff_factor: float,\n) -&gt; np.ndarray:\n    \"\"\"Returns masks that are the size of the original image with a tradeoff factor applied.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n        tradeoff_factor (float): Tradeoff factor for resizing masks.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    c, mh, mw = protos.shape  # CHW\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n\n    # Order = 1 -&gt; bilinear\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=0)\n    masks = masks.transpose((1, 2, 0))\n    ih, iw = shape\n    h = int(mh * (1 - tradeoff_factor) + ih * tradeoff_factor)\n    w = int(mw * (1 - tradeoff_factor) + iw * tradeoff_factor)\n    size = (h, w)\n    if tradeoff_factor != 0:\n        masks = cv2.resize(masks, size, cv2.INTER_LINEAR)\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=2)\n    masks = masks.transpose((2, 0, 1))\n    c, mh, mw = masks.shape\n    down_sampled_boxes = scale_bboxes(\n        bboxes=deepcopy(bboxes),\n        scale_x=mw / iw,\n        scale_y=mh / ih,\n    )\n    masks = crop_mask(masks, down_sampled_boxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.sigmoid","title":"<code>sigmoid(x)</code>","text":"<p>Computes the sigmoid function for the given input.</p> <p>The sigmoid function is defined as: f(x) = 1 / (1 + exp(-x))</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float or ndarray</code> <p>Input value or array for which the sigmoid function is to be computed.</p> required <p>Returns:</p> Type Description <code>Union[float, number, ndarray]</code> <p>float or numpy.ndarray: The computed sigmoid value(s).</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def sigmoid(x: Union[float, np.ndarray]) -&gt; Union[float, np.number, np.ndarray]:\n    \"\"\"Computes the sigmoid function for the given input.\n\n    The sigmoid function is defined as:\n    f(x) = 1 / (1 + exp(-x))\n\n    Args:\n        x (float or numpy.ndarray): Input value or array for which the sigmoid function is to be computed.\n\n    Returns:\n        float or numpy.ndarray: The computed sigmoid value(s).\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n</code></pre>"},{"location":"reference/inference/core/utils/preprocess/","title":"Preprocess","text":""},{"location":"reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.letterbox_image","title":"<code>letterbox_image(image, desired_size, color=(0, 0, 0))</code>","text":"<p>Resize and pad image to fit the desired size, preserving its aspect ratio.</p> <p>Parameters: - image: numpy array representing the image. - desired_size: tuple (width, height) representing the target dimensions. - color: tuple (B, G, R) representing the color to pad with.</p> <p>Returns: - letterboxed image.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def letterbox_image(\n    image: ImageMetaType,\n    desired_size: Tuple[int, int],\n    color: Tuple[int, int, int] = (0, 0, 0),\n) -&gt; ImageMetaType:\n    \"\"\"\n    Resize and pad image to fit the desired size, preserving its aspect ratio.\n\n    Parameters:\n    - image: numpy array representing the image.\n    - desired_size: tuple (width, height) representing the target dimensions.\n    - color: tuple (B, G, R) representing the color to pad with.\n\n    Returns:\n    - letterboxed image.\n    \"\"\"\n    resized_img = resize_image_keeping_aspect_ratio(\n        image=image,\n        desired_size=desired_size,\n    )\n    new_height, new_width = (\n        resized_img.shape[:2]\n        if isinstance(resized_img, np.ndarray)\n        else resized_img.shape[-2:]\n    )\n    top_padding = (desired_size[1] - new_height) // 2\n    bottom_padding = desired_size[1] - new_height - top_padding\n    left_padding = (desired_size[0] - new_width) // 2\n    right_padding = desired_size[0] - new_width - left_padding\n    if isinstance(resized_img, np.ndarray):\n        return cv2.copyMakeBorder(\n            resized_img,\n            top_padding,\n            bottom_padding,\n            left_padding,\n            right_padding,\n            cv2.BORDER_CONSTANT,\n            value=color,\n        )\n    elif USE_PYTORCH_FOR_PREPROCESSING:\n        return torch.nn.functional.pad(\n            resized_img,\n            (left_padding, right_padding, top_padding, bottom_padding),\n            \"constant\",\n            color[0],\n        )\n    else:\n        raise ValueError(\n            f\"Received an image of unknown type, {type(resized_img)}; \"\n            \"This is most likely a bug. Contact Roboflow team through github issues \"\n            \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n        )\n</code></pre>"},{"location":"reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.prepare","title":"<code>prepare(image, preproc, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Prepares an image by applying a series of preprocessing steps defined in the <code>preproc</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input PIL image object.</p> required <code>preproc</code> <code>dict</code> <p>Dictionary containing preprocessing steps. Example: {     \"resize\": {\"enabled\": true, \"width\": 416, \"height\": 416, \"format\": \"Stretch to\"},     \"static-crop\": {\"y_min\": 25, \"x_max\": 75, \"y_max\": 75, \"enabled\": true, \"x_min\": 25},     \"auto-orient\": {\"enabled\": true},     \"grayscale\": {\"enabled\": true},     \"contrast\": {\"enabled\": true, \"type\": \"Adaptive Equalization\"} }</p> required <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>PIL.Image.Image: The preprocessed image object.</p> <code>tuple</code> <code>Tuple[int, int]</code> <p>The dimensions of the image.</p> Note <p>The function uses global flags like <code>DISABLE_PREPROC_AUTO_ORIENT</code>, <code>DISABLE_PREPROC_STATIC_CROP</code>, etc. to conditionally enable or disable certain preprocessing steps.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def prepare(\n    image: np.ndarray,\n    preproc,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Prepares an image by applying a series of preprocessing steps defined in the `preproc` dictionary.\n\n    Args:\n        image (PIL.Image.Image): The input PIL image object.\n        preproc (dict): Dictionary containing preprocessing steps. Example:\n            {\n                \"resize\": {\"enabled\": true, \"width\": 416, \"height\": 416, \"format\": \"Stretch to\"},\n                \"static-crop\": {\"y_min\": 25, \"x_max\": 75, \"y_max\": 75, \"enabled\": true, \"x_min\": 25},\n                \"auto-orient\": {\"enabled\": true},\n                \"grayscale\": {\"enabled\": true},\n                \"contrast\": {\"enabled\": true, \"type\": \"Adaptive Equalization\"}\n            }\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        PIL.Image.Image: The preprocessed image object.\n        tuple: The dimensions of the image.\n\n    Note:\n        The function uses global flags like `DISABLE_PREPROC_AUTO_ORIENT`, `DISABLE_PREPROC_STATIC_CROP`, etc.\n        to conditionally enable or disable certain preprocessing steps.\n    \"\"\"\n    try:\n        if isinstance(image, np.ndarray):\n            h, w = image.shape[0:2]\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            h, w = image.shape[-2:]\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(image)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n        img_dims = (h, w)\n        if static_crop_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        ):\n            image = take_static_crop(\n                image=image, crop_parameters=preproc[STATIC_CROP_KEY]\n            )\n        if contrast_adjustments_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_contrast=disable_preproc_contrast,\n        ):\n            adjustment_type = ContrastAdjustmentType(preproc[CONTRAST_KEY][TYPE_KEY])\n            image = apply_contrast_adjustment(\n                image=image, adjustment_type=adjustment_type\n            )\n        if grayscale_conversion_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n        ):\n            image = apply_grayscale_conversion(image=image)\n        return image, img_dims\n    except KeyError as error:\n        raise PreProcessingError(\n            f\"Pre-processing of image failed due to misconfiguration. Missing key: {error}.\"\n        ) from error\n</code></pre>"},{"location":"reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.resize_image_keeping_aspect_ratio","title":"<code>resize_image_keeping_aspect_ratio(image, desired_size)</code>","text":"<p>Resize reserving its aspect ratio.</p> <p>Parameters: - image: numpy array representing the image. - desired_size: tuple (width, height) representing the target dimensions.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def resize_image_keeping_aspect_ratio(\n    image: ImageMetaType,\n    desired_size: Tuple[int, int],\n) -&gt; ImageMetaType:\n    \"\"\"\n    Resize reserving its aspect ratio.\n\n    Parameters:\n    - image: numpy array representing the image.\n    - desired_size: tuple (width, height) representing the target dimensions.\n    \"\"\"\n    if isinstance(image, np.ndarray):\n        img_ratio = image.shape[1] / image.shape[0]\n    elif USE_PYTORCH_FOR_PREPROCESSING:\n        img_ratio = image.shape[-1] / image.shape[-2]\n    else:\n        raise ValueError(\n            f\"Received an image of unknown type, {type(image)}; \"\n            \"This is most likely a bug. Contact Roboflow team through github issues \"\n            \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n        )\n    desired_ratio = desired_size[0] / desired_size[1]\n\n    # Determine the new dimensions\n    if img_ratio &gt;= desired_ratio:\n        # Resize by width\n        new_width = desired_size[0]\n        new_height = int(desired_size[0] / img_ratio)\n    else:\n        # Resize by height\n        new_height = desired_size[1]\n        new_width = int(desired_size[1] * img_ratio)\n\n    # Resize the image to new dimensions\n    if isinstance(image, np.ndarray):\n        return cv2.resize(image, (new_width, new_height))\n    elif USE_PYTORCH_FOR_PREPROCESSING:\n        return torch.nn.functional.interpolate(\n            image, size=(new_height, new_width), mode=\"bilinear\"\n        )\n    else:\n        raise ValueError(\n            f\"Received an image of unknown type, {type(image)}; \"\n            \"This is most likely a bug. Contact Roboflow team through github issues \"\n            \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n        )\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/#inference.core.workflows.core_steps.classical_cv.camera_focus.v1.calculate_brenner_measure","title":"<code>calculate_brenner_measure(input_image, text_color=(255, 255, 255), text_thickness=2)</code>","text":"<p>Brenner's focus measure.</p>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/#inference.core.workflows.core_steps.classical_cv.camera_focus.v1.calculate_brenner_measure--parameters","title":"Parameters","text":"<p>input_image : np.ndarray     The input image in grayscale. text_color : Tuple[int, int, int], optional     The color of the text displaying the Brenner value, in BGR format. Default is white (255, 255, 255). text_thickness : int, optional     The thickness of the text displaying the Brenner value. Default is 2.</p>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/#inference.core.workflows.core_steps.classical_cv.camera_focus.v1.calculate_brenner_measure--returns","title":"Returns","text":"<p>Tuple[np.ndarray, float]     The Brenner image and the Brenner value.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/camera_focus/v1.py</code> <pre><code>def calculate_brenner_measure(\n    input_image: np.ndarray,\n    text_color: Tuple[int, int, int] = (255, 255, 255),\n    text_thickness: int = 2,\n) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Brenner's focus measure.\n\n    Parameters\n    ----------\n    input_image : np.ndarray\n        The input image in grayscale.\n    text_color : Tuple[int, int, int], optional\n        The color of the text displaying the Brenner value, in BGR format. Default is white (255, 255, 255).\n    text_thickness : int, optional\n        The thickness of the text displaying the Brenner value. Default is 2.\n\n    Returns\n    -------\n    Tuple[np.ndarray, float]\n        The Brenner image and the Brenner value.\n    \"\"\"\n    # Convert image to grayscale if it has 3 channels\n    if len(input_image.shape) == 3:\n        input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n\n    # Convert image to 16-bit integer format\n    converted_image = input_image.astype(np.int16)\n\n    # Get the dimensions of the image\n    height, width = converted_image.shape\n\n    # Initialize two matrices for horizontal and vertical focus measures\n    horizontal_diff = np.zeros((height, width))\n    vertical_diff = np.zeros((height, width))\n\n    # Calculate horizontal and vertical focus measures\n    horizontal_diff[:, : width - 2] = np.clip(\n        converted_image[:, 2:] - converted_image[:, :-2], 0, None\n    )\n    vertical_diff[: height - 2, :] = np.clip(\n        converted_image[2:, :] - converted_image[:-2, :], 0, None\n    )\n\n    # Calculate final focus measure\n    focus_measure = np.max((horizontal_diff, vertical_diff), axis=0) ** 2\n\n    # Convert focus measure matrix to 8-bit for visualization\n    focus_measure_image = ((focus_measure / focus_measure.max()) * 255).astype(np.uint8)\n\n    # Display the Brenner value on the top left of the image\n    cv2.putText(\n        focus_measure_image,\n        f\"Focus value: {focus_measure.mean():.2f}\",\n        (10, 30),\n        cv2.FONT_HERSHEY_SIMPLEX,\n        1,\n        text_color,\n        text_thickness,\n    )\n\n    return focus_measure_image, focus_measure.mean()\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/contours/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/contours/v1/#inference.core.workflows.core_steps.classical_cv.contours.v1.find_and_draw_contours","title":"<code>find_and_draw_contours(image, color=(255, 0, 255), thickness=3)</code>","text":"<p>Finds and draws contours on the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input thresholded image.</p> required <code>color</code> <code>tuple</code> <p>Color of the contour lines in BGR. Defaults to purple (255, 0, 255).</p> <code>(255, 0, 255)</code> <code>thickness</code> <code>int</code> <p>Thickness of the contour lines. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, int]</code> <p>Image with contours drawn and number of contours.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/contours/v1.py</code> <pre><code>def find_and_draw_contours(\n    image: np.ndarray, color: Tuple[int, int, int] = (255, 0, 255), thickness: int = 3\n) -&gt; Tuple[np.ndarray, int]:\n    \"\"\"\n    Finds and draws contours on the image.\n\n    Args:\n        image (np.ndarray): Input thresholded image.\n        color (tuple, optional): Color of the contour lines in BGR. Defaults to purple (255, 0, 255).\n        thickness (int, optional): Thickness of the contour lines. Defaults to 3.\n\n    Returns:\n        tuple: Image with contours drawn and number of contours.\n    \"\"\"\n    # If not in grayscale, convert to grayscale\n    if len(image.shape) == 3 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Find contours\n    contours, hierarchy = cv2.findContours(\n        image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n\n    # Draw contours on a copy of the original image\n    contour_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    cv2.drawContours(contour_image, contours, -1, color, thickness)\n\n    # Return the image with contours and the number of contours\n    return contour_image, contours, hierarchy\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/distance_measurement/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/distance_measurement/v1/#inference.core.workflows.core_steps.classical_cv.distance_measurement.v1.has_overlap","title":"<code>has_overlap(bbox1, bbox2)</code>","text":"<p>Check if two bounding boxes overlap.</p> <p>Parameters:</p> Name Type Description Default <code>bbox1</code> <code>Tuple[int, int, int, int]</code> <p>A tuple of (x_min, y_min, x_max, y_max) for the first bounding box.</p> required <code>bbox2</code> <code>Tuple[int, int, int, int]</code> <p>A tuple of (x_min, y_min, x_max, y_max) for the second bounding box.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the bounding boxes overlap, False otherwise.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/distance_measurement/v1.py</code> <pre><code>def has_overlap(\n    bbox1: Tuple[int, int, int, int], bbox2: Tuple[int, int, int, int]\n) -&gt; bool:\n    \"\"\"\n    Check if two bounding boxes overlap.\n\n    Args:\n        bbox1: A tuple of (x_min, y_min, x_max, y_max) for the first bounding box.\n        bbox2: A tuple of (x_min, y_min, x_max, y_max) for the second bounding box.\n\n    Returns:\n        True if the bounding boxes overlap, False otherwise.\n    \"\"\"\n    x1_min, y1_min, x1_max, y1_max = bbox1\n    x2_min, y2_min, x2_max, y2_max = bbox2\n\n    if x1_max &lt; x2_min or x2_max &lt; x1_min:\n        return False\n    if y1_max &lt; y2_min or y2_max &lt; y1_min:\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/image_blur/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/image_blur/v1/#inference.core.workflows.core_steps.classical_cv.image_blur.v1.apply_blur","title":"<code>apply_blur(image, blur_type, ksize=5)</code>","text":"<p>Applies the specified blur to the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image.</p> required <code>blur_type</code> <code>str</code> <p>Type of blur ('average', 'gaussian', 'median', 'bilateral').</p> required <code>ksize</code> <code>int</code> <p>Kernel size for the blur. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Blurred image.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/image_blur/v1.py</code> <pre><code>def apply_blur(image: np.ndarray, blur_type: str, ksize: int = 5) -&gt; np.ndarray:\n    \"\"\"\n    Applies the specified blur to the image.\n\n    Args:\n        image: Input image.\n        blur_type (str): Type of blur ('average', 'gaussian', 'median', 'bilateral').\n        ksize (int, optional): Kernel size for the blur. Defaults to 5.\n\n    Returns:\n        np.ndarray: Blurred image.\n    \"\"\"\n\n    if blur_type == \"average\":\n        blurred_image = cv2.blur(image, (ksize, ksize))\n    elif blur_type == \"gaussian\":\n        blurred_image = cv2.GaussianBlur(image, (ksize, ksize), 0)\n    elif blur_type == \"median\":\n        blurred_image = cv2.medianBlur(image, ksize)\n    elif blur_type == \"bilateral\":\n        blurred_image = cv2.bilateralFilter(image, ksize, 75, 75)\n    else:\n        raise ValueError(f\"Unknown blur type: {blur_type}\")\n\n    return blurred_image\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/pixel_color_count/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/pixel_color_count/v1/#inference.core.workflows.core_steps.classical_cv.pixel_color_count.v1.count_specific_color_pixels","title":"<code>count_specific_color_pixels(image, target_color, tolerance)</code>","text":"<p>Counts the number of pixels that match the target color within the given tolerance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image.</p> required <code>target_color</code> <code>Union[str, tuple]</code> <p>Target color in hex format (e.g., '#431112') or BGR tuple (e.g., (18, 17, 67)).</p> required <code>tolerance</code> <code>int</code> <p>Tolerance for color matching. Defaults to 10.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of pixels that match the target color.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/pixel_color_count/v1.py</code> <pre><code>def count_specific_color_pixels(\n    image: np.ndarray,\n    target_color: Union[str, Tuple[int, int, int]],\n    tolerance: int,\n) -&gt; int:\n    \"\"\"\n    Counts the number of pixels that match the target color within the given tolerance.\n\n    Args:\n        image: Input image.\n        target_color (Union[str, tuple]): Target color in hex format (e.g., '#431112') or BGR tuple (e.g., (18, 17, 67)).\n        tolerance (int, optional): Tolerance for color matching. Defaults to 10.\n\n    Returns:\n        int: Number of pixels that match the target color.\n    \"\"\"\n    target_color_bgr = convert_color_to_bgr_tuple(color=target_color)\n    lower_bound = np.array(target_color_bgr) - tolerance\n    upper_bound = np.array(target_color_bgr) + tolerance\n\n    # Use vectorized comparison to directly create a mask and count non-zero elements\n    mask = cv2.inRange(image, lower_bound, upper_bound)\n\n    return int(cv2.countNonZero(mask))\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/sift/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/sift/v1/#inference.core.workflows.core_steps.classical_cv.sift.v1.apply_sift","title":"<code>apply_sift(image)</code>","text":"<p>Applies SIFT to the image. Args:     image: Input image. Returns:     np.ndarray: Image with keypoints drawn.     list: Keypoints detected.     np.ndarray: Descriptors of the keypoints.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/sift/v1.py</code> <pre><code>def apply_sift(image: np.ndarray) -&gt; (np.ndarray, list, np.ndarray):\n    \"\"\"\n    Applies SIFT to the image.\n    Args:\n        image: Input image.\n    Returns:\n        np.ndarray: Image with keypoints drawn.\n        list: Keypoints detected.\n        np.ndarray: Descriptors of the keypoints.\n    \"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    kp, des = sift.detectAndCompute(gray, None)\n    img_with_kp = cv2.drawKeypoints(gray, kp, image)\n    # Convert keypoints to the desired format\n    keypoints = [\n        {\n            \"pt\": (point.pt[0], point.pt[1]),\n            \"size\": point.size,\n            \"angle\": point.angle,\n            \"response\": point.response,\n            \"octave\": point.octave,\n            \"class_id\": point.class_id,\n        }\n        for point in kp\n    ]\n    return img_with_kp, keypoints, des\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/sift_comparison/v2/","title":"V2","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/sift_comparison/v2/#inference.core.workflows.core_steps.classical_cv.sift_comparison.v2.apply_sift","title":"<code>apply_sift(image, visualize=False)</code>","text":"<p>Applies SIFT to the image. Args:     image: Input image.     visualize: Whether to visualize keypoints on the image. Returns:     img_with_kp: Image with keypoints drawn (if visualize is True).     kp: List of cv2.KeyPoint objects.     keypoints_dicts: List of keypoints as dictionaries.     des: Descriptors of the keypoints.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/sift_comparison/v2.py</code> <pre><code>def apply_sift(\n    image: np.ndarray, visualize=False\n) -&gt; (Optional[np.ndarray], list, list, np.ndarray):\n    \"\"\"\n    Applies SIFT to the image.\n    Args:\n        image: Input image.\n        visualize: Whether to visualize keypoints on the image.\n    Returns:\n        img_with_kp: Image with keypoints drawn (if visualize is True).\n        kp: List of cv2.KeyPoint objects.\n        keypoints_dicts: List of keypoints as dictionaries.\n        des: Descriptors of the keypoints.\n    \"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    kp, des = sift.detectAndCompute(gray, None)\n    img_with_kp = None\n    if visualize:\n        img_with_kp = cv2.drawKeypoints(gray, kp, None)\n    # Convert keypoints to the desired format\n    keypoints_dicts = [\n        {\n            \"pt\": (point.pt[0], point.pt[1]),\n            \"size\": point.size,\n            \"angle\": point.angle,\n            \"response\": point.response,\n            \"octave\": point.octave,\n            \"class_id\": point.class_id,\n        }\n        for point in kp\n    ]\n    return img_with_kp, kp, keypoints_dicts, des\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/size_measurement/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/size_measurement/v1/#inference.core.workflows.core_steps.classical_cv.size_measurement.v1.compute_aligned_dimensions","title":"<code>compute_aligned_dimensions(contour)</code>","text":"<p>Compute the width and height of an object based on its contour, ensuring proper orientation.</p> <p>This function: 1. Finds the minimum area rectangle that encloses the contour 2. Determines which edges correspond to width and height by analyzing their angles 3. Returns dimensions where width is the more horizontal edge and height is the more vertical edge</p> <p>Parameters:</p> Name Type Description Default <code>contour</code> <code>ndarray</code> <p>Array of points representing the object's contour</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: A tuple of (width_pixels, height_pixels) where: - width_pixels: Length of the more horizontal edge - height_pixels: Length of the more vertical edge</p> Note <p>The function uses angle analysis to ensure consistent width/height assignment regardless of the object's rotation. The edge closer to horizontal (0\u00b0 or 180\u00b0) is always considered the width.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/size_measurement/v1.py</code> <pre><code>def compute_aligned_dimensions(contour: np.ndarray) -&gt; Tuple[float, float]:\n    \"\"\"\n    Compute the width and height of an object based on its contour, ensuring proper orientation.\n\n    This function:\n    1. Finds the minimum area rectangle that encloses the contour\n    2. Determines which edges correspond to width and height by analyzing their angles\n    3. Returns dimensions where width is the more horizontal edge and height is the more vertical edge\n\n    Args:\n        contour (np.ndarray): Array of points representing the object's contour\n\n    Returns:\n        Tuple[float, float]: A tuple of (width_pixels, height_pixels) where:\n            - width_pixels: Length of the more horizontal edge\n            - height_pixels: Length of the more vertical edge\n\n    Note:\n        The function uses angle analysis to ensure consistent width/height assignment\n        regardless of the object's rotation. The edge closer to horizontal (0\u00b0 or 180\u00b0)\n        is always considered the width.\n    \"\"\"\n    rect = cv.minAreaRect(contour)\n    box = cv.boxPoints(rect)\n    box = np.array(box, dtype=np.float32)\n\n    edge1 = box[1] - box[0]\n    edge2 = box[2] - box[1]\n\n    len_edge1 = np.linalg.norm(edge1)\n    len_edge2 = np.linalg.norm(edge2)\n\n    angle1 = np.degrees(np.arctan2(edge1[1], edge1[0]))\n    angle2 = np.degrees(np.arctan2(edge2[1], edge2[0]))\n\n    h_score1 = horizontal_score(angle1)\n    h_score2 = horizontal_score(angle2)\n\n    if h_score1 &lt; h_score2:\n        width_pixels = len_edge1\n        height_pixels = len_edge2\n    else:\n        width_pixels = len_edge2\n        height_pixels = len_edge1\n\n    return float(width_pixels), float(height_pixels)\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/size_measurement/v1/#inference.core.workflows.core_steps.classical_cv.size_measurement.v1.get_detection_dimensions","title":"<code>get_detection_dimensions(detection, index)</code>","text":"<p>Retrieve the width and height dimensions of a detected object in pixels.</p> <p>Parameters:</p> Name Type Description Default <code>detection</code> <code>Detections</code> <p>Detection object containing masks and/or bounding boxes</p> required <code>index</code> <code>int</code> <p>Index of the specific detection to analyze</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[float], Optional[float]]</code> <p>Tuple[float, float]: A tuple of (width_pixels, height_pixels) where: - width_pixels: Width of the object in pixels - height_pixels: Height of the object in pixels</p> Notes <p>The function uses two methods to compute dimensions: 1. If a segmentation mask is available:    - Extracts the largest contour from the mask    - Uses compute_aligned_dimensions() to get orientation-aware measurements 2. If no mask is available:    - Falls back to using the bounding box dimensions    - Simply computes width and height as box edges</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/size_measurement/v1.py</code> <pre><code>def get_detection_dimensions(\n    detection: sv.Detections, index: int\n) -&gt; Tuple[Optional[float], Optional[float]]:\n    \"\"\"\n    Retrieve the width and height dimensions of a detected object in pixels.\n\n    Args:\n        detection (sv.Detections): Detection object containing masks and/or bounding boxes\n        index (int): Index of the specific detection to analyze\n\n    Returns:\n        Tuple[float, float]: A tuple of (width_pixels, height_pixels) where:\n            - width_pixels: Width of the object in pixels\n            - height_pixels: Height of the object in pixels\n\n    Notes:\n        The function uses two methods to compute dimensions:\n        1. If a segmentation mask is available:\n           - Extracts the largest contour from the mask\n           - Uses compute_aligned_dimensions() to get orientation-aware measurements\n        2. If no mask is available:\n           - Falls back to using the bounding box dimensions\n           - Simply computes width and height as box edges\n    \"\"\"\n    if detection.mask is not None:\n        mask = detection.mask[index].astype(np.uint8)\n        contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n        if contours:\n            largest_contour = max(contours, key=cv.contourArea)\n            if cv.contourArea(largest_contour) &gt; 0:\n                return compute_aligned_dimensions(largest_contour)\n\n    else:\n        bbox = detection.xyxy[index]\n        w = bbox[2] - bbox[0]\n        h = bbox[3] - bbox[1]\n        return float(w), float(h)\n\n    return None, None\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/size_measurement/v1/#inference.core.workflows.core_steps.classical_cv.size_measurement.v1.horizontal_score","title":"<code>horizontal_score(angle)</code>","text":"<p>Determine how close an angle is to horizontal (0 or 180 degrees). Lower score means more horizontal.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/size_measurement/v1.py</code> <pre><code>def horizontal_score(angle: float) -&gt; float:\n    \"\"\"\n    Determine how close an angle is to horizontal (0 or 180 degrees).\n    Lower score means more horizontal.\n    \"\"\"\n    mod_angle = abs(angle % 180)\n    return min(mod_angle, 180 - mod_angle)\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/size_measurement/v1/#inference.core.workflows.core_steps.classical_cv.size_measurement.v1.parse_reference_dimensions","title":"<code>parse_reference_dimensions(reference_dimensions)</code>","text":"<p>Parse reference dimensions from various input formats.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/size_measurement/v1.py</code> <pre><code>def parse_reference_dimensions(\n    reference_dimensions: Union[str, Tuple[float, float], List[float]],\n) -&gt; Tuple[float, float]:\n    \"\"\"Parse reference dimensions from various input formats.\"\"\"\n    if isinstance(reference_dimensions, str):\n        parts = reference_dimensions.split(\",\")\n        if len(parts) != 2:\n            raise ValueError(\n                \"reference_dimensions must be a string in the format 'width,height'\"\n            )\n        try:\n            reference_dimensions = [float(p.strip()) for p in parts]\n        except ValueError:\n            raise ValueError(\"Invalid format for reference_dimensions\")\n\n    if len(reference_dimensions) != 2:\n        raise ValueError(\"reference_dimensions must have two values (width, height)\")\n\n    return tuple(reference_dimensions)\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/classical_cv/threshold/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/classical_cv/threshold/v1/#inference.core.workflows.core_steps.classical_cv.threshold.v1.apply_thresholding","title":"<code>apply_thresholding(image, threshold_type, thresh_value, max_value)</code>","text":"<p>Applies the specified thresholding to the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image in grayscale.</p> required <code>threshold_type</code> <code>str</code> <p>Type of thresholding ('binary', 'binary_inv', 'trunc', 'tozero', 'tozero_inv', 'adaptive_mean', 'adaptive_gaussian', 'otsu').</p> required <code>thresh_value</code> <code>int</code> <p>Threshold value.</p> required <code>max_value</code> <code>int</code> <p>Maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Image with thresholding applied.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/threshold/v1.py</code> <pre><code>def apply_thresholding(\n    image: np.ndarray, threshold_type: str, thresh_value: int, max_value: int\n) -&gt; np.ndarray:\n    \"\"\"\n    Applies the specified thresholding to the image.\n\n    Args:\n        image (np.ndarray): Input image in grayscale.\n        threshold_type (str): Type of thresholding ('binary', 'binary_inv', 'trunc', 'tozero', 'tozero_inv', 'adaptive_mean', 'adaptive_gaussian', 'otsu').\n        thresh_value (int, optional): Threshold value.\n        max_value (int, optional): Maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.\n\n    Returns:\n        np.ndarray: Image with thresholding applied.\n    \"\"\"\n    if threshold_type == \"binary\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_BINARY\n        )\n    elif threshold_type == \"binary_inv\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_BINARY_INV\n        )\n    elif threshold_type == \"trunc\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_TRUNC\n        )\n    elif threshold_type == \"tozero\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_TOZERO\n        )\n    elif threshold_type == \"tozero_inv\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_TOZERO_INV\n        )\n    elif threshold_type == \"adaptive_mean\":\n        thresh_image = cv2.adaptiveThreshold(\n            image, max_value, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2\n        )\n    elif threshold_type == \"adaptive_gaussian\":\n        thresh_image = cv2.adaptiveThreshold(\n            image,\n            max_value,\n            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n            cv2.THRESH_BINARY,\n            11,\n            2,\n        )\n    elif threshold_type == \"otsu\":\n        _, thresh_image = cv2.threshold(\n            image, 0, max_value, cv2.THRESH_BINARY + cv2.THRESH_OTSU\n        )\n    else:\n        raise ValueError(f\"Unknown threshold type: {threshold_type}\")\n\n    return thresh_image\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/common/utils/","title":"Utils","text":""},{"location":"reference/inference/core/workflows/core_steps/common/utils/#inference.core.workflows.core_steps.common.utils.remove_unexpected_keys_from_dictionary","title":"<code>remove_unexpected_keys_from_dictionary(dictionary, expected_keys)</code>","text":"<p>This function mutates input <code>dictionary</code></p> Source code in <code>inference/core/workflows/core_steps/common/utils.py</code> <pre><code>def remove_unexpected_keys_from_dictionary(\n    dictionary: dict,\n    expected_keys: set,\n) -&gt; dict:\n    \"\"\"This function mutates input `dictionary`\"\"\"\n    unexpected_keys = set(dictionary.keys()).difference(expected_keys)\n    for unexpected_key in unexpected_keys:\n        del dictionary[unexpected_key]\n    return dictionary\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/fusion/detections_stitch/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/fusion/detections_stitch/v1/#inference.core.workflows.core_steps.fusion.detections_stitch.v1.move_detections","title":"<code>move_detections(detections, offset, resolution_wh)</code>","text":"<p>Copied from: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L17-L16 to avoid fragile contract with supervision, as this function is not element of public API.</p> Source code in <code>inference/core/workflows/core_steps/fusion/detections_stitch/v1.py</code> <pre><code>def move_detections(\n    detections: sv.Detections,\n    offset: Optional[np.ndarray],\n    resolution_wh: Optional[Tuple[int, int]],\n) -&gt; sv.Detections:\n    \"\"\"\n    Copied from: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L17-L16\n    to avoid fragile contract with supervision, as this function is not element of public\n    API.\n    \"\"\"\n    if len(detections) == 0:\n        return detections\n    if offset is None:\n        raise ValueError(\"To move non-empty detections offset is needed, but not given\")\n    detections.xyxy = move_boxes(xyxy=detections.xyxy, offset=offset)\n    if detections.mask is not None:\n        if resolution_wh is None:\n            raise ValueError(\n                \"To move non-empty detections with segmentation mask, resolution_wh is needed, but not given.\"\n            )\n        detections.mask = move_masks(\n            masks=detections.mask, offset=offset, resolution_wh=resolution_wh\n        )\n    return detections\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/models/foundation/gaze/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/models/foundation/gaze/v1/#inference.core.workflows.core_steps.models.foundation.gaze.v1.convert_gaze_detections_to_sv_detections_and_angles","title":"<code>convert_gaze_detections_to_sv_detections_and_angles(images, gaze_predictions)</code>","text":"<p>Convert gaze detection results to supervision detections and angle lists.</p> Source code in <code>inference/core/workflows/core_steps/models/foundation/gaze/v1.py</code> <pre><code>def convert_gaze_detections_to_sv_detections_and_angles(\n    images: Batch[WorkflowImageData],\n    gaze_predictions: List[dict],\n) -&gt; Tuple[List[sv.Detections], List[List[float]], List[List[float]]]:\n    \"\"\"Convert gaze detection results to supervision detections and angle lists.\"\"\"\n    face_predictions = []\n    yaw_degrees = []\n    pitch_degrees = []\n\n    for single_image, predictions in zip(images, gaze_predictions):\n        height, width = single_image.numpy_image.shape[:2]\n\n        # Format predictions for this image\n        image_face_preds = {\n            \"predictions\": [],\n            \"image\": {\"width\": width, \"height\": height},\n        }\n        batch_yaw = []\n        batch_pitch = []\n\n        for p in predictions:  # predictions is already a list\n            p_dict = p.model_dump(by_alias=True, exclude_none=True)\n            for pred in p_dict[\"predictions\"]:\n                face = pred[\"face\"]\n\n                # Face detection with landmarks\n                face_pred = {\n                    \"x\": face[\"x\"],\n                    \"y\": face[\"y\"],\n                    \"width\": face[\"width\"],\n                    \"height\": face[\"height\"],\n                    \"confidence\": face[\"confidence\"],\n                    \"class\": \"face\",\n                    \"class_id\": 0,\n                    \"keypoints\": [\n                        {\n                            \"x\": l[\"x\"],\n                            \"y\": l[\"y\"],\n                            \"confidence\": face[\"confidence\"],\n                            \"class\": str(i),\n                            \"class_id\": i,\n                        }\n                        for i, l in enumerate(face[\"landmarks\"])\n                    ],\n                }\n\n                image_face_preds[\"predictions\"].append(face_pred)\n\n                # Store angles in degrees\n                batch_yaw.append(pred[\"yaw\"] * 180 / np.pi)\n                batch_pitch.append(pred[\"pitch\"] * 180 / np.pi)\n\n        face_predictions.append(image_face_preds)\n        yaw_degrees.append(batch_yaw)\n        pitch_degrees.append(batch_pitch)\n\n    # Process predictions\n    face_preds = convert_inference_detections_batch_to_sv_detections(face_predictions)\n\n    # Add keypoints to supervision detections\n    for prediction, detections in zip(face_predictions, face_preds):\n        add_inference_keypoints_to_sv_detections(\n            inference_prediction=prediction[\"predictions\"],\n            detections=detections,\n        )\n\n    face_preds = attach_prediction_type_info_to_sv_detections_batch(\n        predictions=face_preds,\n        prediction_type=\"facial-landmark\",\n    )\n    face_preds = attach_parents_coordinates_to_batch_of_sv_detections(\n        images=images,\n        predictions=face_preds,\n    )\n\n    return face_preds, yaw_degrees, pitch_degrees\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/models/foundation/openai/v3/","title":"V3","text":""},{"location":"reference/inference/core/workflows/core_steps/models/foundation/stability_ai/inpainting/v1/","title":"V1","text":"<p>Credits to: https://github.com/Fafruch for origin idea</p>"},{"location":"reference/inference/core/workflows/core_steps/models/foundation/stability_ai/outpainting/v1/","title":"V1","text":"<p>Credits to: https://github.com/Fafruch for origin idea</p>"},{"location":"reference/inference/core/workflows/core_steps/sinks/roboflow/dataset_upload/v1/","title":"V1","text":"<ul> <li>WARNING!                            *</li> </ul> <p>This module contains the utility functions used by RoboflowDatasetUploadBlockV2.</p> <p>We do not recommend making multiple blocks dependent on the same code, but the change between v1 and v2 was basically the default value of some parameter - hence we decided not to replicate the code.</p> <p>If you need to modify this module beware that you may introduce change to RoboflowDatasetUploadBlockV2! If that happens, probably that's the time to disentangle those blocks and copy the code.</p>"},{"location":"reference/inference/core/workflows/core_steps/transformations/detections_merge/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/transformations/detections_merge/v1/#inference.core.workflows.core_steps.transformations.detections_merge.v1.calculate_union_bbox","title":"<code>calculate_union_bbox(detections)</code>","text":"<p>Calculate a single bounding box that contains all input detections.</p> Source code in <code>inference/core/workflows/core_steps/transformations/detections_merge/v1.py</code> <pre><code>def calculate_union_bbox(detections: sv.Detections) -&gt; np.ndarray:\n    \"\"\"Calculate a single bounding box that contains all input detections.\"\"\"\n    if len(detections) == 0:\n        return np.array([], dtype=np.float32).reshape(0, 4)\n\n    # Get all bounding boxes\n    xyxy = detections.xyxy\n\n    # Calculate the union by taking min/max coordinates\n    x1 = np.min(xyxy[:, 0])\n    y1 = np.min(xyxy[:, 1])\n    x2 = np.max(xyxy[:, 2])\n    y2 = np.max(xyxy[:, 3])\n\n    return np.array([[x1, y1, x2, y2]])\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/detections_merge/v1/#inference.core.workflows.core_steps.transformations.detections_merge.v1.get_lowest_confidence_index","title":"<code>get_lowest_confidence_index(detections)</code>","text":"<p>Get the index of the detection with the lowest confidence.</p> Source code in <code>inference/core/workflows/core_steps/transformations/detections_merge/v1.py</code> <pre><code>def get_lowest_confidence_index(detections: sv.Detections) -&gt; int:\n    \"\"\"Get the index of the detection with the lowest confidence.\"\"\"\n    if detections.confidence is None:\n        return 0\n    return int(np.argmin(detections.confidence))\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/image_slicer/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/transformations/image_slicer/v1/#inference.core.workflows.core_steps.transformations.image_slicer.v1.generate_offsets","title":"<code>generate_offsets(resolution_wh, slice_wh, overlap_ratio_wh)</code>","text":"<p>Original code: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L204-L203 to avoid fragile contract with supervision, as this function is not element of public API.</p> <p>Generate offset coordinates for slicing an image based on the given resolution, slice dimensions, and overlap ratios.</p> <p>Parameters:</p> Name Type Description Default <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple representing the width and height of the image to be sliced.</p> required <code>slice_wh</code> <code>Tuple[int, int]</code> <p>Dimensions of each slice measured in pixels. The</p> required <code>overlap_ratio_wh</code> <code>Optional[Tuple[float, float]]</code> <p>A tuple representing the desired overlap ratio for width and height between consecutive slices. Each value should be in the range [0, 1), where 0 means no overlap and a value close to 1 means high overlap.</p> required <p>Returns:     np.ndarray: An array of shape <code>(n, 4)</code> containing coordinates for each         slice in the format <code>[xmin, ymin, xmax, ymax]</code>.</p> Note <p>The function ensures that slices do not exceed the boundaries of the     original image. As a result, the final slices in the row and column     dimensions might be smaller than the specified slice dimensions if the     image's width or height is not a multiple of the slice's width or     height minus the overlap.</p> Source code in <code>inference/core/workflows/core_steps/transformations/image_slicer/v1.py</code> <pre><code>def generate_offsets(\n    resolution_wh: Tuple[int, int],\n    slice_wh: Tuple[int, int],\n    overlap_ratio_wh: Optional[Tuple[float, float]],\n) -&gt; np.ndarray:\n    \"\"\"\n    Original code: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L204-L203\n    to avoid fragile contract with supervision, as this function is not element of public\n    API.\n\n    Generate offset coordinates for slicing an image based on the given resolution,\n    slice dimensions, and overlap ratios.\n\n    Args:\n        resolution_wh (Tuple[int, int]): A tuple representing the width and height\n            of the image to be sliced.\n        slice_wh (Tuple[int, int]): Dimensions of each slice measured in pixels. The\n        tuple should be in the format `(width, height)`.\n        overlap_ratio_wh (Optional[Tuple[float, float]]): A tuple representing the\n            desired overlap ratio for width and height between consecutive slices.\n            Each value should be in the range [0, 1), where 0 means no overlap and\n            a value close to 1 means high overlap.\n    Returns:\n        np.ndarray: An array of shape `(n, 4)` containing coordinates for each\n            slice in the format `[xmin, ymin, xmax, ymax]`.\n\n    Note:\n        The function ensures that slices do not exceed the boundaries of the\n            original image. As a result, the final slices in the row and column\n            dimensions might be smaller than the specified slice dimensions if the\n            image's width or height is not a multiple of the slice's width or\n            height minus the overlap.\n    \"\"\"\n    slice_width, slice_height = slice_wh\n    image_width, image_height = resolution_wh\n    overlap_width = int(overlap_ratio_wh[0] * slice_width)\n    overlap_height = int(overlap_ratio_wh[1] * slice_height)\n    width_stride = slice_width - overlap_width\n    height_stride = slice_height - overlap_height\n    ws = np.arange(0, image_width, width_stride)\n    hs = np.arange(0, image_height, height_stride)\n    xmin, ymin = np.meshgrid(ws, hs)\n    xmax = np.clip(xmin + slice_width, 0, image_width)\n    ymax = np.clip(ymin + slice_height, 0, image_height)\n    return np.stack([xmin, ymin, xmax, ymax], axis=-1).reshape(-1, 4)\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/image_slicer/v2/","title":"V2","text":""},{"location":"reference/inference/core/workflows/core_steps/transformations/image_slicer/v2/#inference.core.workflows.core_steps.transformations.image_slicer.v2.generate_offsets","title":"<code>generate_offsets(resolution_wh, slice_wh, overlap_ratio_wh)</code>","text":"<p>This is modification of the function from block v1, which makes sure that the \"border\" crops are pushed towards the center of the image, making sure:     * all crops will be the same size     * deduplication of crops coordinates is done</p> Source code in <code>inference/core/workflows/core_steps/transformations/image_slicer/v2.py</code> <pre><code>def generate_offsets(\n    resolution_wh: Tuple[int, int],\n    slice_wh: Tuple[int, int],\n    overlap_ratio_wh: Tuple[float, float],\n) -&gt; np.ndarray:\n    \"\"\"\n    This is modification of the function from block v1, which\n    makes sure that the \"border\" crops are pushed towards the center of\n    the image, making sure:\n        * all crops will be the same size\n        * deduplication of crops coordinates is done\n    \"\"\"\n    slice_width, slice_height = slice_wh\n    image_width, image_height = resolution_wh\n    slice_width = min(slice_width, image_width)\n    slice_height = min(slice_height, image_height)\n    overlap_width = int(overlap_ratio_wh[0] * slice_width)\n    overlap_height = int(overlap_ratio_wh[1] * slice_height)\n    width_stride = slice_width - overlap_width\n    height_stride = slice_height - overlap_height\n    ws = np.arange(0, image_width, width_stride)\n    ws_left_over = np.clip(ws + slice_width - image_width, 0, slice_width)\n    hs = np.arange(0, image_height, height_stride)\n    hs_left_over = np.clip(hs + slice_height - image_height, 0, slice_height)\n    anchors_ws = ws - ws_left_over\n    anchors_hs = hs - hs_left_over\n    xmin, ymin = np.meshgrid(anchors_ws, anchors_hs)\n    xmax = np.clip(xmin + slice_width, 0, image_width)\n    ymax = np.clip(ymin + slice_height, 0, image_height)\n    results = np.stack([xmin, ymin, xmax, ymax], axis=-1).reshape(-1, 4)\n    deduplicated_results = []\n    already_seen = set()\n    for xyxy in results:\n        xyxy_tuple = tuple(xyxy)\n        if xyxy_tuple in already_seen:\n            continue\n        deduplicated_results.append(xyxy)\n        already_seen.add(xyxy_tuple)\n    return np.array(deduplicated_results)\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/qr_code_generator/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/transformations/qr_code_generator/v1/#inference.core.workflows.core_steps.transformations.qr_code_generator.v1.generate_qr_code","title":"<code>generate_qr_code(text, version=None, box_size=10, error_correct='M', border=4, fill_color='BLACK', back_color='WHITE')</code>","text":"<p>Generate a QR code PNG image from text input.</p> Source code in <code>inference/core/workflows/core_steps/transformations/qr_code_generator/v1.py</code> <pre><code>def generate_qr_code(\n    text: str,\n    version: Optional[int] = None,\n    box_size: int = 10,\n    error_correct: str = \"M\",\n    border: int = 4,\n    fill_color: str = \"BLACK\",\n    back_color: str = \"WHITE\",\n) -&gt; WorkflowImageData:\n    \"\"\"Generate a QR code PNG image from text input.\"\"\"\n    global _ERROR_LEVELS, _QR_CACHE\n\n    # Check cache first\n    cached_result = _QR_CACHE.get(\n        text, version, box_size, error_correct, border, fill_color, back_color\n    )\n    if cached_result is not None:\n        return cached_result\n\n    try:\n        import qrcode\n    except ImportError:\n        raise ImportError(\n            \"qrcode library is required for QR code generation. \"\n            \"Install it with: pip install qrcode\"\n        )\n    if _ERROR_LEVELS is None:\n        _ERROR_LEVELS = _get_error_levels()\n\n    # Parse colors using the common utility that handles hex, rgb, bgr, and standard names\n    try:\n        # Convert to supervision Color object, then to RGB tuple for qrcode library\n        fill_sv_color = str_to_color(fill_color)\n        fill = fill_sv_color.as_rgb()  # Returns (R, G, B) tuple\n    except (ValueError, AttributeError):\n        # Fallback to original string if not a recognized format\n        # This allows qrcode library to handle CSS3 color names directly\n        fill = fill_color\n\n    try:\n        back_sv_color = str_to_color(back_color)\n        back = back_sv_color.as_rgb()  # Returns (R, G, B) tuple\n    except (ValueError, AttributeError):\n        # Fallback to original string if not a recognized format\n        back = back_color\n\n    error_level = _ERROR_LEVELS.get(\n        error_correct.upper(), qrcode.constants.ERROR_CORRECT_M\n    )\n\n    # Create QR code\n    qr = qrcode.QRCode(\n        version=version,\n        error_correction=error_level,\n        box_size=box_size,\n        border=border,\n    )\n\n    qr.add_data(text)\n    qr.make(fit=(version is None))\n\n    # Generate image using default image factory\n    img = qr.make_image(\n        fill_color=fill,\n        back_color=back,\n    ).convert(\n        \"RGB\"\n    )  # Ensure always RGB\n\n    # Direct conversion from PIL.Image to numpy array (much faster than encode/decode)\n    numpy_image = np.array(img)\n\n    # Convert from RGB (PIL format) to BGR (OpenCV/WorkflowImageData format)\n    # PIL creates RGB images, but WorkflowImageData expects BGR format\n    numpy_image = numpy_image[:, :, ::-1]  # RGB -&gt; BGR\n\n    # Defensive: numpy_image should never be None; original code checks for None on OpenCV decode failure\n    if numpy_image is None or numpy_image.size == 0:\n        raise ValueError(\"Failed to generate QR code image\")\n\n    # Create WorkflowImageData\n    parent_metadata = ImageParentMetadata(parent_id=f\"qr_code.{uuid4()}\")\n    result = WorkflowImageData(\n        parent_metadata=parent_metadata,\n        numpy_image=numpy_image,\n    )\n\n    # Store in cache\n    _QR_CACHE.put(\n        text, version, box_size, error_correct, border, fill_color, back_color, result\n    )\n\n    return result\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.get_line_separator","title":"<code>get_line_separator(reading_direction)</code>","text":"<p>Get the appropriate separator based on reading direction.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def get_line_separator(reading_direction: str) -&gt; str:\n    \"\"\"Get the appropriate separator based on reading direction.\"\"\"\n    return \"\\n\" if reading_direction in [\"left_to_right\", \"right_to_left\"] else \" \"\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.group_detections_by_line","title":"<code>group_detections_by_line(xyxy, reading_direction, tolerance)</code>","text":"<p>Group detections into lines based on primary coordinate.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def group_detections_by_line(\n    xyxy: np.ndarray,\n    reading_direction: str,\n    tolerance: int,\n) -&gt; Dict[float, Dict[str, List]]:\n    \"\"\"Group detections into lines based on primary coordinate.\"\"\"\n    # After prepare_coordinates swap, we always group by y ([:, 1])\n    primary_coord = xyxy[:, 1]  # This is y for horizontal, swapped x for vertical\n\n    # Round primary coordinate to group into lines\n    rounded_primary = np.round(primary_coord / tolerance) * tolerance\n\n    boxes_by_line = {}\n    # Group bounding boxes and associated indices by line\n    for i, (bbox, line_pos) in enumerate(zip(xyxy, rounded_primary)):\n        if line_pos not in boxes_by_line:\n            boxes_by_line[line_pos] = {\"xyxy\": [bbox], \"idx\": [i]}\n        else:\n            boxes_by_line[line_pos][\"xyxy\"].append(bbox)\n            boxes_by_line[line_pos][\"idx\"].append(i)\n\n    return boxes_by_line\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.prepare_coordinates","title":"<code>prepare_coordinates(xyxy, reading_direction)</code>","text":"<p>Prepare coordinates based on reading direction.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def prepare_coordinates(\n    xyxy: np.ndarray,\n    reading_direction: str,\n) -&gt; np.ndarray:\n    \"\"\"Prepare coordinates based on reading direction.\"\"\"\n    if reading_direction in [\"vertical_top_to_bottom\", \"vertical_bottom_to_top\"]:\n        # Swap x and y coordinates: [x1,y1,x2,y2] -&gt; [y1,x1,y2,x2]\n        return xyxy[:, [1, 0, 3, 2]]\n    return xyxy\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.sort_line_detections","title":"<code>sort_line_detections(line_xyxy, reading_direction)</code>","text":"<p>Sort detections within a line based on reading direction.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def sort_line_detections(\n    line_xyxy: np.ndarray,\n    reading_direction: str,\n) -&gt; np.ndarray:\n    \"\"\"Sort detections within a line based on reading direction.\"\"\"\n    # After prepare_coordinates swap, we always sort by x ([:, 0])\n    if reading_direction in [\"left_to_right\", \"vertical_top_to_bottom\"]:\n        return line_xyxy[:, 0].argsort()  # Sort by x1 (original x or swapped y)\n    else:  # right_to_left or vertical_bottom_to_top\n        return (-line_xyxy[:, 0]).argsort()  # Sort by -x1 (original -x or swapped -y)\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.stitch_ocr_detections","title":"<code>stitch_ocr_detections(detections, reading_direction='left_to_right', tolerance=10, delimiter='')</code>","text":"<p>Stitch OCR detections into coherent text based on spatial arrangement.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>Supervision Detections object containing OCR results</p> required <code>reading_direction</code> <code>str</code> <p>Direction to read text (\"left_to_right\", \"right_to_left\",              \"vertical_top_to_bottom\", \"vertical_bottom_to_top\")</p> <code>'left_to_right'</code> <code>tolerance</code> <code>int</code> <p>Vertical tolerance for grouping text into lines</p> <code>10</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict containing stitched OCR text under 'ocr_text' key</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def stitch_ocr_detections(\n    detections: sv.Detections,\n    reading_direction: str = \"left_to_right\",\n    tolerance: int = 10,\n    delimiter: str = \"\",\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Stitch OCR detections into coherent text based on spatial arrangement.\n\n    Args:\n        detections: Supervision Detections object containing OCR results\n        reading_direction: Direction to read text (\"left_to_right\", \"right_to_left\",\n                         \"vertical_top_to_bottom\", \"vertical_bottom_to_top\")\n        tolerance: Vertical tolerance for grouping text into lines\n\n    Returns:\n        Dict containing stitched OCR text under 'ocr_text' key\n    \"\"\"\n    if len(detections) == 0:\n        return {\"ocr_text\": \"\"}\n\n    xyxy = detections.xyxy.round().astype(dtype=int)\n    class_names = detections.data[\"class_name\"]\n\n    # Prepare coordinates based on reading direction\n    xyxy = prepare_coordinates(xyxy, reading_direction)\n\n    # Group detections into lines\n    boxes_by_line = group_detections_by_line(xyxy, reading_direction, tolerance)\n    # Sort lines based on reading direction\n    lines = sorted(\n        boxes_by_line.keys(), reverse=reading_direction in [\"vertical_bottom_to_top\"]\n    )\n\n    # Build final text\n    ordered_class_names = []\n    for i, key in enumerate(lines):\n        line_data = boxes_by_line[key]\n        line_xyxy = np.array(line_data[\"xyxy\"])\n        line_idx = np.array(line_data[\"idx\"])\n\n        # Sort detections within line\n        sort_idx = sort_line_detections(line_xyxy, reading_direction)\n\n        # Add sorted class names for this line\n        ordered_class_names.extend(class_names[line_idx[sort_idx]])\n\n        # Add line separator if not last line\n        if i &lt; len(lines) - 1:\n            ordered_class_names.append(get_line_separator(reading_direction))\n\n    return {\"ocr_text\": delimiter.join(ordered_class_names)}\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/","title":"V1","text":""},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.create_label_visualization","title":"<code>create_label_visualization(sorted_predictions, text_position, text, w, h, initial_offset, total_spacing, text_scale, text_padding)</code>","text":"<p>Create visualization layout for classification labels.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def create_label_visualization(\n    sorted_predictions: List[dict],\n    text_position: str,\n    text: str,\n    w: int,\n    h: int,\n    initial_offset: float,\n    total_spacing: float,\n    text_scale: float,\n    text_padding: int,\n) -&gt; Tuple[np.ndarray, List[str], List[dict]]:\n    \"\"\"Create visualization layout for classification labels.\"\"\"\n    if text_position in [\"BOTTOM_LEFT\", \"BOTTOM_CENTER\", \"BOTTOM_RIGHT\"]:\n        return handle_bottom_position(\n            sorted_predictions, text, w, h, initial_offset, total_spacing\n        )\n    elif text_position in [\"CENTER\", \"CENTER_LEFT\", \"CENTER_RIGHT\"]:\n        return handle_center_position(\n            sorted_predictions,\n            text,\n            text_position,\n            w,\n            h,\n            total_spacing,\n            text_scale,\n            text_padding,\n        )\n    else:  # Top positions\n        return handle_top_position(\n            sorted_predictions, text, w, h, initial_offset, total_spacing\n        )\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.detect_prediction_type","title":"<code>detect_prediction_type(predictions)</code>","text":"<p>Detect whether predictions are single-label or multi-label based on structure.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>The predictions dictionary</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>'single-label' or 'multi-label'</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def detect_prediction_type(predictions: dict) -&gt; str:\n    \"\"\"\n    Detect whether predictions are single-label or multi-label based on structure.\n\n    Args:\n        predictions (dict): The predictions dictionary\n\n    Returns:\n        str: 'single-label' or 'multi-label'\n    \"\"\"\n    if isinstance(predictions.get(\"predictions\"), list):\n        return \"single-label\"\n    return \"multi-label\"\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.format_labels","title":"<code>format_labels(predictions, text='Class and Confidence')</code>","text":"<p>Format labels based on specified text option.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>List of prediction dictionaries containing 'class' and 'confidence'</p> required <code>text</code> <code>str</code> <p>One of \"class\", \"confidence\", or \"class and confidence\"</p> <code>'Class and Confidence'</code> <p>Returns:</p> Name Type Description <code>list</code> <p>Formatted label strings</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def format_labels(predictions, text=\"Class and Confidence\"):\n    \"\"\"\n    Format labels based on specified text option.\n\n    Args:\n        predictions (list): List of prediction dictionaries containing 'class' and 'confidence'\n        text (str): One of \"class\", \"confidence\", or \"class and confidence\"\n\n    Returns:\n        list: Formatted label strings\n    \"\"\"\n    if text == \"Class\":\n        labels = [f\"{p['class']}\" for p in predictions]\n    elif text == \"Confidence\":\n        labels = [f\"{p['confidence']:.2f}\" for p in predictions]\n    elif text == \"Class and Confidence\":\n        labels = [f\"{p['class']} {p['confidence']:.2f}\" for p in predictions]\n    else:\n        raise ValueError(\n            \"text must be one of: 'class', 'confidence', or 'class and confidence'\"\n        )\n\n    return labels\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.format_multi_label_predictions","title":"<code>format_multi_label_predictions(predictions)</code>","text":"<p>Transform multi-label predictions from predicted_classes into standard format.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>The predictions dictionary</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: Formatted predictions list</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def format_multi_label_predictions(predictions: dict) -&gt; List[dict]:\n    \"\"\"\n    Transform multi-label predictions from predicted_classes into standard format.\n\n    Args:\n        predictions (dict): The predictions dictionary\n\n    Returns:\n        List[dict]: Formatted predictions list\n    \"\"\"\n    formatted_predictions = []\n    for class_name in predictions[\"predicted_classes\"]:\n        pred_info = predictions[\"predictions\"][class_name]\n        formatted_predictions.append(\n            {\n                \"class\": class_name,\n                \"class_id\": pred_info[\"class_id\"],\n                \"confidence\": pred_info[\"confidence\"],\n            }\n        )\n    return formatted_predictions\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.handle_bottom_position","title":"<code>handle_bottom_position(sorted_predictions, text, w, h, initial_offset, total_spacing)</code>","text":"<p>Handle visualization layout for bottom positions.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def handle_bottom_position(\n    sorted_predictions: List[dict],\n    text: str,\n    w: int,\n    h: int,\n    initial_offset: float,\n    total_spacing: float,\n) -&gt; Tuple[np.ndarray, List[str], List[dict]]:\n    \"\"\"Handle visualization layout for bottom positions.\"\"\"\n    reversed_predictions = sorted_predictions[::-1]\n    xyxy = np.array(\n        [\n            [0, 0, w, h - (initial_offset + i * total_spacing)]\n            for i in range(len(reversed_predictions))\n        ]\n    )\n    labels = format_labels(reversed_predictions, text)\n    return xyxy, labels, reversed_predictions\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.handle_center_position","title":"<code>handle_center_position(sorted_predictions, text, text_position, w, h, total_spacing, text_scale, text_padding)</code>","text":"<p>Handle visualization layout for center positions.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def handle_center_position(\n    sorted_predictions: List[dict],\n    text: str,\n    text_position: str,\n    w: int,\n    h: int,\n    total_spacing: float,\n    text_scale: float,\n    text_padding: int,\n) -&gt; Tuple[np.ndarray, List[str], List[dict]]:\n    \"\"\"Handle visualization layout for center positions.\"\"\"\n    labels = format_labels(sorted_predictions, text)\n    n_predictions = len(sorted_predictions)\n    total_height = total_spacing * n_predictions\n    start_y = max(0, min((h - total_height) / 2, h - total_height))\n\n    max_label_length = max(len(label) for label in labels)\n    char_width = 15\n    label_width = (max_label_length * char_width * text_scale) + (text_padding * 2)\n    extra_padding = 20 + max(0, 10 - text_padding) * 3\n\n    if text_position == \"CENTER_LEFT\":\n        x_start = label_width + extra_padding\n        xyxy = np.array(\n            [\n                [\n                    x_start,\n                    start_y + i * total_spacing,\n                    w,\n                    start_y + (i + 1) * total_spacing,\n                ]\n                for i in range(n_predictions)\n            ]\n        )\n    elif text_position == \"CENTER_RIGHT\":\n        x_end = w - (label_width + extra_padding)\n        xyxy = np.array(\n            [\n                [\n                    0,\n                    start_y + i * total_spacing,\n                    x_end,\n                    start_y + (i + 1) * total_spacing,\n                ]\n                for i in range(n_predictions)\n            ]\n        )\n    else:  # CENTER\n        xyxy = np.array(\n            [\n                [0, start_y + i * total_spacing, w, start_y + (i + 1) * total_spacing]\n                for i in range(n_predictions)\n            ]\n        )\n\n    return xyxy, labels, sorted_predictions\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.handle_top_position","title":"<code>handle_top_position(sorted_predictions, text, w, h, initial_offset, total_spacing)</code>","text":"<p>Handle visualization layout for top positions.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def handle_top_position(\n    sorted_predictions: List[dict],\n    text: str,\n    w: int,\n    h: int,\n    initial_offset: float,\n    total_spacing: float,\n) -&gt; Tuple[np.ndarray, List[str], List[dict]]:\n    \"\"\"Handle visualization layout for top positions.\"\"\"\n    xyxy = np.array(\n        [\n            [0, initial_offset + i * total_spacing, w, h]\n            for i in range(len(sorted_predictions))\n        ]\n    )\n    labels = format_labels(sorted_predictions, text)\n    return xyxy, labels, sorted_predictions\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/classification_label/v1/#inference.core.workflows.core_steps.visualizations.classification_label.v1.validate_prediction_format","title":"<code>validate_prediction_format(predictions, task_type)</code>","text":"<p>Validate that the predictions format matches the specified task type.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>dict</code> <p>The predictions dictionary</p> required <code>task_type</code> <code>str</code> <p>The specified task type ('single-label' or 'multi-label')</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If prediction format doesn't match task type</p> Source code in <code>inference/core/workflows/core_steps/visualizations/classification_label/v1.py</code> <pre><code>def validate_prediction_format(predictions: dict, task_type: str) -&gt; None:\n    \"\"\"\n    Validate that the predictions format matches the specified task type.\n\n    Args:\n        predictions (dict): The predictions dictionary\n        task_type (str): The specified task type ('single-label' or 'multi-label')\n\n    Raises:\n        ValueError: If prediction format doesn't match task type\n    \"\"\"\n    actual_type = detect_prediction_type(predictions)\n\n    if actual_type != task_type:\n        if actual_type == \"single-label\":\n            raise ValueError(\n                \"Received single-label predictions but task_type is set to 'multi-label'. Please correct the task_type setting.\"\n            )\n        else:\n            raise ValueError(\n                \"Received multi-label predictions but task_type is set to 'single-label'. Please correct the task_type setting.\"\n            )\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/","title":"Background color","text":""},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/#inference.core.workflows.core_steps.visualizations.common.annotators.background_color.BackgroundColorAnnotator","title":"<code>BackgroundColorAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing background colors outside of detected box or mask regions.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/background_color.py</code> <pre><code>class BackgroundColorAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing background colors outside of detected box or mask regions.\n    !!! warning\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Color = Color.BLACK,\n        opacity: float = 0.5,\n        force_box: bool = False,\n    ):\n        \"\"\"\n        Args:\n            color (Color): The color to use for annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        \"\"\"\n        self.color: Color = color\n        self.opacity = opacity\n        self.force_box = force_box\n\n    def annotate(self, scene: np.ndarray, detections: Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with masks based on the provided detections.\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n        Example:\n            ```python\n            import supervision as sv\n            image = ...\n            detections = sv.Detections(...)\n            background_color_annotator = sv.BackgroundColorAnnotator()\n            annotated_frame = background_color_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n        ![background-color-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/background-color-annotator-example-purple.png)\n        \"\"\"\n\n        colored_mask = np.full_like(scene, self.color.as_bgr(), dtype=np.uint8)\n\n        cv2.addWeighted(\n            scene, 1 - self.opacity, colored_mask, self.opacity, 0, dst=colored_mask\n        )\n\n        if detections.mask is None or self.force_box:\n            for detection_idx in range(len(detections)):\n                x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n                colored_mask[y1:y2, x1:x2] = scene[y1:y2, x1:x2]\n        else:\n            for mask in detections.mask:\n                colored_mask[mask] = scene[mask]\n\n        return colored_mask\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/#inference.core.workflows.core_steps.visualizations.common.annotators.background_color.BackgroundColorAnnotator.__init__","title":"<code>__init__(color=Color.BLACK, opacity=0.5, force_box=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Color</code> <p>The color to use for annotating detections.</p> <code>BLACK</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/background_color.py</code> <pre><code>def __init__(\n    self,\n    color: Color = Color.BLACK,\n    opacity: float = 0.5,\n    force_box: bool = False,\n):\n    \"\"\"\n    Args:\n        color (Color): The color to use for annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n    \"\"\"\n    self.color: Color = color\n    self.opacity = opacity\n    self.force_box = force_box\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/#inference.core.workflows.core_steps.visualizations.common.annotators.background_color.BackgroundColorAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the given scene with masks based on the provided detections. Args:     scene (ImageType): The image where masks will be drawn.         <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code>         or <code>PIL.Image.Image</code>.     detections (Detections): Object detections to annotate. Returns:     The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code>         or <code>PIL.Image.Image</code>) Example:     <pre><code>import supervision as sv\nimage = ...\ndetections = sv.Detections(...)\nbackground_color_annotator = sv.BackgroundColorAnnotator()\nannotated_frame = background_color_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> </p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/background_color.py</code> <pre><code>def annotate(self, scene: np.ndarray, detections: Detections) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with masks based on the provided detections.\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n    Example:\n        ```python\n        import supervision as sv\n        image = ...\n        detections = sv.Detections(...)\n        background_color_annotator = sv.BackgroundColorAnnotator()\n        annotated_frame = background_color_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n    ![background-color-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/background-color-annotator-example-purple.png)\n    \"\"\"\n\n    colored_mask = np.full_like(scene, self.color.as_bgr(), dtype=np.uint8)\n\n    cv2.addWeighted(\n        scene, 1 - self.opacity, colored_mask, self.opacity, 0, dst=colored_mask\n    )\n\n    if detections.mask is None or self.force_box:\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            colored_mask[y1:y2, x1:x2] = scene[y1:y2, x1:x2]\n    else:\n        for mask in detections.mask:\n            colored_mask[mask] = scene[mask]\n\n    return colored_mask\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/","title":"Halo","text":""},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/#inference.core.workflows.core_steps.visualizations.common.annotators.halo.HaloAnnotator","title":"<code>HaloAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing Halos on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/halo.py</code> <pre><code>class HaloAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing Halos on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        opacity: float = 0.8,\n        kernel_size: int = 40,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            kernel_size (int): The size of the average pooling kernel used for creating\n                the halo.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.opacity = opacity\n        self.color_lookup: ColorLookup = color_lookup\n        self.kernel_size: int = kernel_size\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with halos based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            halo_annotator = sv.HaloAnnotator()\n            annotated_frame = halo_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![halo-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/halo-annotator-example-purple.png)\n        \"\"\"\n        assert isinstance(scene, np.ndarray)\n        colored_mask = np.zeros_like(scene, dtype=np.uint8)\n        fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n            scene.shape[0], scene.shape[1]\n        )\n\n        for detection_idx in np.flip(np.argsort(detections.area)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n            if detections.mask is None:\n                x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n                mask = np.zeros(scene.shape[:2], dtype=bool)\n                mask[y1:y2, x1:x2] = True\n            else:\n                mask = detections.mask[detection_idx]\n            fmask = np.logical_or(fmask, mask)\n            color_bgr = color.as_bgr()\n            colored_mask[mask] = color_bgr\n\n        colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n        colored_mask[fmask] = [0, 0, 0]\n        gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n        alpha = self.opacity * gray / gray.max()\n        alpha_mask = alpha[:, :, np.newaxis]\n        blended_scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n        np.copyto(scene, blended_scene)\n        return scene\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/#inference.core.workflows.core_steps.visualizations.common.annotators.halo.HaloAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, opacity=0.8, kernel_size=40, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.8</code> <code>kernel_size</code> <code>int</code> <p>The size of the average pooling kernel used for creating the halo.</p> <code>40</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/halo.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    opacity: float = 0.8,\n    kernel_size: int = 40,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        kernel_size (int): The size of the average pooling kernel used for creating\n            the halo.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.opacity = opacity\n    self.color_lookup: ColorLookup = color_lookup\n    self.kernel_size: int = kernel_size\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/#inference.core.workflows.core_steps.visualizations.common.annotators.halo.HaloAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with halos based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where masks will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nhalo_annotator = sv.HaloAnnotator()\nannotated_frame = halo_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/halo.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with halos based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        halo_annotator = sv.HaloAnnotator()\n        annotated_frame = halo_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![halo-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/halo-annotator-example-purple.png)\n    \"\"\"\n    assert isinstance(scene, np.ndarray)\n    colored_mask = np.zeros_like(scene, dtype=np.uint8)\n    fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n        scene.shape[0], scene.shape[1]\n    )\n\n    for detection_idx in np.flip(np.argsort(detections.area)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n        if detections.mask is None:\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            mask = np.zeros(scene.shape[:2], dtype=bool)\n            mask[y1:y2, x1:x2] = True\n        else:\n            mask = detections.mask[detection_idx]\n        fmask = np.logical_or(fmask, mask)\n        color_bgr = color.as_bgr()\n        colored_mask[mask] = color_bgr\n\n    colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n    colored_mask[fmask] = [0, 0, 0]\n    gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n    alpha = self.opacity * gray / gray.max()\n    alpha_mask = alpha[:, :, np.newaxis]\n    blended_scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n    np.copyto(scene, blended_scene)\n    return scene\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/","title":"Model comparison","text":""},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/#inference.core.workflows.core_steps.visualizations.common.annotators.model_comparison.ModelComparisonAnnotator","title":"<code>ModelComparisonAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for annotating images by highlighting regions predicted by two different models. This annotator visually distinguishes areas uniquely predicted by each model as well as the background where neither model made a prediction.</p> <p>Attributes:</p> Name Type Description <code>color_a</code> <code>Color</code> <p>Color used to highlight predictions made only by Model A.</p> <code>color_b</code> <code>Color</code> <p>Color used to highlight predictions made only by Model B.</p> <code>background_color</code> <code>Color</code> <p>Color used for parts of the image where neither model made a prediction.</p> <code>opacity</code> <code>float</code> <p>Opacity level of the overlays, ranging between 0 and 1.</p> <code>force_box</code> <code>bool</code> <p>If True, forces the use of bounding boxes for predictions even if masks are available.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison.py</code> <pre><code>class ModelComparisonAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for annotating images by highlighting regions predicted by two different models.\n    This annotator visually distinguishes areas uniquely predicted by each model as well as\n    the background where neither model made a prediction.\n\n    Attributes:\n        color_a (Color): Color used to highlight predictions made only by Model A.\n        color_b (Color): Color used to highlight predictions made only by Model B.\n        background_color (Color): Color used for parts of the image where neither model made a prediction.\n        opacity (float): Opacity level of the overlays, ranging between 0 and 1.\n        force_box (bool): If True, forces the use of bounding boxes for predictions even if masks are available.\n    \"\"\"\n\n    def __init__(\n        self,\n        color_a: Color = Color.GREEN,\n        color_b: Color = Color.RED,\n        background_color: Color = Color.BLACK,\n        opacity: float = 0.7,\n        force_box: bool = False,\n    ):\n        \"\"\"\n        Initializes the ModelComparisonAnnotator with the specified colors, opacity, and behavior.\n\n        Args:\n            color_a (Color): Color used to highlight predictions made only by Model A.\n            color_b (Color): Color used to highlight predictions made only by Model B.\n            background_color (Color): Color for parts of the image not covered by any prediction.\n            opacity (float): Opacity of the overlay mask, must be between 0 and 1.\n            force_box (bool): Whether to use bounding boxes instead of masks if masks are available.\n        \"\"\"\n        self.color_a: Color = color_a\n        self.color_b: Color = color_b\n        self.background_color: Color = background_color\n        self.opacity = opacity\n        self.force_box = force_box\n\n    def annotate(\n        self, scene: np.ndarray, detections_a: Detections, detections_b: Detections\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with highlights representing predictions from two models.\n\n        Args:\n            scene (np.ndarray): Original image as a NumPy array (H x W x C).\n            detections_a (Detections): Predictions from Model A.\n            detections_b (Detections): Predictions from Model B.\n\n        Returns:\n            np.ndarray: Annotated image as a NumPy array.\n        \"\"\"\n\n        # Initialize single-channel masks\n        neither_predicted = np.ones(\n            scene.shape[:2], dtype=np.uint8\n        )  # 1 where neither model predicts\n        a_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n        b_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n\n        # Populate masks based on detections from Model A\n        if detections_a.mask is None or self.force_box:\n            for detection_idx in range(len(detections_a)):\n                x1, y1, x2, y2 = detections_a.xyxy[detection_idx].astype(int)\n                a_predicted[y1:y2, x1:x2] = 1\n                neither_predicted[y1:y2, x1:x2] = 0\n        else:\n            for mask in detections_a.mask:\n                a_predicted[mask.astype(bool)] = 1\n                neither_predicted[mask.astype(bool)] = 0\n\n        # Populate masks based on detections from Model B\n        if detections_b.mask is None or self.force_box:\n            for detection_idx in range(len(detections_b)):\n                x1, y1, x2, y2 = detections_b.xyxy[detection_idx].astype(int)\n                b_predicted[y1:y2, x1:x2] = 1\n                neither_predicted[y1:y2, x1:x2] = 0\n        else:\n            for mask in detections_b.mask:\n                b_predicted[mask.astype(bool)] = 1\n                neither_predicted[mask.astype(bool)] = 0\n\n        # Define combined masks\n        only_a_predicted = a_predicted &amp; (a_predicted ^ b_predicted)\n        only_b_predicted = b_predicted &amp; (b_predicted ^ a_predicted)\n\n        # Prepare overlay colors\n        background_color_bgr = self.background_color.as_bgr()  # Tuple like (B, G, R)\n        color_a_bgr = self.color_a.as_bgr()\n        color_b_bgr = self.color_b.as_bgr()\n\n        # Create full-color overlay images\n        overlay_background = np.full_like(scene, background_color_bgr, dtype=np.uint8)\n        overlay_a = np.full_like(scene, color_a_bgr, dtype=np.uint8)\n        overlay_b = np.full_like(scene, color_b_bgr, dtype=np.uint8)\n\n        # Function to blend and apply overlay based on mask\n        def apply_overlay(base_img, overlay_img, mask, opacity):\n            \"\"\"\n            Blends the overlay with the base image where the mask is set.\n\n            Args:\n                base_img (np.ndarray): Original image.\n                overlay_img (np.ndarray): Overlay color image.\n                mask (np.ndarray): Single-channel mask where to apply the overlay.\n                opacity (float): Opacity of the overlay (0 to 1).\n\n            Returns:\n                np.ndarray: Image with overlay applied.\n            \"\"\"\n            # Blend the entire images\n            blended = cv2.addWeighted(base_img, 1 - opacity, overlay_img, opacity, 0)\n            # Expand mask to three channels\n            mask_3ch = np.stack([mask] * 3, axis=-1)  # Shape: H x W x 3\n            # Ensure mask is boolean\n            mask_bool = mask_3ch.astype(bool)\n            # Apply blended regions where mask is True\n            base_img[mask_bool] = blended[mask_bool]\n            return base_img\n\n        # Apply background overlay where neither model predicted\n        scene = apply_overlay(\n            scene, overlay_background, neither_predicted, self.opacity\n        )\n\n        # Apply overlay for only Model A predictions\n        scene = apply_overlay(scene, overlay_a, only_a_predicted, self.opacity)\n\n        # Apply overlay for only Model B predictions\n        scene = apply_overlay(scene, overlay_b, only_b_predicted, self.opacity)\n\n        # Areas where both models predicted remain unchanged (no overlay)\n\n        return scene\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/#inference.core.workflows.core_steps.visualizations.common.annotators.model_comparison.ModelComparisonAnnotator.__init__","title":"<code>__init__(color_a=Color.GREEN, color_b=Color.RED, background_color=Color.BLACK, opacity=0.7, force_box=False)</code>","text":"<p>Initializes the ModelComparisonAnnotator with the specified colors, opacity, and behavior.</p> <p>Parameters:</p> Name Type Description Default <code>color_a</code> <code>Color</code> <p>Color used to highlight predictions made only by Model A.</p> <code>GREEN</code> <code>color_b</code> <code>Color</code> <p>Color used to highlight predictions made only by Model B.</p> <code>RED</code> <code>background_color</code> <code>Color</code> <p>Color for parts of the image not covered by any prediction.</p> <code>BLACK</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask, must be between 0 and 1.</p> <code>0.7</code> <code>force_box</code> <code>bool</code> <p>Whether to use bounding boxes instead of masks if masks are available.</p> <code>False</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison.py</code> <pre><code>def __init__(\n    self,\n    color_a: Color = Color.GREEN,\n    color_b: Color = Color.RED,\n    background_color: Color = Color.BLACK,\n    opacity: float = 0.7,\n    force_box: bool = False,\n):\n    \"\"\"\n    Initializes the ModelComparisonAnnotator with the specified colors, opacity, and behavior.\n\n    Args:\n        color_a (Color): Color used to highlight predictions made only by Model A.\n        color_b (Color): Color used to highlight predictions made only by Model B.\n        background_color (Color): Color for parts of the image not covered by any prediction.\n        opacity (float): Opacity of the overlay mask, must be between 0 and 1.\n        force_box (bool): Whether to use bounding boxes instead of masks if masks are available.\n    \"\"\"\n    self.color_a: Color = color_a\n    self.color_b: Color = color_b\n    self.background_color: Color = background_color\n    self.opacity = opacity\n    self.force_box = force_box\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/#inference.core.workflows.core_steps.visualizations.common.annotators.model_comparison.ModelComparisonAnnotator.annotate","title":"<code>annotate(scene, detections_a, detections_b)</code>","text":"<p>Annotates the given scene with highlights representing predictions from two models.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>Original image as a NumPy array (H x W x C).</p> required <code>detections_a</code> <code>Detections</code> <p>Predictions from Model A.</p> required <code>detections_b</code> <code>Detections</code> <p>Predictions from Model B.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Annotated image as a NumPy array.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison.py</code> <pre><code>def annotate(\n    self, scene: np.ndarray, detections_a: Detections, detections_b: Detections\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with highlights representing predictions from two models.\n\n    Args:\n        scene (np.ndarray): Original image as a NumPy array (H x W x C).\n        detections_a (Detections): Predictions from Model A.\n        detections_b (Detections): Predictions from Model B.\n\n    Returns:\n        np.ndarray: Annotated image as a NumPy array.\n    \"\"\"\n\n    # Initialize single-channel masks\n    neither_predicted = np.ones(\n        scene.shape[:2], dtype=np.uint8\n    )  # 1 where neither model predicts\n    a_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n    b_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n\n    # Populate masks based on detections from Model A\n    if detections_a.mask is None or self.force_box:\n        for detection_idx in range(len(detections_a)):\n            x1, y1, x2, y2 = detections_a.xyxy[detection_idx].astype(int)\n            a_predicted[y1:y2, x1:x2] = 1\n            neither_predicted[y1:y2, x1:x2] = 0\n    else:\n        for mask in detections_a.mask:\n            a_predicted[mask.astype(bool)] = 1\n            neither_predicted[mask.astype(bool)] = 0\n\n    # Populate masks based on detections from Model B\n    if detections_b.mask is None or self.force_box:\n        for detection_idx in range(len(detections_b)):\n            x1, y1, x2, y2 = detections_b.xyxy[detection_idx].astype(int)\n            b_predicted[y1:y2, x1:x2] = 1\n            neither_predicted[y1:y2, x1:x2] = 0\n    else:\n        for mask in detections_b.mask:\n            b_predicted[mask.astype(bool)] = 1\n            neither_predicted[mask.astype(bool)] = 0\n\n    # Define combined masks\n    only_a_predicted = a_predicted &amp; (a_predicted ^ b_predicted)\n    only_b_predicted = b_predicted &amp; (b_predicted ^ a_predicted)\n\n    # Prepare overlay colors\n    background_color_bgr = self.background_color.as_bgr()  # Tuple like (B, G, R)\n    color_a_bgr = self.color_a.as_bgr()\n    color_b_bgr = self.color_b.as_bgr()\n\n    # Create full-color overlay images\n    overlay_background = np.full_like(scene, background_color_bgr, dtype=np.uint8)\n    overlay_a = np.full_like(scene, color_a_bgr, dtype=np.uint8)\n    overlay_b = np.full_like(scene, color_b_bgr, dtype=np.uint8)\n\n    # Function to blend and apply overlay based on mask\n    def apply_overlay(base_img, overlay_img, mask, opacity):\n        \"\"\"\n        Blends the overlay with the base image where the mask is set.\n\n        Args:\n            base_img (np.ndarray): Original image.\n            overlay_img (np.ndarray): Overlay color image.\n            mask (np.ndarray): Single-channel mask where to apply the overlay.\n            opacity (float): Opacity of the overlay (0 to 1).\n\n        Returns:\n            np.ndarray: Image with overlay applied.\n        \"\"\"\n        # Blend the entire images\n        blended = cv2.addWeighted(base_img, 1 - opacity, overlay_img, opacity, 0)\n        # Expand mask to three channels\n        mask_3ch = np.stack([mask] * 3, axis=-1)  # Shape: H x W x 3\n        # Ensure mask is boolean\n        mask_bool = mask_3ch.astype(bool)\n        # Apply blended regions where mask is True\n        base_img[mask_bool] = blended[mask_bool]\n        return base_img\n\n    # Apply background overlay where neither model predicted\n    scene = apply_overlay(\n        scene, overlay_background, neither_predicted, self.opacity\n    )\n\n    # Apply overlay for only Model A predictions\n    scene = apply_overlay(scene, overlay_a, only_a_predicted, self.opacity)\n\n    # Apply overlay for only Model B predictions\n    scene = apply_overlay(scene, overlay_b, only_b_predicted, self.opacity)\n\n    # Areas where both models predicted remain unchanged (no overlay)\n\n    return scene\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/","title":"Polygon","text":""},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/#inference.core.workflows.core_steps.visualizations.common.annotators.polygon.PolygonAnnotator","title":"<code>PolygonAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing polygons on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/polygon.py</code> <pre><code>class PolygonAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing polygons on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the polygon lines.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with polygons based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where polygons will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            polygon_annotator = sv.PolygonAnnotator()\n            annotated_frame = polygon_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![polygon-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/polygon-annotator-example-purple.png)\n        \"\"\"\n        assert isinstance(scene, np.ndarray)\n\n        for detection_idx in range(len(detections)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n\n            if detections.mask is None:\n                x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n                cv2.rectangle(\n                    img=scene,\n                    pt1=(x1, y1),\n                    pt2=(x2, y2),\n                    color=color.as_bgr(),\n                    thickness=self.thickness,\n                )\n            else:\n                mask = detections.mask[detection_idx]\n                for polygon in mask_to_polygons(mask=mask):\n                    scene = draw_polygon(\n                        scene=scene,\n                        polygon=polygon,\n                        color=color,\n                        thickness=self.thickness,\n                    )\n\n        return scene\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/#inference.core.workflows.core_steps.visualizations.common.annotators.polygon.PolygonAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the polygon lines.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/polygon.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the polygon lines.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/#inference.core.workflows.core_steps.visualizations.common.annotators.polygon.PolygonAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with polygons based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where polygons will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npolygon_annotator = sv.PolygonAnnotator()\nannotated_frame = polygon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/polygon.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with polygons based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where polygons will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        polygon_annotator = sv.PolygonAnnotator()\n        annotated_frame = polygon_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![polygon-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/polygon-annotator-example-purple.png)\n    \"\"\"\n    assert isinstance(scene, np.ndarray)\n\n    for detection_idx in range(len(detections)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n\n        if detections.mask is None:\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n        else:\n            mask = detections.mask[detection_idx]\n            for polygon in mask_to_polygons(mask=mask):\n                scene = draw_polygon(\n                    scene=scene,\n                    polygon=polygon,\n                    color=color,\n                    thickness=self.thickness,\n                )\n\n    return scene\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/compiler/cache/","title":"Cache","text":""},{"location":"reference/inference/core/workflows/execution_engine/v1/compiler/cache/#inference.core.workflows.execution_engine.v1.compiler.cache.BasicWorkflowsCache","title":"<code>BasicWorkflowsCache</code>","text":"<p>               Bases: <code>Generic[V]</code></p> <p>Base cache which is capable of hashing compound payloads based on list of injected hash functions. Hash functions are to produce stable hashing strings. Each function is invoked on <code>get_hash_key(...)</code> kwarg (use named args only!), output string is concatenated and md5 value is calculated.</p> <p>Cache is size bounded, each entry lives until <code>cache_size</code> new entries appear.</p> <p>Raises <code>WorkflowEnvironmentConfigurationError</code> when <code>get_hash_key(...)</code> is not provided with params corresponding to all hash functions.</p> <p>Thread safe thanks to thread lock on <code>get(...)</code> and <code>cache(...)</code>.</p> Source code in <code>inference/core/workflows/execution_engine/v1/compiler/cache.py</code> <pre><code>class BasicWorkflowsCache(Generic[V]):\n    \"\"\"\n    Base cache which is capable of hashing compound payloads based on\n    list of injected hash functions. Hash functions are to produce stable hashing strings.\n    Each function is invoked on `get_hash_key(...)` kwarg (use named args only!),\n    output string is concatenated and md5 value is calculated.\n\n    Cache is size bounded, each entry lives until `cache_size` new entries appear.\n\n    Raises `WorkflowEnvironmentConfigurationError` when `get_hash_key(...)` is not\n    provided with params corresponding to all hash functions.\n\n    Thread safe thanks to thread lock on `get(...)` and `cache(...)`.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache_size: int,\n        hash_functions: List[Tuple[str, Callable[[Any], str]]],\n    ):\n        self._keys_buffer = deque(maxlen=max(cache_size, 1))\n        self._cache: Dict[str, V] = {}\n        self._hash_functions = hash_functions\n        self._cache_lock = Lock()\n\n    def get_hash_key(self, **kwargs) -&gt; str:\n        hash_chunks = []\n        for key_name, hashing_function in self._hash_functions:\n            if key_name not in kwargs:\n                raise WorkflowEnvironmentConfigurationError(\n                    public_message=f\"Cache is miss configured.\",\n                    context=\"workflows_cache | hash_key_generation\",\n                )\n            hash_value = hashing_function(kwargs[key_name])\n            hash_chunks.append(hash_value)\n        return hashlib.md5(\"&lt;|&gt;\".join(hash_chunks).encode(\"utf-8\")).hexdigest()\n\n    def get(self, key: str) -&gt; Optional[V]:\n        with self._cache_lock:\n            return self._cache.get(key)\n\n    def cache(self, key: str, value: V) -&gt; None:\n        with self._cache_lock:\n            if len(self._keys_buffer) == self._keys_buffer.maxlen:\n                to_pop = self._keys_buffer.popleft()\n                del self._cache[to_pop]\n            self._keys_buffer.append(key)\n            self._cache[key] = value\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/compiler/graph_traversal/","title":"Graph traversal","text":""},{"location":"reference/inference/core/workflows/execution_engine/v1/compiler/graph_traversal/#inference.core.workflows.execution_engine.v1.compiler.graph_traversal.traverse_graph_ensuring_parents_are_reached_first","title":"<code>traverse_graph_ensuring_parents_are_reached_first(graph, start_node)</code>","text":"<p>This function works under assumption of common super-input node in the graph - otherwise, there is no common entry point to put as <code>start_node</code>.</p> Source code in <code>inference/core/workflows/execution_engine/v1/compiler/graph_traversal.py</code> <pre><code>def traverse_graph_ensuring_parents_are_reached_first(\n    graph: DiGraph,\n    start_node: str,\n) -&gt; List[str]:\n    \"\"\"\n    This function works under assumption of common super-input node in the graph - otherwise,\n    there is no common entry point to put as `start_node`.\n    \"\"\"\n    graph_copy = graph.copy()\n    distance_key = \"distance\"\n    graph_copy = assign_max_distances_from_start(\n        graph=graph_copy,\n        start_node=start_node,\n        distance_key=distance_key,\n    )\n    nodes_groups = group_nodes_by_sorted_key_value(graph=graph_copy, key=distance_key)\n    return [node for node_group in nodes_groups for node in node_group]\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/block_assembler/","title":"Block assembler","text":""},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/block_assembler/#inference.core.workflows.execution_engine.v1.dynamic_blocks.block_assembler.ensure_dynamic_blocks_allowed","title":"<code>ensure_dynamic_blocks_allowed(dynamic_blocks_definitions)</code>","text":"<p>Ensure that dynamic blocks are allowed based on configuration.</p> <p>Dynamic blocks are allowed if: 1. Local custom Python execution is enabled (ALLOW_CUSTOM_PYTHON_EXECUTION_IN_WORKFLOWS=True) 2. OR Modal execution mode is set (WORKFLOWS_CUSTOM_PYTHON_EXECUTION_MODE=modal)</p> <p>This allows secure execution via Modal sandboxes even when local execution is disabled.</p> Source code in <code>inference/core/workflows/execution_engine/v1/dynamic_blocks/block_assembler.py</code> <pre><code>def ensure_dynamic_blocks_allowed(dynamic_blocks_definitions: List[dict]) -&gt; None:\n    \"\"\"Ensure that dynamic blocks are allowed based on configuration.\n\n    Dynamic blocks are allowed if:\n    1. Local custom Python execution is enabled (ALLOW_CUSTOM_PYTHON_EXECUTION_IN_WORKFLOWS=True)\n    2. OR Modal execution mode is set (WORKFLOWS_CUSTOM_PYTHON_EXECUTION_MODE=modal)\n\n    This allows secure execution via Modal sandboxes even when local execution is disabled.\n    \"\"\"\n    if not dynamic_blocks_definitions:\n        return\n\n    # Check if we're using Modal for secure remote execution\n    is_modal_mode = WORKFLOWS_CUSTOM_PYTHON_EXECUTION_MODE == \"modal\"\n\n    # Allow if either local execution is enabled OR Modal mode is set\n    if not ALLOW_CUSTOM_PYTHON_EXECUTION_IN_WORKFLOWS and not is_modal_mode:\n        raise WorkflowEnvironmentConfigurationError(\n            public_message=\"Cannot use dynamic blocks with custom Python code in this installation of `workflows`. \"\n            \"This can be changed by either setting environmental variable \"\n            \"`ALLOW_CUSTOM_PYTHON_EXECUTION_IN_WORKFLOWS=True` for local execution \"\n            \"or `WORKFLOWS_CUSTOM_PYTHON_EXECUTION_MODE=modal` for secure remote execution.\",\n            context=\"workflow_compilation | dynamic_blocks_compilation\",\n        )\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor/","title":"Modal executor","text":"<p>Modal executor for Custom Python Blocks in Workflows using Web Endpoints.</p> <p>This module handles the execution of untrusted user code in Modal sandboxes using web endpoints for better security and no size limitations.</p>"},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor/#inference.core.workflows.execution_engine.v1.dynamic_blocks.modal_executor.ModalExecutor","title":"<code>ModalExecutor</code>","text":"<p>Manages execution of Custom Python Blocks in Modal sandboxes via web endpoints.</p> Source code in <code>inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor.py</code> <pre><code>class ModalExecutor:\n    \"\"\"Manages execution of Custom Python Blocks in Modal sandboxes via web endpoints.\"\"\"\n\n    def __init__(self, workspace_id: Optional[str] = None):\n        \"\"\"Initialize the Modal executor for a specific workspace.\n\n        Args:\n            workspace_id: The workspace ID to namespace execution, defaults to \"anonymous\"\n        \"\"\"\n        self.workspace_id = workspace_id or MODAL_ANONYMOUS_WORKSPACE_NAME\n        self._base_url = None\n\n    def _get_endpoint_url(self, workspace_id: str) -&gt; str:\n        \"\"\"Get the web endpoint URL for a workspace.\n\n        Args:\n            workspace_id: The workspace ID\n\n        Returns:\n            The endpoint URL with query parameter for workspace_id\n        \"\"\"\n        # Get base URL once (it's the same for all workspace_ids)\n        if self._base_url is None:\n            # First check for environment variable override\n            env_url = os.environ.get(\"MODAL_WEB_ENDPOINT_URL\")\n            if env_url:\n                self._base_url = env_url\n            else:\n                # If we couldn't get it dynamically, construct it based on expected pattern\n                if not self._base_url:\n                    # URL pattern: https://{workspace}--{app}-{class}-{method_truncated}.modal.run\n                    # Note: Modal truncates long labels to 63 chars with a hash suffix\n                    workspace = MODAL_WORKSPACE_NAME\n                    app_name = \"webexec\"\n                    class_name = \"executor\"\n                    method_name = \"execute-block\"\n\n                    # The label would be: inference-custom-blocks-web-customblockexecutor-execute-block\n                    # This is 62 chars, which might get truncated\n                    label = f\"{app_name}-{class_name}-{method_name}\"\n                    if (\n                        len(label) &gt; 56\n                    ):  # Modal truncates at 56 chars and adds 7-char hash\n                        import hashlib\n\n                        hash_str = hashlib.sha256(label.encode()).hexdigest()[:6]\n                        label = f\"{label[:56]}-{hash_str}\"\n\n                    self._base_url = f\"https://{workspace}--{label}.modal.run\"\n\n        # Add workspace_id as query parameter\n        return f\"{self._base_url}?workspace_id={workspace_id}\"\n\n    def execute_remote(\n        self,\n        block_type_name: str,\n        python_code: PythonCode,\n        inputs: Dict[str, Any],\n        workspace_id: Optional[str] = None,\n    ) -&gt; BlockResult:\n        \"\"\"Execute a Custom Python Block in a Modal sandbox via web endpoint.\n\n        Args:\n            block_type_name: Name of the block type\n            python_code: The Python code to execute\n            inputs: Input data for the function\n            workspace_id: Optional workspace ID override\n\n        Returns:\n            BlockResult from the execution\n\n        Raises:\n            DynamicBlockError: If Modal is not available or Modal request fails\n            Exception: If remote execution throws an exception\n        \"\"\"\n        # Check if Modal is available\n        if not MODAL_AVAILABLE:\n            raise DynamicBlockError(\n                public_message=\"Modal credentials not configured. Please set MODAL_TOKEN_ID and MODAL_TOKEN_SECRET environment variables.\",\n                context=\"modal_executor | credentials_check\",\n            )\n\n        # Use provided workspace_id or fall back to instance default\n        workspace = workspace_id if workspace_id else self.workspace_id\n\n        try:\n            # Get endpoint URL for this workspace\n            endpoint_url = self._get_endpoint_url(workspace)\n\n            # Custom JSON encoder for inputs\n            inputs_json = serialize_for_modal_remote_execution(inputs)\n\n            # Prepare request payload\n            request_payload = {\n                \"code_str\": python_code.run_function_code,\n                \"imports\": python_code.imports or [],\n                \"run_function_name\": python_code.run_function_name,\n                \"inputs_json\": inputs_json,\n            }\n\n            if (\n                not workspace\n                or workspace == \"anonymous\"\n                or workspace == \"unauthorized\"\n                or workspace == MODAL_ANONYMOUS_WORKSPACE_NAME\n            ):\n                from inference.core.env import MODAL_ALLOW_ANONYMOUS_EXECUTION\n\n                if not MODAL_ALLOW_ANONYMOUS_EXECUTION:\n                    raise DynamicBlockError(\n                        public_message=\"Modal validation requires an API key when anonymous execution is disabled. \"\n                        \"Please provide an API key or enable anonymous execution by setting \"\n                        \"MODAL_ALLOW_ANONYMOUS_EXECUTION=True\",\n                        context=\"modal_executor | validation_authentication\",\n                    )\n\n            # Make HTTP request to Modal endpoint\n            response = requests.post(\n                endpoint_url,\n                json=request_payload,\n                timeout=30,  # 30 second timeout\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Modal-Key\": MODAL_TOKEN_ID,\n                    \"Modal-Secret\": MODAL_TOKEN_SECRET,\n                },\n            )\n\n            # Check HTTP status\n            if response.status_code != 200:\n                raise DynamicBlockError(\n                    public_message=f\"Modal endpoint returned status {response.status_code}: {response.text}\",\n                    context=\"modal_executor | http_request\",\n                )\n\n            # Parse response\n            result = response.json()\n\n            # Check for errors\n            if not result.get(\"success\", False):\n                error_msg = result.get(\"error\", \"Unknown error\")\n                error_type = result.get(\"error_type\", \"RuntimeError\")\n                line_number = result.get(\"line_number\", None)\n                function_name = result.get(\"function_name\", None)\n\n                if line_number and function_name:\n                    message = f\"Error in line {line_number}, in {function_name}: {error_type}: {error_msg}\"\n                else:\n                    message = f\"{error_type}: {error_msg}\"\n\n                # Propagate remote Exception on runtime error. Will be caught by the\n                # core executor and wrapped in StepExecutionError with block metadata.\n                raise Exception(message)\n\n            # Get the result and deserialize from JSON\n            json_result = result.get(\"result\", \"{}\")\n            return deserialize_for_modal_remote_execution(json_result)\n\n        except requests.exceptions.RequestException as e:\n            raise DynamicBlockError(\n                public_message=f\"Failed to connect to Modal endpoint: {str(e)}\",\n                context=\"modal_executor | http_connection\",\n            )\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor/#inference.core.workflows.execution_engine.v1.dynamic_blocks.modal_executor.ModalExecutor.__init__","title":"<code>__init__(workspace_id=None)</code>","text":"<p>Initialize the Modal executor for a specific workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>Optional[str]</code> <p>The workspace ID to namespace execution, defaults to \"anonymous\"</p> <code>None</code> Source code in <code>inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor.py</code> <pre><code>def __init__(self, workspace_id: Optional[str] = None):\n    \"\"\"Initialize the Modal executor for a specific workspace.\n\n    Args:\n        workspace_id: The workspace ID to namespace execution, defaults to \"anonymous\"\n    \"\"\"\n    self.workspace_id = workspace_id or MODAL_ANONYMOUS_WORKSPACE_NAME\n    self._base_url = None\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor/#inference.core.workflows.execution_engine.v1.dynamic_blocks.modal_executor.ModalExecutor.execute_remote","title":"<code>execute_remote(block_type_name, python_code, inputs, workspace_id=None)</code>","text":"<p>Execute a Custom Python Block in a Modal sandbox via web endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>block_type_name</code> <code>str</code> <p>Name of the block type</p> required <code>python_code</code> <code>PythonCode</code> <p>The Python code to execute</p> required <code>inputs</code> <code>Dict[str, Any]</code> <p>Input data for the function</p> required <code>workspace_id</code> <code>Optional[str]</code> <p>Optional workspace ID override</p> <code>None</code> <p>Returns:</p> Type Description <code>BlockResult</code> <p>BlockResult from the execution</p> <p>Raises:</p> Type Description <code>DynamicBlockError</code> <p>If Modal is not available or Modal request fails</p> <code>Exception</code> <p>If remote execution throws an exception</p> Source code in <code>inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor.py</code> <pre><code>def execute_remote(\n    self,\n    block_type_name: str,\n    python_code: PythonCode,\n    inputs: Dict[str, Any],\n    workspace_id: Optional[str] = None,\n) -&gt; BlockResult:\n    \"\"\"Execute a Custom Python Block in a Modal sandbox via web endpoint.\n\n    Args:\n        block_type_name: Name of the block type\n        python_code: The Python code to execute\n        inputs: Input data for the function\n        workspace_id: Optional workspace ID override\n\n    Returns:\n        BlockResult from the execution\n\n    Raises:\n        DynamicBlockError: If Modal is not available or Modal request fails\n        Exception: If remote execution throws an exception\n    \"\"\"\n    # Check if Modal is available\n    if not MODAL_AVAILABLE:\n        raise DynamicBlockError(\n            public_message=\"Modal credentials not configured. Please set MODAL_TOKEN_ID and MODAL_TOKEN_SECRET environment variables.\",\n            context=\"modal_executor | credentials_check\",\n        )\n\n    # Use provided workspace_id or fall back to instance default\n    workspace = workspace_id if workspace_id else self.workspace_id\n\n    try:\n        # Get endpoint URL for this workspace\n        endpoint_url = self._get_endpoint_url(workspace)\n\n        # Custom JSON encoder for inputs\n        inputs_json = serialize_for_modal_remote_execution(inputs)\n\n        # Prepare request payload\n        request_payload = {\n            \"code_str\": python_code.run_function_code,\n            \"imports\": python_code.imports or [],\n            \"run_function_name\": python_code.run_function_name,\n            \"inputs_json\": inputs_json,\n        }\n\n        if (\n            not workspace\n            or workspace == \"anonymous\"\n            or workspace == \"unauthorized\"\n            or workspace == MODAL_ANONYMOUS_WORKSPACE_NAME\n        ):\n            from inference.core.env import MODAL_ALLOW_ANONYMOUS_EXECUTION\n\n            if not MODAL_ALLOW_ANONYMOUS_EXECUTION:\n                raise DynamicBlockError(\n                    public_message=\"Modal validation requires an API key when anonymous execution is disabled. \"\n                    \"Please provide an API key or enable anonymous execution by setting \"\n                    \"MODAL_ALLOW_ANONYMOUS_EXECUTION=True\",\n                    context=\"modal_executor | validation_authentication\",\n                )\n\n        # Make HTTP request to Modal endpoint\n        response = requests.post(\n            endpoint_url,\n            json=request_payload,\n            timeout=30,  # 30 second timeout\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"Modal-Key\": MODAL_TOKEN_ID,\n                \"Modal-Secret\": MODAL_TOKEN_SECRET,\n            },\n        )\n\n        # Check HTTP status\n        if response.status_code != 200:\n            raise DynamicBlockError(\n                public_message=f\"Modal endpoint returned status {response.status_code}: {response.text}\",\n                context=\"modal_executor | http_request\",\n            )\n\n        # Parse response\n        result = response.json()\n\n        # Check for errors\n        if not result.get(\"success\", False):\n            error_msg = result.get(\"error\", \"Unknown error\")\n            error_type = result.get(\"error_type\", \"RuntimeError\")\n            line_number = result.get(\"line_number\", None)\n            function_name = result.get(\"function_name\", None)\n\n            if line_number and function_name:\n                message = f\"Error in line {line_number}, in {function_name}: {error_type}: {error_msg}\"\n            else:\n                message = f\"{error_type}: {error_msg}\"\n\n            # Propagate remote Exception on runtime error. Will be caught by the\n            # core executor and wrapped in StepExecutionError with block metadata.\n            raise Exception(message)\n\n        # Get the result and deserialize from JSON\n        json_result = result.get(\"result\", \"{}\")\n        return deserialize_for_modal_remote_execution(json_result)\n\n    except requests.exceptions.RequestException as e:\n        raise DynamicBlockError(\n            public_message=f\"Failed to connect to Modal endpoint: {str(e)}\",\n            context=\"modal_executor | http_connection\",\n        )\n</code></pre>"},{"location":"reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor/#inference.core.workflows.execution_engine.v1.dynamic_blocks.modal_executor.validate_code_in_modal","title":"<code>validate_code_in_modal(python_code, workspace_id=None)</code>","text":"<p>Validate Python code syntax in a Modal sandbox via web endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>python_code</code> <code>PythonCode</code> <p>The Python code to validate</p> required <code>workspace_id</code> <code>Optional[str]</code> <p>The workspace ID for Modal App</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if code is valid, raises otherwise</p> <p>Raises:</p> Type Description <code>DynamicBlockError</code> <p>If code validation fails</p> Source code in <code>inference/core/workflows/execution_engine/v1/dynamic_blocks/modal_executor.py</code> <pre><code>def validate_code_in_modal(\n    python_code: PythonCode, workspace_id: Optional[str] = None\n) -&gt; bool:\n    \"\"\"Validate Python code syntax in a Modal sandbox via web endpoint.\n\n    Args:\n        python_code: The Python code to validate\n        workspace_id: The workspace ID for Modal App\n\n    Returns:\n        True if code is valid, raises otherwise\n\n    Raises:\n        DynamicBlockError: If code validation fails\n    \"\"\"\n    # Check if Modal is available\n    if not MODAL_AVAILABLE:\n        raise DynamicBlockError(\n            public_message=\"Modal credentials not configured. Please set MODAL_TOKEN_ID and MODAL_TOKEN_SECRET environment variables.\",\n            context=\"modal_executor | credentials_check\",\n        )\n\n    workspace = workspace_id or MODAL_ANONYMOUS_WORKSPACE_NAME\n\n    # Construct the full code to validate (same as in create_dynamic_module)\n    full_code = python_code.run_function_code\n    if python_code.init_function_code:\n        full_code += \"\\n\\n\" + python_code.init_function_code\n\n    # Escape the code for safe embedding in the validation function\n    # Use repr() to properly escape quotes and special characters\n    escaped_code = repr(full_code)\n\n    # Simple validation code that checks syntax\n    validation_code = PythonCode(\n        type=\"PythonCode\",\n        imports=[],\n        run_function_code=f\"\"\"\nimport ast\n\ndef validate_syntax():\n    try:\n        # Try to compile the user code\n        code = {escaped_code}\n        compile(code, \"&lt;string&gt;\", \"exec\")\n        # Try to parse as AST to check structure\n        ast.parse(code)\n        return {{\"valid\": True}}\n    except SyntaxError as e:\n        return {{\"valid\": False, \"error\": str(e), \"line\": e.lineno}}\n    except Exception as e:\n        return {{\"valid\": False, \"error\": str(e)}}\n\"\"\",\n        run_function_name=\"validate_syntax\",\n        init_function_code=None,\n        init_function_name=\"init\",\n    )\n\n    executor = ModalExecutor(workspace_id=workspace)\n\n    try:\n        # For validation, we don't need complex inputs, just pass empty JSON\n        result = executor.execute_remote(\n            block_type_name=\"validation\",\n            python_code=validation_code,\n            inputs={},\n            workspace_id=workspace,\n        )\n\n        if result.get(\"valid\") is False:\n            error_msg = result.get(\"error\", \"Unknown syntax error\")\n            line_no = result.get(\"line\", None)\n            if line_no:\n                error_msg = f\"Line {line_no}: {error_msg}\"\n            raise DynamicBlockError(\n                public_message=f\"Code validation failed: {error_msg}\",\n                context=\"modal_executor | code_validation\",\n            )\n\n        return True\n\n    except Exception as e:\n        if isinstance(e, DynamicBlockError):\n            raise\n        raise DynamicBlockError(\n            public_message=f\"Code validation failed: {str(e)}\",\n            context=\"modal_executor | code_validation\",\n        )\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/dispatch_manager/","title":"Dispatch manager","text":""},{"location":"reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker","title":"<code>ResultsChecker</code>","text":"<p>Class responsible for queuing asyncronous inference runs, keeping track of running requests, and awaiting their results.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>class ResultsChecker:\n    \"\"\"\n    Class responsible for queuing asyncronous inference runs,\n    keeping track of running requests, and awaiting their results.\n    \"\"\"\n\n    def __init__(self, redis: Redis):\n        self.tasks: Dict[str, Event] = {}\n        self.dones = dict()\n        self.errors = dict()\n        self.running = True\n        self.redis = redis\n        self.semaphore: BoundedSemaphore = BoundedSemaphore(NUM_PARALLEL_TASKS)\n\n    def add_task(self, task_id: str, request: InferenceRequest):\n        \"\"\"\n        Wait until there's available cylce to queue a task.\n        When there are cycles, add the task's id to a list to keep track of its results,\n        launch the preprocess celeryt task, set the task's status to in progress in redis.\n        \"\"\"\n        self.semaphore.acquire()\n        self.tasks[task_id] = Event()\n        preprocess.s(request.dict()).delay()\n\n    def get_result(self, task_id: str) -&gt; Any:\n        \"\"\"\n        Check the done tasks and errored tasks for this task id.\n        \"\"\"\n        if task_id in self.dones:\n            return self.dones.pop(task_id)\n        elif task_id in self.errors:\n            message = self.errors.pop(task_id)\n            raise Exception(message)\n        else:\n            raise RuntimeError(\n                \"Task result not found in either success or error dict. Unreachable\"\n            )\n\n    def loop(self):\n        \"\"\"\n        Main loop. Check all in progress tasks for their status, and if their status is final,\n        (either failure or success) then add their results to the appropriate results dictionary.\n        \"\"\"\n        with self.redis.pubsub() as pubsub:\n            pubsub.subscribe(\"results\")\n            for message in pubsub.listen():\n                if message[\"type\"] != \"message\":\n                    continue\n                message = orjson.loads(message[\"data\"])\n                task_id = message.pop(\"task_id\")\n                if task_id not in self.tasks:\n                    continue\n                self.semaphore.release()\n                status = message.pop(\"status\")\n                if status == FAILURE_STATE:\n                    self.errors[task_id] = message[\"payload\"]\n                elif status == SUCCESS_STATE:\n                    self.dones[task_id] = message[\"payload\"]\n                else:\n                    raise RuntimeError(\n                        \"Task result not found in possible states. Unreachable\"\n                    )\n                self.tasks[task_id].set()\n\n    def wait_for_response(self, key: str):\n        event = self.tasks[key]\n        event.wait()\n        del self.tasks[key]\n        return self.get_result(key)\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.add_task","title":"<code>add_task(task_id, request)</code>","text":"<p>Wait until there's available cylce to queue a task. When there are cycles, add the task's id to a list to keep track of its results, launch the preprocess celeryt task, set the task's status to in progress in redis.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>def add_task(self, task_id: str, request: InferenceRequest):\n    \"\"\"\n    Wait until there's available cylce to queue a task.\n    When there are cycles, add the task's id to a list to keep track of its results,\n    launch the preprocess celeryt task, set the task's status to in progress in redis.\n    \"\"\"\n    self.semaphore.acquire()\n    self.tasks[task_id] = Event()\n    preprocess.s(request.dict()).delay()\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.get_result","title":"<code>get_result(task_id)</code>","text":"<p>Check the done tasks and errored tasks for this task id.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>def get_result(self, task_id: str) -&gt; Any:\n    \"\"\"\n    Check the done tasks and errored tasks for this task id.\n    \"\"\"\n    if task_id in self.dones:\n        return self.dones.pop(task_id)\n    elif task_id in self.errors:\n        message = self.errors.pop(task_id)\n        raise Exception(message)\n    else:\n        raise RuntimeError(\n            \"Task result not found in either success or error dict. Unreachable\"\n        )\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.loop","title":"<code>loop()</code>","text":"<p>Main loop. Check all in progress tasks for their status, and if their status is final, (either failure or success) then add their results to the appropriate results dictionary.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>def loop(self):\n    \"\"\"\n    Main loop. Check all in progress tasks for their status, and if their status is final,\n    (either failure or success) then add their results to the appropriate results dictionary.\n    \"\"\"\n    with self.redis.pubsub() as pubsub:\n        pubsub.subscribe(\"results\")\n        for message in pubsub.listen():\n            if message[\"type\"] != \"message\":\n                continue\n            message = orjson.loads(message[\"data\"])\n            task_id = message.pop(\"task_id\")\n            if task_id not in self.tasks:\n                continue\n            self.semaphore.release()\n            status = message.pop(\"status\")\n            if status == FAILURE_STATE:\n                self.errors[task_id] = message[\"payload\"]\n            elif status == SUCCESS_STATE:\n                self.dones[task_id] = message[\"payload\"]\n            else:\n                raise RuntimeError(\n                    \"Task result not found in possible states. Unreachable\"\n                )\n            self.tasks[task_id].set()\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/infer/","title":"Infer","text":""},{"location":"reference/inference/enterprise/parallel/infer/#inference.enterprise.parallel.infer.get_batch","title":"<code>get_batch(redis, model_names)</code>","text":"<p>Run a heuristic to select the best batch to infer on redis[Redis]: redis client model_names[List[str]]: list of models with nonzero number of requests returns:     Tuple[List[Dict], str]     List[Dict] represents a batch of request dicts     str is the model id</p> Source code in <code>inference/enterprise/parallel/infer.py</code> <pre><code>def get_batch(redis: Redis, model_names: List[str]) -&gt; Tuple[List[Dict], str]:\n    \"\"\"\n    Run a heuristic to select the best batch to infer on\n    redis[Redis]: redis client\n    model_names[List[str]]: list of models with nonzero number of requests\n    returns:\n        Tuple[List[Dict], str]\n        List[Dict] represents a batch of request dicts\n        str is the model id\n    \"\"\"\n    batch_sizes = [\n        RoboflowInferenceModel.model_metadata_from_memcache_endpoint(m)[\"batch_size\"]\n        for m in model_names\n    ]\n    batch_sizes = [b if not isinstance(b, str) else BATCH_SIZE for b in batch_sizes]\n    batches = [\n        redis.zrange(f\"infer:{m}\", 0, b - 1, withscores=True)\n        for m, b in zip(model_names, batch_sizes)\n    ]\n    model_index = select_best_inference_batch(batches, batch_sizes)\n    batch = batches[model_index]\n    selected_model = model_names[model_index]\n    redis.zrem(f\"infer:{selected_model}\", *[b[0] for b in batch])\n    redis.hincrby(f\"requests\", selected_model, -len(batch))\n    batch = [orjson.loads(b[0]) for b in batch]\n    return batch, selected_model\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/infer/#inference.enterprise.parallel.infer.write_infer_arrays_and_launch_postprocess","title":"<code>write_infer_arrays_and_launch_postprocess(arrs, request, preproc_return_metadata)</code>","text":"<p>Write inference results to shared memory and launch the postprocessing task</p> Source code in <code>inference/enterprise/parallel/infer.py</code> <pre><code>def write_infer_arrays_and_launch_postprocess(\n    arrs: Tuple[np.ndarray, ...],\n    request: InferenceRequest,\n    preproc_return_metadata: Dict,\n):\n    \"\"\"Write inference results to shared memory and launch the postprocessing task\"\"\"\n    shms = [shared_memory.SharedMemory(create=True, size=arr.nbytes) for arr in arrs]\n    with shm_manager(*shms):\n        shm_metadatas = []\n        for arr, shm in zip(arrs, shms):\n            shared = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)\n            shared[:] = arr[:]\n            shm_metadata = SharedMemoryMetadata(\n                shm_name=shm.name, array_shape=arr.shape, array_dtype=arr.dtype.name\n            )\n            shm_metadatas.append(asdict(shm_metadata))\n\n        postprocess.s(\n            tuple(shm_metadatas), request.dict(), preproc_return_metadata\n        ).delay()\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/utils/","title":"Utils","text":""},{"location":"reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.SharedMemoryMetadata","title":"<code>SharedMemoryMetadata</code>  <code>dataclass</code>","text":"<p>Info needed to load array from shared memory</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@dataclass\nclass SharedMemoryMetadata:\n    \"\"\"Info needed to load array from shared memory\"\"\"\n\n    shm_name: str\n    array_shape: List[int]\n    array_dtype: str\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.failure_handler","title":"<code>failure_handler(redis, *request_ids)</code>","text":"<p>Context manager that updates the status/results key in redis with exception info on failure.</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@contextmanager\ndef failure_handler(redis: Redis, *request_ids: str):\n    \"\"\"\n    Context manager that updates the status/results key in redis with exception\n    info on failure.\n    \"\"\"\n    try:\n        yield\n    except Exception as error:\n        message = type(error).__name__ + \": \" + str(error)\n        for request_id in request_ids:\n            redis.publish(\n                \"results\",\n                json.dumps(\n                    {\"task_id\": request_id, \"status\": FAILURE_STATE, \"payload\": message}\n                ),\n            )\n        raise\n</code></pre>"},{"location":"reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.shm_manager","title":"<code>shm_manager(*shms, unlink_on_success=False)</code>","text":"<p>Context manager that closes and frees shared memory objects.</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@contextmanager\ndef shm_manager(\n    *shms: Union[str, shared_memory.SharedMemory], unlink_on_success: bool = False\n):\n    \"\"\"Context manager that closes and frees shared memory objects.\"\"\"\n    try:\n        loaded_shms = []\n        for shm in shms:\n            errors = []\n            try:\n                if isinstance(shm, str):\n                    shm = shared_memory.SharedMemory(name=shm)\n                loaded_shms.append(shm)\n            except BaseException as error:\n                errors.append(error)\n            if errors:\n                raise Exception(errors)\n\n        yield loaded_shms\n    except:\n        for shm in loaded_shms:\n            shm.close()\n            shm.unlink()\n        raise\n    else:\n        for shm in loaded_shms:\n            shm.close()\n            if unlink_on_success:\n                shm.unlink()\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/PLC_modbus/v1/","title":"V1","text":""},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/PLC_modbus/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.PLC_modbus.v1.ModbusTCPBlockV1","title":"<code>ModbusTCPBlockV1</code>","text":"<p>               Bases: <code>WorkflowBlock</code></p> <p>A Modbus TCP communication block using pymodbus.</p> <p>Supports: - 'read': Reads specified registers. - 'write': Writes values to specified registers. - 'read_and_write': Reads and writes in one execution.</p> <p>On failures, errors are printed and marked as \"ReadFailure\" or \"WriteFailure\".</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/PLC_modbus/v1.py</code> <pre><code>class ModbusTCPBlockV1(WorkflowBlock):\n    \"\"\"A Modbus TCP communication block using pymodbus.\n\n    Supports:\n    - 'read': Reads specified registers.\n    - 'write': Writes values to specified registers.\n    - 'read_and_write': Reads and writes in one execution.\n\n    On failures, errors are printed and marked as \"ReadFailure\" or \"WriteFailure\".\n    \"\"\"\n\n    def __init__(self):\n        self.client: Optional[ModbusClient] = None\n\n    def __del__(self):\n        if self.client:\n            try:\n                self.client.close()\n            except Exception as exc:\n                logger.debug(\"Failed to release modbus client: %s\", exc)\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ModbusTCPBlockManifest\n\n    def run(\n        self,\n        plc_ip: str,\n        plc_port: int,\n        mode: str,\n        registers_to_read: List[int],\n        registers_to_write: Dict[int, int],\n        depends_on: any,\n        image: Optional[WorkflowImageData] = None,\n        metadata: Optional[VideoMetadata] = None,\n    ) -&gt; dict:\n        read_results = {}\n        write_results = {}\n\n        if not self.client:\n            self.client: ModbusClient = ModbusClient(plc_ip, port=plc_port)\n            if not self.client.connect():\n                print(\"Failed to connect to PLC\")\n                return {\"modbus_results\": [{\"error\": \"ConnectionFailure\"}]}\n\n        # If mode involves reading\n        if mode in [\"read\", \"read_and_write\"]:\n            for address in registers_to_read:\n                try:\n                    response = self.client.read_holding_registers(address)\n                    if not response.isError():\n                        read_results[address] = (\n                            response.registers[0] if response.registers else None\n                        )\n                    else:\n                        print(f\"Error reading register {address}: {response}\")\n                        read_results[address] = \"ReadFailure\"\n                except Exception as e:\n                    print(f\"Exception reading register {address}: {e}\")\n                    read_results[address] = \"ReadFailure\"\n\n        # If mode involves writing\n        if mode in [\"write\", \"read_and_write\"]:\n            for address, value in registers_to_write.items():\n                try:\n                    response = self.client.write_register(address, value)\n                    if not response.isError():\n                        write_results[address] = \"WriteSuccess\"\n                    else:\n                        print(\n                            f\"Error writing register {address} with value {value}: {response}\"\n                        )\n                        write_results[address] = \"WriteFailure\"\n                except Exception as e:\n                    print(\n                        f\"Exception writing register {address} with value {value}: {e}\"\n                    )\n                    write_results[address] = \"WriteFailure\"\n\n        modbus_output = {}\n        if read_results:\n            modbus_output[\"read\"] = read_results\n        if write_results:\n            modbus_output[\"write\"] = write_results\n\n        return {\"modbus_results\": [modbus_output]}\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1/","title":"V1","text":""},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.PLCethernetIP.v1.PLCBlockManifest","title":"<code>PLCBlockManifest</code>","text":"<p>               Bases: <code>WorkflowBlockManifest</code></p> <p>Manifest for a PLC communication block using Ethernet/IP.</p> <p>The block can be used in one of three modes: - 'read': Only reads specified tags. - 'write': Only writes specified tags. - 'read_and_write': Performs both reading and writing in one execution.</p> <p><code>tags_to_read</code> and <code>tags_to_write</code> are applicable depending on the mode chosen.</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1.py</code> <pre><code>class PLCBlockManifest(WorkflowBlockManifest):\n    \"\"\"Manifest for a PLC communication block using Ethernet/IP.\n\n    The block can be used in one of three modes:\n    - 'read': Only reads specified tags.\n    - 'write': Only writes specified tags.\n    - 'read_and_write': Performs both reading and writing in one execution.\n\n    `tags_to_read` and `tags_to_write` are applicable depending on the mode chosen.\n    \"\"\"\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"name\": \"PLC EthernetIP\",\n            \"version\": \"v1\",\n            \"short_description\": \"Generic PLC read/write block using pylogix over Ethernet/IP.\",\n            \"long_description\": LONG_DESCRIPTION,\n            \"license\": \"Roboflow Enterprise License\",\n            \"block_type\": \"sinks\",\n            \"ui_manifest\": {\n                \"section\": \"industrial\",\n                \"icon\": \"fal fa-microchip\",\n                \"blockPriority\": 13,\n                \"enterprise_only\": True,\n                \"local_only\": True,\n            },\n        }\n    )\n\n    type: Literal[\"roboflow_core/sinks@v1\"]\n\n    plc_ip: Union[str, WorkflowParameterSelector(kind=[STRING_KIND])] = Field(\n        description=\"IP address of the target PLC.\", examples=[\"192.168.1.10\"]\n    )\n\n    mode: Literal[\"read\", \"write\", \"read_and_write\"] = Field(\n        description=\"Mode of operation: 'read', 'write', or 'read_and_write'.\",\n        examples=[\"read\", \"write\", \"read_and_write\"],\n    )\n\n    tags_to_read: Union[\n        List[str], WorkflowParameterSelector(kind=[LIST_OF_VALUES_KIND])\n    ] = Field(\n        default=[],\n        description=\"List of PLC tag names to read. Applicable if mode='read' or mode='read_and_write'.\",\n        examples=[[\"camera_msg\", \"sku_number\"]],\n    )\n\n    tags_to_write: Union[\n        Dict[str, Union[int, float, str]],\n        WorkflowParameterSelector(kind=[LIST_OF_VALUES_KIND]),\n    ] = Field(\n        default={},\n        description=\"Dictionary of tags and the values to write. Applicable if mode='write' or mode='read_and_write'.\",\n        examples=[{\"camera_fault\": True, \"defect_count\": 5}],\n    )\n\n    depends_on: Selector() = Field(\n        description=\"Reference to the step output this block depends on.\",\n        examples=[\"$steps.some_previous_step\"],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(\n                name=\"plc_results\",\n                kind=[LIST_OF_VALUES_KIND],\n            ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.PLCethernetIP.v1.PLCBlockV1","title":"<code>PLCBlockV1</code>","text":"<p>               Bases: <code>WorkflowBlock</code></p> <p>A PLC communication workflow block using Ethernet/IP and pylogix.</p> <p>Depending on the selected mode: - 'read': Reads specified tags. - 'write': Writes provided values to specified tags. - 'read_and_write': Reads and writes in one go.</p> <p>In case of failures, errors are printed to terminal and the corresponding tag entry in the output is set to \"ReadFailure\" or \"WriteFailure\".</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1.py</code> <pre><code>class PLCBlockV1(WorkflowBlock):\n    \"\"\"A PLC communication workflow block using Ethernet/IP and pylogix.\n\n    Depending on the selected mode:\n    - 'read': Reads specified tags.\n    - 'write': Writes provided values to specified tags.\n    - 'read_and_write': Reads and writes in one go.\n\n    In case of failures, errors are printed to terminal and the corresponding tag entry in the output is set to \"ReadFailure\" or \"WriteFailure\".\n    \"\"\"\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return PLCBlockManifest\n\n    def _read_single_tag(self, comm, tag):\n        try:\n            response = comm.Read(tag)\n            if response.Status == \"Success\":\n                return response.Value\n            logger.error(f\"Error reading tag '%s': %s\", tag, response.Status)\n            return \"ReadFailure\"\n        except Exception as e:\n            logger.error(f\"Unhandled error reading tag '%s': %s\", tag, e)\n            return \"ReadFailure\"\n\n    def _write_single_tag(self, comm, tag, value):\n        try:\n            response = comm.Write(tag, value)\n            if response.Status == \"Success\":\n                return \"WriteSuccess\"\n            logger.error(\n                \"Error writing tag '%s' with value '%s': %s\",\n                tag,\n                value,\n                response.Status,\n            )\n            return \"WriteFailure\"\n        except Exception as e:\n            logger.error(f\"Unhandled error writing tag '%s': %s\", tag, e)\n            return \"WriteFailure\"\n\n    def run(\n        self,\n        plc_ip: str,\n        mode: str,\n        tags_to_read: List[str],\n        tags_to_write: Dict[str, Union[int, float, str]],\n        depends_on: any,\n        image: Optional[WorkflowImageData] = None,\n        metadata: Optional[VideoMetadata] = None,\n    ) -&gt; dict:\n        \"\"\"Run PLC read/write operations using pylogix over Ethernet/IP.\n\n        Args:\n            plc_ip (str): PLC IP address.\n            mode (str): 'read', 'write', or 'read_and_write'.\n            tags_to_read (List[str]): Tags to read if applicable.\n            tags_to_write (Dict[str, Union[int, float, str]]): Tags to write if applicable.\n            depends_on (any): The step output this block depends on.\n            image (Optional[WorkflowImageData]): Not required for this block.\n            metadata (Optional[VideoMetadata]): Not required for this block.\n\n        Returns:\n            dict: A dictionary with `plc_results` as a list containing one dictionary. That dictionary has 'read' and/or 'write' keys.\n        \"\"\"\n        read_results = {}\n        write_results = {}\n\n        with pylogix.PLC() as comm:\n            comm.IPAddress = plc_ip\n\n            if mode in [\"read\", \"read_and_write\"]:\n                read_results = {\n                    tag: self._read_single_tag(comm, tag) for tag in tags_to_read\n                }\n\n            if mode in [\"write\", \"read_and_write\"]:\n                write_results = {\n                    tag: self._write_single_tag(comm, tag, value)\n                    for tag, value in tags_to_write.items()\n                }\n\n        plc_output = {}\n        if read_results:\n            plc_output[\"read\"] = read_results\n        if write_results:\n            plc_output[\"write\"] = write_results\n\n        return {\"plc_results\": [plc_output]}\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.PLCethernetIP.v1.PLCBlockV1.run","title":"<code>run(plc_ip, mode, tags_to_read, tags_to_write, depends_on, image=None, metadata=None)</code>","text":"<p>Run PLC read/write operations using pylogix over Ethernet/IP.</p> <p>Parameters:</p> Name Type Description Default <code>plc_ip</code> <code>str</code> <p>PLC IP address.</p> required <code>mode</code> <code>str</code> <p>'read', 'write', or 'read_and_write'.</p> required <code>tags_to_read</code> <code>List[str]</code> <p>Tags to read if applicable.</p> required <code>tags_to_write</code> <code>Dict[str, Union[int, float, str]]</code> <p>Tags to write if applicable.</p> required <code>depends_on</code> <code>any</code> <p>The step output this block depends on.</p> required <code>image</code> <code>Optional[WorkflowImageData]</code> <p>Not required for this block.</p> <code>None</code> <code>metadata</code> <code>Optional[VideoMetadata]</code> <p>Not required for this block.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with <code>plc_results</code> as a list containing one dictionary. That dictionary has 'read' and/or 'write' keys.</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/PLCethernetIP/v1.py</code> <pre><code>def run(\n    self,\n    plc_ip: str,\n    mode: str,\n    tags_to_read: List[str],\n    tags_to_write: Dict[str, Union[int, float, str]],\n    depends_on: any,\n    image: Optional[WorkflowImageData] = None,\n    metadata: Optional[VideoMetadata] = None,\n) -&gt; dict:\n    \"\"\"Run PLC read/write operations using pylogix over Ethernet/IP.\n\n    Args:\n        plc_ip (str): PLC IP address.\n        mode (str): 'read', 'write', or 'read_and_write'.\n        tags_to_read (List[str]): Tags to read if applicable.\n        tags_to_write (Dict[str, Union[int, float, str]]): Tags to write if applicable.\n        depends_on (any): The step output this block depends on.\n        image (Optional[WorkflowImageData]): Not required for this block.\n        metadata (Optional[VideoMetadata]): Not required for this block.\n\n    Returns:\n        dict: A dictionary with `plc_results` as a list containing one dictionary. That dictionary has 'read' and/or 'write' keys.\n    \"\"\"\n    read_results = {}\n    write_results = {}\n\n    with pylogix.PLC() as comm:\n        comm.IPAddress = plc_ip\n\n        if mode in [\"read\", \"read_and_write\"]:\n            read_results = {\n                tag: self._read_single_tag(comm, tag) for tag in tags_to_read\n            }\n\n        if mode in [\"write\", \"read_and_write\"]:\n            write_results = {\n                tag: self._write_single_tag(comm, tag, value)\n                for tag, value in tags_to_write.items()\n            }\n\n    plc_output = {}\n    if read_results:\n        plc_output[\"read\"] = read_results\n    if write_results:\n        plc_output[\"write\"] = write_results\n\n    return {\"plc_results\": [plc_output]}\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1/","title":"V1","text":""},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.microsoft_sql_server.v1.SQLServerConnectionError","title":"<code>SQLServerConnectionError</code>","text":"<p>               Bases: <code>SQLServerError</code></p> <p>Exception raised for connection-related errors</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1.py</code> <pre><code>class SQLServerConnectionError(SQLServerError):\n    \"\"\"Exception raised for connection-related errors\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.microsoft_sql_server.v1.SQLServerError","title":"<code>SQLServerError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for SQL Server related errors</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1.py</code> <pre><code>class SQLServerError(Exception):\n    \"\"\"Base exception for SQL Server related errors\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1/#inference.enterprise.workflows.enterprise_blocks.sinks.microsoft_sql_server.v1.SQLServerInsertError","title":"<code>SQLServerInsertError</code>","text":"<p>               Bases: <code>SQLServerError</code></p> <p>Exception raised for insert operation errors</p> Source code in <code>inference/enterprise/workflows/enterprise_blocks/sinks/microsoft_sql_server/v1.py</code> <pre><code>class SQLServerInsertError(SQLServerError):\n    \"\"\"Exception raised for insert operation errors\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/","title":"Clip model","text":""},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip","title":"<code>Clip</code>","text":"<p>               Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX ClipModel model.</p> <p>This class is responsible for handling the ONNX ClipModel model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>visual_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for visual inference.</p> <code>textual_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for textual inference.</p> <code>resolution</code> <code>int</code> <p>The resolution of the input image.</p> <code>clip_preprocess</code> <code>function</code> <p>Function to preprocess the image.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>class Clip(OnnxRoboflowCoreModel):\n    \"\"\"Roboflow ONNX ClipModel model.\n\n    This class is responsible for handling the ONNX ClipModel model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        visual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for visual inference.\n        textual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for textual inference.\n        resolution (int): The resolution of the input image.\n        clip_preprocess (function): Function to preprocess the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        model_id: str = CLIP_MODEL_ID,\n        onnxruntime_execution_providers: List[\n            str\n        ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n        **kwargs,\n    ):\n        \"\"\"Initializes the Clip with the given arguments and keyword arguments.\"\"\"\n        self.onnxruntime_execution_providers = onnxruntime_execution_providers\n        t1 = perf_counter()\n        super().__init__(*args, model_id=model_id, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n        self.visual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"visual.onnx\"),\n            providers=self.onnxruntime_execution_providers,\n        )\n        self._visual_session_lock = Lock()\n        self.textual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"textual.onnx\"),\n            providers=self.onnxruntime_execution_providers,\n        )\n        self._textual_session_lock = Lock()\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n        self.clip_preprocess = clip.clip._transform(self.resolution)\n        self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n        self.task_type = \"embedding\"\n\n    def compare(\n        self,\n        subject: Any,\n        prompt: Any,\n        subject_type: str = \"image\",\n        prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n        **kwargs,\n    ) -&gt; Union[List[float], Dict[str, float]]:\n        \"\"\"\n        Compares the subject with the prompt to calculate similarity scores.\n\n        Args:\n            subject (Any): The subject data to be compared. Can be either an image or text.\n            prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n            subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n            prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.\n\n        Raises:\n            ValueError: If subject_type or prompt_type is neither \"image\" nor \"text\".\n            ValueError: If the number of prompts exceeds the maximum batch size.\n        \"\"\"\n\n        if subject_type == \"image\":\n            subject_embeddings = self.embed_image(subject)\n        elif subject_type == \"text\":\n            subject_embeddings = self.embed_text(subject)\n        else:\n            raise ValueError(\n                \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n            )\n\n        if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n            prompt_keys = prompt.keys()\n            prompt = [prompt[k] for k in prompt_keys]\n            prompt_obj = \"dict\"\n        else:\n            prompt = prompt\n            if not isinstance(prompt, list):\n                prompt = [prompt]\n            prompt_obj = \"list\"\n\n        if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n\n        if prompt_type == \"image\":\n            prompt_embeddings = self.embed_image(prompt)\n        elif prompt_type == \"text\":\n            prompt_embeddings = self.embed_text(prompt)\n        else:\n            raise ValueError(\n                \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n            )\n\n        similarities = [\n            cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n        ]\n\n        if prompt_obj == \"dict\":\n            similarities = dict(zip(prompt_keys, similarities))\n\n        return similarities\n\n    def make_compare_response(\n        self, similarities: Union[List[float], Dict[str, float]]\n    ) -&gt; ClipCompareResponse:\n        \"\"\"\n        Creates a ClipCompareResponse object from the provided similarity data.\n\n        Args:\n            similarities (Union[List[float], Dict[str, float]]): A list or dictionary containing similarity scores.\n\n        Returns:\n            ClipCompareResponse: An instance of the ClipCompareResponse with the given similarity scores.\n\n        Example:\n            Assuming `ClipCompareResponse` expects a dictionary of string-float pairs:\n\n            &gt;&gt;&gt; make_compare_response({\"image1\": 0.98, \"image2\": 0.76})\n            ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})\n        \"\"\"\n        response = ClipCompareResponse(similarity=similarities)\n        return response\n\n    def embed_image(\n        self,\n        image: Any,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embeds an image or a list of images using the Clip model.\n\n        Args:\n            image (Any): The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the image(s) as a numpy array.\n\n        Raises:\n            ValueError: If the number of images in the list exceeds the maximum batch size.\n\n        Notes:\n            The function measures performance using perf_counter and also has support for ONNX session to get embeddings.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(image, list):\n            if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n                )\n            imgs = [self.preproc_image(i) for i in image]\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in = self.preproc_image(image)\n\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        with self._visual_session_lock:\n            return self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        with self._visual_session_lock:\n            embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n        return (embeddings,)\n\n    def make_embed_image_response(\n        self, embeddings: np.ndarray\n    ) -&gt; ClipEmbeddingResponse:\n        \"\"\"\n        Converts the given embeddings into a ClipEmbeddingResponse object.\n\n        Args:\n            embeddings (np.ndarray): A numpy array containing the embeddings for an image or images.\n\n        Returns:\n            ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n        Example:\n            &gt;&gt;&gt; embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n            &gt;&gt;&gt; make_embed_image_response(embeddings_array)\n            ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n        \"\"\"\n        response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n\n        return response\n\n    def embed_text(\n        self,\n        text: Union[str, List[str]],\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embeds a text or a list of texts using the Clip model.\n\n        Args:\n            text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the text or texts as a numpy array.\n\n        Raises:\n            ValueError: If the number of text strings in the list exceeds the maximum batch size.\n\n        Notes:\n            The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.\n        \"\"\"\n        if isinstance(text, list):\n            texts = text\n        else:\n            texts = [text]\n        results = []\n        for texts_batch in create_batches(\n            sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n        ):\n            tokenized_batch = clip.tokenize(texts_batch).numpy().astype(np.int32)\n            onnx_input_text = {\n                self.textual_onnx_session.get_inputs()[0].name: tokenized_batch\n            }\n            with self._textual_session_lock:\n                embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n            results.append(embeddings)\n        return np.concatenate(results, axis=0)\n\n    def make_embed_text_response(self, embeddings: np.ndarray) -&gt; ClipEmbeddingResponse:\n        \"\"\"\n        Converts the given text embeddings into a ClipEmbeddingResponse object.\n\n        Args:\n            embeddings (np.ndarray): A numpy array containing the embeddings for a text or texts.\n\n        Returns:\n            ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n        Example:\n            &gt;&gt;&gt; embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n            &gt;&gt;&gt; make_embed_text_response(embeddings_array)\n            ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n        \"\"\"\n        response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\"textual.onnx\", \"visual.onnx\"]\n\n    def infer_from_request(\n        self, request: ClipInferenceRequest\n    ) -&gt; ClipEmbeddingResponse:\n        \"\"\"Routes the request to the appropriate inference function.\n\n        Args:\n            request (ClipInferenceRequest): The request object containing the inference details.\n\n        Returns:\n            ClipEmbeddingResponse: The response object containing the embeddings.\n        \"\"\"\n        t1 = perf_counter()\n        if isinstance(request, ClipImageEmbeddingRequest):\n            infer_func = self.embed_image\n            make_response_func = self.make_embed_image_response\n        elif isinstance(request, ClipTextEmbeddingRequest):\n            infer_func = self.embed_text\n            make_response_func = self.make_embed_text_response\n        elif isinstance(request, ClipCompareRequest):\n            infer_func = self.compare\n            make_response_func = self.make_compare_response\n        else:\n            raise ValueError(\n                f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n            )\n        data = infer_func(**request.dict())\n        response = make_response_func(data)\n        response.time = perf_counter() - t1\n        return response\n\n    def make_response(self, embeddings, *args, **kwargs) -&gt; InferenceResponse:\n        return [self.make_embed_image_response(embeddings)]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        return [self.make_embed_image_response(predictions[0])]\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Embeds an image\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        return super().infer(image, **kwargs)\n\n    def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n        \"\"\"Preprocesses an inference request image.\n\n        Args:\n            image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n        Returns:\n            np.ndarray: A numpy array of the preprocessed image pixel data.\n        \"\"\"\n        pil_image = Image.fromarray(load_image_rgb(image))\n        preprocessed_image = self.clip_preprocess(pil_image)\n\n        img_in = np.expand_dims(preprocessed_image, axis=0)\n\n        return img_in.astype(np.float32)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        return self.preproc_image(image), PreprocessReturnMetadata({})\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.__init__","title":"<code>__init__(*args, model_id=CLIP_MODEL_ID, onnxruntime_execution_providers=get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS), **kwargs)</code>","text":"<p>Initializes the Clip with the given arguments and keyword arguments.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    model_id: str = CLIP_MODEL_ID,\n    onnxruntime_execution_providers: List[\n        str\n    ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n    **kwargs,\n):\n    \"\"\"Initializes the Clip with the given arguments and keyword arguments.\"\"\"\n    self.onnxruntime_execution_providers = onnxruntime_execution_providers\n    t1 = perf_counter()\n    super().__init__(*args, model_id=model_id, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n    self.visual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"visual.onnx\"),\n        providers=self.onnxruntime_execution_providers,\n    )\n    self._visual_session_lock = Lock()\n    self.textual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"textual.onnx\"),\n        providers=self.onnxruntime_execution_providers,\n    )\n    self._textual_session_lock = Lock()\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n    self.clip_preprocess = clip.clip._transform(self.resolution)\n    self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n    self.task_type = \"embedding\"\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.compare","title":"<code>compare(subject, prompt, subject_type='image', prompt_type='text', **kwargs)</code>","text":"<p>Compares the subject with the prompt to calculate similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>Any</code> <p>The subject data to be compared. Can be either an image or text.</p> required <code>prompt</code> <code>Any</code> <p>The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.</p> required <code>subject_type</code> <code>str</code> <p>Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".</p> <code>'image'</code> <code>prompt_type</code> <code>Union[str, List[str], Dict[str, Any]]</code> <p>Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".</p> <code>'text'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Dict[str, float]]</code> <p>Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If subject_type or prompt_type is neither \"image\" nor \"text\".</p> <code>ValueError</code> <p>If the number of prompts exceeds the maximum batch size.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def compare(\n    self,\n    subject: Any,\n    prompt: Any,\n    subject_type: str = \"image\",\n    prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n    **kwargs,\n) -&gt; Union[List[float], Dict[str, float]]:\n    \"\"\"\n    Compares the subject with the prompt to calculate similarity scores.\n\n    Args:\n        subject (Any): The subject data to be compared. Can be either an image or text.\n        prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n        subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n        prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.\n\n    Raises:\n        ValueError: If subject_type or prompt_type is neither \"image\" nor \"text\".\n        ValueError: If the number of prompts exceeds the maximum batch size.\n    \"\"\"\n\n    if subject_type == \"image\":\n        subject_embeddings = self.embed_image(subject)\n    elif subject_type == \"text\":\n        subject_embeddings = self.embed_text(subject)\n    else:\n        raise ValueError(\n            \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n        )\n\n    if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n        prompt_keys = prompt.keys()\n        prompt = [prompt[k] for k in prompt_keys]\n        prompt_obj = \"dict\"\n    else:\n        prompt = prompt\n        if not isinstance(prompt, list):\n            prompt = [prompt]\n        prompt_obj = \"list\"\n\n    if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n        raise ValueError(\n            f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n        )\n\n    if prompt_type == \"image\":\n        prompt_embeddings = self.embed_image(prompt)\n    elif prompt_type == \"text\":\n        prompt_embeddings = self.embed_text(prompt)\n    else:\n        raise ValueError(\n            \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n        )\n\n    similarities = [\n        cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n    ]\n\n    if prompt_obj == \"dict\":\n        similarities = dict(zip(prompt_keys, similarities))\n\n    return similarities\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.embed_image","title":"<code>embed_image(image, **kwargs)</code>","text":"<p>Embeds an image or a list of images using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the image(s) as a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of images in the list exceeds the maximum batch size.</p> Notes <p>The function measures performance using perf_counter and also has support for ONNX session to get embeddings.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def embed_image(\n    self,\n    image: Any,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Embeds an image or a list of images using the Clip model.\n\n    Args:\n        image (Any): The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the image(s) as a numpy array.\n\n    Raises:\n        ValueError: If the number of images in the list exceeds the maximum batch size.\n\n    Notes:\n        The function measures performance using perf_counter and also has support for ONNX session to get embeddings.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(image, list):\n        if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n        imgs = [self.preproc_image(i) for i in image]\n        img_in = np.concatenate(imgs, axis=0)\n    else:\n        img_in = self.preproc_image(image)\n\n    onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n    with self._visual_session_lock:\n        return self.visual_onnx_session.run(None, onnx_input_image)[0]\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.embed_text","title":"<code>embed_text(text, **kwargs)</code>","text":"<p>Embeds a text or a list of texts using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, List[str]]</code> <p>The text string or list of text strings to be embedded.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the text or texts as a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of text strings in the list exceeds the maximum batch size.</p> Notes <p>The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def embed_text(\n    self,\n    text: Union[str, List[str]],\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Embeds a text or a list of texts using the Clip model.\n\n    Args:\n        text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the text or texts as a numpy array.\n\n    Raises:\n        ValueError: If the number of text strings in the list exceeds the maximum batch size.\n\n    Notes:\n        The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.\n    \"\"\"\n    if isinstance(text, list):\n        texts = text\n    else:\n        texts = [text]\n    results = []\n    for texts_batch in create_batches(\n        sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n    ):\n        tokenized_batch = clip.tokenize(texts_batch).numpy().astype(np.int32)\n        onnx_input_text = {\n            self.textual_onnx_session.get_inputs()[0].name: tokenized_batch\n        }\n        with self._textual_session_lock:\n            embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n        results.append(embeddings)\n    return np.concatenate(results, axis=0)\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\"textual.onnx\", \"visual.onnx\"]\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Embeds an image - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Embeds an image\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    return super().infer(image, **kwargs)\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Routes the request to the appropriate inference function.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipInferenceRequest</code> <p>The request object containing the inference details.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>The response object containing the embeddings.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def infer_from_request(\n    self, request: ClipInferenceRequest\n) -&gt; ClipEmbeddingResponse:\n    \"\"\"Routes the request to the appropriate inference function.\n\n    Args:\n        request (ClipInferenceRequest): The request object containing the inference details.\n\n    Returns:\n        ClipEmbeddingResponse: The response object containing the embeddings.\n    \"\"\"\n    t1 = perf_counter()\n    if isinstance(request, ClipImageEmbeddingRequest):\n        infer_func = self.embed_image\n        make_response_func = self.make_embed_image_response\n    elif isinstance(request, ClipTextEmbeddingRequest):\n        infer_func = self.embed_text\n        make_response_func = self.make_embed_text_response\n    elif isinstance(request, ClipCompareRequest):\n        infer_func = self.compare\n        make_response_func = self.make_compare_response\n    else:\n        raise ValueError(\n            f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n        )\n    data = infer_func(**request.dict())\n    response = make_response_func(data)\n    response.time = perf_counter() - t1\n    return response\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_compare_response","title":"<code>make_compare_response(similarities)</code>","text":"<p>Creates a ClipCompareResponse object from the provided similarity data.</p> <p>Parameters:</p> Name Type Description Default <code>similarities</code> <code>Union[List[float], Dict[str, float]]</code> <p>A list or dictionary containing similarity scores.</p> required <p>Returns:</p> Name Type Description <code>ClipCompareResponse</code> <code>ClipCompareResponse</code> <p>An instance of the ClipCompareResponse with the given similarity scores.</p> Example <p>Assuming <code>ClipCompareResponse</code> expects a dictionary of string-float pairs:</p> <p>make_compare_response({\"image1\": 0.98, \"image2\": 0.76}) ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_compare_response(\n    self, similarities: Union[List[float], Dict[str, float]]\n) -&gt; ClipCompareResponse:\n    \"\"\"\n    Creates a ClipCompareResponse object from the provided similarity data.\n\n    Args:\n        similarities (Union[List[float], Dict[str, float]]): A list or dictionary containing similarity scores.\n\n    Returns:\n        ClipCompareResponse: An instance of the ClipCompareResponse with the given similarity scores.\n\n    Example:\n        Assuming `ClipCompareResponse` expects a dictionary of string-float pairs:\n\n        &gt;&gt;&gt; make_compare_response({\"image1\": 0.98, \"image2\": 0.76})\n        ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})\n    \"\"\"\n    response = ClipCompareResponse(similarity=similarities)\n    return response\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_embed_image_response","title":"<code>make_embed_image_response(embeddings)</code>","text":"<p>Converts the given embeddings into a ClipEmbeddingResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>A numpy array containing the embeddings for an image or images.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.</p> Example <p>embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]]) make_embed_image_response(embeddings_array) ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_embed_image_response(\n    self, embeddings: np.ndarray\n) -&gt; ClipEmbeddingResponse:\n    \"\"\"\n    Converts the given embeddings into a ClipEmbeddingResponse object.\n\n    Args:\n        embeddings (np.ndarray): A numpy array containing the embeddings for an image or images.\n\n    Returns:\n        ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n    Example:\n        &gt;&gt;&gt; embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n        &gt;&gt;&gt; make_embed_image_response(embeddings_array)\n        ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n    \"\"\"\n    response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n\n    return response\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_embed_text_response","title":"<code>make_embed_text_response(embeddings)</code>","text":"<p>Converts the given text embeddings into a ClipEmbeddingResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>A numpy array containing the embeddings for a text or texts.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.</p> Example <p>embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]]) make_embed_text_response(embeddings_array) ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_embed_text_response(self, embeddings: np.ndarray) -&gt; ClipEmbeddingResponse:\n    \"\"\"\n    Converts the given text embeddings into a ClipEmbeddingResponse object.\n\n    Args:\n        embeddings (np.ndarray): A numpy array containing the embeddings for a text or texts.\n\n    Returns:\n        ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n    Example:\n        &gt;&gt;&gt; embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n        &gt;&gt;&gt; make_embed_text_response(embeddings_array)\n        ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n    \"\"\"\n    response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n    return response\n</code></pre>"},{"location":"reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an inference request image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The object containing information necessary to load the image for inference.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of the preprocessed image pixel data.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n    \"\"\"Preprocesses an inference request image.\n\n    Args:\n        image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n    Returns:\n        np.ndarray: A numpy array of the preprocessed image pixel data.\n    \"\"\"\n    pil_image = Image.fromarray(load_image_rgb(image))\n    preprocessed_image = self.clip_preprocess(pil_image)\n\n    img_in = np.expand_dims(preprocessed_image, axis=0)\n\n    return img_in.astype(np.float32)\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/","title":"Doctr model","text":""},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR","title":"<code>DocTR</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTR(RoboflowCoreModel):\n    def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n        \"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        self.api_key = kwargs.get(\"api_key\")\n        self.dataset_id = \"doctr\"\n        self.version_id = \"default\"\n        self.endpoint = model_id\n        model_id = model_id.lower()\n\n        self.det_model = DocTRDet(api_key=kwargs.get(\"api_key\"))\n        self.rec_model = DocTRRec(api_key=kwargs.get(\"api_key\"))\n\n        os.makedirs(f\"{MODEL_CACHE_DIR}/doctr/models/\", exist_ok=True)\n\n        detector_weights_path = (\n            f\"{MODEL_CACHE_DIR}/doctr/models/{self.det_model.version_id}.pt\"\n        )\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/doctr_det/{self.det_model.version_id}/model.pt\",\n            detector_weights_path,\n        )\n        recognizer_weights_path = (\n            f\"{MODEL_CACHE_DIR}/doctr/models/{self.rec_model.version_id}.pt\"\n        )\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/doctr_rec/{self.rec_model.version_id}/model.pt\",\n            recognizer_weights_path,\n        )\n\n        det_model = db_resnet50(pretrained=False, pretrained_backbone=False)\n        det_model.load_state_dict(\n            torch.load(detector_weights_path, map_location=DEVICE, weights_only=True)\n        )\n\n        reco_model = crnn_vgg16_bn(pretrained=False, pretrained_backbone=False)\n        reco_model.load_state_dict(\n            torch.load(recognizer_weights_path, map_location=DEVICE, weights_only=True)\n        )\n\n        self.model = ocr_predictor(\n            det_arch=det_model,\n            reco_arch=reco_model,\n            pretrained=False,\n        )\n        self.task_type = \"ocr\"\n\n    def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n        self.det_model.clear_cache(delete_from_disk=delete_from_disk)\n        self.rec_model.clear_cache(delete_from_disk=delete_from_disk)\n\n    def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"\n        DocTR pre-processes images as part of its inference pipeline.\n\n        Thus, no preprocessing is required here.\n        \"\"\"\n        pass\n\n    def infer_from_request(\n        self, request: DoctrOCRInferenceRequest\n    ) -&gt; Union[OCRInferenceResponse, List]:\n        if type(request.image) is list:\n            response = []\n            request_copy = copy.copy(request)\n            for image in request.image:\n                request_copy.image = image\n                response.append(self.single_request(request=request_copy))\n            return response\n        return self.single_request(request)\n\n    def single_request(self, request: DoctrOCRInferenceRequest) -&gt; OCRInferenceResponse:\n        t1 = perf_counter()\n        result = self.infer(**request.dict())\n        if not isinstance(result, tuple):\n            result = (result, None, None)\n        # maintaining backwards compatibility with previous implementation\n        if request.generate_bounding_boxes:\n            return OCRInferenceResponse(\n                result=result[0],\n                image=result[1],\n                predictions=result[2],\n                time=perf_counter() - t1,\n            )\n        else:\n            return OCRInferenceResponse(\n                result=result[0],\n                time=perf_counter() - t1,\n            )\n\n    def infer(\n        self, image: Any, **kwargs\n    ) -&gt; Union[\n        str, Tuple[str, InferenceResponseImage, List[ObjectDetectionPrediction]]\n    ]:\n        \"\"\"\n        Run inference on a provided image.\n            - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n        Args:\n            request (DoctrOCRInferenceRequest): The inference request.\n\n        Returns:\n            OCRInferenceResponse: The inference response.\n        \"\"\"\n\n        img = load_image(image)\n\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n            image = Image.fromarray(img[0])\n\n            image.save(f.name)\n\n            doc = DocumentFile.from_images([f.name])\n\n            result = self.model(doc).export()\n\n            blocks = result[\"pages\"][0][\"blocks\"]\n            page_dimensions = result[\"pages\"][0][\"dimensions\"]\n\n            words = [\n                word\n                for block in blocks\n                for line in block[\"lines\"]\n                for word in line[\"words\"]\n            ]\n\n            result = \" \".join([word[\"value\"] for word in words])\n            # maintaining backwards compatibility with previous implementation\n            if not kwargs.get(\"generate_bounding_boxes\", False):\n                return result\n\n            bounding_boxes = [\n                _geometry_to_bbox(page_dimensions, word[\"geometry\"]) for word in words\n            ]\n            objects = [\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": bbox[0] + (bbox[2] - bbox[0]) // 2,\n                        \"y\": bbox[1] + (bbox[3] - bbox[1]) // 2,\n                        \"width\": bbox[2] - bbox[0],\n                        \"height\": bbox[3] - bbox[1],\n                        \"confidence\": float(word[\"objectness_score\"]),\n                        \"class\": word[\"value\"],\n                        \"class_id\": 0,\n                        \"detection_id\": str(uuid.uuid4()),\n                    }\n                )\n                for word, bbox in zip(words, bounding_boxes)\n            ]\n            image_height, image_width = img[0].shape[:2]\n            return (\n                result,\n                InferenceResponseImage(width=image_width, height=image_height),\n                objects,\n            )\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.__init__","title":"<code>__init__(*args, model_id='doctr_rec/crnn_vgg16_bn', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n    \"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    self.api_key = kwargs.get(\"api_key\")\n    self.dataset_id = \"doctr\"\n    self.version_id = \"default\"\n    self.endpoint = model_id\n    model_id = model_id.lower()\n\n    self.det_model = DocTRDet(api_key=kwargs.get(\"api_key\"))\n    self.rec_model = DocTRRec(api_key=kwargs.get(\"api_key\"))\n\n    os.makedirs(f\"{MODEL_CACHE_DIR}/doctr/models/\", exist_ok=True)\n\n    detector_weights_path = (\n        f\"{MODEL_CACHE_DIR}/doctr/models/{self.det_model.version_id}.pt\"\n    )\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/doctr_det/{self.det_model.version_id}/model.pt\",\n        detector_weights_path,\n    )\n    recognizer_weights_path = (\n        f\"{MODEL_CACHE_DIR}/doctr/models/{self.rec_model.version_id}.pt\"\n    )\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/doctr_rec/{self.rec_model.version_id}/model.pt\",\n        recognizer_weights_path,\n    )\n\n    det_model = db_resnet50(pretrained=False, pretrained_backbone=False)\n    det_model.load_state_dict(\n        torch.load(detector_weights_path, map_location=DEVICE, weights_only=True)\n    )\n\n    reco_model = crnn_vgg16_bn(pretrained=False, pretrained_backbone=False)\n    reco_model.load_state_dict(\n        torch.load(recognizer_weights_path, map_location=DEVICE, weights_only=True)\n    )\n\n    self.model = ocr_predictor(\n        det_arch=det_model,\n        reco_arch=reco_model,\n        pretrained=False,\n    )\n    self.task_type = \"ocr\"\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Run inference on a provided image.     - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>DoctrOCRInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Name Type Description <code>OCRInferenceResponse</code> <code>Union[str, Tuple[str, InferenceResponseImage, List[ObjectDetectionPrediction]]]</code> <p>The inference response.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def infer(\n    self, image: Any, **kwargs\n) -&gt; Union[\n    str, Tuple[str, InferenceResponseImage, List[ObjectDetectionPrediction]]\n]:\n    \"\"\"\n    Run inference on a provided image.\n        - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n    Args:\n        request (DoctrOCRInferenceRequest): The inference request.\n\n    Returns:\n        OCRInferenceResponse: The inference response.\n    \"\"\"\n\n    img = load_image(image)\n\n    with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n        image = Image.fromarray(img[0])\n\n        image.save(f.name)\n\n        doc = DocumentFile.from_images([f.name])\n\n        result = self.model(doc).export()\n\n        blocks = result[\"pages\"][0][\"blocks\"]\n        page_dimensions = result[\"pages\"][0][\"dimensions\"]\n\n        words = [\n            word\n            for block in blocks\n            for line in block[\"lines\"]\n            for word in line[\"words\"]\n        ]\n\n        result = \" \".join([word[\"value\"] for word in words])\n        # maintaining backwards compatibility with previous implementation\n        if not kwargs.get(\"generate_bounding_boxes\", False):\n            return result\n\n        bounding_boxes = [\n            _geometry_to_bbox(page_dimensions, word[\"geometry\"]) for word in words\n        ]\n        objects = [\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": bbox[0] + (bbox[2] - bbox[0]) // 2,\n                    \"y\": bbox[1] + (bbox[3] - bbox[1]) // 2,\n                    \"width\": bbox[2] - bbox[0],\n                    \"height\": bbox[3] - bbox[1],\n                    \"confidence\": float(word[\"objectness_score\"]),\n                    \"class\": word[\"value\"],\n                    \"class_id\": 0,\n                    \"detection_id\": str(uuid.uuid4()),\n                }\n            )\n            for word, bbox in zip(words, bounding_boxes)\n        ]\n        image_height, image_width = img[0].shape[:2]\n        return (\n            result,\n            InferenceResponseImage(width=image_width, height=image_height),\n            objects,\n        )\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>DocTR pre-processes images as part of its inference pipeline.</p> <p>Thus, no preprocessing is required here.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n    \"\"\"\n    DocTR pre-processes images as part of its inference pipeline.\n\n    Thus, no preprocessing is required here.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet","title":"<code>DocTRDet</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>DocTR class for document Optical Character Recognition (OCR).</p> <p>Attributes:</p> Name Type Description <code>doctr</code> <p>The DocTR model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTRDet(RoboflowCoreModel):\n    \"\"\"DocTR class for document Optical Character Recognition (OCR).\n\n    Attributes:\n        doctr: The DocTR model.\n        ort_session: ONNX runtime inference session.\n    \"\"\"\n\n    def __init__(self, *args, model_id: str = \"doctr_det/db_resnet50\", **kwargs):\n        \"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        self.get_infer_bucket_file_list()\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n    def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n        super().clear_cache(delete_from_disk=delete_from_disk)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet.__init__","title":"<code>__init__(*args, model_id='doctr_det/db_resnet50', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_det/db_resnet50\", **kwargs):\n    \"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    self.get_infer_bucket_file_list()\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec","title":"<code>DocTRRec</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTRRec(RoboflowCoreModel):\n    def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n        \"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        self.get_infer_bucket_file_list()\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n    def clear_cache(self, delete_from_disk: bool = True) -&gt; None:\n        super().clear_cache(delete_from_disk=delete_from_disk)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec.__init__","title":"<code>__init__(*args, model_id='doctr_rec/crnn_vgg16_bn', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n    \"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    self.get_infer_bucket_file_list()\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"reference/inference/models/easy_ocr/easy_ocr/","title":"Easy ocr","text":""},{"location":"reference/inference/models/easy_ocr/easy_ocr/#inference.models.easy_ocr.easy_ocr.EasyOCR","title":"<code>EasyOCR</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>Roboflow EasyOCR model implementation.</p> <p>This class is responsible for handling the EasyOCR model, including loading the model, preprocessing the input, and performing inference.</p> Source code in <code>inference/models/easy_ocr/easy_ocr.py</code> <pre><code>class EasyOCR(RoboflowCoreModel):\n    \"\"\"Roboflow EasyOCR model implementation.\n\n    This class is responsible for handling the EasyOCR model, including\n    loading the model, preprocessing the input, and performing inference.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str = \"easy_ocr/english_g2\",\n        device: str = DEVICE,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initializes EasyOCR with the given arguments and keyword arguments.\"\"\"\n\n        super().__init__(model_id=model_id.lower(), *args, **kwargs)\n        self.device = device\n        self.task_type = \"ocr\"\n        self.recognizer = model_id.split(\"/\")[1]\n\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/{model_id}/weights.pt\",\n            f\"{MODEL_CACHE_DIR}/{model_id}/{self.recognizer}.pth\",\n        )\n\n    def predict(self, image_in: np.ndarray, prompt=\"\", history=None, **kwargs):\n        language_codes = kwargs.get(\"language_codes\", [\"en\"])\n        quantize = kwargs.get(\"quantize\", False)\n        reader = easyocr.Reader(\n            language_codes,\n            download_enabled=False,\n            user_network_directory=f\"{MODEL_CACHE_DIR}/easy_ocr/{self.recognizer}/\",\n            model_storage_directory=f\"{MODEL_CACHE_DIR}/easy_ocr/{self.recognizer}/\",\n            detect_network=\"craft\",\n            recog_network=self.recognizer,\n            detector=True,\n            recognizer=True,\n            gpu=True,\n            quantize=quantize,\n        )\n\n        results = reader.readtext(image_in)\n        # convert native EasyOCR results from numpy to standard python types\n        results = [\n            (\n                [\n                    [x.item() if not isinstance(x, (int, float)) else x for x in c]\n                    for c in res[0]\n                ],\n                res[1],\n                res[2].item() if not isinstance(res[2], (int, float)) else res[2],\n            )\n            for res in results\n        ]\n\n        return results\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        return predictions, preprocess_return_metadata\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        image = load_image(image)[0]\n        return image, InferenceResponseImage(\n            width=image.shape[1], height=image.shape[0]\n        )\n\n    def infer_from_request(\n        self, request: EasyOCRInferenceRequest\n    ) -&gt; Union[OCRInferenceResponse, List]:\n        if type(request.image) is list:\n            response = []\n            request_copy = copy.copy(request)\n            for image in request.image:\n                request_copy.image = image\n                response.append(self.single_request(request=request_copy))\n            return response\n        return self.single_request(request)\n\n    def single_request(self, request: EasyOCRInferenceRequest) -&gt; OCRInferenceResponse:\n        t1 = perf_counter()\n        prediction_result, image_metadata = self.infer(**request.dict())\n        strings = [res[1] for res in prediction_result]\n        return OCRInferenceResponse(\n            result=\" \".join(strings),\n            image=image_metadata,\n            predictions=[\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": box[0][0] + (box[2][0] - box[0][0]) // 2,\n                        \"y\": box[0][1] + (box[2][1] - box[0][1]) // 2,\n                        \"width\": box[2][0] - box[0][0],\n                        \"height\": box[2][1] - box[0][1],\n                        \"confidence\": float(confidence),\n                        \"class\": string,\n                        \"class_id\": 0,\n                        \"detection_id\": str(uuid.uuid4()),\n                    }\n                )\n                for box, string, confidence in prediction_result\n            ],\n            time=perf_counter() - t1,\n        )\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        return [\"weights.pt\", \"craft_mlt_25k.pth\"]\n</code></pre>"},{"location":"reference/inference/models/easy_ocr/easy_ocr/#inference.models.easy_ocr.easy_ocr.EasyOCR.__init__","title":"<code>__init__(model_id='easy_ocr/english_g2', device=DEVICE, *args, **kwargs)</code>","text":"<p>Initializes EasyOCR with the given arguments and keyword arguments.</p> Source code in <code>inference/models/easy_ocr/easy_ocr.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = \"easy_ocr/english_g2\",\n    device: str = DEVICE,\n    *args,\n    **kwargs,\n):\n    \"\"\"Initializes EasyOCR with the given arguments and keyword arguments.\"\"\"\n\n    super().__init__(model_id=model_id.lower(), *args, **kwargs)\n    self.device = device\n    self.task_type = \"ocr\"\n    self.recognizer = model_id.split(\"/\")[1]\n\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/{model_id}/weights.pt\",\n        f\"{MODEL_CACHE_DIR}/{model_id}/{self.recognizer}.pth\",\n    )\n</code></pre>"},{"location":"reference/inference/models/florence2/utils/","title":"Utils","text":""},{"location":"reference/inference/models/florence2/utils/#inference.models.florence2.utils.import_class_from_file","title":"<code>import_class_from_file(file_path, class_name, alias_name=None)</code>","text":"<p>Emulates what huggingface transformers does to load remote code with trust_remote_code=True, but allows us to use the class directly so that we don't have to load untrusted code.</p> Source code in <code>inference/models/florence2/utils.py</code> <pre><code>def import_class_from_file(file_path, class_name, alias_name=None):\n    \"\"\"\n    Emulates what huggingface transformers does to load remote code with trust_remote_code=True,\n    but allows us to use the class directly so that we don't have to load untrusted code.\n    \"\"\"\n    file_path = os.path.abspath(file_path)\n    module_name = os.path.splitext(os.path.basename(file_path))[0]\n    module_dir = os.path.dirname(file_path)\n    parent_dir = os.path.dirname(module_dir)\n\n    sys.path.insert(0, parent_dir)\n\n    try:\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n\n        # Manually set the __package__ attribute to the parent package\n        module.__package__ = os.path.basename(module_dir)\n\n        spec.loader.exec_module(module)\n        cls = getattr(module, class_name)\n        if alias_name:\n            globals()[alias_name] = cls\n        return cls\n    finally:\n        sys.path.pop(0)\n</code></pre>"},{"location":"reference/inference/models/gaze/gaze/","title":"Gaze","text":""},{"location":"reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze","title":"<code>Gaze</code>","text":"<p>               Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX Gaze model.</p> <p>This class is responsible for handling the ONNX Gaze model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>gaze_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for gaze detection inference.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>class Gaze(OnnxRoboflowCoreModel):\n    \"\"\"Roboflow ONNX Gaze model.\n\n    This class is responsible for handling the ONNX Gaze model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        gaze_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for gaze detection inference.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes the Gaze with the given arguments and keyword arguments.\"\"\"\n\n        t1 = perf_counter()\n        super().__init__(*args, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n\n        # TODO: convert face detector (TensorflowLite) to ONNX model\n\n        self.gaze_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"L2CSNet_gaze360_resnet50_90bins.onnx\"),\n            providers=[\n                (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                    },\n                ),\n                \"CUDAExecutionProvider\",\n                \"OpenVINOExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n        self._gaze_session_lock = Lock()\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        # init face detector\n        self.face_detector = mp.tasks.vision.FaceDetector.create_from_options(\n            mp.tasks.vision.FaceDetectorOptions(\n                base_options=mp.tasks.BaseOptions(\n                    model_asset_path=self.cache_file(\"mediapipe_face_detector.tflite\")\n                ),\n                running_mode=mp.tasks.vision.RunningMode.IMAGE,\n            )\n        )\n\n        # additional settings for gaze detection\n        self._gaze_transformations = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Resize(448),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n        self.task_type = \"gaze-detection\"\n        self.log(f\"GAZE model loaded in {perf_counter() - t1:.2f} seconds\")\n\n    def _crop_face_img(self, np_img: np.ndarray, face: Detection) -&gt; np.ndarray:\n        \"\"\"Extract facial area in an image.\n\n        Args:\n            np_img (np.ndarray): The numpy image.\n            face (mediapipe.tasks.python.components.containers.detections.Detection): The detected face.\n\n        Returns:\n            np.ndarray: Cropped face image.\n        \"\"\"\n        # extract face area\n        bbox = face.bounding_box\n        x_min = bbox.origin_x\n        y_min = bbox.origin_y\n        x_max = bbox.origin_x + bbox.width\n        y_max = bbox.origin_y + bbox.height\n        face_img = np_img[y_min:y_max, x_min:x_max, :]\n        face_img = cv2.resize(face_img, (224, 224))\n        return face_img\n\n    def _detect_gaze(self, np_imgs: List[np.ndarray]) -&gt; List[Tuple[float, float]]:\n        \"\"\"Detect faces and gazes in an image.\n\n        Args:\n            pil_imgs (List[np.ndarray]): The numpy image list, each image is a cropped facial image.\n\n        Returns:\n            List[Tuple[float, float]]: Yaw (radian) and Pitch (radian).\n        \"\"\"\n        ret = []\n        for i in range(0, len(np_imgs), GAZE_MAX_BATCH_SIZE):\n            img_batch = []\n            for j in range(i, min(len(np_imgs), i + GAZE_MAX_BATCH_SIZE)):\n                img = self._gaze_transformations(np_imgs[j])\n                img = np.expand_dims(img, axis=0).astype(np.float32)\n                img_batch.append(img)\n\n            img_batch = np.concatenate(img_batch, axis=0)\n            onnx_input_image = {self.gaze_onnx_session.get_inputs()[0].name: img_batch}\n            with self._gaze_session_lock:\n                yaw, pitch = self.gaze_onnx_session.run(None, onnx_input_image)\n            for j in range(len(img_batch)):\n                ret.append((yaw[j], pitch[j]))\n\n        return ret\n\n    def _make_response(\n        self,\n        faces: List[Detection],\n        gazes: List[Tuple[float, float]],\n        imgW: int,\n        imgH: int,\n        time_total: float,\n        time_face_det: float = None,\n        time_gaze_det: float = None,\n    ) -&gt; GazeDetectionInferenceResponse:\n        \"\"\"Prepare response object from detected faces and corresponding gazes.\n\n        Args:\n            faces (List[Detection]): The detected faces.\n            gazes (List[tuple(float, float)]): The detected gazes (yaw, pitch).\n            imgW (int): The width (px) of original image.\n            imgH (int): The height (px) of original image.\n            time_total (float): The processing time.\n            time_face_det (float): The processing time.\n            time_gaze_det (float): The processing time.\n\n        Returns:\n            GazeDetectionInferenceResponse: The response object including the detected faces and gazes info.\n        \"\"\"\n        predictions = []\n        for face, gaze in zip(faces, gazes):\n            landmarks = []\n            for keypoint in face.keypoints:\n                x = min(max(int(keypoint.x * imgW), 0), imgW - 1)\n                y = min(max(int(keypoint.y * imgH), 0), imgH - 1)\n                landmarks.append(Point(x=x, y=y))\n\n            bbox = face.bounding_box\n            x_center = bbox.origin_x + bbox.width / 2\n            y_center = bbox.origin_y + bbox.height / 2\n            score = face.categories[0].score\n\n            prediction = GazeDetectionPrediction(\n                face=FaceDetectionPrediction(\n                    x=x_center,\n                    y=y_center,\n                    width=bbox.width,\n                    height=bbox.height,\n                    confidence=score,\n                    class_name=\"face\",\n                    landmarks=landmarks,\n                ),\n                yaw=gaze[0],\n                pitch=gaze[1],\n            )\n            predictions.append(prediction)\n\n        response = GazeDetectionInferenceResponse(\n            predictions=predictions,\n            time=time_total,\n            time_face_det=time_face_det,\n            time_gaze_det=time_gaze_det,\n        )\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\n            \"mediapipe_face_detector.tflite\",\n            \"L2CSNet_gaze360_resnet50_90bins.onnx\",\n        ]\n\n    def infer_from_request(\n        self, request: GazeDetectionInferenceRequest\n    ) -&gt; List[GazeDetectionInferenceResponse]:\n        \"\"\"Detect faces and gazes in image(s).\n\n        Args:\n            request (GazeDetectionInferenceRequest): The request object containing the image.\n\n        Returns:\n            List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.\n        \"\"\"\n        if isinstance(request.image, list):\n            if len(request.image) &gt; GAZE_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be inferred with gaze detection at one time is {GAZE_MAX_BATCH_SIZE}\"\n                )\n            imgs = request.image\n        else:\n            imgs = [request.image]\n\n        time_total = perf_counter()\n\n        # load pil images\n        num_img = len(imgs)\n        np_imgs = [load_image_rgb(img) for img in imgs]\n\n        # face detection\n        # TODO: face detection for batch\n        time_face_det = perf_counter()\n        faces = []\n        for np_img in np_imgs:\n            if request.do_run_face_detection:\n                mp_img = mp.Image(\n                    image_format=mp.ImageFormat.SRGB, data=np_img.astype(np.uint8)\n                )\n                faces_per_img = self.face_detector.detect(mp_img).detections\n            else:\n                faces_per_img = [\n                    Detection(\n                        bounding_box=BoundingBox(\n                            origin_x=0,\n                            origin_y=0,\n                            width=np_img.shape[1],\n                            height=np_img.shape[0],\n                        ),\n                        categories=[Category(score=1.0, category_name=\"face\")],\n                        keypoints=[],\n                    )\n                ]\n            faces.append(faces_per_img)\n        time_face_det = (perf_counter() - time_face_det) / num_img\n\n        # gaze detection\n        time_gaze_det = perf_counter()\n        face_imgs = []\n        for i, np_img in enumerate(np_imgs):\n            if request.do_run_face_detection:\n                face_imgs.extend(\n                    [self._crop_face_img(np_img, face) for face in faces[i]]\n                )\n            else:\n                face_imgs.append(cv2.resize(np_img, (224, 224)))\n        gazes = self._detect_gaze(face_imgs)\n        time_gaze_det = (perf_counter() - time_gaze_det) / num_img\n\n        time_total = (perf_counter() - time_total) / num_img\n\n        # prepare response\n        response = []\n        idx_gaze = 0\n        for i in range(len(np_imgs)):\n            imgH, imgW, _ = np_imgs[i].shape\n            faces_per_img = faces[i]\n            gazes_per_img = gazes[idx_gaze : idx_gaze + len(faces_per_img)]\n            response.append(\n                self._make_response(\n                    faces_per_img, gazes_per_img, imgW, imgH, time_total\n                )\n            )\n            idx_gaze += len(faces_per_img)\n\n        return response\n</code></pre>"},{"location":"reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the Gaze with the given arguments and keyword arguments.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes the Gaze with the given arguments and keyword arguments.\"\"\"\n\n    t1 = perf_counter()\n    super().__init__(*args, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n\n    # TODO: convert face detector (TensorflowLite) to ONNX model\n\n    self.gaze_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"L2CSNet_gaze360_resnet50_90bins.onnx\"),\n        providers=[\n            (\n                \"TensorrtExecutionProvider\",\n                {\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                },\n            ),\n            \"CUDAExecutionProvider\",\n            \"OpenVINOExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n    self._gaze_session_lock = Lock()\n\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    # init face detector\n    self.face_detector = mp.tasks.vision.FaceDetector.create_from_options(\n        mp.tasks.vision.FaceDetectorOptions(\n            base_options=mp.tasks.BaseOptions(\n                model_asset_path=self.cache_file(\"mediapipe_face_detector.tflite\")\n            ),\n            running_mode=mp.tasks.vision.RunningMode.IMAGE,\n        )\n    )\n\n    # additional settings for gaze detection\n    self._gaze_transformations = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Resize(448),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n            ),\n        ]\n    )\n    self.task_type = \"gaze-detection\"\n    self.log(f\"GAZE model loaded in {perf_counter() - t1:.2f} seconds\")\n</code></pre>"},{"location":"reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\n        \"mediapipe_face_detector.tflite\",\n        \"L2CSNet_gaze360_resnet50_90bins.onnx\",\n    ]\n</code></pre>"},{"location":"reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Detect faces and gazes in image(s).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GazeDetectionInferenceRequest</code> <p>The request object containing the image.</p> required <p>Returns:</p> Type Description <code>List[GazeDetectionInferenceResponse]</code> <p>List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def infer_from_request(\n    self, request: GazeDetectionInferenceRequest\n) -&gt; List[GazeDetectionInferenceResponse]:\n    \"\"\"Detect faces and gazes in image(s).\n\n    Args:\n        request (GazeDetectionInferenceRequest): The request object containing the image.\n\n    Returns:\n        List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.\n    \"\"\"\n    if isinstance(request.image, list):\n        if len(request.image) &gt; GAZE_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be inferred with gaze detection at one time is {GAZE_MAX_BATCH_SIZE}\"\n            )\n        imgs = request.image\n    else:\n        imgs = [request.image]\n\n    time_total = perf_counter()\n\n    # load pil images\n    num_img = len(imgs)\n    np_imgs = [load_image_rgb(img) for img in imgs]\n\n    # face detection\n    # TODO: face detection for batch\n    time_face_det = perf_counter()\n    faces = []\n    for np_img in np_imgs:\n        if request.do_run_face_detection:\n            mp_img = mp.Image(\n                image_format=mp.ImageFormat.SRGB, data=np_img.astype(np.uint8)\n            )\n            faces_per_img = self.face_detector.detect(mp_img).detections\n        else:\n            faces_per_img = [\n                Detection(\n                    bounding_box=BoundingBox(\n                        origin_x=0,\n                        origin_y=0,\n                        width=np_img.shape[1],\n                        height=np_img.shape[0],\n                    ),\n                    categories=[Category(score=1.0, category_name=\"face\")],\n                    keypoints=[],\n                )\n            ]\n        faces.append(faces_per_img)\n    time_face_det = (perf_counter() - time_face_det) / num_img\n\n    # gaze detection\n    time_gaze_det = perf_counter()\n    face_imgs = []\n    for i, np_img in enumerate(np_imgs):\n        if request.do_run_face_detection:\n            face_imgs.extend(\n                [self._crop_face_img(np_img, face) for face in faces[i]]\n            )\n        else:\n            face_imgs.append(cv2.resize(np_img, (224, 224)))\n    gazes = self._detect_gaze(face_imgs)\n    time_gaze_det = (perf_counter() - time_gaze_det) / num_img\n\n    time_total = (perf_counter() - time_total) / num_img\n\n    # prepare response\n    response = []\n    idx_gaze = 0\n    for i in range(len(np_imgs)):\n        imgH, imgW, _ = np_imgs[i].shape\n        faces_per_img = faces[i]\n        gazes_per_img = gazes[idx_gaze : idx_gaze + len(faces_per_img)]\n        response.append(\n            self._make_response(\n                faces_per_img, gazes_per_img, imgW, imgH, time_total\n            )\n        )\n        idx_gaze += len(faces_per_img)\n\n    return response\n</code></pre>"},{"location":"reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.L2C2Wrapper","title":"<code>L2C2Wrapper</code>","text":"<p>               Bases: <code>L2CS</code></p> <p>Roboflow L2CS Gaze detection model.</p> <p>This class is responsible for converting L2CS model to ONNX model. It is ONLY intended for internal usage.</p> Workflow <p>After training a L2CS model, create an instance of this wrapper class. Load the trained weights file, and save it as ONNX model.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>class L2C2Wrapper(L2CS):\n    \"\"\"Roboflow L2CS Gaze detection model.\n\n    This class is responsible for converting L2CS model to ONNX model.\n    It is ONLY intended for internal usage.\n\n    Workflow:\n        After training a L2CS model, create an instance of this wrapper class.\n        Load the trained weights file, and save it as ONNX model.\n    \"\"\"\n\n    def __init__(self):\n        self.device = torch.device(\"cpu\")\n        self.num_bins = 90\n        super().__init__(\n            torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], self.num_bins\n        )\n        self._gaze_softmax = nn.Softmax(dim=1)\n        self._gaze_idx_tensor = torch.FloatTensor([i for i in range(90)]).to(\n            self.device\n        )\n\n    def forward(self, x):\n        idx_tensor = torch.stack(\n            [self._gaze_idx_tensor for i in range(x.shape[0])], dim=0\n        )\n        gaze_yaw, gaze_pitch = super().forward(x)\n\n        yaw_predicted = self._gaze_softmax(gaze_yaw)\n        yaw_radian = (\n            (torch.sum(yaw_predicted * idx_tensor, dim=1) * 4 - 180) * np.pi / 180\n        )\n\n        pitch_predicted = self._gaze_softmax(gaze_pitch)\n        pitch_radian = (\n            (torch.sum(pitch_predicted * idx_tensor, dim=1) * 4 - 180) * np.pi / 180\n        )\n\n        return yaw_radian, pitch_radian\n\n    def load_L2CS_model(\n        self,\n        file_path=f\"{MODEL_CACHE_DIR}/gaze/L2CS/L2CSNet_gaze360_resnet50_90bins.pkl\",\n    ):\n        super().load_state_dict(torch.load(file_path, map_location=self.device))\n        super().to(self.device)\n\n    def saveas_ONNX_model(\n        self,\n        file_path=f\"{MODEL_CACHE_DIR}/gaze/L2CS/L2CSNet_gaze360_resnet50_90bins.onnx\",\n    ):\n        dummy_input = torch.randn(1, 3, 448, 448)\n        dynamic_axes = {\n            \"input\": {0: \"batch_size\"},\n            \"output_yaw\": {0: \"batch_size\"},\n            \"output_pitch\": {0: \"batch_size\"},\n        }\n        torch.onnx.export(\n            self,\n            dummy_input,\n            file_path,\n            input_names=[\"input\"],\n            output_names=[\"output_yaw\", \"output_pitch\"],\n            dynamic_axes=dynamic_axes,\n            verbose=False,\n        )\n</code></pre>"},{"location":"reference/inference/models/gaze/l2cs/","title":"L2cs","text":""},{"location":"reference/inference/models/gaze/l2cs/#inference.models.gaze.l2cs.L2CS","title":"<code>L2CS</code>","text":"<p>               Bases: <code>Module</code></p> <p>L2CS Gaze Detection Model.</p> <p>This class is responsible for performing gaze detection using the L2CS-Net model. Ref: https://github.com/Ahmednull/L2CS-Net</p> <p>Methods:</p> Name Description <code>forward</code> <p>Performs inference on the given image.</p> Source code in <code>inference/models/gaze/l2cs.py</code> <pre><code>class L2CS(nn.Module):\n    \"\"\"L2CS Gaze Detection Model.\n\n    This class is responsible for performing gaze detection using the L2CS-Net model.\n    Ref: https://github.com/Ahmednull/L2CS-Net\n\n    Methods:\n        forward: Performs inference on the given image.\n    \"\"\"\n\n    def __init__(self, block, layers, num_bins):\n        self.inplanes = 64\n        super(L2CS, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.fc_yaw_gaze = nn.Linear(512 * block.expansion, num_bins)\n        self.fc_pitch_gaze = nn.Linear(512 * block.expansion, num_bins)\n\n        # Vestigial layer from previous experiments\n        self.fc_finetune = nn.Linear(512 * block.expansion + 3, 3)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n\n        # gaze\n        pre_yaw_gaze = self.fc_yaw_gaze(x)\n        pre_pitch_gaze = self.fc_pitch_gaze(x)\n        return pre_yaw_gaze, pre_pitch_gaze\n</code></pre>"},{"location":"reference/inference/models/grounding_dino/grounding_dino/","title":"Grounding dino","text":""},{"location":"reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO","title":"<code>GroundingDINO</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>GroundingDINO class for zero-shot object detection.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The GroundingDINO model.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>class GroundingDINO(RoboflowCoreModel):\n    \"\"\"GroundingDINO class for zero-shot object detection.\n\n    Attributes:\n        model: The GroundingDINO model.\n    \"\"\"\n\n    def __init__(\n        self, *args, model_id=\"grounding_dino/groundingdino_swint_ogc\", **kwargs\n    ):\n        \"\"\"Initializes the GroundingDINO model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n        GROUNDING_DINO_CACHE_DIR = os.path.join(MODEL_CACHE_DIR, model_id)\n\n        GROUNDING_DINO_CONFIG_PATH = os.path.join(\n            GROUNDING_DINO_CACHE_DIR, \"GroundingDINO_SwinT_OGC.py\"\n        )\n\n        if not os.path.exists(GROUNDING_DINO_CACHE_DIR):\n            os.makedirs(GROUNDING_DINO_CACHE_DIR)\n\n        if not os.path.exists(GROUNDING_DINO_CONFIG_PATH):\n            url = \"https://raw.githubusercontent.com/roboflow/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n            urllib.request.urlretrieve(url, GROUNDING_DINO_CONFIG_PATH)\n\n        self.model = Model(\n            model_config_path=GROUNDING_DINO_CONFIG_PATH,\n            model_checkpoint_path=os.path.join(\n                GROUNDING_DINO_CACHE_DIR, \"groundingdino_swint_ogc.pth\"\n            ),\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        )\n        self.task_type = \"object-detection\"\n\n    def preproc_image(self, image: Any):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_bgr(image)\n        return np_image\n\n    def infer_from_request(\n        self,\n        request: GroundingDINOInferenceRequest,\n    ) -&gt; ObjectDetectionInferenceResponse:\n        \"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        \"\"\"\n        result = self.infer(**request.dict())\n        return result\n\n    def infer(\n        self,\n        image: InferenceRequestImage,\n        text: List[str] = None,\n        class_filter: list = None,\n        box_threshold=0.5,\n        text_threshold=0.5,\n        class_agnostic_nms=CLASS_AGNOSTIC_NMS,\n        **kwargs\n    ):\n        \"\"\"\n        Run inference on a provided image.\n            - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n        Args:\n            request (CVInferenceRequest): The inference request.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            GroundingDINOInferenceRequest: The inference response.\n        \"\"\"\n        t1 = perf_counter()\n        image = self.preproc_image(image)\n        img_dims = image.shape\n\n        detections = self.model.predict_with_classes(\n            image=image,\n            classes=text,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n        )\n\n        self.class_names = text\n\n        if class_agnostic_nms:\n            detections = detections.with_nms(class_agnostic=True)\n        else:\n            detections = detections.with_nms()\n\n        xywh_bboxes = [xyxy_to_xywh(detection) for detection in detections.xyxy]\n\n        t2 = perf_counter() - t1\n\n        responses = ObjectDetectionInferenceResponse(\n            predictions=[\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": xywh_bboxes[i][0],\n                        \"y\": xywh_bboxes[i][1],\n                        \"width\": xywh_bboxes[i][2],\n                        \"height\": xywh_bboxes[i][3],\n                        \"confidence\": detections.confidence[i],\n                        \"class\": self.class_names[int(detections.class_id[i])],\n                        \"class_id\": int(detections.class_id[i]),\n                    }\n                )\n                for i, pred in enumerate(detections.xyxy)\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n                and detections.class_id[i] is not None\n            ],\n            image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n            time=t2,\n        )\n        return responses\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"groundingdino_swint_ogc.pth\"]\n</code></pre>"},{"location":"reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.__init__","title":"<code>__init__(*args, model_id='grounding_dino/groundingdino_swint_ogc', **kwargs)</code>","text":"<p>Initializes the GroundingDINO model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def __init__(\n    self, *args, model_id=\"grounding_dino/groundingdino_swint_ogc\", **kwargs\n):\n    \"\"\"Initializes the GroundingDINO model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n\n    GROUNDING_DINO_CACHE_DIR = os.path.join(MODEL_CACHE_DIR, model_id)\n\n    GROUNDING_DINO_CONFIG_PATH = os.path.join(\n        GROUNDING_DINO_CACHE_DIR, \"GroundingDINO_SwinT_OGC.py\"\n    )\n\n    if not os.path.exists(GROUNDING_DINO_CACHE_DIR):\n        os.makedirs(GROUNDING_DINO_CACHE_DIR)\n\n    if not os.path.exists(GROUNDING_DINO_CONFIG_PATH):\n        url = \"https://raw.githubusercontent.com/roboflow/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n        urllib.request.urlretrieve(url, GROUNDING_DINO_CONFIG_PATH)\n\n    self.model = Model(\n        model_config_path=GROUNDING_DINO_CONFIG_PATH,\n        model_checkpoint_path=os.path.join(\n            GROUNDING_DINO_CACHE_DIR, \"groundingdino_swint_ogc.pth\"\n        ),\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    )\n    self.task_type = \"object-detection\"\n</code></pre>"},{"location":"reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"groundingdino_swint_ogc.pth\"]\n</code></pre>"},{"location":"reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.infer","title":"<code>infer(image, text=None, class_filter=None, box_threshold=0.5, text_threshold=0.5, class_agnostic_nms=CLASS_AGNOSTIC_NMS, **kwargs)</code>","text":"<p>Run inference on a provided image.     - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>CVInferenceRequest</code> <p>The inference request.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GroundingDINOInferenceRequest</code> <p>The inference response.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def infer(\n    self,\n    image: InferenceRequestImage,\n    text: List[str] = None,\n    class_filter: list = None,\n    box_threshold=0.5,\n    text_threshold=0.5,\n    class_agnostic_nms=CLASS_AGNOSTIC_NMS,\n    **kwargs\n):\n    \"\"\"\n    Run inference on a provided image.\n        - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n    Args:\n        request (CVInferenceRequest): The inference request.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        GroundingDINOInferenceRequest: The inference response.\n    \"\"\"\n    t1 = perf_counter()\n    image = self.preproc_image(image)\n    img_dims = image.shape\n\n    detections = self.model.predict_with_classes(\n        image=image,\n        classes=text,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n    )\n\n    self.class_names = text\n\n    if class_agnostic_nms:\n        detections = detections.with_nms(class_agnostic=True)\n    else:\n        detections = detections.with_nms()\n\n    xywh_bboxes = [xyxy_to_xywh(detection) for detection in detections.xyxy]\n\n    t2 = perf_counter() - t1\n\n    responses = ObjectDetectionInferenceResponse(\n        predictions=[\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": xywh_bboxes[i][0],\n                    \"y\": xywh_bboxes[i][1],\n                    \"width\": xywh_bboxes[i][2],\n                    \"height\": xywh_bboxes[i][3],\n                    \"confidence\": detections.confidence[i],\n                    \"class\": self.class_names[int(detections.class_id[i])],\n                    \"class_id\": int(detections.class_id[i]),\n                }\n            )\n            for i, pred in enumerate(detections.xyxy)\n            if not class_filter\n            or self.class_names[int(pred[6])] in class_filter\n            and detections.class_id[i] is not None\n        ],\n        image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n        time=t2,\n    )\n    return responses\n</code></pre>"},{"location":"reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def infer_from_request(\n    self,\n    request: GroundingDINOInferenceRequest,\n) -&gt; ObjectDetectionInferenceResponse:\n    \"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    \"\"\"\n    result = self.infer(**request.dict())\n    return result\n</code></pre>"},{"location":"reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def preproc_image(self, image: Any):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_bgr(image)\n    return np_image\n</code></pre>"},{"location":"reference/inference/models/owlv2/owlv2/","title":"Owlv2","text":""},{"location":"reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.OwlV2","title":"<code>OwlV2</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>class OwlV2(RoboflowInferenceModel):\n    task_type = \"object-detection\"\n    box_format = \"xywh\"\n\n    def __init__(self, model_id=f\"owlv2/{OWLV2_VERSION_ID}\", *args, **kwargs):\n        super().__init__(model_id, *args, **kwargs)\n        # TODO: owlv2 makes use of version_id - version_id is being dropped so this class needs to be refactored\n\n        self.owlv2_lock = RLock()\n\n        if self.version_id is None:\n            owlv2_model_id_chunks = model_id.split(\"/\")\n            if len(owlv2_model_id_chunks) != 2:\n                raise InvalidModelIDError(\"Model ID: `%s` is invalid.\", model_id)\n            self.dataset_id = owlv2_model_id_chunks[0]\n            self.version_id = owlv2_model_id_chunks[1]\n        hf_id = os.path.join(\"google\", self.version_id)\n        processor = Owlv2Processor.from_pretrained(hf_id)\n        self.image_size = tuple(processor.image_processor.size.values())\n        self.image_mean = torch.tensor(\n            processor.image_processor.image_mean, device=DEVICE\n        ).view(1, 3, 1, 1)\n        self.image_std = torch.tensor(\n            processor.image_processor.image_std, device=DEVICE\n        ).view(1, 3, 1, 1)\n        self.model = Owlv2Singleton(hf_id).model\n        self.reset_cache()\n\n    def reset_cache(self):\n        # each entry should be on the order of 300*4KB, so 1000 is 400MB of CUDA memory\n        self.image_embed_cache = LimitedSizeDict(size_limit=OWLV2_IMAGE_CACHE_SIZE)\n        # no need for limit here, as we're only storing on CPU\n        self.cpu_image_embed_cache = LimitedSizeDict(\n            size_limit=CPU_IMAGE_EMBED_CACHE_SIZE\n        )\n        # each entry should be on the order of 10 bytes, so 1000 is 10KB\n        self.image_size_cache = LimitedSizeDict(size_limit=OWLV2_IMAGE_CACHE_SIZE)\n        # entry size will vary depending on the number of samples, but 10 should be safe\n        self.class_embeddings_cache = LimitedSizeDict(size_limit=OWLV2_MODEL_CACHE_SIZE)\n\n    def draw_predictions(\n        self,\n        inference_request,\n        inference_response,\n    ) -&gt; bytes:\n        \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n        Args:\n            inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n            inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n        Returns:\n            str: A base64 encoded image string\n        \"\"\"\n        all_class_names = [x.class_name for x in inference_response.predictions]\n        all_class_names = sorted(list(set(all_class_names)))\n\n        return draw_detection_predictions(\n            inference_request=inference_request,\n            inference_response=inference_response,\n            colors={\n                class_name: DEFAULT_COLOR_PALETTE[i % len(DEFAULT_COLOR_PALETTE)]\n                for (i, class_name) in enumerate(all_class_names)\n            },\n        )\n\n    def download_weights(self) -&gt; None:\n        # Download from huggingface\n        pass\n\n    def get_image_embeds(self, image_hash: Hash) -&gt; Optional[torch.Tensor]:\n        if image_hash in self.image_embed_cache:\n            return self.image_embed_cache[image_hash]\n        elif image_hash in self.cpu_image_embed_cache:\n            tensors = self.cpu_image_embed_cache[image_hash]\n            tensors = tuple(t.to(DEVICE) for t in tensors)\n            return tensors\n        else:\n            return None\n\n    def compute_image_size(\n        self, image: Union[np.ndarray, LazyImageRetrievalWrapper]\n    ) -&gt; Tuple[int, int]:\n        if isinstance(image, LazyImageRetrievalWrapper):\n            if (image_size := self.image_size_cache.get(image.image_hash)) is None:\n                np_img = image.image_as_numpy\n                image_size = np_img.shape[:2][::-1]\n                self.image_size_cache[image.image_hash] = image_size\n            return image_size\n        else:\n            return image.shape[:2][::-1]\n\n    @torch.no_grad()\n    def embed_image(self, image: Union[np.ndarray, LazyImageRetrievalWrapper]) -&gt; Hash:\n        if isinstance(image, LazyImageRetrievalWrapper):\n            image_hash = image.image_hash\n        else:\n            image_hash = hash_function(image.tobytes())\n\n        if (image_embeds := self.get_image_embeds(image_hash)) is not None:\n            return image_hash\n\n        np_image = (\n            image.image_as_numpy\n            if isinstance(image, LazyImageRetrievalWrapper)\n            else image\n        )\n        pixel_values = preprocess_image(\n            np_image, self.image_size, self.image_mean, self.image_std\n        )\n\n        # torch 2.4 lets you use \"cuda:0\" as device_type\n        # but this crashes in 2.3\n        # so we parse DEVICE as a string to make it work in both 2.3 and 2.4\n        # as we don't know a priori our torch version\n        device_str = \"cuda\" if str(DEVICE).startswith(\"cuda\") else \"cpu\"\n        # we disable autocast on CPU for stability, although it's possible using bfloat16 would work\n        with torch.autocast(\n            device_type=device_str, dtype=torch.float16, enabled=device_str == \"cuda\"\n        ):\n            image_embeds, _ = self.model.image_embedder(pixel_values=pixel_values)\n            batch_size, h, w, dim = image_embeds.shape\n            image_features = image_embeds.reshape(batch_size, h * w, dim)\n            objectness = self.model.objectness_predictor(image_features)\n            boxes = self.model.box_predictor(image_features, feature_map=image_embeds)\n        image_class_embeds = self.model.class_head.dense0(image_features)\n        image_class_embeds /= (\n            torch.linalg.norm(image_class_embeds, ord=2, dim=-1, keepdim=True) + 1e-6\n        )\n        logit_shift = self.model.class_head.logit_shift(image_features)\n        logit_scale = (\n            self.model.class_head.elu(self.model.class_head.logit_scale(image_features))\n            + 1\n        )\n        objectness = objectness.sigmoid()\n\n        objectness, boxes, image_class_embeds, logit_shift, logit_scale = (\n            filter_tensors_by_objectness(\n                objectness, boxes, image_class_embeds, logit_shift, logit_scale\n            )\n        )\n\n        self.image_embed_cache[image_hash] = (\n            objectness,\n            boxes,\n            image_class_embeds,\n            logit_shift,\n            logit_scale,\n        )\n\n        # Explicitly delete temporary tensors to free memory.\n        del pixel_values, np_image, image_features, image_embeds\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        if isinstance(image, LazyImageRetrievalWrapper):\n            image.unload_numpy_image()  # Clears both _image_as_numpy and image if needed.\n\n        return image_hash\n\n    def get_query_embedding(\n        self, query_spec: QuerySpecType, iou_threshold: float\n    ) -&gt; torch.Tensor:\n        # NOTE: for now we're handling each image seperately\n        query_embeds = []\n        for image_hash, query_boxes in query_spec.items():\n            image_embeds = self.get_image_embeds(image_hash)\n            if image_embeds is None:\n                raise KeyError(\"We didn't embed the image first!\")\n            _objectness, image_boxes, image_class_embeds, _, _ = image_embeds\n\n            query_boxes_tensor = torch.tensor(\n                query_boxes, dtype=image_boxes.dtype, device=image_boxes.device\n            )\n            if image_boxes.numel() == 0 or query_boxes_tensor.numel() == 0:\n                continue\n            iou, _ = box_iou(\n                to_corners(image_boxes), to_corners(query_boxes_tensor)\n            )  # 3000, k\n            ious, indices = torch.max(iou, dim=0)\n            # filter for only iou &gt; 0.4\n            iou_mask = ious &gt; iou_threshold\n            indices = indices[iou_mask]\n            if not indices.numel() &gt; 0:\n                continue\n\n            embeds = image_class_embeds[indices]\n            query_embeds.append(embeds)\n        if not query_embeds:\n            return None\n        query = torch.cat(query_embeds, dim=0)\n        return query\n\n    def infer_from_embed(\n        self,\n        image_hash: Hash,\n        query_embeddings: Dict[str, PosNegDictType],\n        confidence: float,\n        iou_threshold: float,\n        max_detections: int = MAX_DETECTIONS,\n    ) -&gt; List[Dict]:\n        image_embeds = self.get_image_embeds(image_hash)\n        if image_embeds is None:\n            raise KeyError(\"We didn't embed the image first!\")\n        _, image_boxes, image_class_embeds, _, _ = image_embeds\n        class_map, class_names = make_class_map(query_embeddings)\n        all_predicted_boxes, all_predicted_classes, all_predicted_scores = [], [], []\n        for class_name, pos_neg_embedding_dict in query_embeddings.items():\n            boxes, classes, scores = get_class_preds_from_embeds(\n                pos_neg_embedding_dict,\n                image_class_embeds,\n                confidence,\n                image_boxes,\n                class_map,\n                class_name,\n                iou_threshold,\n            )\n\n            all_predicted_boxes.append(boxes)\n            all_predicted_classes.append(classes)\n            all_predicted_scores.append(scores)\n\n        if not all_predicted_boxes:\n            return []\n\n        all_predicted_boxes = torch.cat(all_predicted_boxes, dim=0)\n        all_predicted_classes = torch.cat(all_predicted_classes, dim=0)\n        all_predicted_scores = torch.cat(all_predicted_scores, dim=0)\n\n        # run nms on all predictions\n        survival_indices = torchvision.ops.nms(\n            to_corners(all_predicted_boxes), all_predicted_scores, iou_threshold\n        )\n        all_predicted_boxes = all_predicted_boxes[survival_indices]\n        all_predicted_classes = all_predicted_classes[survival_indices]\n        all_predicted_scores = all_predicted_scores[survival_indices]\n\n        if len(all_predicted_boxes) &gt; max_detections:\n            all_predicted_boxes = all_predicted_boxes[:max_detections]\n            all_predicted_classes = all_predicted_classes[:max_detections]\n            all_predicted_scores = all_predicted_scores[:max_detections]\n\n        # move tensors to numpy before returning\n        all_predicted_boxes = all_predicted_boxes.cpu().numpy()\n        all_predicted_classes = all_predicted_classes.cpu().numpy()\n        all_predicted_scores = all_predicted_scores.cpu().numpy()\n\n        return [\n            {\n                \"class_name\": class_names[int(c)],\n                \"x\": float(x),\n                \"y\": float(y),\n                \"w\": float(w),\n                \"h\": float(h),\n                \"confidence\": float(score),\n            }\n            for c, (x, y, w, h), score in zip(\n                all_predicted_classes, all_predicted_boxes, all_predicted_scores\n            )\n        ]\n\n    def infer(\n        self,\n        image: Any,\n        training_data: Dict,\n        confidence: float = 0.99,\n        iou_threshold: float = 0.3,\n        max_detections: int = MAX_DETECTIONS,\n        **kwargs,\n    ):\n        with self.owlv2_lock:\n            class_embeddings_dict = self.make_class_embeddings_dict(\n                training_data, iou_threshold\n            )\n            return self.infer_from_embedding_dict(\n                image,\n                class_embeddings_dict,\n                confidence,\n                iou_threshold,\n                max_detections=max_detections,\n            )\n\n    def infer_from_embedding_dict(\n        self,\n        image: Any,\n        class_embeddings_dict: Dict[str, PosNegDictType],\n        confidence: float,\n        iou_threshold: float,\n        max_detections: int = MAX_DETECTIONS,\n        **kwargs,\n    ):\n        with self.owlv2_lock:\n            if not isinstance(image, list):\n                images = [image]\n            else:\n                images = image\n\n            images = [LazyImageRetrievalWrapper(image) for image in images]\n\n            results = []\n            image_sizes = []\n            for image_wrapper in images:\n                # happy path here is that both image size and image embeddings are cached\n                # in which case we avoid loading the image at all\n                image_size = self.compute_image_size(image_wrapper)\n                image_sizes.append(image_size)\n                image_hash = self.embed_image(image_wrapper)\n                image_wrapper.unload_numpy_image()\n                result = self.infer_from_embed(\n                    image_hash,\n                    class_embeddings_dict,\n                    confidence,\n                    iou_threshold,\n                    max_detections=max_detections,\n                )\n                results.append(result)\n            return self.make_response(\n                results, image_sizes, sorted(list(class_embeddings_dict.keys()))\n            )\n\n    def make_class_embeddings_dict(\n        self,\n        training_data: List[Any],\n        iou_threshold: float,\n        return_image_embeds: bool = False,\n    ) -&gt; Dict[str, PosNegDictType]:\n\n        wrapped_training_data = [\n            {\n                \"image\": LazyImageRetrievalWrapper(train_image[\"image\"]),\n                \"boxes\": train_image[\"boxes\"],\n            }\n            for train_image in training_data\n        ]\n\n        wrapped_training_data_hash = hash_wrapped_training_data(wrapped_training_data)\n\n        if (\n            class_embeddings_dict := self.class_embeddings_cache.get(\n                wrapped_training_data_hash\n            )\n        ) is not None:\n            if return_image_embeds:\n                # Return a dummy empty dict as the second value\n                # or extract it from CPU cache if available\n                return_image_embeds_dict = {}\n                for image_hash in self.cpu_image_embed_cache:\n                    return_image_embeds_dict[image_hash] = self.cpu_image_embed_cache[\n                        image_hash\n                    ]\n                return class_embeddings_dict, return_image_embeds_dict\n            else:\n                return class_embeddings_dict\n\n        class_embeddings_dict = defaultdict(lambda: {\"positive\": [], \"negative\": []})\n\n        bool_to_literal = {True: \"positive\", False: \"negative\"}\n        return_image_embeds_dict = dict()\n\n        for train_image in wrapped_training_data:\n            image_size = self.compute_image_size(train_image[\"image\"])\n            image_hash = self.embed_image(train_image[\"image\"])\n            if return_image_embeds:\n                if (image_embeds := self.get_image_embeds(image_hash)) is None:\n                    raise KeyError(\"We didn't embed the image first!\")\n                return_image_embeds_dict[image_hash] = tuple(\n                    t.to(\"cpu\") for t in image_embeds\n                )\n            # grab and normalize box prompts for this image\n            boxes = train_image[\"boxes\"]\n            coords = [[box[\"x\"], box[\"y\"], box[\"w\"], box[\"h\"]] for box in boxes]\n            coords = [tuple([c / max(image_size) for c in coord]) for coord in coords]\n            classes = [box[\"cls\"] for box in boxes]\n            is_positive = [not box[\"negative\"] for box in boxes]\n            query_spec = {image_hash: coords}\n            # compute the embeddings for the box prompts\n            embeddings = self.get_query_embedding(query_spec, iou_threshold)\n\n            del train_image\n\n            if embeddings is None:\n                continue\n\n            for embedding, class_name, is_pos in zip(embeddings, classes, is_positive):\n                class_embeddings_dict[class_name][bool_to_literal[is_pos]].append(\n                    embedding\n                )\n\n        gc.collect()\n        # Convert lists of embeddings to tensors.\n        class_embeddings_dict = {\n            k: {\n                \"positive\": torch.stack(v[\"positive\"]) if v[\"positive\"] else None,\n                \"negative\": torch.stack(v[\"negative\"]) if v[\"negative\"] else None,\n            }\n            for k, v in class_embeddings_dict.items()\n        }\n\n        self.class_embeddings_cache[wrapped_training_data_hash] = class_embeddings_dict\n        if return_image_embeds:\n            return class_embeddings_dict, return_image_embeds_dict\n\n        return class_embeddings_dict\n\n    def make_response(self, predictions, image_sizes, class_names):\n        responses = [\n            ObjectDetectionInferenceResponse(\n                predictions=[\n                    ObjectDetectionPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": pred[\"x\"] * max(image_sizes[ind]),\n                            \"y\": pred[\"y\"] * max(image_sizes[ind]),\n                            \"width\": pred[\"w\"] * max(image_sizes[ind]),\n                            \"height\": pred[\"h\"] * max(image_sizes[ind]),\n                            \"confidence\": pred[\"confidence\"],\n                            \"class\": pred[\"class_name\"],\n                            \"class_id\": class_names.index(pred[\"class_name\"]),\n                        }\n                    )\n                    for pred in batch_predictions\n                ],\n                image=InferenceResponseImage(\n                    width=image_sizes[ind][0], height=image_sizes[ind][1]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n</code></pre>"},{"location":"reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.OwlV2.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw predictions from an inference response onto the original image provided by an inference request</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>ObjectDetectionInferenceRequest</code> <p>The inference request containing the image on which to draw predictions</p> required <code>inference_response</code> <code>ObjectDetectionInferenceResponse</code> <p>The inference response containing predictions to be drawn</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>bytes</code> <p>A base64 encoded image string</p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>def draw_predictions(\n    self,\n    inference_request,\n    inference_response,\n) -&gt; bytes:\n    \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n    Args:\n        inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n        inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n    Returns:\n        str: A base64 encoded image string\n    \"\"\"\n    all_class_names = [x.class_name for x in inference_response.predictions]\n    all_class_names = sorted(list(set(all_class_names)))\n\n    return draw_detection_predictions(\n        inference_request=inference_request,\n        inference_response=inference_response,\n        colors={\n            class_name: DEFAULT_COLOR_PALETTE[i % len(DEFAULT_COLOR_PALETTE)]\n            for (i, class_name) in enumerate(all_class_names)\n        },\n    )\n</code></pre>"},{"location":"reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.SerializedOwlV2","title":"<code>SerializedOwlV2</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>class SerializedOwlV2(RoboflowInferenceModel):\n    task_type = \"object-detection\"\n    box_format = \"xywh\"\n\n    # Cache of OwlV2 instances to avoid creating new ones for each serialize_training_data call\n    # This improves performance by reusing model instances across serialization operations\n    _base_owlv2_instances = {}\n\n    @classmethod\n    def get_or_create_owlv2_instance(cls, roboflow_id: str) -&gt; OwlV2:\n        \"\"\"Get an existing OwlV2 instance from cache or create a new one if it doesn't exist.\n\n        Args:\n            roboflow_id: The model ID for the OwlV2 model\n\n        Returns:\n            An OwlV2 instance\n        \"\"\"\n        if roboflow_id in cls._base_owlv2_instances:\n            return cls._base_owlv2_instances[roboflow_id]\n        else:\n            owlv2 = OwlV2(model_id=roboflow_id)\n            cls._base_owlv2_instances[roboflow_id] = owlv2\n            return owlv2\n\n    @classmethod\n    def serialize_training_data(\n        cls,\n        training_data: List[Any],\n        hf_id: str = f\"google/{OWLV2_VERSION_ID}\",\n        iou_threshold: float = 0.3,\n        save_dir: str = os.path.join(MODEL_CACHE_DIR, \"owl-v2-serialized-data\"),\n        previous_embeddings_file: str = None,\n    ):\n        roboflow_id = hf_id.replace(\"google/\", \"owlv2/\")\n\n        owlv2 = cls.get_or_create_owlv2_instance(roboflow_id)\n\n        if previous_embeddings_file is not None:\n            if DEVICE == \"cpu\":\n                model_data = torch.load(\n                    previous_embeddings_file, map_location=\"cpu\", weights_only=False\n                )\n            else:\n                model_data = torch.load(previous_embeddings_file, weights_only=False)\n\n            train_data_dict = model_data[\"train_data_dict\"]\n            owlv2.cpu_image_embed_cache = model_data[\"image_embeds\"]\n\n        train_data_dict, image_embeds = owlv2.make_class_embeddings_dict(\n            training_data, iou_threshold, return_image_embeds=True\n        )\n        return cls.save_model(\n            hf_id, roboflow_id, train_data_dict, image_embeds, save_dir\n        )\n\n    @classmethod\n    def save_model(\n        cls,\n        hf_id: str,\n        roboflow_id: str,\n        train_data_dict: Dict,\n        image_embeds: Dict,\n        save_dir: str,\n    ):\n        train_data_dict = {\n            \"huggingface_id\": hf_id,\n            \"train_data_dict\": train_data_dict,\n            \"class_names\": list(train_data_dict.keys()),\n            \"roboflow_id\": roboflow_id,\n            \"image_embeds\": image_embeds,\n        }\n        train_data_path = os.path.join(save_dir, cls.weights_file_path)\n        os.makedirs(save_dir, exist_ok=True)\n        torch.save(train_data_dict, train_data_path)\n        return train_data_path\n\n    def infer_from_request(\n        self,\n        request: ObjectDetectionInferenceRequest,\n    ) -&gt; Union[\n        List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse\n    ]:\n        return super().infer_from_request(request)\n\n    def __init__(self, model_id, *args, **kwargs):\n        super().__init__(model_id, *args, **kwargs)\n        self.get_model_artifacts(**kwargs)\n\n    def get_infer_bucket_file_list(self):\n        return []\n\n    def download_model_artefacts_from_s3(self):\n        raise NotImplementedError(\"Owlv2 not currently supported on hosted inference\")\n\n    def download_model_artifacts_from_roboflow_api(\n        self,\n        countinference: Optional[bool] = None,\n        service_secret: Optional[str] = None,\n        **kwargs,\n    ):\n        logger.info(\"Downloading OWLv2 model artifacts\")\n\n        # Use the same lock file pattern as in clear_cache\n        lock_dir = MODEL_CACHE_DIR + \"/_file_locks\"  # Dedicated lock directory\n        os.makedirs(lock_dir, exist_ok=True)  # Ensure lock directory exists.\n        lock_file = os.path.join(lock_dir, f\"{os.path.basename(self.cache_dir)}.lock\")\n        try:\n            lock = FileLock(lock_file, timeout=120)  # 120 second timeout for downloads\n            with lock:\n                if self.version_id is not None:\n                    api_data = get_roboflow_model_data(\n                        api_key=self.api_key,\n                        model_id=self.endpoint,\n                        endpoint_type=ModelEndpointType.OWLV2,\n                        device_id=self.device_id,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    api_data = api_data[\"owlv2\"]\n                    if \"model\" not in api_data:\n                        raise ModelArtefactError(\n                            \"Could not find `model` key in roboflow API model description response.\"\n                        )\n                    logger.info(\"Downloading OWLv2 model weights for %s\", self.endpoint)\n                    model_weights_response = get_from_url(\n                        api_data[\"model\"], json_response=False\n                    )\n                else:\n                    logger.info(\"Getting OWLv2 model data for\")\n                    api_data = get_roboflow_instant_model_data(\n                        api_key=self.api_key,\n                        model_id=self.endpoint,\n                        countinference=countinference,\n                        service_secret=service_secret,\n                    )\n                    if (\n                        \"modelFiles\" not in api_data\n                        or \"owlv2\" not in api_data[\"modelFiles\"]\n                        or \"model\" not in api_data[\"modelFiles\"][\"owlv2\"]\n                    ):\n                        raise ModelArtefactError(\n                            \"Could not find `modelFiles` key or `modelFiles`.`owlv2` or `modelFiles`.`owlv2`.`model` key in roboflow API model description response.\"\n                        )\n                    logger.info(\"Downloading OWLv2 model weights for %s\", self.endpoint)\n                    model_weights_response = get_from_url(\n                        api_data[\"modelFiles\"][\"owlv2\"][\"model\"], json_response=False\n                    )\n                save_bytes_in_cache(\n                    content=model_weights_response.content,\n                    file=self.weights_file,\n                    model_id=self.endpoint,\n                )\n                logger.info(\"OWLv2 model weights saved to cache\")\n        except Exception as e:\n            logger.error(\"Error downloading OWLv2 model artifacts: %s\", e)\n            raise\n        finally:\n            try:\n                if os.path.exists(lock_file):\n                    os.unlink(lock_file)  # Clean up lock file\n            except OSError:\n                pass  # Best effort cleanup\n\n    def load_model_artifacts_from_cache(self):\n        if DEVICE == \"cpu\":\n            self.model_data = torch.load(\n                self.cache_file(self.weights_file),\n                map_location=\"cpu\",\n                weights_only=False,\n            )\n        else:\n            self.model_data = torch.load(\n                self.cache_file(self.weights_file), weights_only=False\n            )\n        self.class_names = self.model_data[\"class_names\"]\n        self.train_data_dict = self.model_data[\"train_data_dict\"]\n        self.huggingface_id = self.model_data[\"huggingface_id\"]\n        self.roboflow_id = self.model_data[\"roboflow_id\"]\n        # Use the same cached OwlV2 instance mechanism to avoid creating duplicates\n        self.owlv2 = self.__class__.get_or_create_owlv2_instance(self.roboflow_id)\n        self.owlv2.cpu_image_embed_cache = self.model_data[\"image_embeds\"]\n\n    weights_file_path = \"weights.pt\"\n\n    @property\n    def weights_file(self):\n        return self.weights_file_path\n\n    def infer(\n        self,\n        image,\n        confidence: float = 0.99,\n        iou_threshold: float = 0.3,\n        max_detections: int = MAX_DETECTIONS,\n        **kwargs,\n    ):\n        logger.info(\"Inferring OWLv2 model\")\n        result = self.owlv2.infer_from_embedding_dict(\n            image,\n            self.train_data_dict,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_detections=max_detections,\n            **kwargs,\n        )\n        logger.info(\"OWLv2 model inference complete\")\n        return result\n\n    def draw_predictions(\n        self,\n        inference_request: ObjectDetectionInferenceRequest,\n        inference_response: ObjectDetectionInferenceResponse,\n    ):\n        return self.owlv2.draw_predictions(\n            inference_request,\n            inference_response,\n        )\n\n    def save_small_model_without_image_embeds(\n        self, save_dir: str = os.path.join(MODEL_CACHE_DIR, \"owl-v2-serialized-data\")\n    ):\n        self.owlv2.cpu_image_embed_cache = LimitedSizeDict(\n            size_limit=CPU_IMAGE_EMBED_CACHE_SIZE\n        )\n        return self.save_model(\n            self.huggingface_id,\n            self.roboflow_id,\n            self.train_data_dict,\n            self.owlv2.cpu_image_embed_cache,\n            save_dir,\n        )\n</code></pre>"},{"location":"reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.SerializedOwlV2.get_or_create_owlv2_instance","title":"<code>get_or_create_owlv2_instance(roboflow_id)</code>  <code>classmethod</code>","text":"<p>Get an existing OwlV2 instance from cache or create a new one if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>roboflow_id</code> <code>str</code> <p>The model ID for the OwlV2 model</p> required <p>Returns:</p> Type Description <code>OwlV2</code> <p>An OwlV2 instance</p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>@classmethod\ndef get_or_create_owlv2_instance(cls, roboflow_id: str) -&gt; OwlV2:\n    \"\"\"Get an existing OwlV2 instance from cache or create a new one if it doesn't exist.\n\n    Args:\n        roboflow_id: The model ID for the OwlV2 model\n\n    Returns:\n        An OwlV2 instance\n    \"\"\"\n    if roboflow_id in cls._base_owlv2_instances:\n        return cls._base_owlv2_instances[roboflow_id]\n    else:\n        owlv2 = OwlV2(model_id=roboflow_id)\n        cls._base_owlv2_instances[roboflow_id] = owlv2\n        return owlv2\n</code></pre>"},{"location":"reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.preprocess_image","title":"<code>preprocess_image(np_image, image_size, image_mean, image_std)</code>","text":"<p>Preprocess an image for OWLv2 by resizing, normalizing, and padding it. This is much faster than using the Owlv2Processor directly, as we ensure we use GPU if available.</p> <p>Parameters:</p> Name Type Description Default <code>np_image</code> <code>ndarray</code> <p>The image to preprocess, with shape (H, W, 3)</p> required <code>image_size</code> <code>tuple[int, int]</code> <p>The target size of the image</p> required <code>image_mean</code> <code>Tensor</code> <p>The mean of the image, on DEVICE, with shape (1, 3, 1, 1)</p> required <code>image_std</code> <code>Tensor</code> <p>The standard deviation of the image, on DEVICE, with shape (1, 3, 1, 1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The preprocessed image, on DEVICE, with shape (1, 3, H, W)</p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>def preprocess_image(\n    np_image: np.ndarray,\n    image_size: Tuple[int, int],\n    image_mean: torch.Tensor,\n    image_std: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Preprocess an image for OWLv2 by resizing, normalizing, and padding it.\n    This is much faster than using the Owlv2Processor directly, as we ensure we use GPU if available.\n\n    Args:\n        np_image (np.ndarray): The image to preprocess, with shape (H, W, 3)\n        image_size (tuple[int, int]): The target size of the image\n        image_mean (torch.Tensor): The mean of the image, on DEVICE, with shape (1, 3, 1, 1)\n        image_std (torch.Tensor): The standard deviation of the image, on DEVICE, with shape (1, 3, 1, 1)\n\n    Returns:\n        torch.Tensor: The preprocessed image, on DEVICE, with shape (1, 3, H, W)\n    \"\"\"\n    current_size = np_image.shape[:2]\n\n    r = min(image_size[0] / current_size[0], image_size[1] / current_size[1])\n    target_size = (int(r * current_size[0]), int(r * current_size[1]))\n\n    torch_image = (\n        torch.tensor(np_image)\n        .permute(2, 0, 1)\n        .unsqueeze(0)\n        .to(DEVICE)\n        .to(dtype=torch.float32)\n        / 255.0\n    )\n    torch_image = F.interpolate(\n        torch_image, size=target_size, mode=\"bilinear\", align_corners=False\n    )\n\n    padded_image_tensor = torch.ones((1, 3, *image_size), device=DEVICE) * 0.5\n    padded_image_tensor[:, :, : torch_image.shape[2], : torch_image.shape[3]] = (\n        torch_image\n    )\n\n    padded_image_tensor = (padded_image_tensor - image_mean) / image_std\n\n    return padded_image_tensor\n</code></pre>"},{"location":"reference/inference/models/paligemma/paligemma/","title":"Paligemma","text":""},{"location":"reference/inference/models/paligemma/paligemma/#inference.models.paligemma.paligemma.LoRAPaliGemma","title":"<code>LoRAPaliGemma</code>","text":"<p>               Bases: <code>LoRATransformerModel</code></p> <p>By using you agree to the terms listed at https://ai.google.dev/gemma/terms</p> Source code in <code>inference/models/paligemma/paligemma.py</code> <pre><code>class LoRAPaliGemma(LoRATransformerModel):\n    \"\"\"By using you agree to the terms listed at https://ai.google.dev/gemma/terms\"\"\"\n\n    generation_includes_input = True\n    transformers_class = PaliGemmaForConditionalGeneration\n    load_base_from_roboflow = True\n</code></pre>"},{"location":"reference/inference/models/paligemma/paligemma/#inference.models.paligemma.paligemma.PaliGemma","title":"<code>PaliGemma</code>","text":"<p>               Bases: <code>TransformerModel</code></p> <p>By using you agree to the terms listed at https://ai.google.dev/gemma/terms</p> Source code in <code>inference/models/paligemma/paligemma.py</code> <pre><code>class PaliGemma(TransformerModel):\n    \"\"\"By using you agree to the terms listed at https://ai.google.dev/gemma/terms\"\"\"\n\n    generation_includes_input = True\n    transformers_class = PaliGemmaForConditionalGeneration\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/","title":"Perception encoder","text":""},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder","title":"<code>PerceptionEncoder</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>Roboflow Perception Encoder model implementation.</p> <p>This class is responsible for handling the Percpetion Encoder model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>CLIP</code> <p>The PE-CLIP model instance.</p> <code>preprocess</code> <code>function</code> <p>Function to preprocess the image.</p> <code>tokenizer</code> <code>function</code> <p>Function to tokenize text.</p> <code>device</code> <code>str</code> <p>The device to run inference on (cuda/cpu).</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>class PerceptionEncoder(RoboflowCoreModel):\n    \"\"\"Roboflow Perception Encoder model implementation.\n\n    This class is responsible for handling the Percpetion Encoder model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        model (pe.CLIP): The PE-CLIP model instance.\n        preprocess (function): Function to preprocess the image.\n        tokenizer (function): Function to tokenize text.\n        device (str): The device to run inference on (cuda/cpu).\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str = PERCEPTION_ENCODER_MODEL_ID,\n        device: str = DEVICE,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initializes the PerceptionEncoder with the given arguments and keyword arguments.\"\"\"\n        t1 = perf_counter()\n        super().__init__(model_id=model_id.lower(), *args, **kwargs)\n        self.device = device\n        self.log(\"Creating PE-CLIP model\")\n        # Parse model config from model_id (format: perception-encoder/PE-Core-L14-336)\n        model_config = model_id.split(\"/\")[-1]\n        checkpoint_path = os.path.join(self.cache_dir, \"model.pt\")\n        self.model = pe.CLIP.from_config(\n            model_config, pretrained=True, checkpoint_path=checkpoint_path\n        )\n        self.model = self.model.to(device)\n        self.model.eval()\n\n        self.preprocessor = transforms.get_image_transform(self.model.image_size)\n        self.tokenizer = transforms.get_text_tokenizer(self.model.context_length)\n\n        self.task_type = \"embedding\"\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\"\"\"\n        return [\"model.pt\"]  # No files needed as model is downloaded from HuggingFace\n\n    def initialize_model(self, **kwargs) -&gt; None:\n        \"\"\"Initialize the model. Not needed for PE-CLIP as it's loaded in __init__.\"\"\"\n        pass\n\n    def preproc_image(self, image: InferenceRequestImage) -&gt; torch.Tensor:\n        \"\"\"Preprocesses an inference request image.\"\"\"\n        pil_image = Image.fromarray(load_image_rgb(image))\n        preprocessed_image = self.preprocessor(pil_image)\n        return preprocessed_image.unsqueeze(0)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[torch.Tensor, PreprocessReturnMetadata]:\n        return self.preproc_image(image), PreprocessReturnMetadata({})\n\n    def compare(\n        self,\n        subject: Any,\n        prompt: Any,\n        subject_type: str = \"image\",\n        prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n        **kwargs,\n    ) -&gt; Union[List[float], Dict[str, float]]:\n        \"\"\"\n        Compares the subject with the prompt to calculate similarity scores.\n\n        Args:\n            subject (Any): The subject data to be compared. Can be either an image or text.\n            prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n            subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n            prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s).\n        \"\"\"\n        if subject_type == \"image\":\n            subject_embeddings = self.embed_image(subject)\n        elif subject_type == \"text\":\n            subject_embeddings = self.embed_text(subject)\n        else:\n            raise ValueError(\n                f\"subject_type must be either 'image' or 'text', but got {subject_type}\"\n            )\n\n        if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n            prompt_keys = prompt.keys()\n            prompt = [prompt[k] for k in prompt_keys]\n            prompt_obj = \"dict\"\n        else:\n            if not isinstance(prompt, list):\n                prompt = [prompt]\n            prompt_obj = \"list\"\n\n        if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n\n        if prompt_type == \"image\":\n            prompt_embeddings = self.embed_image(prompt)\n        elif prompt_type == \"text\":\n            prompt_embeddings = self.embed_text(prompt)\n        else:\n            raise ValueError(\n                f\"prompt_type must be either 'image' or 'text', but got {prompt_type}\"\n            )\n\n        similarities = [\n            cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n        ]\n\n        if prompt_obj == \"dict\":\n            similarities = dict(zip(prompt_keys, similarities))\n\n        return similarities\n\n    def make_compare_response(\n        self, similarities: Union[List[float], Dict[str, float]]\n    ) -&gt; PerceptionEncoderCompareResponse:\n        \"\"\"Creates a PerceptionEncoderCompareResponse object from the provided similarity data.\"\"\"\n        response = PerceptionEncoderCompareResponse(similarity=similarities)\n        return response\n\n    def embed_image(\n        self,\n        image: Any,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embeds an image or a list of images using the PE-CLIP model.\n\n        Args:\n            image (Any): The image or list of images to be embedded.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the image(s) as a numpy array.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(image, list):\n            if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n                )\n            imgs = [self.preproc_image(i) for i in image]\n            img_in = torch.cat(imgs, dim=0).to(self.device)\n        else:\n            img_in = self.preproc_image(image).to(self.device)\n\n        if self.device == \"cpu\" or self.device == \"mps\":\n            with torch.inference_mode():\n                image_features, _, _ = self.model(img_in, None)\n                # Convert to float32 before converting to numpy\n                embeddings = image_features.float().cpu().numpy()\n        else:\n            with torch.inference_mode(), torch.autocast(self.device):\n                image_features, _, _ = self.model(img_in, None)\n                # Convert to float32 before converting to numpy\n                embeddings = image_features.float().cpu().numpy()\n\n        return embeddings\n\n    def embed_text(\n        self,\n        text: Union[str, List[str]],\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embeds a text or a list of texts using the PE-CLIP model.\n\n        Args:\n            text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the text or texts as a numpy array.\n        \"\"\"\n        if isinstance(text, list):\n            texts = text\n        else:\n            texts = [text]\n\n        results = []\n        for texts_batch in create_batches(\n            sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n        ):\n            tokenized = self.tokenizer(texts_batch).to(self.device)\n            # Use float32 for CPU, bfloat16 for CUDA\n            if self.device == \"cpu\" or self.device == \"mps\":\n                with torch.no_grad():\n                    _, text_features, _ = self.model(None, tokenized)\n            else:\n                with torch.inference_mode(), torch.autocast(self.device):\n                    _, text_features, _ = self.model(None, tokenized)\n\n            # Convert to float32 before converting to numpy\n            embeddings = text_features.float().cpu().numpy()\n            results.append(embeddings)\n\n        return np.concatenate(results, axis=0)\n\n    def predict(self, img_in: torch.Tensor, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Predict embeddings for an input tensor.\n\n        Args:\n            img_in (torch.Tensor): The input tensor to get embeddings for.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray]: A tuple containing the embeddings as a numpy array.\n        \"\"\"\n        img_in = img_in.to(self.device)\n        if self.device == \"cpu\" or self.device == \"mps\":\n            with torch.inference_mode():\n                image_features, _, _ = self.model(img_in, None)\n        else:\n            with torch.inference_mode(), torch.autocast(self.device):\n                image_features, _, _ = self.model(img_in, None)\n\n        embeddings = image_features.float().cpu().numpy()\n        return (embeddings,)\n\n    def make_embed_image_response(\n        self, embeddings: np.ndarray\n    ) -&gt; PerceptionEncoderEmbeddingResponse:\n        \"\"\"Converts the given embeddings into a PerceptionEncoderEmbeddingResponse object.\"\"\"\n        response = PerceptionEncoderEmbeddingResponse(embeddings=embeddings.tolist())\n        return response\n\n    def make_embed_text_response(\n        self, embeddings: np.ndarray\n    ) -&gt; PerceptionEncoderEmbeddingResponse:\n        \"\"\"Converts the given text embeddings into a PerceptionEncoderEmbeddingResponse object.\"\"\"\n        response = PerceptionEncoderEmbeddingResponse(embeddings=embeddings.tolist())\n        return response\n\n    def infer_from_request(\n        self, request: PerceptionEncoderInferenceRequest\n    ) -&gt; PerceptionEncoderEmbeddingResponse:\n        \"\"\"Routes the request to the appropriate inference function.\"\"\"\n        t1 = perf_counter()\n        if isinstance(request, PerceptionEncoderImageEmbeddingRequest):\n            infer_func = self.embed_image\n            make_response_func = self.make_embed_image_response\n        elif isinstance(request, PerceptionEncoderTextEmbeddingRequest):\n            infer_func = self.embed_text\n            make_response_func = self.make_embed_text_response\n        elif isinstance(request, PerceptionEncoderCompareRequest):\n            infer_func = self.compare\n            make_response_func = self.make_compare_response\n        else:\n            raise ValueError(\n                f\"Request type {type(request)} is not a valid PerceptionEncoderInferenceRequest\"\n            )\n        data = infer_func(**request.dict())\n        response = make_response_func(data)\n        response.time = perf_counter() - t1\n        return response\n\n    def make_response(self, embeddings, *args, **kwargs) -&gt; InferenceResponse:\n        return [self.make_embed_image_response(embeddings)]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        return [self.make_embed_image_response(predictions[0])]\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Embeds an image\"\"\"\n        return super().infer(image, **kwargs)\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.__init__","title":"<code>__init__(model_id=PERCEPTION_ENCODER_MODEL_ID, device=DEVICE, *args, **kwargs)</code>","text":"<p>Initializes the PerceptionEncoder with the given arguments and keyword arguments.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def __init__(\n    self,\n    model_id: str = PERCEPTION_ENCODER_MODEL_ID,\n    device: str = DEVICE,\n    *args,\n    **kwargs,\n):\n    \"\"\"Initializes the PerceptionEncoder with the given arguments and keyword arguments.\"\"\"\n    t1 = perf_counter()\n    super().__init__(model_id=model_id.lower(), *args, **kwargs)\n    self.device = device\n    self.log(\"Creating PE-CLIP model\")\n    # Parse model config from model_id (format: perception-encoder/PE-Core-L14-336)\n    model_config = model_id.split(\"/\")[-1]\n    checkpoint_path = os.path.join(self.cache_dir, \"model.pt\")\n    self.model = pe.CLIP.from_config(\n        model_config, pretrained=True, checkpoint_path=checkpoint_path\n    )\n    self.model = self.model.to(device)\n    self.model.eval()\n\n    self.preprocessor = transforms.get_image_transform(self.model.image_size)\n    self.tokenizer = transforms.get_text_tokenizer(self.model.context_length)\n\n    self.task_type = \"embedding\"\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.compare","title":"<code>compare(subject, prompt, subject_type='image', prompt_type='text', **kwargs)</code>","text":"<p>Compares the subject with the prompt to calculate similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>Any</code> <p>The subject data to be compared. Can be either an image or text.</p> required <code>prompt</code> <code>Any</code> <p>The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.</p> required <code>subject_type</code> <code>str</code> <p>Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".</p> <code>'image'</code> <code>prompt_type</code> <code>Union[str, List[str], Dict[str, Any]]</code> <p>Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".</p> <code>'text'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Dict[str, float]]</code> <p>Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s).</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def compare(\n    self,\n    subject: Any,\n    prompt: Any,\n    subject_type: str = \"image\",\n    prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n    **kwargs,\n) -&gt; Union[List[float], Dict[str, float]]:\n    \"\"\"\n    Compares the subject with the prompt to calculate similarity scores.\n\n    Args:\n        subject (Any): The subject data to be compared. Can be either an image or text.\n        prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n        subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n        prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s).\n    \"\"\"\n    if subject_type == \"image\":\n        subject_embeddings = self.embed_image(subject)\n    elif subject_type == \"text\":\n        subject_embeddings = self.embed_text(subject)\n    else:\n        raise ValueError(\n            f\"subject_type must be either 'image' or 'text', but got {subject_type}\"\n        )\n\n    if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n        prompt_keys = prompt.keys()\n        prompt = [prompt[k] for k in prompt_keys]\n        prompt_obj = \"dict\"\n    else:\n        if not isinstance(prompt, list):\n            prompt = [prompt]\n        prompt_obj = \"list\"\n\n    if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n        raise ValueError(\n            f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n        )\n\n    if prompt_type == \"image\":\n        prompt_embeddings = self.embed_image(prompt)\n    elif prompt_type == \"text\":\n        prompt_embeddings = self.embed_text(prompt)\n    else:\n        raise ValueError(\n            f\"prompt_type must be either 'image' or 'text', but got {prompt_type}\"\n        )\n\n    similarities = [\n        cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n    ]\n\n    if prompt_obj == \"dict\":\n        similarities = dict(zip(prompt_keys, similarities))\n\n    return similarities\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.embed_image","title":"<code>embed_image(image, **kwargs)</code>","text":"<p>Embeds an image or a list of images using the PE-CLIP model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be embedded.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the image(s) as a numpy array.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def embed_image(\n    self,\n    image: Any,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Embeds an image or a list of images using the PE-CLIP model.\n\n    Args:\n        image (Any): The image or list of images to be embedded.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the image(s) as a numpy array.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(image, list):\n        if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n        imgs = [self.preproc_image(i) for i in image]\n        img_in = torch.cat(imgs, dim=0).to(self.device)\n    else:\n        img_in = self.preproc_image(image).to(self.device)\n\n    if self.device == \"cpu\" or self.device == \"mps\":\n        with torch.inference_mode():\n            image_features, _, _ = self.model(img_in, None)\n            # Convert to float32 before converting to numpy\n            embeddings = image_features.float().cpu().numpy()\n    else:\n        with torch.inference_mode(), torch.autocast(self.device):\n            image_features, _, _ = self.model(img_in, None)\n            # Convert to float32 before converting to numpy\n            embeddings = image_features.float().cpu().numpy()\n\n    return embeddings\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.embed_text","title":"<code>embed_text(text, **kwargs)</code>","text":"<p>Embeds a text or a list of texts using the PE-CLIP model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, List[str]]</code> <p>The text string or list of text strings to be embedded.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the text or texts as a numpy array.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def embed_text(\n    self,\n    text: Union[str, List[str]],\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Embeds a text or a list of texts using the PE-CLIP model.\n\n    Args:\n        text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the text or texts as a numpy array.\n    \"\"\"\n    if isinstance(text, list):\n        texts = text\n    else:\n        texts = [text]\n\n    results = []\n    for texts_batch in create_batches(\n        sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n    ):\n        tokenized = self.tokenizer(texts_batch).to(self.device)\n        # Use float32 for CPU, bfloat16 for CUDA\n        if self.device == \"cpu\" or self.device == \"mps\":\n            with torch.no_grad():\n                _, text_features, _ = self.model(None, tokenized)\n        else:\n            with torch.inference_mode(), torch.autocast(self.device):\n                _, text_features, _ = self.model(None, tokenized)\n\n        # Convert to float32 before converting to numpy\n        embeddings = text_features.float().cpu().numpy()\n        results.append(embeddings)\n\n    return np.concatenate(results, axis=0)\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\"\"\"\n    return [\"model.pt\"]  # No files needed as model is downloaded from HuggingFace\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Embeds an image</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Embeds an image\"\"\"\n    return super().infer(image, **kwargs)\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Routes the request to the appropriate inference function.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def infer_from_request(\n    self, request: PerceptionEncoderInferenceRequest\n) -&gt; PerceptionEncoderEmbeddingResponse:\n    \"\"\"Routes the request to the appropriate inference function.\"\"\"\n    t1 = perf_counter()\n    if isinstance(request, PerceptionEncoderImageEmbeddingRequest):\n        infer_func = self.embed_image\n        make_response_func = self.make_embed_image_response\n    elif isinstance(request, PerceptionEncoderTextEmbeddingRequest):\n        infer_func = self.embed_text\n        make_response_func = self.make_embed_text_response\n    elif isinstance(request, PerceptionEncoderCompareRequest):\n        infer_func = self.compare\n        make_response_func = self.make_compare_response\n    else:\n        raise ValueError(\n            f\"Request type {type(request)} is not a valid PerceptionEncoderInferenceRequest\"\n        )\n    data = infer_func(**request.dict())\n    response = make_response_func(data)\n    response.time = perf_counter() - t1\n    return response\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.initialize_model","title":"<code>initialize_model(**kwargs)</code>","text":"<p>Initialize the model. Not needed for PE-CLIP as it's loaded in init.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def initialize_model(self, **kwargs) -&gt; None:\n    \"\"\"Initialize the model. Not needed for PE-CLIP as it's loaded in __init__.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.make_compare_response","title":"<code>make_compare_response(similarities)</code>","text":"<p>Creates a PerceptionEncoderCompareResponse object from the provided similarity data.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def make_compare_response(\n    self, similarities: Union[List[float], Dict[str, float]]\n) -&gt; PerceptionEncoderCompareResponse:\n    \"\"\"Creates a PerceptionEncoderCompareResponse object from the provided similarity data.\"\"\"\n    response = PerceptionEncoderCompareResponse(similarity=similarities)\n    return response\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.make_embed_image_response","title":"<code>make_embed_image_response(embeddings)</code>","text":"<p>Converts the given embeddings into a PerceptionEncoderEmbeddingResponse object.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def make_embed_image_response(\n    self, embeddings: np.ndarray\n) -&gt; PerceptionEncoderEmbeddingResponse:\n    \"\"\"Converts the given embeddings into a PerceptionEncoderEmbeddingResponse object.\"\"\"\n    response = PerceptionEncoderEmbeddingResponse(embeddings=embeddings.tolist())\n    return response\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.make_embed_text_response","title":"<code>make_embed_text_response(embeddings)</code>","text":"<p>Converts the given text embeddings into a PerceptionEncoderEmbeddingResponse object.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def make_embed_text_response(\n    self, embeddings: np.ndarray\n) -&gt; PerceptionEncoderEmbeddingResponse:\n    \"\"\"Converts the given text embeddings into a PerceptionEncoderEmbeddingResponse object.\"\"\"\n    response = PerceptionEncoderEmbeddingResponse(embeddings=embeddings.tolist())\n    return response\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Predict embeddings for an input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>Tensor</code> <p>The input tensor to get embeddings for.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: A tuple containing the embeddings as a numpy array.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def predict(self, img_in: torch.Tensor, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Predict embeddings for an input tensor.\n\n    Args:\n        img_in (torch.Tensor): The input tensor to get embeddings for.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray]: A tuple containing the embeddings as a numpy array.\n    \"\"\"\n    img_in = img_in.to(self.device)\n    if self.device == \"cpu\" or self.device == \"mps\":\n        with torch.inference_mode():\n            image_features, _, _ = self.model(img_in, None)\n    else:\n        with torch.inference_mode(), torch.autocast(self.device):\n            image_features, _, _ = self.model(img_in, None)\n\n    embeddings = image_features.float().cpu().numpy()\n    return (embeddings,)\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/perception_encoder/#inference.models.perception_encoder.perception_encoder.PerceptionEncoder.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an inference request image.</p> Source code in <code>inference/models/perception_encoder/perception_encoder.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage) -&gt; torch.Tensor:\n    \"\"\"Preprocesses an inference request image.\"\"\"\n    pil_image = Image.fromarray(load_image_rgb(image))\n    preprocessed_image = self.preprocessor(pil_image)\n    return preprocessed_image.unsqueeze(0)\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/config/","title":"Config","text":"<p>Include all available vision encoder configurations.</p>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/config/#inference.models.perception_encoder.vision_encoder.config.PEConfig","title":"<code>PEConfig</code>  <code>dataclass</code>","text":"<p>Vision Tower Config.</p> Source code in <code>inference/models/perception_encoder/vision_encoder/config.py</code> <pre><code>@dataclass\nclass PEConfig:\n    \"\"\"Vision Tower Config.\"\"\"\n\n    patch_size: int\n    width: int\n    layers: int\n    heads: int\n    mlp_ratio: float\n    output_dim: Optional[int]\n\n    ls_init_value: float = None\n    drop_path: float = 0.0\n\n    image_size: int = (224,)\n    use_abs_posemb: bool = True\n    use_cls_token: bool = False\n    use_rope2d: bool = True\n\n    pool_type: str = \"attn\"\n    attn_pooler_heads: int = 8\n\n    use_ln_pre: bool = True\n    use_ln_post: bool = True\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/config/#inference.models.perception_encoder.vision_encoder.config.PETextConfig","title":"<code>PETextConfig</code>  <code>dataclass</code>","text":"<p>Text Tower Config.</p> Source code in <code>inference/models/perception_encoder/vision_encoder/config.py</code> <pre><code>@dataclass\nclass PETextConfig:\n    \"\"\"Text Tower Config.\"\"\"\n\n    context_length: int\n    width: int\n    heads: int\n    layers: int\n\n    output_dim: int\n\n    mlp_ratio: float = 4.0\n    vocab_size: int = 49408\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/pe/","title":"Pe","text":""},{"location":"reference/inference/models/perception_encoder/vision_encoder/pe/#inference.models.perception_encoder.vision_encoder.pe.SelfAttention","title":"<code>SelfAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements sequence packed attention and RoPe</p> Source code in <code>inference/models/perception_encoder/vision_encoder/pe.py</code> <pre><code>class SelfAttention(nn.Module):\n    r\"\"\"\n    Implements sequence packed attention and RoPe\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        rope: Optional[nn.Module] = None,\n    ):\n        super(SelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n\n        # To make this compatibile with nn.MultiHeadAttention\n        self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n        self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n\n        self.rope = rope\n        self.scale = self.head_dim ** (-0.5)\n\n    def init_tensors(self):\n        xavier_uniform_(self.in_proj_weight)\n        constant_(self.in_proj_bias, 0.0)\n        constant_(self.out_proj.bias, 0.0)\n\n    def forward(self, x, attn_mask=None):\n        batch, seq, embed_dim = x.shape\n        proj = F.linear(x, self.in_proj_weight, self.in_proj_bias)\n\n        # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n        proj = (\n            proj.unflatten(-1, (3, embed_dim))\n            .unsqueeze(0)\n            .transpose(0, -2)\n            .squeeze(-2)\n            .contiguous()\n        )\n        q, k, v = proj[0], proj[1], proj[2]\n\n        # Use \"q_\" so that we don't accidentally quit in pdb :)\n        q = rearrange(q, \"b s (h d) -&gt; b h s d\", h=self.num_heads)\n        k = rearrange(k, \"b s (h d) -&gt; b h s d\", h=self.num_heads)\n        v = rearrange(v, \"b s (h d) -&gt; b h s d\", h=self.num_heads)\n\n        if self.rope:\n            q, k = self.rope(q, k)\n\n        attn = F.scaled_dot_product_attention(\n            q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False, scale=self.scale\n        )\n        attn = rearrange(attn, \"b h s d -&gt; b s (h d)\")\n\n        return F.linear(attn, self.out_proj.weight, self.out_proj.bias)\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/pe/#inference.models.perception_encoder.vision_encoder.pe.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>inference/models/perception_encoder/vision_encoder/pe.py</code> <pre><code>class Transformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = nn.LayerNorm,\n        drop_path: float = 0.0,\n        rope: Optional[nn.Module] = None,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n\n        self.resblocks = nn.ModuleList(\n            [\n                ResidualAttentionBlock(\n                    width,\n                    heads,\n                    mlp_ratio,\n                    ls_init_value=ls_init_value,\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    drop_path=drop_path,\n                    rope=rope,\n                )\n                for _ in range(layers)\n            ]\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def truncate(self, layer_idx: int):\n        \"\"\"Delete layers so the last layer is the given layer index.\"\"\"\n        self.layers = ((self.layers + layer_idx) % self.layers) + 1\n        self.resblocks = nn.ModuleList(self.resblocks[: self.layers])\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        attn_mask: Optional[torch.Tensor] = None,\n        layer_idx: int = -1,\n    ):\n        stop_idx = (self.layers + layer_idx) % self.layers\n\n        for i, r in enumerate(self.resblocks):\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                # TODO: handle kwargs https://github.com/pytorch/pytorch/issues/79887#issuecomment-1161758372\n                x = checkpoint(r, x, None, None, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n\n            if i == stop_idx:\n                break\n\n        return x\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/pe/#inference.models.perception_encoder.vision_encoder.pe.Transformer.truncate","title":"<code>truncate(layer_idx)</code>","text":"<p>Delete layers so the last layer is the given layer index.</p> Source code in <code>inference/models/perception_encoder/vision_encoder/pe.py</code> <pre><code>@torch.jit.ignore\ndef truncate(self, layer_idx: int):\n    \"\"\"Delete layers so the last layer is the given layer index.\"\"\"\n    self.layers = ((self.layers + layer_idx) % self.layers) + 1\n    self.resblocks = nn.ModuleList(self.resblocks[: self.layers])\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/pe/#inference.models.perception_encoder.vision_encoder.pe.VisionTransformer","title":"<code>VisionTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>inference/models/perception_encoder/vision_encoder/pe.py</code> <pre><code>class VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        patch_size: int,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = partial(nn.LayerNorm, eps=1e-5),\n        use_ln_pre: bool = True,\n        use_ln_post: bool = True,\n        ls_init_value: float = None,\n        drop_path: float = 0.0,\n        image_size: int = 448,  # Pretrain image size only; you can pass in any image size\n        use_abs_posemb: bool = True,\n        use_rope2d: bool = True,\n        use_cls_token: bool = False,\n        output_dim: Optional[int] = 1280,\n        attn_pooler_heads: int = 8,\n        pool_type: Literal[\"attn\", \"tok\", \"avg\", \"none\"] = \"attn\",\n    ):\n        super().__init__()\n        assert pool_type in (\"attn\", \"tok\", \"avg\", \"none\")\n        self.pool_type = pool_type\n        self.patch_size = patch_size\n\n        self.output_dim = output_dim or width\n        self.proj_dim = output_dim\n        self.heads = heads\n        self.width = width\n        self.layers = layers\n\n        self.use_abs_posemb = use_abs_posemb\n        self.use_cls_token = use_cls_token\n        self.use_rope2d = use_rope2d\n        self.image_size = image_size\n\n        self.conv1 = nn.Conv2d(\n            in_channels=3,\n            out_channels=width,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=False,\n        )\n        self.rope = (\n            Rope2D(\n                dim=width // heads,\n                use_cls_token=self.use_cls_token,\n            )\n            if self.use_rope2d\n            else None\n        )\n\n        self.ln_pre = norm_layer(width) if use_ln_pre else nn.Identity()\n        self.ln_post = norm_layer(self.width) if use_ln_post else nn.Identity()\n\n        self.transformer = Transformer(\n            width,\n            layers,\n            heads,\n            mlp_ratio,\n            ls_init_value=ls_init_value,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            drop_path=drop_path,\n            rope=self.rope,\n        )\n\n        if pool_type == \"attn\":\n            self.attn_pool = AttentionPooling(\n                embed_dim=width,\n                num_heads=attn_pooler_heads,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.attn_pool = None\n\n        self.init_tensors()\n\n    def init_tensors(self):\n        def init_submodule_tensors(module):\n            for name, child in module.named_children():\n                if hasattr(child, \"init_tensors\"):\n                    logger.debug(f\"Initializing tensors for submodule: {name}\")\n                    child.init_tensors()\n                init_submodule_tensors(child)\n\n        init_submodule_tensors(self)\n        self.rope.init_tensors()\n\n        # class embeddings and positional embeddings\n        init_scale = self.width**-0.5\n\n        if self.use_cls_token:\n            self.class_embedding = nn.Parameter(init_scale * torch.randn(self.width))\n\n        if self.use_abs_posemb:\n            self.posemb_grid_size = self.image_size // self.patch_size\n            self.positional_embedding = nn.Parameter(\n                init_scale\n                * torch.randn(\n                    int(self.use_cls_token) + self.posemb_grid_size**2, self.width\n                )\n            )\n\n        if self.proj_dim is not None:\n            self.proj = nn.Parameter(\n                init_scale * torch.randn(self.width, self.proj_dim)\n            )\n\n    def load_ckpt(self, ckpt_path: str):\n        _sd = torch.load(ckpt_path, weights_only=True)\n        if \"state_dict\" in _sd:\n            _sd = _sd[\"state_dict\"]\n        elif \"weights\" in _sd:\n            _sd = _sd[\"weights\"]\n\n        # for backwards compatibility\n        _sd = {k.replace(\"module.\", \"\"): v for k, v in _sd.items()}\n        if any(k.startswith(\"visual.\") for k in _sd):\n            _sd = {k.replace(\"visual.\", \"\"): v for k, v in _sd.items() if \"visual\" in k}\n\n        m, u = self.load_state_dict(_sd, strict=False)\n        logger.info(f\"Missing keys for loading vision encoder: {m}\")\n        logger.info(f\"Unexpected keys for loading vision encoder: {u}\")\n        print(f\"Missing keys for loading vision encoder: {m}\")\n        print(f\"Unexpected keys for loading vision encoder: {u}\")\n\n    def truncate(self, layer_idx: int):\n        \"\"\"Delete layers so the last layer is the given layer index.\"\"\"\n        self.transformer.truncate(layer_idx)\n        self.layers = self.transformer.layers\n\n    @classmethod\n    def from_config(\n        cls,\n        name: str,\n        pretrained: bool = False,\n        checkpoint_path: Optional[str] = None,\n        **kwdargs,\n    ):\n        if name not in PE_VISION_CONFIG:\n            raise RuntimeError(f\"{name} not found in configs.\")\n\n        args = asdict(PE_VISION_CONFIG[name])\n        args.update(kwdargs)\n\n        model = cls(**args)\n        if pretrained:\n            model.load_ckpt(fetch_pe_checkpoint(name, checkpoint_path))\n\n        return model\n\n    @classmethod\n    def available_configs(cls):\n        return list(PE_VISION_CONFIG.keys())\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.set_grad_checkpointing(enable=enable)\n\n    def _sample_abs_posemb(self, grid_h: int, grid_w: int):\n        \"\"\"Interpolates the absolute position embedding if necessary.\"\"\"\n        if self.posemb_grid_size == grid_h and self.posemb_grid_size == grid_w:\n            return self.positional_embedding[None, ...]\n\n        pos_embed = self.positional_embedding\n        if self.use_cls_token:\n            cls_token_embed, pos_embed = pos_embed[:1], pos_embed[1:]\n\n        pos_embed = (\n            pos_embed.reshape(1, self.posemb_grid_size, self.posemb_grid_size, -1)\n            .permute(0, 3, 1, 2)\n            .contiguous()\n        )\n        pos_embed = F.interpolate(\n            pos_embed, size=(grid_h, grid_w), mode=\"bilinear\", align_corners=False\n        )\n        pos_embed = pos_embed.permute(0, 2, 3, 1).reshape(-1, self.width).contiguous()\n\n        if self.use_cls_token:\n            pos_embed = torch.cat([cls_token_embed, pos_embed], dim=0)\n\n        return pos_embed[None, ...]\n\n    def _pool(self, x: torch.Tensor):\n        if self.pool_type == \"tok\":\n            return x[:, 0]\n        elif self.pool_type == \"avg\":\n            return x.mean(dim=1)\n        elif self.pool_type == \"attn\":\n            return self.attn_pool(x).squeeze(1)\n        elif self.pool_type == \"none\":\n            return x\n        else:\n            raise NotImplementedError\n\n    def forward_features(\n        self,\n        x: torch.Tensor,\n        norm: bool = False,\n        layer_idx: int = -1,\n        strip_cls_token: bool = False,\n    ):\n        batch, _, h, w = x.shape\n        grid_h, grid_w = h // self.patch_size, w // self.patch_size\n\n        x = self.conv1(x)\n        x = x.permute(0, 2, 3, 1).reshape(batch, -1, self.width)\n\n        if self.use_cls_token:\n            x = torch.cat(\n                [self.class_embedding.view(1, 1, -1).expand(batch, -1, -1), x],\n                dim=1,\n            )\n\n        if self.use_abs_posemb:\n            x = x + self._sample_abs_posemb(grid_h, grid_w)\n\n        if self.use_rope2d:\n            self.rope.update_grid(x.device, grid_h, grid_w)\n\n        x = self.ln_pre(x)\n        x = self.transformer(x, layer_idx=layer_idx)\n\n        if norm:\n            x = self.ln_post(x)\n\n        if strip_cls_token and self.use_cls_token:\n            x = x[:, 1:, :]\n\n        return x\n\n    def forward(self, x: torch.Tensor, **kwargs):\n        x = self.forward_features(x, norm=True, **kwargs)\n        x = self._pool(x)\n\n        if self.proj_dim is not None:\n            x = x @ self.proj\n\n        return x\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/pe/#inference.models.perception_encoder.vision_encoder.pe.VisionTransformer.truncate","title":"<code>truncate(layer_idx)</code>","text":"<p>Delete layers so the last layer is the given layer index.</p> Source code in <code>inference/models/perception_encoder/vision_encoder/pe.py</code> <pre><code>def truncate(self, layer_idx: int):\n    \"\"\"Delete layers so the last layer is the given layer index.\"\"\"\n    self.transformer.truncate(layer_idx)\n    self.layers = self.transformer.layers\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/rope/","title":"Rope","text":""},{"location":"reference/inference/models/perception_encoder/vision_encoder/rope/#inference.models.perception_encoder.vision_encoder.rope.Rope2D","title":"<code>Rope2D</code>","text":"<p>Helper class to apply RoPE2D as well as interpolate on the fly.</p> Source code in <code>inference/models/perception_encoder/vision_encoder/rope.py</code> <pre><code>class Rope2D:\n    \"\"\"Helper class to apply RoPE2D as well as interpolate on the fly.\"\"\"\n\n    def __init__(self, dim, use_cls_token=False):\n        self.dim = dim\n        self.use_cls_token = use_cls_token\n        self.grid_size = None\n        self.freq = None\n\n    def init_tensors(self):\n        self.rope = RotaryEmbedding(self.dim // 2)\n\n    def update_grid(self, device, grid_h, grid_w):\n        if self.grid_size != (grid_h, grid_w):\n            self.grid_size = (grid_h, grid_w)\n\n            self.rope = self.rope.to(device)\n\n            if self.use_cls_token:\n                # +1 to leave space for the cls token to be (0, 0)\n                grid_y_range = torch.arange(grid_h, device=device) + 1\n                grid_x_range = torch.arange(grid_w, device=device) + 1\n            else:\n                grid_y_range = torch.arange(grid_h, device=device)\n                grid_x_range = torch.arange(grid_w, device=device)\n\n            freqs_y = self.rope(grid_y_range)[:, None].expand(grid_h, grid_w, -1)\n            freqs_x = self.rope(grid_x_range)[None, :].expand(grid_h, grid_w, -1)\n            freq = torch.cat([freqs_x, freqs_y], dim=-1).reshape(grid_h * grid_w, -1)\n\n            if self.use_cls_token:\n                freq = torch.cat(\n                    [torch.zeros(1, freq.shape[-1], device=device), freq], dim=0\n                )\n\n            self.freq = freq[None, ...]\n\n        self.freq = self.freq.to(device)\n\n    def __call__(self, q, k):\n        # batch, heads, seq, dim = q.shape\n        q = apply_rotary_emb(self.freq[:, None, :, :], q)\n        k = apply_rotary_emb(self.freq[:, None, :, :], k)\n\n        return q, k\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/","title":"Tokenizer","text":"<p>CLIP tokenizer</p> <p>Copied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.</p>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.SimpleTokenizer","title":"<code>SimpleTokenizer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>inference/models/perception_encoder/vision_encoder/tokenizer.py</code> <pre><code>class SimpleTokenizer(object):\n    def __init__(\n        self,\n        bpe_path: str = default_bpe(),\n        additional_special_tokens: Optional[List[str]] = None,\n        context_length: Optional[int] = DEFAULT_CONTEXT_LENGTH,\n        clean: str = \"lower\",\n        reduction_mask: str = \"\",\n    ):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"&lt;/w&gt;\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        special_tokens = [\"&lt;start_of_text&gt;\", \"&lt;end_of_text&gt;\"]\n        if additional_special_tokens:\n            special_tokens += additional_special_tokens\n        vocab.extend(special_tokens)\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {t: t for t in special_tokens}\n        special = \"|\".join(special_tokens)\n        self.pat = re.compile(\n            special + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n            re.IGNORECASE,\n        )\n        self.vocab_size = len(self.encoder)\n        self.all_special_ids = [self.encoder[t] for t in special_tokens]\n        self.sot_token_id = self.all_special_ids[0]\n        self.eot_token_id = self.all_special_ids[1]\n        self.context_length = context_length\n        self.clean_fn = get_clean_fn(clean)\n        self.reduction_fn = (\n            get_reduction_mask_fn(reduction_mask) if reduction_mask else None\n        )\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"&lt;/w&gt;\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"&lt;/w&gt;\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i &lt; len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i &lt; len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = self.clean_fn(text)\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(\n                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n            )\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = (\n            bytearray([self.byte_decoder[c] for c in text])\n            .decode(\"utf-8\", errors=\"replace\")\n            .replace(\"&lt;/w&gt;\", \" \")\n        )\n        return text\n\n    def __call__(\n        self, texts: Union[str, List[str]], context_length: Optional[int] = None\n    ) -&gt; torch.LongTensor:\n        \"\"\"Returns the tokenized representation of given input string(s)\n\n        Parameters\n        ----------\n        texts : Union[str, List[str]]\n            An input string or a list of input strings to tokenize\n        context_length : int\n            The context length to use; all CLIP models use 77 as the context length\n\n        Returns\n        -------\n        A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n        \"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n\n        context_length = context_length or self.context_length\n        assert context_length, \"Please set a valid context length\"\n\n        if self.reduction_fn is not None:\n            # use reduction strategy for tokenize if set, otherwise default to truncation below\n            return self.reduction_fn(\n                texts,\n                context_length=context_length,\n                sot_token_id=self.sot_token_id,\n                eot_token_id=self.eot_token_id,\n                encode_fn=self.encode,\n            )\n\n        all_tokens = [\n            [self.sot_token_id] + self.encode(text) + [self.eot_token_id]\n            for text in texts\n        ]\n        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n        for i, tokens in enumerate(all_tokens):\n            if len(tokens) &gt; context_length:\n                tokens = tokens[:context_length]  # Truncate\n                tokens[-1] = self.eot_token_id\n            result[i, : len(tokens)] = torch.tensor(tokens)\n\n        return result\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.SimpleTokenizer.__call__","title":"<code>__call__(texts, context_length=None)</code>","text":"<p>Returns the tokenized representation of given input string(s)</p>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.SimpleTokenizer.__call__--parameters","title":"Parameters","text":"<p>texts : Union[str, List[str]]     An input string or a list of input strings to tokenize context_length : int     The context length to use; all CLIP models use 77 as the context length</p>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.SimpleTokenizer.__call__--returns","title":"Returns","text":"<p>A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]</p> Source code in <code>inference/models/perception_encoder/vision_encoder/tokenizer.py</code> <pre><code>def __call__(\n    self, texts: Union[str, List[str]], context_length: Optional[int] = None\n) -&gt; torch.LongTensor:\n    \"\"\"Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    context_length = context_length or self.context_length\n    assert context_length, \"Please set a valid context length\"\n\n    if self.reduction_fn is not None:\n        # use reduction strategy for tokenize if set, otherwise default to truncation below\n        return self.reduction_fn(\n            texts,\n            context_length=context_length,\n            sot_token_id=self.sot_token_id,\n            eot_token_id=self.eot_token_id,\n            encode_fn=self.encode,\n        )\n\n    all_tokens = [\n        [self.sot_token_id] + self.encode(text) + [self.eot_token_id]\n        for text in texts\n    ]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) &gt; context_length:\n            tokens = tokens[:context_length]  # Truncate\n            tokens[-1] = self.eot_token_id\n        result[i, : len(tokens)] = torch.tensor(tokens)\n\n    return result\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.bytes_to_unicode","title":"<code>bytes_to_unicode()</code>  <code>cached</code>","text":"<p>Returns list of utf-8 byte and a corresponding list of unicode strings. The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup tables between utf-8 bytes and unicode strings. And avoids mapping to whitespace/control characters the bpe code barfs on.</p> Source code in <code>inference/models/perception_encoder/vision_encoder/tokenizer.py</code> <pre><code>@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a significant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1))\n        + list(range(ord(\"\u00a1\"), ord(\"\u00ac\") + 1))\n        + list(range(ord(\"\u00ae\"), ord(\"\u00ff\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.canonicalize_text","title":"<code>canonicalize_text(text, *, keep_punctuation_exact_string=None)</code>","text":"<p>Returns canonicalized <code>text</code> (lowercase and punctuation removed).</p> <p>From: https://github.com/google-research/big_vision/blob/53f18caf27a9419231bbf08d3388b07671616d3d/big_vision/evaluators/proj/image_text/prompt_engineering.py#L94</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>string to be canonicalized.</p> required <code>keep_punctuation_exact_string</code> <p>If provided, then this exact string kept. For example providing '{}' will keep any occurrences of '{}' (but will still remove '{' and '}' that appear separately).</p> <code>None</code> Source code in <code>inference/models/perception_encoder/vision_encoder/tokenizer.py</code> <pre><code>def canonicalize_text(text, *, keep_punctuation_exact_string=None):\n    \"\"\"Returns canonicalized `text` (lowercase and punctuation removed).\n\n    From: https://github.com/google-research/big_vision/blob/53f18caf27a9419231bbf08d3388b07671616d3d/big_vision/evaluators/proj/image_text/prompt_engineering.py#L94\n\n    Args:\n      text: string to be canonicalized.\n      keep_punctuation_exact_string: If provided, then this exact string kept.\n        For example providing '{}' will keep any occurrences of '{}' (but will\n        still remove '{' and '}' that appear separately).\n    \"\"\"\n    text = text.replace(\"_\", \" \")\n    if keep_punctuation_exact_string:\n        text = keep_punctuation_exact_string.join(\n            part.translate(str.maketrans(\"\", \"\", string.punctuation))\n            for part in text.split(keep_punctuation_exact_string)\n        )\n    else:\n        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    text = text.lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.get_pairs","title":"<code>get_pairs(word)</code>","text":"<p>Return set of symbol pairs in a word. Word is represented as tuple of symbols (symbols being variable-length strings).</p> Source code in <code>inference/models/perception_encoder/vision_encoder/tokenizer.py</code> <pre><code>def get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n</code></pre>"},{"location":"reference/inference/models/perception_encoder/vision_encoder/tokenizer/#inference.models.perception_encoder.vision_encoder.tokenizer.get_reduction_mask_fn","title":"<code>get_reduction_mask_fn(type)</code>","text":"<p>Choose strategy for dropping (masking) tokens to achieve target context length</p> Source code in <code>inference/models/perception_encoder/vision_encoder/tokenizer.py</code> <pre><code>def get_reduction_mask_fn(type: str):\n    \"\"\"Choose strategy for dropping (masking) tokens to achieve target context length\"\"\"\n    assert type in (\"simple\", \"random\", \"shuffle\")\n    if type == \"simple\":\n        return simple_mask_tokenize  # randomly select block [start:end]\n    elif type == \"random\":\n        return random_mask_tokenize  # randomly drop tokens (keep order)\n    elif type == \"shuffle\":\n        return partial(\n            random_mask_tokenize, shuffle=True\n        )  # randomly drop tokens (shuffle order)\n</code></pre>"},{"location":"reference/inference/models/qwen25vl/qwen25vl/","title":"Qwen25vl","text":""},{"location":"reference/inference/models/resnet/resnet_classification/","title":"Resnet classification","text":""},{"location":"reference/inference/models/resnet/resnet_classification/#inference.models.resnet.resnet_classification.ResNetClassification","title":"<code>ResNetClassification</code>","text":"<p>               Bases: <code>ClassificationBaseOnnxRoboflowInferenceModel</code></p> <p>VitClassification handles classification inference for Vision Transformer (ViT) models using ONNX.</p> Inherits <p>ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference. ClassificationMixin: Mixin class providing classification-specific methods.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>A flag that specifies if the model should handle multiclass classification.</p> Source code in <code>inference/models/resnet/resnet_classification.py</code> <pre><code>class ResNetClassification(ClassificationBaseOnnxRoboflowInferenceModel):\n    \"\"\"VitClassification handles classification inference\n    for Vision Transformer (ViT) models using ONNX.\n\n    Inherits:\n        ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference.\n        ClassificationMixin: Mixin class providing classification-specific methods.\n\n    Attributes:\n        multiclass (bool): A flag that specifies if the model should handle multiclass classification.\n    \"\"\"\n\n    preprocess_means = [0.485, 0.456, 0.406]\n    preprocess_stds = [0.229, 0.224, 0.225]\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes the VitClassification instance.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Determines the weights file to be used based on the availability of AWS keys.\n\n        If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'.\n        Otherwise, it returns the path to 'best.onnx'.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and LAMBDA:\n            return \"weights.onnx\"\n        else:\n            return \"best.onnx\"\n</code></pre>"},{"location":"reference/inference/models/resnet/resnet_classification/#inference.models.resnet.resnet_classification.ResNetClassification.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Determines the weights file to be used based on the availability of AWS keys.</p> <p>If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'. Otherwise, it returns the path to 'best.onnx'.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"reference/inference/models/resnet/resnet_classification/#inference.models.resnet.resnet_classification.ResNetClassification.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the VitClassification instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/resnet/resnet_classification.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes the VitClassification instance.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/","title":"Rfdetr","text":""},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRInstanceSegmentation","title":"<code>RFDETRInstanceSegmentation</code>","text":"<p>               Bases: <code>RFDETRObjectDetection</code>, <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>class RFDETRInstanceSegmentation(\n    RFDETRObjectDetection, InstanceSegmentationBaseOnnxRoboflowInferenceModel\n):\n    def initialize_model(self, **kwargs) -&gt; None:\n        super().initialize_model(**kwargs)\n        mask_shape = self.onnx_session.get_outputs()[2].shape\n        self.mask_shape = mask_shape[2:]\n\n    def predict(self, img_in: ImageMetaType, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session with the RFDETR model.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class IDs.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )\n        bboxes = predictions[0]\n        logits = predictions[1]\n        masks = predictions[2]\n\n        return (bboxes, logits, masks)\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n        bboxes, logits, masks = predictions\n        bboxes = bboxes.astype(np.float32)\n        logits = logits.astype(np.float32)\n\n        batch_size, num_queries, num_classes = logits.shape\n        logits_sigmoid = self.sigmoid_stable(logits)\n\n        img_dims = preproc_return_metadata[\"img_dims\"]\n\n        processed_predictions = []\n\n        for batch_idx in range(batch_size):\n            orig_h, orig_w = img_dims[batch_idx]\n\n            logits_flat = logits_sigmoid[batch_idx].reshape(-1)\n\n            # Use argpartition for better performance when max_detections is smaller than logits_flat\n            partition_indices = np.argpartition(-logits_flat, max_detections)[\n                :max_detections\n            ]\n            sorted_indices = partition_indices[\n                np.argsort(-logits_flat[partition_indices])\n            ]\n            topk_scores = logits_flat[sorted_indices]\n\n            conf_mask = topk_scores &gt; confidence\n            sorted_indices = sorted_indices[conf_mask]\n            topk_scores = topk_scores[conf_mask]\n\n            topk_boxes = sorted_indices // num_classes\n            topk_labels = sorted_indices % num_classes\n\n            if self.is_one_indexed:\n                class_filter_mask = topk_labels != self.background_class_index\n\n                topk_labels[topk_labels &gt; self.background_class_index] -= 1\n                topk_scores = topk_scores[class_filter_mask]\n                topk_labels = topk_labels[class_filter_mask]\n                topk_boxes = topk_boxes[class_filter_mask]\n\n            selected_boxes = bboxes[batch_idx, topk_boxes]\n            selected_masks = masks[batch_idx, topk_boxes]\n            if selected_masks.size != 0:\n                if kwargs.get(\"mask_decode_mode\", \"accurate\") == \"accurate\":\n                    target_res = (orig_w, orig_h)\n                    new_masks = []\n                    for mask in selected_masks:\n                        new_masks.append(\n                            cv2.resize(mask, target_res, interpolation=cv2.INTER_LINEAR)\n                        )\n                    selected_masks = np.stack(new_masks, axis=0)\n                elif kwargs.get(\"mask_decode_mode\", \"accurate\") == \"tradeoff\":\n                    tradeoff_factor = kwargs.get(\"tradeoff_factor\", 0.0)\n                    mask_res = (selected_masks.shape[2], selected_masks.shape[1])\n                    full_res = (orig_w, orig_h)\n                    target_res = (\n                        int(\n                            mask_res[0] * (1 - tradeoff_factor)\n                            + full_res[0] * tradeoff_factor\n                        ),\n                        int(\n                            mask_res[1] * (1 - tradeoff_factor)\n                            + full_res[1] * tradeoff_factor\n                        ),\n                    )\n                    new_masks = []\n                    for mask in selected_masks:\n                        new_masks.append(\n                            cv2.resize(mask, target_res, interpolation=cv2.INTER_LINEAR)\n                        )\n                    selected_masks = np.stack(new_masks, axis=0)\n\n            selected_masks = selected_masks &gt; 0\n\n            cxcy = selected_boxes[:, :2]\n            wh = selected_boxes[:, 2:]\n            xy_min = cxcy - 0.5 * wh\n            xy_max = cxcy + 0.5 * wh\n            boxes_xyxy = np.concatenate([xy_min, xy_max], axis=1)\n\n            if self.resize_method == \"Stretch to\":\n                scale_fct = np.array([orig_w, orig_h, orig_w, orig_h], dtype=np.float32)\n                boxes_xyxy *= scale_fct\n            else:\n                input_h, input_w = self.img_size_h, self.img_size_w\n\n                scale = min(input_w / orig_w, input_h / orig_h)\n                scaled_w = int(orig_w * scale)\n                scaled_h = int(orig_h * scale)\n\n                pad_x = (input_w - scaled_w) / 2\n                pad_y = (input_h - scaled_h) / 2\n\n                boxes_input = boxes_xyxy * np.array(\n                    [input_w, input_h, input_w, input_h], dtype=np.float32\n                )\n\n                boxes_input[:, 0] -= pad_x\n                boxes_input[:, 1] -= pad_y\n                boxes_input[:, 2] -= pad_x\n                boxes_input[:, 3] -= pad_y\n\n                boxes_xyxy = boxes_input / scale\n\n            np.clip(\n                boxes_xyxy,\n                [0, 0, 0, 0],\n                [orig_w, orig_h, orig_w, orig_h],\n                out=boxes_xyxy,\n            )\n\n            batch_predictions = np.column_stack(\n                (\n                    boxes_xyxy,\n                    topk_scores,\n                    np.zeros((len(topk_scores), 1), dtype=np.float32),\n                    topk_labels,\n                )\n            )\n            batch_predictions = batch_predictions[\n                batch_predictions[:, 6] &lt; len(self.class_names)\n            ]\n            selected_masks = selected_masks[\n                batch_predictions[:, 6] &lt; len(self.class_names)\n            ]\n\n            outputs = []\n            for pred, mask in zip(batch_predictions, selected_masks):\n                outputs.append(list(pred) + [mask])\n\n            processed_predictions.append(outputs)\n\n        res = self.make_response(processed_predictions, img_dims, **kwargs)\n        return res\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n        \"\"\"\n\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n\n        predictions = predictions[\n            : len(img_dims)\n        ]  # If the batch size was fixed we have empty preds at the end\n\n        batch_mask_preds = []\n        for image_ind in range(len(img_dims)):\n            masks = [pred[7] for pred in predictions[image_ind]]\n            orig_h, orig_w = img_dims[image_ind]\n\n            mask_preds = []\n            for mask in masks:\n                points = mask2poly(mask.astype(np.uint8))\n                new_points = []\n                prediction_h, prediction_w = mask.shape[0], mask.shape[1]\n                for point in points:\n                    if self.resize_method == \"Stretch to\":\n                        new_x = point[0] * (orig_w / prediction_w)\n                        new_y = point[1] * (orig_h / prediction_h)\n                    else:\n                        scale = max(orig_w / prediction_w, orig_h / prediction_h)\n                        pad_x = (orig_w - prediction_w * scale) / 2\n                        pad_y = (orig_h - prediction_h * scale) / 2\n                        new_x = point[0] * scale + pad_x\n                        new_y = point[1] * scale + pad_y\n                    new_points.append(np.array([new_x, new_y]))\n                mask_preds.append(new_points)\n            batch_mask_preds.append(mask_preds)\n\n        responses = [\n            InstanceSegmentationInferenceResponse(\n                predictions=[\n                    InstanceSegmentationPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                            \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                        }\n                    )\n                    for pred, mask in zip(batch_predictions, batch_mask_preds[ind])\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRInstanceSegmentation.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.</p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n    \"\"\"\n\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n\n    predictions = predictions[\n        : len(img_dims)\n    ]  # If the batch size was fixed we have empty preds at the end\n\n    batch_mask_preds = []\n    for image_ind in range(len(img_dims)):\n        masks = [pred[7] for pred in predictions[image_ind]]\n        orig_h, orig_w = img_dims[image_ind]\n\n        mask_preds = []\n        for mask in masks:\n            points = mask2poly(mask.astype(np.uint8))\n            new_points = []\n            prediction_h, prediction_w = mask.shape[0], mask.shape[1]\n            for point in points:\n                if self.resize_method == \"Stretch to\":\n                    new_x = point[0] * (orig_w / prediction_w)\n                    new_y = point[1] * (orig_h / prediction_h)\n                else:\n                    scale = max(orig_w / prediction_w, orig_h / prediction_h)\n                    pad_x = (orig_w - prediction_w * scale) / 2\n                    pad_y = (orig_h - prediction_h * scale) / 2\n                    new_x = point[0] * scale + pad_x\n                    new_y = point[1] * scale + pad_y\n                new_points.append(np.array([new_x, new_y]))\n            mask_preds.append(new_points)\n        batch_mask_preds.append(mask_preds)\n\n    responses = [\n        InstanceSegmentationInferenceResponse(\n            predictions=[\n                InstanceSegmentationPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                        \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                    }\n                )\n                for pred, mask in zip(batch_predictions, batch_mask_preds[ind])\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRInstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session with the RFDETR model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class IDs.</p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>def predict(self, img_in: ImageMetaType, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session with the RFDETR model.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class IDs.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )\n    bboxes = predictions[0]\n    logits = predictions[1]\n    masks = predictions[2]\n\n    return (bboxes, logits, masks)\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRObjectDetection","title":"<code>RFDETRObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection with the RFDETR model.</p> <p>This class is responsible for performing object detection using the RFDETR model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>class RFDETRObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection with the RFDETR model.\n\n    This class is responsible for performing object detection using the RFDETR model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    preprocess_means = [0.485, 0.456, 0.406]\n    preprocess_stds = [0.229, 0.224, 0.225]\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the RFDETR model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def preproc_image(\n        self,\n        image: Union[Any, InferenceRequestImage],\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        \"\"\"\n        Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n        Args:\n            image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n        \"\"\"\n        if isinstance(image, Image.Image) and USE_PYTORCH_FOR_PREPROCESSING:\n            if CUDA_IS_AVAILABLE:\n                np_image = torch.from_numpy(np.asarray(image, copy=False)).cuda()\n            else:\n                np_image = torch.from_numpy(np.asarray(image, copy=False))\n            is_bgr = False\n        else:\n            np_image, is_bgr = load_image(\n                image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient\n                or \"auto-orient\" not in self.preproc.keys()\n                or DISABLE_PREPROC_AUTO_ORIENT,\n            )\n        if USE_PYTORCH_FOR_PREPROCESSING:\n            if not isinstance(np_image, torch.Tensor):\n                np_image = torch.from_numpy(np_image)\n            if torch.cuda.is_available():\n                np_image = np_image.cuda()\n\n        preprocessed_image, img_dims = self.preprocess_image(\n            np_image,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        if USE_PYTORCH_FOR_PREPROCESSING:\n            preprocessed_image = (\n                preprocessed_image.permute(2, 0, 1).unsqueeze(0).contiguous()\n            )\n            preprocessed_image = preprocessed_image.float()\n\n            preprocessed_image /= 255.0\n\n            means = torch.tensor(\n                self.preprocess_means, device=preprocessed_image.device\n            ).view(3, 1, 1)\n            stds = torch.tensor(\n                self.preprocess_stds, device=preprocessed_image.device\n            ).view(3, 1, 1)\n            preprocessed_image = (preprocessed_image - means) / stds\n        else:\n            preprocessed_image = preprocessed_image.astype(np.float32)\n            preprocessed_image /= 255.0\n\n            preprocessed_image[:, :, 0] = (\n                preprocessed_image[:, :, 0] - self.preprocess_means[0]\n            ) / self.preprocess_stds[0]\n            preprocessed_image[:, :, 1] = (\n                preprocessed_image[:, :, 1] - self.preprocess_means[1]\n            ) / self.preprocess_stds[1]\n            preprocessed_image[:, :, 2] = (\n                preprocessed_image[:, :, 2] - self.preprocess_means[2]\n            ) / self.preprocess_stds[2]\n\n        if self.resize_method == \"Stretch to\":\n            if isinstance(preprocessed_image, np.ndarray):\n                preprocessed_image = preprocessed_image.astype(np.float32)\n                resized = cv2.resize(\n                    preprocessed_image,\n                    (self.img_size_w, self.img_size_h),\n                )\n            elif USE_PYTORCH_FOR_PREPROCESSING:\n                resized = torch.nn.functional.interpolate(\n                    preprocessed_image,\n                    size=(self.img_size_h, self.img_size_w),\n                    mode=\"bilinear\",\n                )\n            else:\n                raise ValueError(\n                    f\"Received an image of unknown type, {type(preprocessed_image)}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n\n        elif self.resize_method == \"Fit (black edges) in\":\n            resized = letterbox_image(\n                preprocessed_image, (self.img_size_w, self.img_size_h)\n            )\n        elif self.resize_method == \"Fit (white edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(255, 255, 255),\n            )\n        elif self.resize_method == \"Fit (grey edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(114, 114, 114),\n            )\n\n        if is_bgr:\n            if isinstance(resized, np.ndarray):\n                resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n            else:\n                resized = resized[:, [2, 1, 0], :, :]\n\n        if isinstance(resized, np.ndarray):\n            img_in = np.transpose(resized, (2, 0, 1))\n            img_in = img_in.astype(np.float32)\n            img_in = np.expand_dims(img_in, axis=0)\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            img_in = resized.float()\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(resized)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n        return img_in, img_dims\n\n    def preprocess(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        fix_batch_size: bool = False,\n        **kwargs,\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if not USE_PYTORCH_FOR_PREPROCESSING:\n            img_in = img_in.astype(np.float32)\n        else:\n            img_in = img_in.float()\n\n        if self.batching_enabled:\n            batch_padding = 0\n            if FIX_BATCH_SIZE or fix_batch_size:\n                if MAX_BATCH_SIZE == float(\"inf\"):\n                    logger.warning(\n                        \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                    )\n                    batch_padding = 0\n                else:\n                    batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n            if batch_padding &lt; 0:\n                raise ValueError(\n                    f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                    f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                    f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                    f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n                )\n            else:\n                raise ValueError(\n                    f\"Received an image of unknown type, {type(img_in)}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"disable_preproc_static_crop\": disable_preproc_static_crop,\n            }\n        )\n\n    def predict(self, img_in: ImageMetaType, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session with the RFDETR model.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class IDs.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )\n        bboxes = predictions[0]\n        logits = predictions[1]\n\n        return (bboxes, logits)\n\n    def sigmoid_stable(self, x):\n        # More efficient, branchless, numerically stable sigmoid computation\n        z = np.exp(-np.abs(x))\n        return np.where(x &gt;= 0, 1 / (1 + z), z / (1 + z))\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        bboxes, logits = predictions\n        bboxes = bboxes.astype(np.float32)\n        logits = logits.astype(np.float32)\n\n        batch_size, num_queries, num_classes = logits.shape\n        logits_sigmoid = self.sigmoid_stable(logits)\n\n        img_dims = preproc_return_metadata[\"img_dims\"]\n\n        processed_predictions = []\n\n        for batch_idx in range(batch_size):\n            orig_h, orig_w = img_dims[batch_idx]\n\n            logits_flat = logits_sigmoid[batch_idx].reshape(-1)\n\n            # Use argpartition for better performance when max_detections is smaller than logits_flat\n            partition_indices = np.argpartition(-logits_flat, max_detections)[\n                :max_detections\n            ]\n            sorted_indices = partition_indices[\n                np.argsort(-logits_flat[partition_indices])\n            ]\n            topk_scores = logits_flat[sorted_indices]\n\n            conf_mask = topk_scores &gt; confidence\n            sorted_indices = sorted_indices[conf_mask]\n            topk_scores = topk_scores[conf_mask]\n\n            topk_boxes = sorted_indices // num_classes\n            topk_labels = sorted_indices % num_classes\n\n            if self.is_one_indexed:\n                class_filter_mask = topk_labels != self.background_class_index\n\n                topk_labels[topk_labels &gt; self.background_class_index] -= 1\n                topk_scores = topk_scores[class_filter_mask]\n                topk_labels = topk_labels[class_filter_mask]\n                topk_boxes = topk_boxes[class_filter_mask]\n\n            selected_boxes = bboxes[batch_idx, topk_boxes]\n\n            cxcy = selected_boxes[:, :2]\n            wh = selected_boxes[:, 2:]\n            xy_min = cxcy - 0.5 * wh\n            xy_max = cxcy + 0.5 * wh\n            boxes_xyxy = np.concatenate([xy_min, xy_max], axis=1)\n\n            if self.resize_method == \"Stretch to\":\n                scale_fct = np.array([orig_w, orig_h, orig_w, orig_h], dtype=np.float32)\n                boxes_xyxy *= scale_fct\n            else:\n                input_h, input_w = self.img_size_h, self.img_size_w\n\n                scale = min(input_w / orig_w, input_h / orig_h)\n                scaled_w = int(orig_w * scale)\n                scaled_h = int(orig_h * scale)\n\n                pad_x = (input_w - scaled_w) / 2\n                pad_y = (input_h - scaled_h) / 2\n\n                boxes_input = boxes_xyxy * np.array(\n                    [input_w, input_h, input_w, input_h], dtype=np.float32\n                )\n\n                boxes_input[:, 0] -= pad_x\n                boxes_input[:, 1] -= pad_y\n                boxes_input[:, 2] -= pad_x\n                boxes_input[:, 3] -= pad_y\n\n                boxes_xyxy = boxes_input / scale\n\n            np.clip(\n                boxes_xyxy,\n                [0, 0, 0, 0],\n                [orig_w, orig_h, orig_w, orig_h],\n                out=boxes_xyxy,\n            )\n\n            batch_predictions = np.column_stack(\n                (\n                    boxes_xyxy,\n                    topk_scores,\n                    np.zeros((len(topk_scores), 1), dtype=np.float32),\n                    topk_labels,\n                )\n            )\n            batch_predictions = batch_predictions[\n                batch_predictions[:, 6] &lt; len(self.class_names)\n            ]\n\n            processed_predictions.append(batch_predictions)\n\n        res = self.make_response(processed_predictions, img_dims, **kwargs)\n        return res\n\n    def initialize_model(self, **kwargs) -&gt; None:\n        \"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n        logger.debug(\"Getting model artefacts\")\n        self.get_model_artifacts(**kwargs)\n\n        input_resolution = self.environment.get(\"RESOLUTION\")\n        if input_resolution is None:\n            input_resolution = self.preproc.get(\"resize\", {}).get(\"width\")\n        if isinstance(input_resolution, (list, tuple)):\n            input_resolution = input_resolution[0]\n        try:\n            input_resolution = int(input_resolution)\n        except (TypeError, ValueError):\n            input_resolution = None\n        if (\n            input_resolution is not None\n            and input_resolution &gt;= RFDETR_ONNX_MAX_RESOLUTION\n        ):\n            logger.error(\n                \"NOT loading '%s' model, input resolution is '%s', ONNX max resolution limit set to '%s' (limit can be increased via RFDETR_ONNX_MAX_RESOLUTION env variable)\",\n                self.endpoint,\n                input_resolution,\n                RFDETR_ONNX_MAX_RESOLUTION,\n            )\n            raise CannotInitialiseModelError(f\"Resolution too high for RFDETR\")\n\n        logger.debug(\"Creating inference session\")\n        if self.load_weights or not self.has_model_metadata:\n            t1_session = perf_counter()\n            providers = get_onnxruntime_execution_providers(\n                ONNXRUNTIME_EXECUTION_PROVIDERS\n            )\n\n            if not self.load_weights:\n                providers = [\n                    \"CPUExecutionProvider\"\n                ]  # \"OpenVINOExecutionProvider\" dropped until further investigation is done\n\n            try:\n                session_options = onnxruntime.SessionOptions()\n                session_options.log_severity_level = 3\n                # TensorRT does better graph optimization for its EP than onnx\n                if has_trt(providers):\n                    session_options.graph_optimization_level = (\n                        onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n                    )\n                expanded_execution_providers = []\n                for ep in self.onnxruntime_execution_providers:\n                    if ep == \"TensorrtExecutionProvider\":\n                        ep = (\n                            \"TensorrtExecutionProvider\",\n                            {\n                                \"trt_max_workspace_size\": str(1 &lt;&lt; 30),\n                                \"trt_engine_cache_enable\": True,\n                                \"trt_engine_cache_path\": os.path.join(\n                                    TENSORRT_CACHE_PATH, self.endpoint\n                                ),\n                                \"trt_fp16_enable\": True,\n                                \"trt_dump_subgraphs\": False,\n                                \"trt_force_sequential_engine_build\": False,\n                                \"trt_dla_enable\": False,\n                            },\n                        )\n                    expanded_execution_providers.append(ep)\n\n                if \"OpenVINOExecutionProvider\" in expanded_execution_providers:\n                    expanded_execution_providers.remove(\"OpenVINOExecutionProvider\")\n\n                self.onnx_session = onnxruntime.InferenceSession(\n                    self.cache_file(self.weights_file),\n                    providers=expanded_execution_providers,\n                    sess_options=session_options,\n                )\n            except Exception as e:\n                self.clear_cache()\n                raise ModelArtefactError(\n                    f\"Unable to load ONNX session. Cause: {e}\"\n                ) from e\n            logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n            inputs = self.onnx_session.get_inputs()[0]\n            input_shape = inputs.shape\n            self.batch_size = input_shape[0]\n            self.img_size_h = input_shape[2]\n            self.img_size_w = input_shape[3]\n            self.input_name = inputs.name\n            if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n                if \"resize\" in self.preproc:\n                    self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                    self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n                else:\n                    self.img_size_h = 640\n                    self.img_size_w = 640\n\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n\n            model_metadata = {\n                \"batch_size\": self.batch_size,\n                \"img_size_h\": self.img_size_h,\n                \"img_size_w\": self.img_size_w,\n            }\n            logger.debug(f\"Writing model metadata to memcache\")\n            self.write_model_metadata_to_memcache(model_metadata)\n            if not self.load_weights:  # had to load weights to get metadata\n                del self.onnx_session\n        else:\n            if not self.has_model_metadata:\n                raise ValueError(\n                    \"This should be unreachable, should get weights if we don't have model metadata\"\n                )\n            logger.debug(f\"Loading model metadata from memcache\")\n            metadata = self.model_metadata_from_memcache()\n            self.batch_size = metadata[\"batch_size\"]\n            self.img_size_h = metadata[\"img_size_h\"]\n            self.img_size_w = metadata[\"img_size_w\"]\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n\n        if ROBOFLOW_BACKGROUND_CLASS in self.class_names:\n            self.is_one_indexed = True\n            self.background_class_index = self.class_names.index(\n                ROBOFLOW_BACKGROUND_CLASS\n            )\n            self.class_names = (\n                self.class_names[: self.background_class_index]\n                + self.class_names[self.background_class_index + 1 :]\n            )\n        else:\n            self.is_one_indexed = False\n        logger.debug(\"Model initialisation finished.\")\n\n    def validate_model_classes(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRObjectDetection.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the RFDETR model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRObjectDetection.initialize_model","title":"<code>initialize_model(**kwargs)</code>","text":"<p>Initializes the ONNX model, setting up the inference session and other necessary properties.</p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>def initialize_model(self, **kwargs) -&gt; None:\n    \"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n    logger.debug(\"Getting model artefacts\")\n    self.get_model_artifacts(**kwargs)\n\n    input_resolution = self.environment.get(\"RESOLUTION\")\n    if input_resolution is None:\n        input_resolution = self.preproc.get(\"resize\", {}).get(\"width\")\n    if isinstance(input_resolution, (list, tuple)):\n        input_resolution = input_resolution[0]\n    try:\n        input_resolution = int(input_resolution)\n    except (TypeError, ValueError):\n        input_resolution = None\n    if (\n        input_resolution is not None\n        and input_resolution &gt;= RFDETR_ONNX_MAX_RESOLUTION\n    ):\n        logger.error(\n            \"NOT loading '%s' model, input resolution is '%s', ONNX max resolution limit set to '%s' (limit can be increased via RFDETR_ONNX_MAX_RESOLUTION env variable)\",\n            self.endpoint,\n            input_resolution,\n            RFDETR_ONNX_MAX_RESOLUTION,\n        )\n        raise CannotInitialiseModelError(f\"Resolution too high for RFDETR\")\n\n    logger.debug(\"Creating inference session\")\n    if self.load_weights or not self.has_model_metadata:\n        t1_session = perf_counter()\n        providers = get_onnxruntime_execution_providers(\n            ONNXRUNTIME_EXECUTION_PROVIDERS\n        )\n\n        if not self.load_weights:\n            providers = [\n                \"CPUExecutionProvider\"\n            ]  # \"OpenVINOExecutionProvider\" dropped until further investigation is done\n\n        try:\n            session_options = onnxruntime.SessionOptions()\n            session_options.log_severity_level = 3\n            # TensorRT does better graph optimization for its EP than onnx\n            if has_trt(providers):\n                session_options.graph_optimization_level = (\n                    onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n                )\n            expanded_execution_providers = []\n            for ep in self.onnxruntime_execution_providers:\n                if ep == \"TensorrtExecutionProvider\":\n                    ep = (\n                        \"TensorrtExecutionProvider\",\n                        {\n                            \"trt_max_workspace_size\": str(1 &lt;&lt; 30),\n                            \"trt_engine_cache_enable\": True,\n                            \"trt_engine_cache_path\": os.path.join(\n                                TENSORRT_CACHE_PATH, self.endpoint\n                            ),\n                            \"trt_fp16_enable\": True,\n                            \"trt_dump_subgraphs\": False,\n                            \"trt_force_sequential_engine_build\": False,\n                            \"trt_dla_enable\": False,\n                        },\n                    )\n                expanded_execution_providers.append(ep)\n\n            if \"OpenVINOExecutionProvider\" in expanded_execution_providers:\n                expanded_execution_providers.remove(\"OpenVINOExecutionProvider\")\n\n            self.onnx_session = onnxruntime.InferenceSession(\n                self.cache_file(self.weights_file),\n                providers=expanded_execution_providers,\n                sess_options=session_options,\n            )\n        except Exception as e:\n            self.clear_cache()\n            raise ModelArtefactError(\n                f\"Unable to load ONNX session. Cause: {e}\"\n            ) from e\n        logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n        inputs = self.onnx_session.get_inputs()[0]\n        input_shape = inputs.shape\n        self.batch_size = input_shape[0]\n        self.img_size_h = input_shape[2]\n        self.img_size_w = input_shape[3]\n        self.input_name = inputs.name\n        if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n            if \"resize\" in self.preproc:\n                self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n            else:\n                self.img_size_h = 640\n                self.img_size_w = 640\n\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n\n        model_metadata = {\n            \"batch_size\": self.batch_size,\n            \"img_size_h\": self.img_size_h,\n            \"img_size_w\": self.img_size_w,\n        }\n        logger.debug(f\"Writing model metadata to memcache\")\n        self.write_model_metadata_to_memcache(model_metadata)\n        if not self.load_weights:  # had to load weights to get metadata\n            del self.onnx_session\n    else:\n        if not self.has_model_metadata:\n            raise ValueError(\n                \"This should be unreachable, should get weights if we don't have model metadata\"\n            )\n        logger.debug(f\"Loading model metadata from memcache\")\n        metadata = self.model_metadata_from_memcache()\n        self.batch_size = metadata[\"batch_size\"]\n        self.img_size_h = metadata[\"img_size_h\"]\n        self.img_size_w = metadata[\"img_size_w\"]\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n\n    if ROBOFLOW_BACKGROUND_CLASS in self.class_names:\n        self.is_one_indexed = True\n        self.background_class_index = self.class_names.index(\n            ROBOFLOW_BACKGROUND_CLASS\n        )\n        self.class_names = (\n            self.class_names[: self.background_class_index]\n            + self.class_names[self.background_class_index + 1 :]\n        )\n    else:\n        self.is_one_indexed = False\n    logger.debug(\"Model initialisation finished.\")\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session with the RFDETR model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class IDs.</p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>def predict(self, img_in: ImageMetaType, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session with the RFDETR model.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class IDs.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )\n    bboxes = predictions[0]\n    logits = predictions[1]\n\n    return (bboxes, logits)\n</code></pre>"},{"location":"reference/inference/models/rfdetr/rfdetr/#inference.models.rfdetr.rfdetr.RFDETRObjectDetection.preproc_image","title":"<code>preproc_image(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Any, InferenceRequestImage]</code> <p>An object containing information necessary to load the image for inference.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.</p> Source code in <code>inference/models/rfdetr/rfdetr.py</code> <pre><code>def preproc_image(\n    self,\n    image: Union[Any, InferenceRequestImage],\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n    Args:\n        image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n    \"\"\"\n    if isinstance(image, Image.Image) and USE_PYTORCH_FOR_PREPROCESSING:\n        if CUDA_IS_AVAILABLE:\n            np_image = torch.from_numpy(np.asarray(image, copy=False)).cuda()\n        else:\n            np_image = torch.from_numpy(np.asarray(image, copy=False))\n        is_bgr = False\n    else:\n        np_image, is_bgr = load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient\n            or \"auto-orient\" not in self.preproc.keys()\n            or DISABLE_PREPROC_AUTO_ORIENT,\n        )\n    if USE_PYTORCH_FOR_PREPROCESSING:\n        if not isinstance(np_image, torch.Tensor):\n            np_image = torch.from_numpy(np_image)\n        if torch.cuda.is_available():\n            np_image = np_image.cuda()\n\n    preprocessed_image, img_dims = self.preprocess_image(\n        np_image,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    if USE_PYTORCH_FOR_PREPROCESSING:\n        preprocessed_image = (\n            preprocessed_image.permute(2, 0, 1).unsqueeze(0).contiguous()\n        )\n        preprocessed_image = preprocessed_image.float()\n\n        preprocessed_image /= 255.0\n\n        means = torch.tensor(\n            self.preprocess_means, device=preprocessed_image.device\n        ).view(3, 1, 1)\n        stds = torch.tensor(\n            self.preprocess_stds, device=preprocessed_image.device\n        ).view(3, 1, 1)\n        preprocessed_image = (preprocessed_image - means) / stds\n    else:\n        preprocessed_image = preprocessed_image.astype(np.float32)\n        preprocessed_image /= 255.0\n\n        preprocessed_image[:, :, 0] = (\n            preprocessed_image[:, :, 0] - self.preprocess_means[0]\n        ) / self.preprocess_stds[0]\n        preprocessed_image[:, :, 1] = (\n            preprocessed_image[:, :, 1] - self.preprocess_means[1]\n        ) / self.preprocess_stds[1]\n        preprocessed_image[:, :, 2] = (\n            preprocessed_image[:, :, 2] - self.preprocess_means[2]\n        ) / self.preprocess_stds[2]\n\n    if self.resize_method == \"Stretch to\":\n        if isinstance(preprocessed_image, np.ndarray):\n            preprocessed_image = preprocessed_image.astype(np.float32)\n            resized = cv2.resize(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n            )\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            resized = torch.nn.functional.interpolate(\n                preprocessed_image,\n                size=(self.img_size_h, self.img_size_w),\n                mode=\"bilinear\",\n            )\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(preprocessed_image)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n    elif self.resize_method == \"Fit (black edges) in\":\n        resized = letterbox_image(\n            preprocessed_image, (self.img_size_w, self.img_size_h)\n        )\n    elif self.resize_method == \"Fit (white edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(255, 255, 255),\n        )\n    elif self.resize_method == \"Fit (grey edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(114, 114, 114),\n        )\n\n    if is_bgr:\n        if isinstance(resized, np.ndarray):\n            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        else:\n            resized = resized[:, [2, 1, 0], :, :]\n\n    if isinstance(resized, np.ndarray):\n        img_in = np.transpose(resized, (2, 0, 1))\n        img_in = img_in.astype(np.float32)\n        img_in = np.expand_dims(img_in, axis=0)\n    elif USE_PYTORCH_FOR_PREPROCESSING:\n        img_in = resized.float()\n    else:\n        raise ValueError(\n            f\"Received an image of unknown type, {type(resized)}; \"\n            \"This is most likely a bug. Contact Roboflow team through github issues \"\n            \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n        )\n    return img_in, img_dims\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/","title":"Segment anything","text":""},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything","title":"<code>SegmentAnything</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>SegmentAnything class for handling segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>sam</code> <p>The segmentation model.</p> <code>predictor</code> <p>The predictor for the segmentation model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> <code>embedding_cache</code> <p>Cache for embeddings.</p> <code>image_size_cache</code> <p>Cache for image sizes.</p> <code>embedding_cache_keys</code> <p>Keys for the embedding cache.</p> <code>low_res_logits_cache</code> <p>Cache for low resolution logits.</p> <code>segmentation_cache_keys</code> <p>Keys for the segmentation cache.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>class SegmentAnything(RoboflowCoreModel):\n    \"\"\"SegmentAnything class for handling segmentation tasks.\n\n    Attributes:\n        sam: The segmentation model.\n        predictor: The predictor for the segmentation model.\n        ort_session: ONNX runtime inference session.\n        embedding_cache: Cache for embeddings.\n        image_size_cache: Cache for image sizes.\n        embedding_cache_keys: Keys for the embedding cache.\n        low_res_logits_cache: Cache for low resolution logits.\n        segmentation_cache_keys: Keys for the segmentation cache.\n    \"\"\"\n\n    def __init__(self, *args, model_id: str = f\"sam/{SAM_VERSION_ID}\", **kwargs):\n        \"\"\"Initializes the SegmentAnything.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, model_id=model_id, **kwargs)\n        self.sam = sam_model_registry[self.version_id](\n            checkpoint=self.cache_file(\"encoder.pth\")\n        )\n        self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.predictor = SamPredictor(self.sam)\n        self.ort_session = onnxruntime.InferenceSession(\n            self.cache_file(\"decoder.onnx\"),\n            providers=[\n                \"CUDAExecutionProvider\",\n                \"OpenVINOExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n        self._state_lock = Lock()\n        self.embedding_cache = {}\n        self.image_size_cache = {}\n        self.embedding_cache_keys = []\n\n        self.low_res_logits_cache = {}\n        self.segmentation_cache_keys = []\n        self.task_type = \"unsupervised-segmentation\"\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: List of file names.\n        \"\"\"\n        return [\"encoder.pth\", \"decoder.onnx\"]\n\n    def embed_image(self, image: Any, image_id: Optional[str] = None, **kwargs):\n        \"\"\"\n        Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n        the cached result will be returned.\n\n        Args:\n            image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n            image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                      with this ID. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                               and the second element is the shape (height, width) of the processed image.\n\n        Notes:\n            - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n\n        Example:\n            &gt;&gt;&gt; img_array = ... # some image array\n            &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n            (array([...]), (224, 224))\n        \"\"\"\n        if image_id and image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n            )\n        img_in = self.preproc_image(image)\n        self.predictor.set_image(img_in)\n        embedding = self.predictor.get_image_embedding().cpu().numpy()\n        if image_id:\n            self.embedding_cache[image_id] = embedding\n            self.image_size_cache[image_id] = img_in.shape[:2]\n            self.embedding_cache_keys.append(image_id)\n            if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.embedding_cache_keys.pop(0)\n                del self.embedding_cache[cache_key]\n                del self.image_size_cache[cache_key]\n        return (embedding, img_in.shape[:2])\n\n    def infer_from_request(self, request: SamInferenceRequest):\n        \"\"\"Performs inference based on the request type.\n\n        Args:\n            request (SamInferenceRequest): The inference request.\n\n        Returns:\n            Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n        \"\"\"\n        with self._state_lock:\n            t1 = perf_counter()\n            if isinstance(request, SamEmbeddingRequest):\n                embedding, _ = self.embed_image(**request.dict())\n                inference_time = perf_counter() - t1\n                if request.format == \"json\":\n                    return SamEmbeddingResponse(\n                        embeddings=embedding.tolist(), time=inference_time\n                    )\n                elif request.format == \"binary\":\n                    binary_vector = BytesIO()\n                    np.save(binary_vector, embedding)\n                    binary_vector.seek(0)\n                    return SamEmbeddingResponse(\n                        embeddings=binary_vector.getvalue(), time=inference_time\n                    )\n            elif isinstance(request, SamSegmentationRequest):\n                masks, low_res_masks = self.segment_image(**request.dict())\n                if request.format == \"json\":\n                    masks = masks &gt; self.predictor.model.mask_threshold\n                    masks = masks2poly(masks)\n                    low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n                    low_res_masks = masks2poly(low_res_masks)\n                elif request.format == \"binary\":\n                    binary_vector = BytesIO()\n                    np.savez_compressed(\n                        binary_vector, masks=masks, low_res_masks=low_res_masks\n                    )\n                    binary_vector.seek(0)\n                    binary_data = binary_vector.getvalue()\n                    return binary_data\n                else:\n                    raise ValueError(f\"Invalid format {request.format}\")\n\n                response = SamSegmentationResponse(\n                    masks=[m.tolist() for m in masks],\n                    low_res_masks=[m.tolist() for m in low_res_masks],\n                    time=perf_counter() - t1,\n                )\n                return response\n\n    def preproc_image(self, image: InferenceRequestImage):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image\n\n    def segment_image(\n        self,\n        image: Any,\n        embeddings: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        embeddings_format: Optional[str] = \"json\",\n        has_mask_input: Optional[bool] = False,\n        image_id: Optional[str] = None,\n        mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n        mask_input_format: Optional[str] = \"json\",\n        orig_im_size: Optional[List[int]] = None,\n        point_coords: Optional[List[List[float]]] = [],\n        point_labels: Optional[List[int]] = [],\n        use_mask_input_cache: Optional[bool] = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Segments an image based on provided embeddings, points, masks, or cached results.\n        If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n        Args:\n            image (Any): The image to be segmented.\n            embeddings (Optional[Union[np.ndarray, List[List[float]]]]): The embeddings of the image.\n                Defaults to None, in which case the image is used to compute embeddings.\n            embeddings_format (Optional[str]): Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.\n            has_mask_input (Optional[bool]): Specifies whether mask input is provided. Defaults to False.\n            image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n            mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input mask for the image.\n            mask_input_format (Optional[str]): Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.\n            orig_im_size (Optional[List[int]]): Original size of the image when providing embeddings directly.\n            point_coords (Optional[List[List[float]]]): Coordinates of points in the image. Defaults to an empty list.\n            point_labels (Optional[List[int]]): Labels associated with the provided points. Defaults to an empty list.\n            use_mask_input_cache (Optional[bool]): Flag to determine if cached mask input should be used. Defaults to True.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image\n                                          and the second element is the low resolution segmentation masks.\n\n        Raises:\n            ValueError: If necessary inputs are missing or inconsistent.\n\n        Notes:\n            - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n              on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n        \"\"\"\n        if not embeddings:\n            if not image and not image_id:\n                raise ValueError(\n                    \"Must provide either image, cached image_id, or embeddings\"\n                )\n            elif image_id and not image and image_id not in self.embedding_cache:\n                raise ValueError(\n                    f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n                )\n            embedding, original_image_size = self.embed_image(\n                image=image, image_id=image_id\n            )\n        else:\n            if not orig_im_size:\n                raise ValueError(\n                    \"Must provide original image size if providing embeddings\"\n                )\n            original_image_size = orig_im_size\n            if embeddings_format == \"json\":\n                embedding = np.array(embeddings)\n            elif embeddings_format == \"binary\":\n                embedding = np.load(BytesIO(embeddings))\n\n        point_coords = point_coords\n        point_coords.append([0, 0])\n        point_coords = np.array(point_coords, dtype=np.float32)\n        point_coords = np.expand_dims(point_coords, axis=0)\n        point_coords = self.predictor.transform.apply_coords(\n            point_coords,\n            original_image_size,\n        )\n\n        point_labels = point_labels\n        point_labels.append(-1)\n        point_labels = np.array(point_labels, dtype=np.float32)\n        point_labels = np.expand_dims(point_labels, axis=0)\n\n        if has_mask_input:\n            if (\n                image_id\n                and image_id in self.low_res_logits_cache\n                and use_mask_input_cache\n            ):\n                mask_input = self.low_res_logits_cache[image_id]\n            elif not mask_input and (\n                not image_id or image_id not in self.low_res_logits_cache\n            ):\n                raise ValueError(\"Must provide either mask_input or cached image_id\")\n            else:\n                if mask_input_format == \"json\":\n                    polys = mask_input\n                    mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                    for i, poly in enumerate(polys):\n                        poly = ShapelyPolygon(poly)\n                        raster = rasterio.features.rasterize(\n                            [poly], out_shape=(256, 256)\n                        )\n                        mask_input[0, i, :, :] = raster\n                elif mask_input_format == \"binary\":\n                    binary_data = base64.b64decode(mask_input)\n                    mask_input = np.load(BytesIO(binary_data))\n        else:\n            mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n        ort_inputs = {\n            \"image_embeddings\": embedding.astype(np.float32),\n            \"point_coords\": point_coords.astype(np.float32),\n            \"point_labels\": point_labels,\n            \"mask_input\": mask_input.astype(np.float32),\n            \"has_mask_input\": (\n                np.zeros(1, dtype=np.float32)\n                if not has_mask_input\n                else np.ones(1, dtype=np.float32)\n            ),\n            \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n        }\n        masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n        if image_id:\n            self.low_res_logits_cache[image_id] = low_res_logits\n            if image_id not in self.segmentation_cache_keys:\n                self.segmentation_cache_keys.append(image_id)\n            if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.segmentation_cache_keys.pop(0)\n                del self.low_res_logits_cache[cache_key]\n        masks = masks[0]\n        low_res_masks = low_res_logits[0]\n\n        return masks, low_res_masks\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.__init__","title":"<code>__init__(*args, model_id=f'sam/{SAM_VERSION_ID}', **kwargs)</code>","text":"<p>Initializes the SegmentAnything.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def __init__(self, *args, model_id: str = f\"sam/{SAM_VERSION_ID}\", **kwargs):\n    \"\"\"Initializes the SegmentAnything.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, model_id=model_id, **kwargs)\n    self.sam = sam_model_registry[self.version_id](\n        checkpoint=self.cache_file(\"encoder.pth\")\n    )\n    self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.predictor = SamPredictor(self.sam)\n    self.ort_session = onnxruntime.InferenceSession(\n        self.cache_file(\"decoder.onnx\"),\n        providers=[\n            \"CUDAExecutionProvider\",\n            \"OpenVINOExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n    self._state_lock = Lock()\n    self.embedding_cache = {}\n    self.image_size_cache = {}\n    self.embedding_cache_keys = []\n\n    self.low_res_logits_cache = {}\n    self.segmentation_cache_keys = []\n    self.task_type = \"unsupervised-segmentation\"\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.embed_image","title":"<code>embed_image(image, image_id=None, **kwargs)</code>","text":"<p>Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached, the cached result will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be embedded. The format should be compatible with the preproc_image method.</p> required <code>image_id</code> <code>Optional[str]</code> <p>An identifier for the image. If provided, the embedding result will be cached                       with this ID. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image                                and the second element is the shape (height, width) of the processed image.</p> Notes <ul> <li>Embeddings and image sizes are cached to improve performance on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Example <p>img_array = ... # some image array embed_image(img_array, image_id=\"sample123\") (array([...]), (224, 224))</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def embed_image(self, image: Any, image_id: Optional[str] = None, **kwargs):\n    \"\"\"\n    Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n    the cached result will be returned.\n\n    Args:\n        image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n        image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                  with this ID. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                           and the second element is the shape (height, width) of the processed image.\n\n    Notes:\n        - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n\n    Example:\n        &gt;&gt;&gt; img_array = ... # some image array\n        &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n        (array([...]), (224, 224))\n    \"\"\"\n    if image_id and image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n        )\n    img_in = self.preproc_image(image)\n    self.predictor.set_image(img_in)\n    embedding = self.predictor.get_image_embedding().cpu().numpy()\n    if image_id:\n        self.embedding_cache[image_id] = embedding\n        self.image_size_cache[image_id] = img_in.shape[:2]\n        self.embedding_cache_keys.append(image_id)\n        if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.embedding_cache_keys.pop(0)\n            del self.embedding_cache[cache_key]\n            del self.image_size_cache[cache_key]\n    return (embedding, img_in.shape[:2])\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of file names.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: List of file names.\n    \"\"\"\n    return [\"encoder.pth\", \"decoder.onnx\"]\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Performs inference based on the request type.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Type Description <p>Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def infer_from_request(self, request: SamInferenceRequest):\n    \"\"\"Performs inference based on the request type.\n\n    Args:\n        request (SamInferenceRequest): The inference request.\n\n    Returns:\n        Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n    \"\"\"\n    with self._state_lock:\n        t1 = perf_counter()\n        if isinstance(request, SamEmbeddingRequest):\n            embedding, _ = self.embed_image(**request.dict())\n            inference_time = perf_counter() - t1\n            if request.format == \"json\":\n                return SamEmbeddingResponse(\n                    embeddings=embedding.tolist(), time=inference_time\n                )\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.save(binary_vector, embedding)\n                binary_vector.seek(0)\n                return SamEmbeddingResponse(\n                    embeddings=binary_vector.getvalue(), time=inference_time\n                )\n        elif isinstance(request, SamSegmentationRequest):\n            masks, low_res_masks = self.segment_image(**request.dict())\n            if request.format == \"json\":\n                masks = masks &gt; self.predictor.model.mask_threshold\n                masks = masks2poly(masks)\n                low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n                low_res_masks = masks2poly(low_res_masks)\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.savez_compressed(\n                    binary_vector, masks=masks, low_res_masks=low_res_masks\n                )\n                binary_vector.seek(0)\n                binary_data = binary_vector.getvalue()\n                return binary_data\n            else:\n                raise ValueError(f\"Invalid format {request.format}\")\n\n            response = SamSegmentationResponse(\n                masks=[m.tolist() for m in masks],\n                low_res_masks=[m.tolist() for m in low_res_masks],\n                time=perf_counter() - t1,\n            )\n            return response\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image\n</code></pre>"},{"location":"reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.segment_image","title":"<code>segment_image(image, embeddings=None, embeddings_format='json', has_mask_input=False, image_id=None, mask_input=None, mask_input_format='json', orig_im_size=None, point_coords=[], point_labels=[], use_mask_input_cache=True, **kwargs)</code>","text":"<p>Segments an image based on provided embeddings, points, masks, or cached results. If embeddings are not directly provided, the function can derive them from the input image or cache.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be segmented.</p> required <code>embeddings</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>The embeddings of the image. Defaults to None, in which case the image is used to compute embeddings.</p> <code>None</code> <code>embeddings_format</code> <code>Optional[str]</code> <p>Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.</p> <code>'json'</code> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Specifies whether mask input is provided. Defaults to False.</p> <code>False</code> <code>image_id</code> <code>Optional[str]</code> <p>A cached identifier for the image. Useful for accessing cached embeddings or masks.</p> <code>None</code> <code>mask_input</code> <code>Optional[Union[ndarray, List[List[List[float]]]]]</code> <p>Input mask for the image.</p> <code>None</code> <code>mask_input_format</code> <code>Optional[str]</code> <p>Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.</p> <code>'json'</code> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>Original size of the image when providing embeddings directly.</p> <code>None</code> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>Coordinates of points in the image. Defaults to an empty list.</p> <code>[]</code> <code>point_labels</code> <code>Optional[List[int]]</code> <p>Labels associated with the provided points. Defaults to an empty list.</p> <code>[]</code> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Flag to determine if cached mask input should be used. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image                           and the second element is the low resolution segmentation masks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary inputs are missing or inconsistent.</p> Notes <ul> <li>Embeddings, segmentations, and low-resolution logits can be cached to improve performance   on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def segment_image(\n    self,\n    image: Any,\n    embeddings: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    embeddings_format: Optional[str] = \"json\",\n    has_mask_input: Optional[bool] = False,\n    image_id: Optional[str] = None,\n    mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n    mask_input_format: Optional[str] = \"json\",\n    orig_im_size: Optional[List[int]] = None,\n    point_coords: Optional[List[List[float]]] = [],\n    point_labels: Optional[List[int]] = [],\n    use_mask_input_cache: Optional[bool] = True,\n    **kwargs,\n):\n    \"\"\"\n    Segments an image based on provided embeddings, points, masks, or cached results.\n    If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n    Args:\n        image (Any): The image to be segmented.\n        embeddings (Optional[Union[np.ndarray, List[List[float]]]]): The embeddings of the image.\n            Defaults to None, in which case the image is used to compute embeddings.\n        embeddings_format (Optional[str]): Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.\n        has_mask_input (Optional[bool]): Specifies whether mask input is provided. Defaults to False.\n        image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n        mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input mask for the image.\n        mask_input_format (Optional[str]): Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.\n        orig_im_size (Optional[List[int]]): Original size of the image when providing embeddings directly.\n        point_coords (Optional[List[List[float]]]): Coordinates of points in the image. Defaults to an empty list.\n        point_labels (Optional[List[int]]): Labels associated with the provided points. Defaults to an empty list.\n        use_mask_input_cache (Optional[bool]): Flag to determine if cached mask input should be used. Defaults to True.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image\n                                      and the second element is the low resolution segmentation masks.\n\n    Raises:\n        ValueError: If necessary inputs are missing or inconsistent.\n\n    Notes:\n        - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n          on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n    \"\"\"\n    if not embeddings:\n        if not image and not image_id:\n            raise ValueError(\n                \"Must provide either image, cached image_id, or embeddings\"\n            )\n        elif image_id and not image and image_id not in self.embedding_cache:\n            raise ValueError(\n                f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n            )\n        embedding, original_image_size = self.embed_image(\n            image=image, image_id=image_id\n        )\n    else:\n        if not orig_im_size:\n            raise ValueError(\n                \"Must provide original image size if providing embeddings\"\n            )\n        original_image_size = orig_im_size\n        if embeddings_format == \"json\":\n            embedding = np.array(embeddings)\n        elif embeddings_format == \"binary\":\n            embedding = np.load(BytesIO(embeddings))\n\n    point_coords = point_coords\n    point_coords.append([0, 0])\n    point_coords = np.array(point_coords, dtype=np.float32)\n    point_coords = np.expand_dims(point_coords, axis=0)\n    point_coords = self.predictor.transform.apply_coords(\n        point_coords,\n        original_image_size,\n    )\n\n    point_labels = point_labels\n    point_labels.append(-1)\n    point_labels = np.array(point_labels, dtype=np.float32)\n    point_labels = np.expand_dims(point_labels, axis=0)\n\n    if has_mask_input:\n        if (\n            image_id\n            and image_id in self.low_res_logits_cache\n            and use_mask_input_cache\n        ):\n            mask_input = self.low_res_logits_cache[image_id]\n        elif not mask_input and (\n            not image_id or image_id not in self.low_res_logits_cache\n        ):\n            raise ValueError(\"Must provide either mask_input or cached image_id\")\n        else:\n            if mask_input_format == \"json\":\n                polys = mask_input\n                mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                for i, poly in enumerate(polys):\n                    poly = ShapelyPolygon(poly)\n                    raster = rasterio.features.rasterize(\n                        [poly], out_shape=(256, 256)\n                    )\n                    mask_input[0, i, :, :] = raster\n            elif mask_input_format == \"binary\":\n                binary_data = base64.b64decode(mask_input)\n                mask_input = np.load(BytesIO(binary_data))\n    else:\n        mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n    ort_inputs = {\n        \"image_embeddings\": embedding.astype(np.float32),\n        \"point_coords\": point_coords.astype(np.float32),\n        \"point_labels\": point_labels,\n        \"mask_input\": mask_input.astype(np.float32),\n        \"has_mask_input\": (\n            np.zeros(1, dtype=np.float32)\n            if not has_mask_input\n            else np.ones(1, dtype=np.float32)\n        ),\n        \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n    }\n    masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n    if image_id:\n        self.low_res_logits_cache[image_id] = low_res_logits\n        if image_id not in self.segmentation_cache_keys:\n            self.segmentation_cache_keys.append(image_id)\n        if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.segmentation_cache_keys.pop(0)\n            del self.low_res_logits_cache[cache_key]\n    masks = masks[0]\n    low_res_masks = low_res_logits[0]\n\n    return masks, low_res_masks\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/","title":"Segment anything2","text":""},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2","title":"<code>SegmentAnything2</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>SegmentAnything class for handling segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>sam</code> <p>The segmentation model.</p> <code>predictor</code> <p>The predictor for the segmentation model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> <code>embedding_cache</code> <p>Cache for embeddings.</p> <code>image_size_cache</code> <p>Cache for image sizes.</p> <code>embedding_cache_keys</code> <p>Keys for the embedding cache.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>class SegmentAnything2(RoboflowCoreModel):\n    \"\"\"SegmentAnything class for handling segmentation tasks.\n\n    Attributes:\n        sam: The segmentation model.\n        predictor: The predictor for the segmentation model.\n        ort_session: ONNX runtime inference session.\n        embedding_cache: Cache for embeddings.\n        image_size_cache: Cache for image sizes.\n        embedding_cache_keys: Keys for the embedding cache.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        model_id: str = f\"sam2/{SAM2_VERSION_ID}\",\n        low_res_logits_cache_size: int = SAM2_MAX_LOGITS_CACHE_SIZE,\n        embedding_cache_size: int = SAM2_MAX_EMBEDDING_CACHE_SIZE,\n        **kwargs,\n    ):\n        \"\"\"Initializes the SegmentAnything.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, model_id=model_id, **kwargs)\n        checkpoint = self.cache_file(\"weights.pt\")\n        model_cfg = {\n            \"hiera_large\": \"sam2_hiera_l.yaml\",\n            \"hiera_small\": \"sam2_hiera_s.yaml\",\n            \"hiera_tiny\": \"sam2_hiera_t.yaml\",\n            \"hiera_b_plus\": \"sam2_hiera_b+.yaml\",\n        }[self.version_id]\n\n        self.sam = build_sam2(model_cfg, checkpoint, device=DEVICE)\n        self.low_res_logits_cache_size = low_res_logits_cache_size\n        self.embedding_cache_size = embedding_cache_size\n\n        self.predictor = SAM2ImagePredictor(self.sam)\n\n        self.embedding_cache = {}\n        self.image_size_cache = {}\n        self.embedding_cache_keys = []\n        self.low_res_logits_cache: Dict[Tuple[str, str], LogitsCacheType] = {}\n        self.low_res_logits_cache_keys = []\n        self._state_lock = Lock()\n        self.task_type = \"unsupervised-segmentation\"\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: List of file names.\n        \"\"\"\n        return [\"weights.pt\"]\n\n    def embed_image(\n        self,\n        image: Optional[InferenceRequestImage],\n        image_id: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n        the cached result will be returned.\n\n        Args:\n            image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n            image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                      with this ID. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                               and the second element is the shape (height, width) of the processed image.\n\n        Notes:\n            - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM2_MAX_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n\n        Example:\n            &gt;&gt;&gt; img_array = ... # some image array\n            &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n            (array([...]), (224, 224))\n        \"\"\"\n        if image_id and image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n                image_id,\n            )\n\n        img_in = self.preproc_image(image)\n        if image_id is None:\n            image_id = hashlib.md5(img_in.tobytes()).hexdigest()[:12]\n\n        if image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n                image_id,\n            )\n\n        with torch.inference_mode():\n            self.predictor.set_image(img_in)\n            embedding_dict = self.predictor._features\n\n        self.embedding_cache[image_id] = embedding_dict\n        self.image_size_cache[image_id] = img_in.shape[:2]\n        if image_id in self.embedding_cache_keys:\n            self.embedding_cache_keys.remove(image_id)\n        self.embedding_cache_keys.append(image_id)\n        if len(self.embedding_cache_keys) &gt; self.embedding_cache_size:\n            cache_key = self.embedding_cache_keys.pop(0)\n            del self.embedding_cache[cache_key]\n            del self.image_size_cache[cache_key]\n        return (embedding_dict, img_in.shape[:2], image_id)\n\n    def infer_from_request(self, request: Sam2InferenceRequest):\n        \"\"\"Performs inference based on the request type.\n\n        Args:\n            request (SamInferenceRequest): The inference request.\n\n        Returns:\n            Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n        \"\"\"\n        with self._state_lock:\n            t1 = perf_counter()\n            if isinstance(request, Sam2EmbeddingRequest):\n                _, _, image_id = self.embed_image(**request.dict())\n                inference_time = perf_counter() - t1\n                return Sam2EmbeddingResponse(time=inference_time, image_id=image_id)\n            elif isinstance(request, Sam2SegmentationRequest):\n                masks, scores, low_resolution_logits = self.segment_image(\n                    **request.dict()\n                )\n                if request.format == \"json\":\n                    return turn_segmentation_results_into_api_response(\n                        masks=masks,\n                        scores=scores,\n                        mask_threshold=self.predictor.mask_threshold,\n                        inference_start_timestamp=t1,\n                    )\n                elif request.format == \"rle\":\n                    return turn_segmentation_results_into_rle_response(\n                        masks=masks,\n                        scores=scores,\n                        mask_threshold=self.predictor.mask_threshold,\n                        inference_start_timestamp=t1,\n                    )\n                elif request.format == \"binary\":\n                    binary_vector = BytesIO()\n                    np.savez_compressed(\n                        binary_vector, masks=masks, low_res_masks=low_resolution_logits\n                    )\n                    binary_vector.seek(0)\n                    binary_data = binary_vector.getvalue()\n                    return binary_data\n                else:\n                    raise ValueError(f\"Invalid format {request.format}\")\n\n            else:\n                raise ValueError(f\"Invalid request type {type(request)}\")\n\n    def preproc_image(self, image: InferenceRequestImage):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image\n\n    def segment_image(\n        self,\n        image: Optional[InferenceRequestImage],\n        image_id: Optional[str] = None,\n        prompts: Optional[Union[Sam2PromptSet, dict]] = None,\n        multimask_output: Optional[bool] = True,\n        mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n        save_logits_to_cache: bool = False,\n        load_logits_from_cache: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Segments an image based on provided embeddings, points, masks, or cached results.\n        If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n        Args:\n            image (Any): The image to be segmented.\n            image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n            prompts (Optional[List[Sam2Prompt]]): List of prompts to use for segmentation. Defaults to None.\n            mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input low_res_logits for the image.\n            multimask_output: (bool): Flag to decide if multiple masks proposal to be predicted (among which the most\n                promising will be returned\n            )\n            use_logits_cache: (bool): Flag to decide to use cached logits from prior prompting\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuple of np.array, where:\n                - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence\n                    for each prompt element\n                - second element is of size (prompt_set_size, ) and represents ths score for most confident mask\n                    of each prompt element\n                - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits\n                    for most confident mask of each prompt element\n\n        Raises:\n            ValueError: If necessary inputs are missing or inconsistent.\n\n        Notes:\n            - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n              on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n        \"\"\"\n        load_logits_from_cache = (\n            load_logits_from_cache and not DISABLE_SAM2_LOGITS_CACHE\n        )\n        save_logits_to_cache = save_logits_to_cache and not DISABLE_SAM2_LOGITS_CACHE\n        with torch.inference_mode():\n            if image is None and not image_id:\n                raise ValueError(\"Must provide either image or  cached image_id\")\n            elif image_id and image is None and image_id not in self.embedding_cache:\n                raise ValueError(\n                    f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n                )\n            embedding, original_image_size, image_id = self.embed_image(\n                image=image, image_id=image_id\n            )\n\n            self.predictor._is_image_set = True\n            self.predictor._features = embedding\n            self.predictor._orig_hw = [original_image_size]\n            self.predictor._is_batch = False\n            args = dict()\n            prompt_set: Sam2PromptSet\n            if prompts:\n                if type(prompts) is dict:\n                    prompt_set = Sam2PromptSet(**prompts)\n                    args = prompt_set.to_sam2_inputs()\n                else:\n                    prompt_set = prompts\n                    args = prompts.to_sam2_inputs()\n            else:\n                prompt_set = Sam2PromptSet()\n\n            if mask_input is None and load_logits_from_cache:\n                mask_input = maybe_load_low_res_logits_from_cache(\n                    image_id, prompt_set, self.low_res_logits_cache\n                )\n\n            args = pad_points(args)\n            if not any(args.values()):\n                args = {\"point_coords\": [[0, 0]], \"point_labels\": [-1], \"box\": None}\n            masks, scores, low_resolution_logits = self.predictor.predict(\n                mask_input=mask_input,\n                multimask_output=multimask_output,\n                return_logits=True,\n                normalize_coords=True,\n                **args,\n            )\n            masks, scores, low_resolution_logits = choose_most_confident_sam_prediction(\n                masks=masks,\n                scores=scores,\n                low_resolution_logits=low_resolution_logits,\n            )\n\n            if save_logits_to_cache:\n                self.add_low_res_logits_to_cache(\n                    low_resolution_logits, image_id, prompt_set\n                )\n\n            return masks, scores, low_resolution_logits\n\n    def add_low_res_logits_to_cache(\n        self, logits: np.ndarray, image_id: str, prompt_set: Sam2PromptSet\n    ) -&gt; None:\n        logits = logits[:, None, :, :]\n        prompt_id = hash_prompt_set(image_id, prompt_set)\n        self.low_res_logits_cache[prompt_id] = {\n            \"logits\": logits,\n            \"prompt_set\": prompt_set,\n        }\n        if prompt_id in self.low_res_logits_cache_keys:\n            self.low_res_logits_cache_keys.remove(prompt_id)\n        self.low_res_logits_cache_keys.append(prompt_id)\n        if len(self.low_res_logits_cache_keys) &gt; self.low_res_logits_cache_size:\n            cache_key = self.low_res_logits_cache_keys.pop(0)\n            del self.low_res_logits_cache[cache_key]\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.__init__","title":"<code>__init__(*args, model_id=f'sam2/{SAM2_VERSION_ID}', low_res_logits_cache_size=SAM2_MAX_LOGITS_CACHE_SIZE, embedding_cache_size=SAM2_MAX_EMBEDDING_CACHE_SIZE, **kwargs)</code>","text":"<p>Initializes the SegmentAnything.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    model_id: str = f\"sam2/{SAM2_VERSION_ID}\",\n    low_res_logits_cache_size: int = SAM2_MAX_LOGITS_CACHE_SIZE,\n    embedding_cache_size: int = SAM2_MAX_EMBEDDING_CACHE_SIZE,\n    **kwargs,\n):\n    \"\"\"Initializes the SegmentAnything.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, model_id=model_id, **kwargs)\n    checkpoint = self.cache_file(\"weights.pt\")\n    model_cfg = {\n        \"hiera_large\": \"sam2_hiera_l.yaml\",\n        \"hiera_small\": \"sam2_hiera_s.yaml\",\n        \"hiera_tiny\": \"sam2_hiera_t.yaml\",\n        \"hiera_b_plus\": \"sam2_hiera_b+.yaml\",\n    }[self.version_id]\n\n    self.sam = build_sam2(model_cfg, checkpoint, device=DEVICE)\n    self.low_res_logits_cache_size = low_res_logits_cache_size\n    self.embedding_cache_size = embedding_cache_size\n\n    self.predictor = SAM2ImagePredictor(self.sam)\n\n    self.embedding_cache = {}\n    self.image_size_cache = {}\n    self.embedding_cache_keys = []\n    self.low_res_logits_cache: Dict[Tuple[str, str], LogitsCacheType] = {}\n    self.low_res_logits_cache_keys = []\n    self._state_lock = Lock()\n    self.task_type = \"unsupervised-segmentation\"\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.embed_image","title":"<code>embed_image(image, image_id=None, **kwargs)</code>","text":"<p>Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached, the cached result will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be embedded. The format should be compatible with the preproc_image method.</p> required <code>image_id</code> <code>Optional[str]</code> <p>An identifier for the image. If provided, the embedding result will be cached                       with this ID. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image                                and the second element is the shape (height, width) of the processed image.</p> Notes <ul> <li>Embeddings and image sizes are cached to improve performance on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM2_MAX_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Example <p>img_array = ... # some image array embed_image(img_array, image_id=\"sample123\") (array([...]), (224, 224))</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def embed_image(\n    self,\n    image: Optional[InferenceRequestImage],\n    image_id: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n    the cached result will be returned.\n\n    Args:\n        image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n        image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                  with this ID. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                           and the second element is the shape (height, width) of the processed image.\n\n    Notes:\n        - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM2_MAX_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n\n    Example:\n        &gt;&gt;&gt; img_array = ... # some image array\n        &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n        (array([...]), (224, 224))\n    \"\"\"\n    if image_id and image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n            image_id,\n        )\n\n    img_in = self.preproc_image(image)\n    if image_id is None:\n        image_id = hashlib.md5(img_in.tobytes()).hexdigest()[:12]\n\n    if image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n            image_id,\n        )\n\n    with torch.inference_mode():\n        self.predictor.set_image(img_in)\n        embedding_dict = self.predictor._features\n\n    self.embedding_cache[image_id] = embedding_dict\n    self.image_size_cache[image_id] = img_in.shape[:2]\n    if image_id in self.embedding_cache_keys:\n        self.embedding_cache_keys.remove(image_id)\n    self.embedding_cache_keys.append(image_id)\n    if len(self.embedding_cache_keys) &gt; self.embedding_cache_size:\n        cache_key = self.embedding_cache_keys.pop(0)\n        del self.embedding_cache[cache_key]\n        del self.image_size_cache[cache_key]\n    return (embedding_dict, img_in.shape[:2], image_id)\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of file names.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: List of file names.\n    \"\"\"\n    return [\"weights.pt\"]\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Performs inference based on the request type.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Type Description <p>Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def infer_from_request(self, request: Sam2InferenceRequest):\n    \"\"\"Performs inference based on the request type.\n\n    Args:\n        request (SamInferenceRequest): The inference request.\n\n    Returns:\n        Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n    \"\"\"\n    with self._state_lock:\n        t1 = perf_counter()\n        if isinstance(request, Sam2EmbeddingRequest):\n            _, _, image_id = self.embed_image(**request.dict())\n            inference_time = perf_counter() - t1\n            return Sam2EmbeddingResponse(time=inference_time, image_id=image_id)\n        elif isinstance(request, Sam2SegmentationRequest):\n            masks, scores, low_resolution_logits = self.segment_image(\n                **request.dict()\n            )\n            if request.format == \"json\":\n                return turn_segmentation_results_into_api_response(\n                    masks=masks,\n                    scores=scores,\n                    mask_threshold=self.predictor.mask_threshold,\n                    inference_start_timestamp=t1,\n                )\n            elif request.format == \"rle\":\n                return turn_segmentation_results_into_rle_response(\n                    masks=masks,\n                    scores=scores,\n                    mask_threshold=self.predictor.mask_threshold,\n                    inference_start_timestamp=t1,\n                )\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.savez_compressed(\n                    binary_vector, masks=masks, low_res_masks=low_resolution_logits\n                )\n                binary_vector.seek(0)\n                binary_data = binary_vector.getvalue()\n                return binary_data\n            else:\n                raise ValueError(f\"Invalid format {request.format}\")\n\n        else:\n            raise ValueError(f\"Invalid request type {type(request)}\")\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.segment_image","title":"<code>segment_image(image, image_id=None, prompts=None, multimask_output=True, mask_input=None, save_logits_to_cache=False, load_logits_from_cache=False, **kwargs)</code>","text":"<p>Segments an image based on provided embeddings, points, masks, or cached results. If embeddings are not directly provided, the function can derive them from the input image or cache.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be segmented.</p> required <code>image_id</code> <code>Optional[str]</code> <p>A cached identifier for the image. Useful for accessing cached embeddings or masks.</p> <code>None</code> <code>prompts</code> <code>Optional[List[Sam2Prompt]]</code> <p>List of prompts to use for segmentation. Defaults to None.</p> <code>None</code> <code>mask_input</code> <code>Optional[Union[ndarray, List[List[List[float]]]]]</code> <p>Input low_res_logits for the image.</p> <code>None</code> <code>multimask_output</code> <code>Optional[bool]</code> <p>(bool): Flag to decide if multiple masks proposal to be predicted (among which the most promising will be returned</p> <code>True</code> <code>use_logits_cache</code> <p>(bool): Flag to decide to use cached logits from prior prompting</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuple of np.array, where: - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence     for each prompt element - second element is of size (prompt_set_size, ) and represents ths score for most confident mask     of each prompt element - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits     for most confident mask of each prompt element</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary inputs are missing or inconsistent.</p> Notes <ul> <li>Embeddings, segmentations, and low-resolution logits can be cached to improve performance   on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def segment_image(\n    self,\n    image: Optional[InferenceRequestImage],\n    image_id: Optional[str] = None,\n    prompts: Optional[Union[Sam2PromptSet, dict]] = None,\n    multimask_output: Optional[bool] = True,\n    mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n    save_logits_to_cache: bool = False,\n    load_logits_from_cache: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Segments an image based on provided embeddings, points, masks, or cached results.\n    If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n    Args:\n        image (Any): The image to be segmented.\n        image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n        prompts (Optional[List[Sam2Prompt]]): List of prompts to use for segmentation. Defaults to None.\n        mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input low_res_logits for the image.\n        multimask_output: (bool): Flag to decide if multiple masks proposal to be predicted (among which the most\n            promising will be returned\n        )\n        use_logits_cache: (bool): Flag to decide to use cached logits from prior prompting\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuple of np.array, where:\n            - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence\n                for each prompt element\n            - second element is of size (prompt_set_size, ) and represents ths score for most confident mask\n                of each prompt element\n            - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits\n                for most confident mask of each prompt element\n\n    Raises:\n        ValueError: If necessary inputs are missing or inconsistent.\n\n    Notes:\n        - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n          on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n    \"\"\"\n    load_logits_from_cache = (\n        load_logits_from_cache and not DISABLE_SAM2_LOGITS_CACHE\n    )\n    save_logits_to_cache = save_logits_to_cache and not DISABLE_SAM2_LOGITS_CACHE\n    with torch.inference_mode():\n        if image is None and not image_id:\n            raise ValueError(\"Must provide either image or  cached image_id\")\n        elif image_id and image is None and image_id not in self.embedding_cache:\n            raise ValueError(\n                f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n            )\n        embedding, original_image_size, image_id = self.embed_image(\n            image=image, image_id=image_id\n        )\n\n        self.predictor._is_image_set = True\n        self.predictor._features = embedding\n        self.predictor._orig_hw = [original_image_size]\n        self.predictor._is_batch = False\n        args = dict()\n        prompt_set: Sam2PromptSet\n        if prompts:\n            if type(prompts) is dict:\n                prompt_set = Sam2PromptSet(**prompts)\n                args = prompt_set.to_sam2_inputs()\n            else:\n                prompt_set = prompts\n                args = prompts.to_sam2_inputs()\n        else:\n            prompt_set = Sam2PromptSet()\n\n        if mask_input is None and load_logits_from_cache:\n            mask_input = maybe_load_low_res_logits_from_cache(\n                image_id, prompt_set, self.low_res_logits_cache\n            )\n\n        args = pad_points(args)\n        if not any(args.values()):\n            args = {\"point_coords\": [[0, 0]], \"point_labels\": [-1], \"box\": None}\n        masks, scores, low_resolution_logits = self.predictor.predict(\n            mask_input=mask_input,\n            multimask_output=multimask_output,\n            return_logits=True,\n            normalize_coords=True,\n            **args,\n        )\n        masks, scores, low_resolution_logits = choose_most_confident_sam_prediction(\n            masks=masks,\n            scores=scores,\n            low_resolution_logits=low_resolution_logits,\n        )\n\n        if save_logits_to_cache:\n            self.add_low_res_logits_to_cache(\n                low_resolution_logits, image_id, prompt_set\n            )\n\n        return masks, scores, low_resolution_logits\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.choose_most_confident_sam_prediction","title":"<code>choose_most_confident_sam_prediction(masks, scores, low_resolution_logits)</code>","text":"<p>This function is supposed to post-process SAM2 inference and choose most confident mask regardless of <code>multimask_output</code> parameter value Args:     masks: np array with values 0.0 and 1.0 representing predicted mask of size         (prompt_set_size, proposed_maks, h, w) or (proposed_maks, h, w) - depending on         prompt set size - unfortunately, prompt_set_size=1 causes squeeze operation         in SAM2 library, so to handle inference uniformly, we need to compensate with         this function.     scores: array of size (prompt_set_size, proposed_maks) or (proposed_maks, ) depending         on prompt set size - this array gives confidence score for mask proposal     low_resolution_logits: array of size (prompt_set_size, proposed_maks, 256, 256) or         (proposed_maks, 256, 256) - depending on prompt set size. These low resolution logits          can be passed to a subsequent iteration as mask input. Returns:     Tuple of np.array, where:         - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence             for each prompt element         - second element is of size (prompt_set_size, ) and represents ths score for most confident mask             of each prompt element         - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits             for most confident mask of each prompt element</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def choose_most_confident_sam_prediction(\n    masks: np.ndarray,\n    scores: np.ndarray,\n    low_resolution_logits: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This function is supposed to post-process SAM2 inference and choose most confident\n    mask regardless of `multimask_output` parameter value\n    Args:\n        masks: np array with values 0.0 and 1.0 representing predicted mask of size\n            (prompt_set_size, proposed_maks, h, w) or (proposed_maks, h, w) - depending on\n            prompt set size - unfortunately, prompt_set_size=1 causes squeeze operation\n            in SAM2 library, so to handle inference uniformly, we need to compensate with\n            this function.\n        scores: array of size (prompt_set_size, proposed_maks) or (proposed_maks, ) depending\n            on prompt set size - this array gives confidence score for mask proposal\n        low_resolution_logits: array of size (prompt_set_size, proposed_maks, 256, 256) or\n            (proposed_maks, 256, 256) - depending on prompt set size. These low resolution logits\n             can be passed to a subsequent iteration as mask input.\n    Returns:\n        Tuple of np.array, where:\n            - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence\n                for each prompt element\n            - second element is of size (prompt_set_size, ) and represents ths score for most confident mask\n                of each prompt element\n            - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits\n                for most confident mask of each prompt element\n    \"\"\"\n    if len(masks.shape) == 3:\n        masks = np.expand_dims(masks, axis=0)\n        scores = np.expand_dims(scores, axis=0)\n        low_resolution_logits = np.expand_dims(low_resolution_logits, axis=0)\n    selected_masks, selected_scores, selected_low_resolution_logits = [], [], []\n    for mask, score, low_resolution_logit in zip(masks, scores, low_resolution_logits):\n        selected_mask, selected_score, selected_low_resolution_logit = (\n            choose_most_confident_prompt_set_element_prediction(\n                mask=mask,\n                score=score,\n                low_resolution_logit=low_resolution_logit,\n            )\n        )\n        selected_masks.append(selected_mask)\n        selected_scores.append(selected_score)\n        selected_low_resolution_logits.append(selected_low_resolution_logit)\n    return (\n        np.asarray(selected_masks),\n        np.asarray(selected_scores),\n        np.asarray(selected_low_resolution_logits),\n    )\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.find_prior_prompt_in_cache","title":"<code>find_prior_prompt_in_cache(initial_prompt_set, image_id, cache)</code>","text":"<p>Performs search over the cache to see if prior used prompts are subset of this one.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def find_prior_prompt_in_cache(\n    initial_prompt_set: Sam2PromptSet,\n    image_id: str,\n    cache: Dict[Tuple[str, str], LogitsCacheType],\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Performs search over the cache to see if prior used prompts are subset of this one.\n    \"\"\"\n\n    logits_for_image = [cache[k] for k in cache if k[0] == image_id]\n    maxed_size = 0\n    best_match: Optional[np.ndarray] = None\n    desired_size = initial_prompt_set.num_points() - 1\n    for cached_dict in logits_for_image[::-1]:\n        logits = cached_dict[\"logits\"]\n        prompt_set: Sam2PromptSet = cached_dict[\"prompt_set\"]\n        is_viable = is_prompt_strict_subset(prompt_set, initial_prompt_set)\n        if not is_viable:\n            continue\n\n        size = prompt_set.num_points()\n        # short circuit search if we find prompt with one less point (most recent possible mask)\n        if size == desired_size:\n            return logits\n        if size &gt;= maxed_size:\n            maxed_size = size\n            best_match = logits\n\n    return best_match\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.hash_prompt_set","title":"<code>hash_prompt_set(image_id, prompt_set)</code>","text":"<p>Computes unique hash from a prompt set.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def hash_prompt_set(image_id: str, prompt_set: Sam2PromptSet) -&gt; Tuple[str, str]:\n    \"\"\"Computes unique hash from a prompt set.\"\"\"\n    md5_hash = hashlib.md5()\n    md5_hash.update(str(prompt_set).encode(\"utf-8\"))\n    return image_id, md5_hash.hexdigest()[:12]\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.maybe_load_low_res_logits_from_cache","title":"<code>maybe_load_low_res_logits_from_cache(image_id, prompt_set, cache)</code>","text":"<p>Loads prior masks from the cache by searching over possibel prior prompts.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def maybe_load_low_res_logits_from_cache(\n    image_id: str,\n    prompt_set: Sam2PromptSet,\n    cache: Dict[Tuple[str, str], LogitsCacheType],\n) -&gt; Optional[np.ndarray]:\n    \"Loads prior masks from the cache by searching over possibel prior prompts.\"\n    prompts = prompt_set.prompts\n    if not prompts:\n        return None\n\n    return find_prior_prompt_in_cache(prompt_set, image_id, cache)\n</code></pre>"},{"location":"reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.pad_points","title":"<code>pad_points(args)</code>","text":"<p>Pad arguments to be passed to sam2 model with not_a_point label (-1). This is necessary when there are multiple prompts per image so that a tensor can be created.</p> <p>Also pads empty point lists with a dummy non-point entry.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def pad_points(args: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Pad arguments to be passed to sam2 model with not_a_point label (-1).\n    This is necessary when there are multiple prompts per image so that a tensor can be created.\n\n\n    Also pads empty point lists with a dummy non-point entry.\n    \"\"\"\n    args = copy.deepcopy(args)\n    if args[\"point_coords\"] is not None:\n        max_len = max(max(len(prompt) for prompt in args[\"point_coords\"]), 1)\n        for prompt in args[\"point_coords\"]:\n            for _ in range(max_len - len(prompt)):\n                prompt.append([0, 0])\n        for label in args[\"point_labels\"]:\n            for _ in range(max_len - len(label)):\n                label.append(-1)\n    else:\n        if args[\"point_labels\"] is not None:\n            raise ValueError(\n                \"Can't have point labels without corresponding point coordinates\"\n            )\n    return args\n</code></pre>"},{"location":"reference/inference/models/vit/vit_classification/","title":"Vit classification","text":""},{"location":"reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification","title":"<code>VitClassification</code>","text":"<p>               Bases: <code>ClassificationBaseOnnxRoboflowInferenceModel</code></p> <p>VitClassification handles classification inference for Vision Transformer (ViT) models using ONNX.</p> Inherits <p>ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference. ClassificationMixin: Mixin class providing classification-specific methods.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>A flag that specifies if the model should handle multiclass classification.</p> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>class VitClassification(ClassificationBaseOnnxRoboflowInferenceModel):\n    \"\"\"VitClassification handles classification inference\n    for Vision Transformer (ViT) models using ONNX.\n\n    Inherits:\n        ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference.\n        ClassificationMixin: Mixin class providing classification-specific methods.\n\n    Attributes:\n        multiclass (bool): A flag that specifies if the model should handle multiclass classification.\n    \"\"\"\n\n    preprocess_means = [0.5, 0.5, 0.5]\n    preprocess_stds = [0.5, 0.5, 0.5]\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes the VitClassification instance.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Determines the weights file to be used based on the availability of AWS keys.\n\n        If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'.\n        Otherwise, it returns the path to 'best.onnx'.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and LAMBDA:\n            return \"weights.onnx\"\n        else:\n            return \"best.onnx\"\n</code></pre>"},{"location":"reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Determines the weights file to be used based on the availability of AWS keys.</p> <p>If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'. Otherwise, it returns the path to 'best.onnx'.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the VitClassification instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes the VitClassification instance.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/","title":"Yolact instance segmentation","text":""},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT","title":"<code>YOLACT</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method)</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>class YOLACT(OnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method)\"\"\"\n\n    task_type = \"instance-segmentation\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = False,\n        confidence: float = 0.5,\n        iou_threshold: float = 0.5,\n        max_candidates: int = 3000,\n        max_detections: int = 300,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[List[dict]]:\n        \"\"\"\n        Performs instance segmentation inference on a given image, post-processes the results,\n        and returns the segmented instances as dictionaries containing their properties.\n\n        Args:\n            image (Any): The image or list of images to segment.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to perform class-agnostic non-max suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for filtering weak detections. Defaults to 0.5.\n            iou_threshold (float, optional): Intersection-over-union threshold for non-max suppression. Defaults to 0.5.\n            max_candidates (int, optional): Maximum number of candidate detections to consider. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections to return after non-max suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the input image(s). Defaults to False.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains:\n                - x, y: Center coordinates of the instance.\n                - width, height: Width and height of the bounding box around the instance.\n                - class: Name of the detected class.\n                - confidence: Confidence score of the detection.\n                - points: List of points describing the segmented mask's boundary.\n                - class_id: ID corresponding to the detected class.\n            If `return_image_dims` is True, the function returns a tuple where the first element is the list of detections and the\n            second element is the list of image dimensions.\n\n        Notes:\n            - The function supports processing multiple images in a batch.\n            - If an input list of images is provided, the function returns a list of lists,\n              where each inner list corresponds to the detections for a specific image.\n            - The function internally uses an ONNX model for inference.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        if isinstance(image, list):\n            imgs_with_dims = [self.preproc_image(i) for i in image]\n            imgs, img_dims = zip(*imgs_with_dims)\n            if isinstance(imgs[0], np.ndarray):\n                img_in = np.concatenate(imgs, axis=0)\n            elif USE_PYTORCH_FOR_PREPROCESSING:\n                img_in = torch.cat(imgs, dim=0)\n            else:\n                raise ValueError(\n                    f\"Received a list of images of unknown type, {type(imgs[0])}; \"\n                    \"This is most likely a bug. Contact Roboflow team through github issues \"\n                    \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n                )\n        else:\n            img_in, img_dims = self.preproc_image(image)\n            img_dims = [img_dims]\n\n        # IN BGR order (for some reason)\n        mean = (103.94, 116.78, 123.68)\n        std = (57.38, 57.12, 58.40)\n\n        if isinstance(img_in, np.ndarray):\n            img_in = img_in.astype(np.float32)\n        elif USE_PYTORCH_FOR_PREPROCESSING:\n            img_in = img_in.float()\n        else:\n            raise ValueError(\n                f\"Received an image of unknown type, {type(img_in)}; \"\n                \"This is most likely a bug. Contact Roboflow team through github issues \"\n                \"(https://github.com/roboflow/inference/issues) providing full context of the problem\"\n            )\n\n        # Our channels are RGB, so apply mean and std accordingly\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[2]) / std[2]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[0]) / std[0]\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"im_shape\": img_in.shape,\n            }\n        )\n\n    def predict(\n        self, img_in: np.ndarray, **kwargs\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        with self._session_lock:\n            return run_session_via_iobinding(self.onnx_session, self.input_name, img_in)\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n        loc_data = np.float32(predictions[0])\n        conf_data = np.float32(predictions[1])\n        mask_data = np.float32(predictions[2])\n        prior_data = np.float32(predictions[3])\n        proto_data = np.float32(predictions[4])\n\n        batch_size = loc_data.shape[0]\n        num_priors = prior_data.shape[0]\n\n        boxes = np.zeros((batch_size, num_priors, 4))\n        for batch_idx in range(batch_size):\n            boxes[batch_idx, :, :] = self.decode_predicted_bboxes(\n                loc_data[batch_idx], prior_data\n            )\n\n        conf_preds = np.reshape(\n            conf_data, (batch_size, num_priors, self.num_classes + 1)\n        )\n        class_confs = conf_preds[:, :, 1:]  # remove background class\n        box_confs = np.expand_dims(\n            np.max(class_confs, axis=2), 2\n        )  # get max conf for each box\n\n        predictions = np.concatenate((boxes, box_confs, class_confs, mask_data), axis=2)\n\n        img_in_shape = preprocess_return_metadata[\"im_shape\"]\n        predictions[:, :, 0] *= img_in_shape[2]\n        predictions[:, :, 1] *= img_in_shape[3]\n        predictions[:, :, 2] *= img_in_shape[2]\n        predictions[:, :, 3] *= img_in_shape[3]\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=kwargs[\"confidence\"],\n            iou_thresh=kwargs[\"iou_threshold\"],\n            class_agnostic=kwargs[\"class_agnostic_nms\"],\n            max_detections=kwargs[\"max_detections\"],\n            max_candidate_detections=kwargs[\"max_candidates\"],\n            num_masks=32,\n            box_format=\"xyxy\",\n        )\n        predictions = np.array(predictions)\n        batch_preds = []\n        if predictions.shape != (1, 0):\n            for batch_idx, img_dim in enumerate(preprocess_return_metadata[\"img_dims\"]):\n                boxes = predictions[batch_idx, :, :4]\n                scores = predictions[batch_idx, :, 4]\n                classes = predictions[batch_idx, :, 6]\n                masks = predictions[batch_idx, :, 7:]\n                proto = proto_data[batch_idx]\n                decoded_masks = self.decode_masks(boxes, masks, proto, img_in_shape[2:])\n                polys = masks2poly(decoded_masks)\n                infer_shape = (self.img_size_w, self.img_size_h)\n                boxes = post_process_bboxes(\n                    [boxes], infer_shape, [img_dim], self.preproc, self.resize_method\n                )[0]\n                polys = post_process_polygons(\n                    img_in_shape[2:],\n                    polys,\n                    img_dim,\n                    self.preproc,\n                    resize_method=self.resize_method,\n                )\n                preds = []\n                for box, poly, score, cls in zip(boxes, polys, scores, classes):\n                    confidence = float(score)\n                    class_name = self.class_names[int(cls)]\n                    points = [{\"x\": round(x, 1), \"y\": round(y, 1)} for (x, y) in poly]\n                    pred = {\n                        \"x\": round((box[2] + box[0]) / 2, 1),\n                        \"y\": round((box[3] + box[1]) / 2, 1),\n                        \"width\": int(box[2] - box[0]),\n                        \"height\": int(box[3] - box[1]),\n                        \"class\": class_name,\n                        \"confidence\": round(confidence, 3),\n                        \"points\": points,\n                        \"class_id\": int(cls),\n                    }\n                    preds.append(pred)\n                batch_preds.append(preds)\n        else:\n            batch_preds.append([])\n        img_dims = preprocess_return_metadata[\"img_dims\"]\n        responses = self.make_response(batch_preds, img_dims, **kwargs)\n        if kwargs[\"return_image_dims\"]:\n            return responses, preprocess_return_metadata[\"img_dims\"]\n        else:\n            return responses\n\n    def make_response(\n        self,\n        predictions: List[List[dict]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: List[str] = None,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n        \"\"\"\n        Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions\n        and image dimensions, optionally filtering by class name.\n\n        Args:\n            predictions (List[List[dict]]): A list containing batch predictions, where each inner list contains\n                dictionaries of segmented instances for a given image.\n            img_dims (List[Tuple[int, int]]): List of tuples specifying the dimensions of each image in the format\n                (height, width).\n            class_filter (List[str], optional): A list of class names to filter the predictions by. If not provided,\n                all predictions are included.\n\n        Returns:\n            List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered\n            predictions and corresponding image dimensions for a given image.\n\n        Examples:\n            &gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n            &gt;&gt;&gt; img_dims = [(300, 400), ...]\n            &gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n            &gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n            1\n        \"\"\"\n        responses = [\n            InstanceSegmentationInferenceResponse(\n                predictions=[\n                    InstanceSegmentationPrediction(**p)\n                    for p in batch_pred\n                    if not class_filter or p[\"class_name\"] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[i][1], height=img_dims[i][0]\n                ),\n            )\n            for i, batch_pred in enumerate(predictions)\n        ]\n        return responses\n\n    def decode_masks(self, boxes, masks, proto, img_dim):\n        \"\"\"Decodes the masks from the given parameters.\n\n        Args:\n            boxes (np.array): Bounding boxes.\n            masks (np.array): Masks.\n            proto (np.array): Proto data.\n            img_dim (tuple): Image dimensions.\n\n        Returns:\n            np.array: Decoded masks.\n        \"\"\"\n        ret_mask = np.matmul(proto, np.transpose(masks))\n        ret_mask = 1 / (1 + np.exp(-ret_mask))\n        w, h, _ = ret_mask.shape\n        gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n        pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n        top, left = int(pad[1]), int(pad[0])  # y, x\n        bottom, right = int(h - pad[1]), int(w - pad[0])\n        ret_mask = np.transpose(ret_mask, (2, 0, 1))\n        ret_mask = ret_mask[:, top:bottom, left:right]\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=0)\n        ret_mask = ret_mask.transpose((1, 2, 0))\n        ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=2)\n        ret_mask = ret_mask.transpose((2, 0, 1))\n        ret_mask = crop_mask(ret_mask, boxes)  # CHW\n        ret_mask[ret_mask &lt; 0.5] = 0\n\n        return ret_mask\n\n    def decode_predicted_bboxes(self, loc, priors):\n        \"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n        Args:\n            loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n            priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n        Returns:\n            np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n        \"\"\"\n\n        variances = [0.1, 0.2]\n\n        boxes = np.concatenate(\n            [\n                priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n                priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n            ],\n            1,\n        )\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n\n        return boxes\n</code></pre>"},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.decode_masks","title":"<code>decode_masks(boxes, masks, proto, img_dim)</code>","text":"<p>Decodes the masks from the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>array</code> <p>Bounding boxes.</p> required <code>masks</code> <code>array</code> <p>Masks.</p> required <code>proto</code> <code>array</code> <p>Proto data.</p> required <code>img_dim</code> <code>tuple</code> <p>Image dimensions.</p> required <p>Returns:</p> Type Description <p>np.array: Decoded masks.</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_masks(self, boxes, masks, proto, img_dim):\n    \"\"\"Decodes the masks from the given parameters.\n\n    Args:\n        boxes (np.array): Bounding boxes.\n        masks (np.array): Masks.\n        proto (np.array): Proto data.\n        img_dim (tuple): Image dimensions.\n\n    Returns:\n        np.array: Decoded masks.\n    \"\"\"\n    ret_mask = np.matmul(proto, np.transpose(masks))\n    ret_mask = 1 / (1 + np.exp(-ret_mask))\n    w, h, _ = ret_mask.shape\n    gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n    pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n    top, left = int(pad[1]), int(pad[0])  # y, x\n    bottom, right = int(h - pad[1]), int(w - pad[0])\n    ret_mask = np.transpose(ret_mask, (2, 0, 1))\n    ret_mask = ret_mask[:, top:bottom, left:right]\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=0)\n    ret_mask = ret_mask.transpose((1, 2, 0))\n    ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=2)\n    ret_mask = ret_mask.transpose((2, 0, 1))\n    ret_mask = crop_mask(ret_mask, boxes)  # CHW\n    ret_mask[ret_mask &lt; 0.5] = 0\n\n    return ret_mask\n</code></pre>"},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.decode_predicted_bboxes","title":"<code>decode_predicted_bboxes(loc, priors)</code>","text":"<p>Decode predicted bounding box coordinates using the scheme employed by Yolov2.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>array</code> <p>The predicted bounding boxes of size [num_priors, 4].</p> required <code>priors</code> <code>array</code> <p>The prior box coordinates with size [num_priors, 4].</p> required <p>Returns:</p> Type Description <p>np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_predicted_bboxes(self, loc, priors):\n    \"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n    Args:\n        loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n        priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n    Returns:\n        np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n    \"\"\"\n\n    variances = [0.1, 0.2]\n\n    boxes = np.concatenate(\n        [\n            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n        ],\n        1,\n    )\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n\n    return boxes\n</code></pre>"},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.infer","title":"<code>infer(image, class_agnostic_nms=False, confidence=0.5, iou_threshold=0.5, max_candidates=3000, max_detections=300, return_image_dims=False, **kwargs)</code>","text":"<p>Performs instance segmentation inference on a given image, post-processes the results, and returns the segmented instances as dictionaries containing their properties.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to segment. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to perform class-agnostic non-max suppression. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering weak detections. Defaults to 0.5.</p> <code>0.5</code> <code>iou_threshold</code> <code>float</code> <p>Intersection-over-union threshold for non-max suppression. Defaults to 0.5.</p> <code>0.5</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections to consider. Defaults to 3000.</p> <code>3000</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections to return after non-max suppression. Defaults to 300.</p> <code>300</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the input image(s). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains: - x, y: Center coordinates of the instance. - width, height: Width and height of the bounding box around the instance. - class: Name of the detected class. - confidence: Confidence score of the detection. - points: List of points describing the segmented mask's boundary. - class_id: ID corresponding to the detected class.</p> <code>List[List[dict]]</code> <p>If <code>return_image_dims</code> is True, the function returns a tuple where the first element is the list of detections and the</p> <code>List[List[dict]]</code> <p>second element is the list of image dimensions.</p> Notes <ul> <li>The function supports processing multiple images in a batch.</li> <li>If an input list of images is provided, the function returns a list of lists,   where each inner list corresponds to the detections for a specific image.</li> <li>The function internally uses an ONNX model for inference.</li> </ul> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = False,\n    confidence: float = 0.5,\n    iou_threshold: float = 0.5,\n    max_candidates: int = 3000,\n    max_detections: int = 300,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[List[dict]]:\n    \"\"\"\n    Performs instance segmentation inference on a given image, post-processes the results,\n    and returns the segmented instances as dictionaries containing their properties.\n\n    Args:\n        image (Any): The image or list of images to segment.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to perform class-agnostic non-max suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for filtering weak detections. Defaults to 0.5.\n        iou_threshold (float, optional): Intersection-over-union threshold for non-max suppression. Defaults to 0.5.\n        max_candidates (int, optional): Maximum number of candidate detections to consider. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections to return after non-max suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the input image(s). Defaults to False.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains:\n            - x, y: Center coordinates of the instance.\n            - width, height: Width and height of the bounding box around the instance.\n            - class: Name of the detected class.\n            - confidence: Confidence score of the detection.\n            - points: List of points describing the segmented mask's boundary.\n            - class_id: ID corresponding to the detected class.\n        If `return_image_dims` is True, the function returns a tuple where the first element is the list of detections and the\n        second element is the list of image dimensions.\n\n    Notes:\n        - The function supports processing multiple images in a batch.\n        - If an input list of images is provided, the function returns a list of lists,\n          where each inner list corresponds to the detections for a specific image.\n        - The function internally uses an ONNX model for inference.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, **kwargs)</code>","text":"<p>Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions and image dimensions, optionally filtering by class name.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[dict]]</code> <p>A list containing batch predictions, where each inner list contains dictionaries of segmented instances for a given image.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>List of tuples specifying the dimensions of each image in the format (height, width).</p> required <code>class_filter</code> <code>List[str]</code> <p>A list of class names to filter the predictions by. If not provided, all predictions are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[InstanceSegmentationInferenceResponse]</code> <p>List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered</p> <code>List[InstanceSegmentationInferenceResponse]</code> <p>predictions and corresponding image dimensions for a given image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n&gt;&gt;&gt; img_dims = [(300, 400), ...]\n&gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n&gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n1\n</code></pre> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[dict]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: List[str] = None,\n    **kwargs,\n) -&gt; List[InstanceSegmentationInferenceResponse]:\n    \"\"\"\n    Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions\n    and image dimensions, optionally filtering by class name.\n\n    Args:\n        predictions (List[List[dict]]): A list containing batch predictions, where each inner list contains\n            dictionaries of segmented instances for a given image.\n        img_dims (List[Tuple[int, int]]): List of tuples specifying the dimensions of each image in the format\n            (height, width).\n        class_filter (List[str], optional): A list of class names to filter the predictions by. If not provided,\n            all predictions are included.\n\n    Returns:\n        List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered\n        predictions and corresponding image dimensions for a given image.\n\n    Examples:\n        &gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n        &gt;&gt;&gt; img_dims = [(300, 400), ...]\n        &gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n        &gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n        1\n    \"\"\"\n    responses = [\n        InstanceSegmentationInferenceResponse(\n            predictions=[\n                InstanceSegmentationPrediction(**p)\n                for p in batch_pred\n                if not class_filter or p[\"class_name\"] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[i][1], height=img_dims[i][0]\n            ),\n        )\n        for i, batch_pred in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/","title":"Yolo world","text":""},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld","title":"<code>YOLOWorld</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>YOLO-World class for zero-shot object detection.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The YOLO-World model.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>class YOLOWorld(RoboflowCoreModel):\n    \"\"\"YOLO-World class for zero-shot object detection.\n\n    Attributes:\n        model: The YOLO-World model.\n    \"\"\"\n\n    task_type = \"object-detection\"\n\n    def __init__(self, *args, model_id=\"yolo_world/l\", **kwargs):\n        \"\"\"Initializes the YOLO-World model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n        self.model = YOLO(self.cache_file(\"yolo-world.pt\"))\n        logger.debug(\"Loading CLIP ViT-B/32\")\n        clip_model = Clip(model_id=\"clip/ViT-B-32\")\n        logger.debug(\"CLIP loaded\")\n        self.clip_model = clip_model\n        self.class_names = None\n        self._state_lock = Lock()\n\n    def preproc_image(self, image: Any):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image[:, :, ::-1]\n\n    def infer_from_request(\n        self,\n        request: YOLOWorldInferenceRequest,\n    ) -&gt; ObjectDetectionInferenceResponse:\n        \"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        \"\"\"\n        with self._state_lock:\n            return self.infer(**request.dict())\n\n    def infer(\n        self,\n        image: Any = None,\n        text: list = None,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: Optional[int] = DEFAUlT_MAX_DETECTIONS,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        **kwargs,\n    ):\n        \"\"\"\n        Run inference on a provided image.\n\n        Args:\n            image - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            GroundingDINOInferenceRequest: The inference response.\n        \"\"\"\n        logger.debug(\"YOLOWorld infer() - image preprocessing.\")\n        t1 = perf_counter()\n        image = self.preproc_image(image)\n        logger.debug(\"YOLOWorld infer() - image ready.\")\n        img_dims = image.shape\n\n        if text is not None and text != self.class_names:\n            logger.debug(\"YOLOWorld infer() - classes embeddings are calculated.\")\n            self.set_classes(text)\n            logger.debug(\"YOLOWorld infer() - classes embeddings are ready.\")\n        if self.class_names is None:\n            raise ValueError(\n                \"Class names not set and not provided in the request. Must set class names before inference or provide them via the argument `text`.\"\n            )\n        logger.debug(\"YOLOWorld infer() - prediction starts.\")\n        results = self.model.predict(\n            image,\n            conf=confidence,\n            verbose=False,\n        )[0]\n        logger.debug(\"YOLOWorld infer() - predictions ready.\")\n        t2 = perf_counter() - t1\n\n        logger.debug(\"YOLOWorld infer() - post-processing starting\")\n        if len(results) &gt; 0:\n            bbox_array = np.array([box.xywh.tolist()[0] for box in results.boxes])\n            conf_array = np.array([[float(box.conf)] for box in results.boxes])\n            cls_array = np.array(\n                [\n                    self.get_cls_conf_array(\n                        max_class_id=int(box.cls),\n                        max_class_confidence=float(box.conf),\n                    )\n                    for box in results.boxes\n                ]\n            )\n\n            pred_array = np.concatenate([bbox_array, conf_array, cls_array], axis=1)\n            pred_array = np.expand_dims(pred_array, axis=0)\n            pred_array = w_np_non_max_suppression(\n                pred_array,\n                conf_thresh=confidence,\n                iou_thresh=iou_threshold,\n                class_agnostic=class_agnostic_nms,\n                max_detections=max_detections,\n                max_candidate_detections=max_candidates,\n                box_format=\"xywh\",\n            )[0]\n        else:\n            pred_array = []\n        predictions = []\n        logger.debug(\"YOLOWorld infer() - post-processing done\")\n        for i, pred in enumerate(pred_array):\n            predictions.append(\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n            )\n\n        responses = ObjectDetectionInferenceResponse(\n            predictions=predictions,\n            image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n            time=t2,\n        )\n        return responses\n\n    def set_classes(self, text: list):\n        \"\"\"Set the class names for the model.\n\n        Args:\n            text (list): The class names.\n        \"\"\"\n        class_names_to_calculate_embeddings = []\n        classes_embeddings = {}\n        for class_name in text:\n            class_name_hash = f\"clip-embedding:{get_text_hash(text=class_name)}\"\n            embedding_for_class = cache.get_numpy(class_name_hash)\n            if embedding_for_class is not None:\n                logger.debug(f\"Cache hit for class: {class_name}\")\n                classes_embeddings[class_name] = embedding_for_class\n            else:\n                logger.debug(f\"Cache miss for class: {class_name}\")\n                class_names_to_calculate_embeddings.append(class_name)\n        if len(class_names_to_calculate_embeddings) &gt; 0:\n            logger.debug(\n                f\"Calculating CLIP embeddings for {len(class_names_to_calculate_embeddings)} class names\"\n            )\n            cache_miss_embeddings = self.clip_model.embed_text(\n                text=class_names_to_calculate_embeddings\n            )\n        else:\n            cache_miss_embeddings = []\n        for missing_class_name, calculated_embedding in zip(\n            class_names_to_calculate_embeddings, cache_miss_embeddings\n        ):\n            classes_embeddings[missing_class_name] = calculated_embedding\n            missing_class_name_hash = (\n                f\"clip-embedding:{get_text_hash(text=missing_class_name)}\"\n            )\n            cache.set_numpy(  # caching vectors of shape (512,)\n                missing_class_name_hash,\n                calculated_embedding,\n                expire=EMBEDDINGS_EXPIRE_TIMEOUT,\n            )\n        embeddings_in_order = np.stack(\n            [classes_embeddings[class_name] for class_name in text], axis=0\n        )\n        txt_feats = torch.from_numpy(embeddings_in_order)\n        txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n        self.model.model.txt_feats = txt_feats.reshape(\n            -1, len(text), txt_feats.shape[-1]\n        ).detach()\n        self.model.model.model[-1].nc = len(text)\n        self.class_names = text\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"yolo-world.pt\"]\n\n    def get_cls_conf_array(\n        self, max_class_id: int, max_class_confidence: float\n    ) -&gt; List[float]:\n        arr = [0.0] * len(self.class_names)\n        arr[max_class_id] = max_class_confidence\n        return arr\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.__init__","title":"<code>__init__(*args, model_id='yolo_world/l', **kwargs)</code>","text":"<p>Initializes the YOLO-World model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def __init__(self, *args, model_id=\"yolo_world/l\", **kwargs):\n    \"\"\"Initializes the YOLO-World model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n\n    self.model = YOLO(self.cache_file(\"yolo-world.pt\"))\n    logger.debug(\"Loading CLIP ViT-B/32\")\n    clip_model = Clip(model_id=\"clip/ViT-B-32\")\n    logger.debug(\"CLIP loaded\")\n    self.clip_model = clip_model\n    self.class_names = None\n    self._state_lock = Lock()\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"yolo-world.pt\"]\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.infer","title":"<code>infer(image=None, text=None, confidence=DEFAULT_CONFIDENCE, max_detections=DEFAUlT_MAX_DETECTIONS, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, **kwargs)</code>","text":"<p>Run inference on a provided image.</p> <p>Parameters:</p> Name Type Description Default <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> required <p>Returns:</p> Name Type Description <code>GroundingDINOInferenceRequest</code> <p>The inference response.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def infer(\n    self,\n    image: Any = None,\n    text: list = None,\n    confidence: float = DEFAULT_CONFIDENCE,\n    max_detections: Optional[int] = DEFAUlT_MAX_DETECTIONS,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    **kwargs,\n):\n    \"\"\"\n    Run inference on a provided image.\n\n    Args:\n        image - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        GroundingDINOInferenceRequest: The inference response.\n    \"\"\"\n    logger.debug(\"YOLOWorld infer() - image preprocessing.\")\n    t1 = perf_counter()\n    image = self.preproc_image(image)\n    logger.debug(\"YOLOWorld infer() - image ready.\")\n    img_dims = image.shape\n\n    if text is not None and text != self.class_names:\n        logger.debug(\"YOLOWorld infer() - classes embeddings are calculated.\")\n        self.set_classes(text)\n        logger.debug(\"YOLOWorld infer() - classes embeddings are ready.\")\n    if self.class_names is None:\n        raise ValueError(\n            \"Class names not set and not provided in the request. Must set class names before inference or provide them via the argument `text`.\"\n        )\n    logger.debug(\"YOLOWorld infer() - prediction starts.\")\n    results = self.model.predict(\n        image,\n        conf=confidence,\n        verbose=False,\n    )[0]\n    logger.debug(\"YOLOWorld infer() - predictions ready.\")\n    t2 = perf_counter() - t1\n\n    logger.debug(\"YOLOWorld infer() - post-processing starting\")\n    if len(results) &gt; 0:\n        bbox_array = np.array([box.xywh.tolist()[0] for box in results.boxes])\n        conf_array = np.array([[float(box.conf)] for box in results.boxes])\n        cls_array = np.array(\n            [\n                self.get_cls_conf_array(\n                    max_class_id=int(box.cls),\n                    max_class_confidence=float(box.conf),\n                )\n                for box in results.boxes\n            ]\n        )\n\n        pred_array = np.concatenate([bbox_array, conf_array, cls_array], axis=1)\n        pred_array = np.expand_dims(pred_array, axis=0)\n        pred_array = w_np_non_max_suppression(\n            pred_array,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            box_format=\"xywh\",\n        )[0]\n    else:\n        pred_array = []\n    predictions = []\n    logger.debug(\"YOLOWorld infer() - post-processing done\")\n    for i, pred in enumerate(pred_array):\n        predictions.append(\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": (pred[0] + pred[2]) / 2,\n                    \"y\": (pred[1] + pred[3]) / 2,\n                    \"width\": pred[2] - pred[0],\n                    \"height\": pred[3] - pred[1],\n                    \"confidence\": pred[4],\n                    \"class\": self.class_names[int(pred[6])],\n                    \"class_id\": int(pred[6]),\n                }\n            )\n        )\n\n    responses = ObjectDetectionInferenceResponse(\n        predictions=predictions,\n        image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n        time=t2,\n    )\n    return responses\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def infer_from_request(\n    self,\n    request: YOLOWorldInferenceRequest,\n) -&gt; ObjectDetectionInferenceResponse:\n    \"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    \"\"\"\n    with self._state_lock:\n        return self.infer(**request.dict())\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def preproc_image(self, image: Any):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image[:, :, ::-1]\n</code></pre>"},{"location":"reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.set_classes","title":"<code>set_classes(text)</code>","text":"<p>Set the class names for the model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>list</code> <p>The class names.</p> required Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def set_classes(self, text: list):\n    \"\"\"Set the class names for the model.\n\n    Args:\n        text (list): The class names.\n    \"\"\"\n    class_names_to_calculate_embeddings = []\n    classes_embeddings = {}\n    for class_name in text:\n        class_name_hash = f\"clip-embedding:{get_text_hash(text=class_name)}\"\n        embedding_for_class = cache.get_numpy(class_name_hash)\n        if embedding_for_class is not None:\n            logger.debug(f\"Cache hit for class: {class_name}\")\n            classes_embeddings[class_name] = embedding_for_class\n        else:\n            logger.debug(f\"Cache miss for class: {class_name}\")\n            class_names_to_calculate_embeddings.append(class_name)\n    if len(class_names_to_calculate_embeddings) &gt; 0:\n        logger.debug(\n            f\"Calculating CLIP embeddings for {len(class_names_to_calculate_embeddings)} class names\"\n        )\n        cache_miss_embeddings = self.clip_model.embed_text(\n            text=class_names_to_calculate_embeddings\n        )\n    else:\n        cache_miss_embeddings = []\n    for missing_class_name, calculated_embedding in zip(\n        class_names_to_calculate_embeddings, cache_miss_embeddings\n    ):\n        classes_embeddings[missing_class_name] = calculated_embedding\n        missing_class_name_hash = (\n            f\"clip-embedding:{get_text_hash(text=missing_class_name)}\"\n        )\n        cache.set_numpy(  # caching vectors of shape (512,)\n            missing_class_name_hash,\n            calculated_embedding,\n            expire=EMBEDDINGS_EXPIRE_TIMEOUT,\n        )\n    embeddings_in_order = np.stack(\n        [classes_embeddings[class_name] for class_name in text], axis=0\n    )\n    txt_feats = torch.from_numpy(embeddings_in_order)\n    txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n    self.model.model.txt_feats = txt_feats.reshape(\n        -1, len(text), txt_feats.shape[-1]\n    ).detach()\n    self.model.model.model[-1].nc = len(text)\n    self.class_names = text\n</code></pre>"},{"location":"reference/inference/models/yolov10/yolov10_object_detection/","title":"Yolov10 object detection","text":""},{"location":"reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection","title":"<code>YOLOv10ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv10 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov10/yolov10_object_detection.py</code> <pre><code>class YOLOv10ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv10 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    box_format = \"xyxy\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv10 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )[0]\n\n        return (predictions,)\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        predictions = np.append(predictions, predictions[..., 5:], axis=-1)\n        predictions[..., 5] = predictions[..., 4]\n\n        mask = predictions[..., 4] &gt; confidence\n        predictions = [\n            p[mask[idx]][:max_detections] for idx, p in enumerate(predictions)\n        ]\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions,\n            infer_shape,\n            img_dims,\n            self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def validate_model_classes(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv10 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, confidence=DEFAULT_CONFIDENCE, max_detections=DEFAUlT_MAX_DETECTIONS, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/models/yolov10/yolov10_object_detection.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray, ...],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    confidence: float = DEFAULT_CONFIDENCE,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    predictions = np.append(predictions, predictions[..., 5:], axis=-1)\n    predictions[..., 5] = predictions[..., 4]\n\n    mask = predictions[..., 4] &gt; confidence\n    predictions = [\n        p[mask[idx]][:max_detections] for idx, p in enumerate(predictions)\n    ]\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions,\n        infer_shape,\n        img_dims,\n        self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov10/yolov10_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )[0]\n\n    return (predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov5/yolov5_instance_segmentation/","title":"Yolov5 instance segmentation","text":""},{"location":"reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation","title":"<code>YOLOv5InstanceSegmentation</code>","text":"<p>               Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv5 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>class YOLOv5InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n    \"\"\"YOLOv5 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )\n        return predictions[0], predictions[1]\n</code></pre>"},{"location":"reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )\n    return predictions[0], predictions[1]\n</code></pre>"},{"location":"reference/inference/models/yolov5/yolov5_object_detection/","title":"Yolov5 object detection","text":""},{"location":"reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection","title":"<code>YOLOv5ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>class YOLOv5ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )[0]\n        return (predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )[0]\n    return (predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov7/yolov7_instance_segmentation/","title":"Yolov7 instance segmentation","text":""},{"location":"reference/inference/models/yolov7/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentation","title":"<code>YOLOv7InstanceSegmentation</code>","text":"<p>               Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv7 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv7 model with ONNX runtime.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>class YOLOv7InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n    \"\"\"YOLOv7 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv7 model\n    with ONNX runtime.\n\n    Methods:\n        predict: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )\n        protos = predictions[4]\n        predictions = predictions[0]\n        return predictions, protos\n</code></pre>"},{"location":"reference/inference/models/yolov7/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )\n    protos = predictions[4]\n    predictions = predictions[0]\n    return predictions, protos\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_instance_segmentation/","title":"Yolov8 instance segmentation","text":""},{"location":"reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation","title":"<code>YOLOv8InstanceSegmentation</code>","text":"<p>               Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv8 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>class YOLOv8InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n    \"\"\"YOLOv8 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n        \"\"\"\n        with self._session_lock:\n            predictions, protos = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:-32]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        masks = predictions[:, :, -32:]\n        predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n        return predictions, protos\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n    \"\"\"\n    with self._session_lock:\n        predictions, protos = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:-32]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    masks = predictions[:, :, -32:]\n    predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n    return predictions, protos\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_keypoints_detection/","title":"Yolov8 keypoints detection","text":""},{"location":"reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection","title":"<code>YOLOv8KeypointsDetection</code>","text":"<p>               Bases: <code>KeypointsDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX keypoints detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing keypoints detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>class YOLOv8KeypointsDetection(KeypointsDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX keypoints detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing keypoints detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        number_of_classes = len(self.get_class_names)\n        class_confs = predictions[:, :, 4 : 4 + number_of_classes]\n        keypoints_detections = predictions[:, :, 4 + number_of_classes :]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        bboxes_predictions = np.concatenate(\n            [boxes, confs, class_confs, keypoints_detections], axis=2\n        )\n        return (bboxes_predictions,)\n\n    def keypoints_count(self) -&gt; int:\n        \"\"\"Returns the number of keypoints in the model.\"\"\"\n        if self.keypoints_metadata is None:\n            raise ModelArtefactError(\"Keypoints metadata not available.\")\n        return superset_keypoints_count(self.keypoints_metadata)\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.keypoints_count","title":"<code>keypoints_count()</code>","text":"<p>Returns the number of keypoints in the model.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>def keypoints_count(self) -&gt; int:\n    \"\"\"Returns the number of keypoints in the model.\"\"\"\n    if self.keypoints_metadata is None:\n        raise ModelArtefactError(\"Keypoints metadata not available.\")\n    return superset_keypoints_count(self.keypoints_metadata)\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    number_of_classes = len(self.get_class_names)\n    class_confs = predictions[:, :, 4 : 4 + number_of_classes]\n    keypoints_detections = predictions[:, :, 4 + number_of_classes :]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    bboxes_predictions = np.concatenate(\n        [boxes, confs, class_confs, keypoints_detections], axis=2\n    )\n    return (bboxes_predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_object_detection/","title":"Yolov8 object detection","text":""},{"location":"reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection","title":"<code>YOLOv8ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>class YOLOv8ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: ImageMetaType, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>def predict(self, img_in: ImageMetaType, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov9/yolov9_object_detection/","title":"Yolov9 object detection","text":""},{"location":"reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection","title":"<code>YOLOv9ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv9 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov9/yolov9_object_detection.py</code> <pre><code>class YOLOv9ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv9 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv9 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions.\n        \"\"\"\n        # (b x 8 x 8000)\n        with self._session_lock:\n            predictions = run_session_via_iobinding(\n                self.onnx_session, self.input_name, img_in\n            )[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection.weights_file","title":"<code>weights_file</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv9 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov9/yolov9_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions.\n    \"\"\"\n    # (b x 8 x 8000)\n    with self._session_lock:\n        predictions = run_session_via_iobinding(\n            self.onnx_session, self.input_name, img_in\n        )[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"reference/inference/usage_tracking/redis_queue/","title":"Redis queue","text":""},{"location":"reference/inference/usage_tracking/redis_queue/#inference.usage_tracking.redis_queue.RedisQueue","title":"<code>RedisQueue</code>","text":"<p>Store and forget, keys with specified hash tag are handled by external service</p> Source code in <code>inference/usage_tracking/redis_queue.py</code> <pre><code>class RedisQueue:\n    \"\"\"\n    Store and forget, keys with specified hash tag are handled by external service\n    \"\"\"\n\n    def __init__(\n        self,\n        hash_tag: str = \"UsageCollector\",\n        redis_cache: Optional[RedisCache] = None,\n    ):\n        # prefix must contain hash-tag to avoid CROSSLOT errors when using mget\n        # hash-tag is common part of the key wrapped within '{}'\n        # removing hash-tag will cause clients utilizing mget to fail\n        self._prefix: str = f\"{{{hash_tag}}}:{time.time()}:{uuid4().hex[:5]}\"\n        self._redis_cache: RedisCache = redis_cache or cache\n        self._increment: int = 0\n        self._lock: Lock = Lock()\n\n    def put(self, payload: Any):\n        if not isinstance(payload, str):\n            try:\n                payload = json.dumps(payload)\n            except Exception as exc:\n                logger.error(\"Failed to parse payload '%s' to JSON - %s\", payload, exc)\n                return\n        with self._lock:\n            try:\n                self._increment += 1\n                redis_key = f\"{self._prefix}:{self._increment}\"\n                # https://redis.io/docs/latest/develop/interact/transactions/\n                redis_pipeline = self._redis_cache.client.pipeline()\n                redis_pipeline.set(\n                    name=redis_key,\n                    value=payload,\n                )\n                redis_pipeline.zadd(\n                    name=\"UsageCollector\",\n                    mapping={redis_key: time.time()},\n                )\n                results = redis_pipeline.execute()\n                if not all(results):\n                    # TODO: partial insert, retry\n                    logger.error(\n                        \"Failed to store payload and sorted set (partial insert): %s\",\n                        results,\n                    )\n            except Exception as exc:\n                logger.error(\"Failed to store usage records '%s', %s\", payload, exc)\n\n    @staticmethod\n    def full() -&gt; bool:\n        return False\n\n    def empty(self) -&gt; bool:\n        return True\n\n    def get_nowait(self) -&gt; List[Dict[str, Any]]:\n        return []\n</code></pre>"},{"location":"reference/inference_cli/","title":"Index","text":"<ul> <li>inference_cli<ul> <li>lib<ul> <li>container_adapter</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/inference_cli/lib/container_adapter/","title":"Container adapter","text":""},{"location":"reference/inference_cli/lib/container_adapter/#inference_cli.lib.container_adapter.terminate_running_containers","title":"<code>terminate_running_containers(containers, interactive_mode=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>containers</code> <code>List[Container]</code> <p>List of containers to handle</p> required <code>interactive_mode</code> <code>bool</code> <p>Flag to determine if user prompt should decide on container termination</p> <code>True</code> <p>boolean value that informs if there are containers that have not received SIGKILL</p> Type Description <code>bool</code> <p>as a result of procedure.</p> Source code in <code>inference_cli/lib/container_adapter.py</code> <pre><code>def terminate_running_containers(\n    containers: List[Container], interactive_mode: bool = True\n) -&gt; bool:\n    \"\"\"\n    Args:\n        containers (List[Container]): List of containers to handle\n        interactive_mode (bool): Flag to determine if user prompt should decide on container termination\n\n    Returns: boolean value that informs if there are containers that have not received SIGKILL\n        as a result of procedure.\n    \"\"\"\n    running_inference_containers = [\n        c for c in containers if is_container_running(container=c)\n    ]\n    containers_to_kill = running_inference_containers\n    if interactive_mode:\n        containers_to_kill = [\n            c for c in running_inference_containers if ask_user_to_kill_container(c)\n        ]\n    kill_containers(containers=containers_to_kill)\n    return len(containers_to_kill) &lt; len(running_inference_containers)\n</code></pre>"},{"location":"reference/inference_sdk/","title":"Index","text":"<ul> <li>inference_sdk<ul> <li>config</li> <li>http<ul> <li>client</li> <li>entities</li> <li>errors</li> <li>utils<ul> <li>aliases</li> <li>encoding</li> <li>executors</li> <li>iterables</li> <li>loaders</li> <li>post_processing</li> <li>pre_processing</li> <li>profilling</li> <li>request_building</li> <li>requests</li> </ul> </li> </ul> </li> <li>utils<ul> <li>decorators</li> <li>environment</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/inference_sdk/config/","title":"Config","text":""},{"location":"reference/inference_sdk/config/#inference_sdk.config.InferenceSDKDeprecationWarning","title":"<code>InferenceSDKDeprecationWarning</code>","text":"<p>               Bases: <code>Warning</code></p> <p>Class used for warning of deprecated features in the Inference SDK</p> Source code in <code>inference_sdk/config.py</code> <pre><code>class InferenceSDKDeprecationWarning(Warning):\n    \"\"\"Class used for warning of deprecated features in the Inference SDK\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/client/","title":"Client","text":""},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient","title":"<code>InferenceHTTPClient</code>","text":"<p>HTTP client for making inference requests to Roboflow's API.</p> <p>This client handles authentication, request formatting, and error handling for interacting with Roboflow's inference endpoints. It supports both synchronous and asynchronous requests.</p> <p>Attributes:</p> Name Type Description <code>inference_configuration</code> <code>InferenceConfiguration</code> <p>Configuration settings for inference requests.</p> <code>client_mode</code> <code>HTTPClientMode</code> <p>The API version mode being used (V0 or V1).</p> <code>selected_model</code> <code>Optional[str]</code> <p>Currently selected model identifier, if any.</p> Example <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n</code></pre> Source code in <code>inference_sdk/http/client.py</code> <pre><code>class InferenceHTTPClient:\n    \"\"\"HTTP client for making inference requests to Roboflow's API.\n\n    This client handles authentication, request formatting, and error handling for\n    interacting with Roboflow's inference endpoints. It supports both synchronous\n    and asynchronous requests.\n\n    Attributes:\n        inference_configuration (InferenceConfiguration): Configuration settings for\n            inference requests.\n        client_mode (HTTPClientMode): The API version mode being used (V0 or V1).\n        selected_model (Optional[str]): Currently selected model identifier, if any.\n\n    Example:\n        ```python\n        from inference_sdk import InferenceHTTPClient\n\n        client = InferenceHTTPClient(\n            api_url=\"http://localhost:9001\", # use local inference server\n            # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n        )\n\n        result = client.run_workflow(\n            workspace_name=\"roboflow-docs\",\n            workflow_id=\"model-comparison\",\n            images={\n                \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            parameters={\n                \"model1\": \"yolov8n-640\",\n                \"model2\": \"yolov11n-640\"\n            }\n        )\n        ```\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        api_url: str,\n        api_key: Optional[str] = None,\n    ) -&gt; \"InferenceHTTPClient\":\n        \"\"\"Initialize a new InferenceHTTPClient instance.\n\n        Args:\n            api_url (str): The base URL for the inference API.\n            api_key (Optional[str], optional): API key for authentication. Defaults to None.\n\n        Returns:\n            InferenceHTTPClient: A new instance of the InferenceHTTPClient.\n        \"\"\"\n        return cls(api_url=api_url, api_key=api_key)\n\n    def __init__(\n        self,\n        api_url: str,\n        api_key: Optional[str] = None,\n    ):\n        \"\"\"Initialize a new InferenceHTTPClient instance.\n\n        Args:\n            api_url (str): The base URL for the inference API.\n            api_key (Optional[str], optional): API key for authentication. Defaults to None.\n        \"\"\"\n        self.__api_url = api_url\n        self.__api_key = api_key\n        self.__inference_configuration = InferenceConfiguration.init_default()\n        self.__client_mode = _determine_client_mode(api_url=api_url)\n        self.__selected_model: Optional[str] = None\n\n    @property\n    def inference_configuration(self) -&gt; InferenceConfiguration:\n        \"\"\"Get the current inference configuration.\n\n        Returns:\n            InferenceConfiguration: The current inference configuration settings.\n        \"\"\"\n        return self.__inference_configuration\n\n    @property\n    def client_mode(self) -&gt; HTTPClientMode:\n        \"\"\"Get the current client mode.\n\n        Returns:\n            HTTPClientMode: The current API version mode (V0 or V1).\n        \"\"\"\n        return self.__client_mode\n\n    @property\n    def selected_model(self) -&gt; Optional[str]:\n        \"\"\"Get the currently selected model identifier.\n\n        Returns:\n            Optional[str]: The identifier of the currently selected model, if any.\n        \"\"\"\n        return self.__selected_model\n\n    @contextmanager\n    def use_configuration(\n        self, inference_configuration: InferenceConfiguration\n    ) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n        \"\"\"Temporarily use a different inference configuration.\n\n        Args:\n            inference_configuration (InferenceConfiguration): The temporary configuration to use.\n\n        Yields:\n            Generator[InferenceHTTPClient, None, None]: The client instance with temporary configuration.\n        \"\"\"\n        previous_configuration = self.__inference_configuration\n        self.__inference_configuration = inference_configuration\n        try:\n            yield self\n        finally:\n            self.__inference_configuration = previous_configuration\n\n    def configure(\n        self, inference_configuration: InferenceConfiguration\n    ) -&gt; \"InferenceHTTPClient\":\n        \"\"\"Configure the client with new inference settings.\n\n        Args:\n            inference_configuration (InferenceConfiguration): The new configuration to apply.\n\n        Returns:\n            InferenceHTTPClient: The client instance with updated configuration.\n        \"\"\"\n        self.__inference_configuration = inference_configuration\n        return self\n\n    def select_api_v0(self) -&gt; \"InferenceHTTPClient\":\n        \"\"\"Select API version 0 for client operations.\n\n        Returns:\n            InferenceHTTPClient: The client instance with API v0 selected.\n        \"\"\"\n        self.__client_mode = HTTPClientMode.V0\n        return self\n\n    def select_api_v1(self) -&gt; \"InferenceHTTPClient\":\n        \"\"\"Select API version 1 for client operations.\n\n        Returns:\n            InferenceHTTPClient: The client instance with API v1 selected.\n        \"\"\"\n        self.__client_mode = HTTPClientMode.V1\n        return self\n\n    @contextmanager\n    def use_api_v0(self) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n        \"\"\"Temporarily use API version 0 for client operations.\n\n        Yields:\n            Generator[InferenceHTTPClient, None, None]: The client instance temporarily using API v0.\n        \"\"\"\n        previous_client_mode = self.__client_mode\n        self.__client_mode = HTTPClientMode.V0\n        try:\n            yield self\n        finally:\n            self.__client_mode = previous_client_mode\n\n    @contextmanager\n    def use_api_v1(self) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n        \"\"\"Temporarily use API version 1 for client operations.\n\n        Yields:\n            Generator[InferenceHTTPClient, None, None]: The client instance temporarily using API v1.\n        \"\"\"\n        previous_client_mode = self.__client_mode\n        self.__client_mode = HTTPClientMode.V1\n        try:\n            yield self\n        finally:\n            self.__client_mode = previous_client_mode\n\n    def select_model(self, model_id: str) -&gt; \"InferenceHTTPClient\":\n        \"\"\"Select a model for inference operations.\n\n        Args:\n            model_id (str): The identifier of the model to select.\n\n        Returns:\n            InferenceHTTPClient: The client instance with the selected model.\n        \"\"\"\n        self.__selected_model = model_id\n        return self\n\n    @contextmanager\n    def use_model(self, model_id: str) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n        \"\"\"Temporarily use a specific model for inference operations.\n\n        Args:\n            model_id (str): The identifier of the model to use.\n\n        Yields:\n            Generator[InferenceHTTPClient, None, None]: The client instance temporarily using the specified model.\n        \"\"\"\n        previous_model = self.__selected_model\n        self.__selected_model = model_id\n        try:\n            yield self\n        finally:\n            self.__selected_model = previous_model\n\n    @wrap_errors\n    def get_server_info(self) -&gt; ServerInfo:\n        \"\"\"Get information about the inference server.\n\n        Returns:\n            ServerInfo: Information about the server configuration and status.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        response = requests.get(f\"{self.__api_url}/info\")\n        response.raise_for_status()\n        response_payload = response.json()\n        return ServerInfo.from_dict(response_payload)\n\n    def infer_on_stream(\n        self,\n        input_uri: str,\n        model_id: Optional[str] = None,\n    ) -&gt; Generator[Tuple[Union[str, int], np.ndarray, dict], None, None]:\n        \"\"\"Run inference on a video stream or sequence of images.\n\n        Args:\n            input_uri (str): URI of the input stream or directory.\n            model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n        Yields:\n            Generator[Tuple[Union[str, int], np.ndarray, dict], None, None]: Tuples of (frame reference, frame data, prediction).\n        \"\"\"\n        for reference, frame in load_stream_inference_input(\n            input_uri=input_uri,\n            image_extensions=self.__inference_configuration.image_extensions_for_directory_scan,\n        ):\n            prediction = self.infer(\n                inference_input=frame,\n                model_id=model_id,\n            )\n            yield reference, frame, prediction\n\n    @wrap_errors\n    def infer(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Run inference on one or more images.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n            model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: Inference results for the input image(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        if self.__client_mode is HTTPClientMode.V0:\n            return self.infer_from_api_v0(\n                inference_input=inference_input,\n                model_id=model_id,\n            )\n        return self.infer_from_api_v1(\n            inference_input=inference_input,\n            model_id=model_id,\n        )\n\n    @wrap_errors_async\n    async def infer_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Run inference asynchronously on one or more images.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n            model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: Inference results for the input image(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        if self.__client_mode is HTTPClientMode.V0:\n            return await self.infer_from_api_v0_async(\n                inference_input=inference_input,\n                model_id=model_id,\n            )\n        return await self.infer_from_api_v1_async(\n            inference_input=inference_input,\n            model_id=model_id,\n        )\n\n    def infer_from_api_v0(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Run inference using API v0.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n            model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: Inference results for the input image(s).\n\n        Raises:\n            ModelNotSelectedError: If no model is selected.\n            APIKeyNotProvided: If API key is required but not provided.\n            InvalidModelIdentifier: If the model identifier format is invalid.\n        \"\"\"\n        requests_data = self._prepare_infer_from_api_v0_request_data(\n            inference_input=inference_input,\n            model_id=model_id,\n        )\n        responses = self._execute_infer_from_api_request(\n            requests_data=requests_data,\n        )\n        results = []\n        for request_data, response in zip(requests_data, responses):\n            if response_contains_jpeg_image(response=response):\n                visualisation = transform_visualisation_bytes(\n                    visualisation=response.content,\n                    expected_format=self.__inference_configuration.output_visualisation_format,\n                )\n                parsed_response = {\"visualization\": visualisation}\n            else:\n                parsed_response = response.json()\n                if parsed_response.get(\"visualization\") is not None:\n                    parsed_response[\"visualization\"] = transform_base64_visualisation(\n                        visualisation=parsed_response[\"visualization\"],\n                        expected_format=self.__inference_configuration.output_visualisation_format,\n                    )\n            parsed_response = adjust_prediction_to_client_scaling_factor(\n                prediction=parsed_response,\n                scaling_factor=request_data.image_scaling_factors[0],\n            )\n            results.append(parsed_response)\n        return unwrap_single_element_list(sequence=results)\n\n    def _execute_infer_from_api_request(\n        self,\n        requests_data: List[RequestData],\n    ) -&gt; List[Response]:\n        responses = execute_requests_packages(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        return responses\n\n    def _prepare_infer_from_api_v0_request_data(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; List[RequestData]:\n        model_id_to_be_used = model_id or self.__selected_model\n        _ensure_model_is_selected(model_id=model_id_to_be_used)\n        _ensure_api_key_provided(api_key=self.__api_key)\n        model_id_to_be_used = resolve_roboflow_model_alias(model_id=model_id_to_be_used)\n        model_id_chunks = model_id_to_be_used.split(\"/\")\n        if len(model_id_chunks) != 2:\n            raise InvalidModelIdentifier(\n                f\"Invalid model id: {model_id}. Expected format: project_id/model_version_id.\"\n            )\n        max_height, max_width = _determine_client_downsizing_parameters(\n            client_downsizing_disabled=self.__inference_configuration.client_downsizing_disabled,\n            model_description=None,\n            default_max_input_size=self.__inference_configuration.default_max_input_size,\n        )\n        encoded_inference_inputs = load_static_inference_input(\n            inference_input=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        params = {\n            \"api_key\": self.__api_key,\n        }\n        params.update(self.__inference_configuration.to_legacy_call_parameters())\n\n        execution_id_value = execution_id.get()\n        headers = DEFAULT_HEADERS\n        if execution_id_value:\n            headers = headers.copy()\n            headers[EXECUTION_ID_HEADER] = execution_id_value\n\n        requests_data = prepare_requests_data(\n            url=f\"{self.__api_url}/{model_id_chunks[0]}/{model_id_chunks[1]}\",\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=headers,\n            parameters=params,\n            payload=None,\n            max_batch_size=1,\n            image_placement=ImagePlacement.DATA,\n        )\n        return requests_data\n\n    async def infer_from_api_v0_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Run inference using API v0 asynchronously.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n            model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: Inference results for the input image(s).\n\n        Raises:\n            ModelNotSelectedError: If no model is selected.\n            APIKeyNotProvided: If API key is required but not provided.\n            InvalidModelIdentifier: If the model identifier format is invalid.\n        \"\"\"\n        model_id_to_be_used = model_id or self.__selected_model\n        _ensure_model_is_selected(model_id=model_id_to_be_used)\n        _ensure_api_key_provided(api_key=self.__api_key)\n        model_id_to_be_used = resolve_roboflow_model_alias(model_id=model_id_to_be_used)\n        model_id_chunks = model_id_to_be_used.split(\"/\")\n        if len(model_id_chunks) != 2:\n            raise InvalidModelIdentifier(\n                f\"Invalid model id: {model_id}. Expected format: project_id/model_version_id.\"\n            )\n        max_height, max_width = _determine_client_downsizing_parameters(\n            client_downsizing_disabled=self.__inference_configuration.client_downsizing_disabled,\n            model_description=None,\n            default_max_input_size=self.__inference_configuration.default_max_input_size,\n        )\n        encoded_inference_inputs = await load_static_inference_input_async(\n            inference_input=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        params = {\n            \"api_key\": self.__api_key,\n        }\n        params.update(self.__inference_configuration.to_legacy_call_parameters())\n\n        execution_id_value = execution_id.get()\n        headers = DEFAULT_HEADERS\n        if execution_id_value:\n            headers = headers.copy()\n            headers[EXECUTION_ID_HEADER] = execution_id_value\n\n        requests_data = prepare_requests_data(\n            url=f\"{self.__api_url}/{model_id_chunks[0]}/{model_id_chunks[1]}\",\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=headers,\n            parameters=params,\n            payload=None,\n            max_batch_size=1,\n            image_placement=ImagePlacement.DATA,\n        )\n        responses = await execute_requests_packages_async(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        results = []\n        for request_data, response in zip(requests_data, responses):\n            if not issubclass(type(response), dict):\n                visualisation = transform_visualisation_bytes(\n                    visualisation=response,\n                    expected_format=self.__inference_configuration.output_visualisation_format,\n                )\n                parsed_response = {\"visualization\": visualisation}\n            else:\n                parsed_response = response\n                if parsed_response.get(\"visualization\") is not None:\n                    parsed_response[\"visualization\"] = transform_base64_visualisation(\n                        visualisation=parsed_response[\"visualization\"],\n                        expected_format=self.__inference_configuration.output_visualisation_format,\n                    )\n            parsed_response = adjust_prediction_to_client_scaling_factor(\n                prediction=parsed_response,\n                scaling_factor=request_data.image_scaling_factors[0],\n            )\n            results.append(parsed_response)\n        return unwrap_single_element_list(sequence=results)\n\n    def infer_from_api_v1(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        requests_data = self._prepare_infer_from_api_v1_request_data(\n            inference_input=inference_input,\n            model_id=model_id,\n        )\n        responses = self._execute_infer_from_api_request(\n            requests_data=requests_data,\n        )\n        results = []\n        for request_data, response in zip(requests_data, responses):\n            parsed_response = response.json()\n            if not issubclass(type(parsed_response), list):\n                parsed_response = [parsed_response]\n            for parsed_response_element, scaling_factor in zip(\n                parsed_response, request_data.image_scaling_factors\n            ):\n                if parsed_response_element.get(\"visualization\") is not None:\n                    parsed_response_element[\"visualization\"] = (\n                        transform_base64_visualisation(\n                            visualisation=parsed_response_element[\"visualization\"],\n                            expected_format=self.__inference_configuration.output_visualisation_format,\n                        )\n                    )\n                parsed_response_element = adjust_prediction_to_client_scaling_factor(\n                    prediction=parsed_response_element,\n                    scaling_factor=scaling_factor,\n                )\n                results.append(parsed_response_element)\n        return unwrap_single_element_list(sequence=results)\n\n    def _prepare_infer_from_api_v1_request_data(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; List[RequestData]:\n        self.__ensure_v1_client_mode()\n        model_id_to_be_used = model_id or self.__selected_model\n        _ensure_model_is_selected(model_id=model_id_to_be_used)\n        model_id_to_be_used = resolve_roboflow_model_alias(model_id=model_id_to_be_used)\n        model_description = self.get_model_description(model_id=model_id_to_be_used)\n        max_height, max_width = _determine_client_downsizing_parameters(\n            client_downsizing_disabled=self.__inference_configuration.client_downsizing_disabled,\n            model_description=model_description,\n            default_max_input_size=self.__inference_configuration.default_max_input_size,\n        )\n        if model_description.task_type not in NEW_INFERENCE_ENDPOINTS:\n            raise ModelTaskTypeNotSupportedError(\n                f\"Model task {model_description.task_type} is not supported by API v1 client.\"\n            )\n        encoded_inference_inputs = load_static_inference_input(\n            inference_input=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        payload = {\n            \"api_key\": self.__api_key,\n            \"model_id\": model_id_to_be_used,\n        }\n        endpoint = NEW_INFERENCE_ENDPOINTS[model_description.task_type]\n        payload.update(\n            self.__inference_configuration.to_api_call_parameters(\n                client_mode=self.__client_mode,\n                task_type=model_description.task_type,\n            )\n        )\n        requests_data = prepare_requests_data(\n            url=f\"{self.__api_url}{endpoint}\",\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=self.__inference_configuration.max_batch_size,\n            image_placement=ImagePlacement.JSON,\n        )\n        return requests_data\n\n    async def infer_from_api_v1_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model_id: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        self.__ensure_v1_client_mode()\n        model_id_to_be_used = model_id or self.__selected_model\n        _ensure_model_is_selected(model_id=model_id_to_be_used)\n        model_id_to_be_used = resolve_roboflow_model_alias(model_id=model_id_to_be_used)\n        model_description = await self.get_model_description_async(\n            model_id=model_id_to_be_used\n        )\n        max_height, max_width = _determine_client_downsizing_parameters(\n            client_downsizing_disabled=self.__inference_configuration.client_downsizing_disabled,\n            model_description=model_description,\n            default_max_input_size=self.__inference_configuration.default_max_input_size,\n        )\n        if model_description.task_type not in NEW_INFERENCE_ENDPOINTS:\n            raise ModelTaskTypeNotSupportedError(\n                f\"Model task {model_description.task_type} is not supported by API v1 client.\"\n            )\n        encoded_inference_inputs = await load_static_inference_input_async(\n            inference_input=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        payload = {\n            \"api_key\": self.__api_key,\n            \"model_id\": model_id_to_be_used,\n        }\n        endpoint = NEW_INFERENCE_ENDPOINTS[model_description.task_type]\n        payload.update(\n            self.__inference_configuration.to_api_call_parameters(\n                client_mode=self.__client_mode,\n                task_type=model_description.task_type,\n            )\n        )\n        requests_data = prepare_requests_data(\n            url=f\"{self.__api_url}{endpoint}\",\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=self.__inference_configuration.max_batch_size,\n            image_placement=ImagePlacement.JSON,\n        )\n        responses = await execute_requests_packages_async(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        results = []\n        for request_data, parsed_response in zip(requests_data, responses):\n            if not issubclass(type(parsed_response), list):\n                parsed_response = [parsed_response]\n            for parsed_response_element, scaling_factor in zip(\n                parsed_response, request_data.image_scaling_factors\n            ):\n                if parsed_response_element.get(\"visualization\") is not None:\n                    parsed_response_element[\"visualization\"] = (\n                        transform_base64_visualisation(\n                            visualisation=parsed_response_element[\"visualization\"],\n                            expected_format=self.__inference_configuration.output_visualisation_format,\n                        )\n                    )\n                parsed_response_element = adjust_prediction_to_client_scaling_factor(\n                    prediction=parsed_response_element,\n                    scaling_factor=scaling_factor,\n                )\n                results.append(parsed_response_element)\n        return unwrap_single_element_list(sequence=results)\n\n    def get_model_description(\n        self, model_id: str, allow_loading: bool = True\n    ) -&gt; ModelDescription:\n        \"\"\"Get the description of a model.\n\n        Args:\n            model_id (str): The identifier of the model.\n            allow_loading (bool, optional): Whether to load the model if not already loaded. Defaults to True.\n\n        Returns:\n            ModelDescription: Description of the model.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            ModelNotInitializedError: If the model is not initialized and cannot be loaded.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        registered_models = self.list_loaded_models()\n        matching_model = filter_model_descriptions(\n            descriptions=registered_models.models,\n            model_id=de_aliased_model_id,\n        )\n        if matching_model is None and allow_loading is True:\n            registered_models = self.load_model(model_id=de_aliased_model_id)\n            matching_model = filter_model_descriptions(\n                descriptions=registered_models.models,\n                model_id=de_aliased_model_id,\n            )\n        if matching_model is not None:\n            return matching_model\n        raise ModelNotInitializedError(\n            f\"Model {model_id} (de-aliased: {de_aliased_model_id}) is not initialised and cannot \"\n            f\"retrieve its description.\"\n        )\n\n    async def get_model_description_async(\n        self, model_id: str, allow_loading: bool = True\n    ) -&gt; ModelDescription:\n        \"\"\"Get the description of a model asynchronously.\n\n        Args:\n            model_id (str): The identifier of the model.\n            allow_loading (bool, optional): Whether to load the model if not already loaded. Defaults to True.\n\n        Returns:\n            ModelDescription: Description of the model.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            ModelNotInitializedError: If the model is not initialized and cannot be loaded.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        registered_models = await self.list_loaded_models_async()\n        matching_model = filter_model_descriptions(\n            descriptions=registered_models.models,\n            model_id=de_aliased_model_id,\n        )\n        if matching_model is None and allow_loading is True:\n            registered_models = await self.load_model_async(\n                model_id=de_aliased_model_id\n            )\n            matching_model = filter_model_descriptions(\n                descriptions=registered_models.models,\n                model_id=de_aliased_model_id,\n            )\n        if matching_model is not None:\n            return matching_model\n        raise ModelNotInitializedError(\n            f\"Model {model_id} (de-aliased: {de_aliased_model_id}) is not initialised and cannot \"\n            f\"retrieve its description.\"\n        )\n\n    @wrap_errors\n    def list_loaded_models(self) -&gt; RegisteredModels:\n        \"\"\"List all models currently loaded on the server.\n\n        Returns:\n            RegisteredModels: Information about registered models.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        response = requests.get(\n            f\"{self.__api_url}/model/registry?api_key={self.__api_key}\"\n        )\n        response.raise_for_status()\n        response_payload = response.json()\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors_async\n    async def list_loaded_models_async(self) -&gt; RegisteredModels:\n        \"\"\"List all models currently loaded on the server asynchronously.\n\n        Returns:\n            RegisteredModels: Information about registered models.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        async with aiohttp.ClientSession() as session:\n            async with session.get(\n                f\"{self.__api_url}/model/registry?api_key={self.__api_key}\"\n            ) as response:\n                response.raise_for_status()\n                response_payload = await response.json()\n                return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors\n    def load_model(\n        self, model_id: str, set_as_default: bool = False\n    ) -&gt; RegisteredModels:\n        \"\"\"Load a model onto the server.\n\n        Args:\n            model_id (str): The identifier of the model to load.\n            set_as_default (bool, optional): Whether to set this model as the default. Defaults to False.\n\n        Returns:\n            RegisteredModels: Updated information about registered models.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        response = requests.post(\n            f\"{self.__api_url}/model/add\",\n            json={\n                \"model_id\": de_aliased_model_id,\n                \"api_key\": self.__api_key,\n            },\n            headers=DEFAULT_HEADERS,\n        )\n        response.raise_for_status()\n        response_payload = response.json()\n        if set_as_default:\n            self.__selected_model = de_aliased_model_id\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors_async\n    async def load_model_async(\n        self, model_id: str, set_as_default: bool = False\n    ) -&gt; RegisteredModels:\n        \"\"\"Load a model onto the server asynchronously.\n\n        Args:\n            model_id (str): The identifier of the model to load.\n            set_as_default (bool, optional): Whether to set this model as the default. Defaults to False.\n\n        Returns:\n            RegisteredModels: Updated information about registered models.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        payload = {\n            \"model_id\": de_aliased_model_id,\n            \"api_key\": self.__api_key,\n        }\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.__api_url}/model/add\",\n                json=payload,\n                headers=DEFAULT_HEADERS,\n            ) as response:\n                response.raise_for_status()\n                response_payload = await response.json()\n        if set_as_default:\n            self.__selected_model = de_aliased_model_id\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors\n    def unload_model(self, model_id: str) -&gt; RegisteredModels:\n        \"\"\"Unload a model from the server.\n\n        Args:\n            model_id (str): The identifier of the model to unload.\n\n        Returns:\n            RegisteredModels: Updated information about registered models.\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        response = requests.post(\n            f\"{self.__api_url}/model/remove\",\n            json={\n                \"model_id\": de_aliased_model_id,\n            },\n            headers=DEFAULT_HEADERS,\n        )\n        response.raise_for_status()\n        response_payload = response.json()\n        if (\n            de_aliased_model_id == self.__selected_model\n            or model_id == self.__selected_model\n        ):\n            self.__selected_model = None\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors_async\n    async def unload_model_async(self, model_id: str) -&gt; RegisteredModels:\n        self.__ensure_v1_client_mode()\n        de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                f\"{self.__api_url}/model/remove\",\n                json={\n                    \"model_id\": de_aliased_model_id,\n                },\n                headers=DEFAULT_HEADERS,\n            ) as response:\n                response.raise_for_status()\n                response_payload = await response.json()\n        if (\n            de_aliased_model_id == self.__selected_model\n            or model_id == self.__selected_model\n        ):\n            self.__selected_model = None\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors\n    def unload_all_models(self) -&gt; RegisteredModels:\n        self.__ensure_v1_client_mode()\n        response = requests.post(f\"{self.__api_url}/model/clear\")\n        response.raise_for_status()\n        response_payload = response.json()\n        self.__selected_model = None\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors_async\n    async def unload_all_models_async(self) -&gt; RegisteredModels:\n        self.__ensure_v1_client_mode()\n        async with aiohttp.ClientSession() as session:\n            async with session.post(f\"{self.__api_url}/model/clear\") as response:\n                response.raise_for_status()\n                response_payload = await response.json()\n        self.__selected_model = None\n        return RegisteredModels.from_dict(response_payload)\n\n    @wrap_errors\n    def ocr_image(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model: str = \"doctr\",\n        version: Optional[str] = None,\n        quantize: Optional[bool] = None,\n        generate_bounding_boxes: Optional[bool] = None,\n        language_codes: Optional[List[str]] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Run OCR on input image(s).\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for OCR.\n            model (str, optional): OCR model to use ('doctr' or 'trocr'). Defaults to \"doctr\".\n            version (Optional[str], optional): Model version to use. Defaults to None.\n                For trocr, supported versions are: 'trocr-small-printed', 'trocr-base-printed', 'trocr-large-printed'.\n            quantize: (Optional[bool]): flag of EasyOCR to decide which version of model to load\n            generate_bounding_boxes: (Optional[bool]): flag of some models (like DocTR) to decide if output variant\n                with sv.Detections(...) compatible bounding boxes should be returned (due to historical reasons, some\n                old implementations were flattening detected OCR structure into text and were only returning that as\n                results).\n            language_codes: (Optional[List[str]]): Parameter of EasyOCR that dictates the code of languages that\n                model should recognise (leave blank for default for given OCR model version).\n        Returns:\n            Union[dict, List[dict]]: OCR results for the input image(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        encoded_inference_inputs = load_static_inference_input(\n            inference_input=inference_input,\n        )\n        payload = self.__initialise_payload()\n        if version:\n            key = f\"{model.lower()}_version_id\"\n            payload[key] = version\n        if quantize is not None:\n            payload[\"quantize\"] = quantize\n        if generate_bounding_boxes is not None:\n            payload[\"generate_bounding_boxes\"] = generate_bounding_boxes\n        if language_codes is not None:\n            payload[\"language_codes\"] = language_codes\n        model_path = resolve_ocr_path(model_name=model)\n        url = self.__wrap_url_with_api_key(f\"{self.__api_url}{model_path}\")\n        requests_data = prepare_requests_data(\n            url=url,\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=1,\n            image_placement=ImagePlacement.JSON,\n        )\n        responses = execute_requests_packages(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        results = [r.json() for r in responses]\n        return unwrap_single_element_list(sequence=results)\n\n    @wrap_errors_async\n    async def ocr_image_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        model: str = \"doctr\",\n        version: Optional[str] = None,\n        quantize: Optional[bool] = None,\n        generate_bounding_boxes: Optional[bool] = None,\n        language_codes: Optional[List[str]] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Run OCR on input image(s) asynchronously.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for OCR.\n            model (str, optional): OCR model to use ('doctr' or 'trocr'). Defaults to \"doctr\".\n            version (Optional[str], optional): Model version to use. Defaults to None.\n                For trocr, supported versions are: 'trocr-small-printed', 'trocr-base-printed', 'trocr-large-printed'.\n            quantize: (Optional[bool]): flag of EasyOCR to decide which version of model to load\n            generate_bounding_boxes: (Optional[bool]): flag of some models (like DocTR) to decide if output variant\n                with sv.Detections(...) compatible bounding boxes should be returned (due to historical reasons, some\n                old implementations were flattening detected OCR structure into text and were only returning that as\n                results).\n            language_codes: (Optional[List[str]]): Parameter of EasyOCR that dictates the code of languages that\n                model should recognise (leave blank for default for given OCR model version).\n        Returns:\n            Union[dict, List[dict]]: OCR results for the input image(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        encoded_inference_inputs = await load_static_inference_input_async(\n            inference_input=inference_input,\n        )\n        payload = self.__initialise_payload()\n        if version:\n            key = f\"{model.lower()}_version_id\"\n            payload[key] = version\n        if quantize is not None:\n            payload[\"quantize\"] = quantize\n        if generate_bounding_boxes is not None:\n            payload[\"generate_bounding_boxes\"] = generate_bounding_boxes\n        if language_codes is not None:\n            payload[\"language_codes\"] = language_codes\n        model_path = resolve_ocr_path(model_name=model)\n        url = self.__wrap_url_with_api_key(f\"{self.__api_url}{model_path}\")\n        requests_data = prepare_requests_data(\n            url=url,\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=1,\n            image_placement=ImagePlacement.JSON,\n        )\n        responses = await execute_requests_packages_async(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        return unwrap_single_element_list(sequence=responses)\n\n    @wrap_errors\n    def detect_gazes(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Detect gazes in input image(s).\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for gaze detection.\n\n        Returns:\n            Union[dict, List[dict]]: Gaze detection results for the input image(s).\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()  # Lambda does not support Gaze, so we require v1 mode of client\n        result = self._post_images(\n            inference_input=inference_input, endpoint=\"/gaze/gaze_detection\"\n        )\n        return combine_gaze_detections(detections=result)\n\n    @wrap_errors_async\n    async def detect_gazes_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Detect gazes in input image(s) asynchronously.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for gaze detection.\n\n        Returns:\n            Union[dict, List[dict]]: Gaze detection results for the input image(s).\n\n        Raises:\n            WrongClientModeError: If not in API v1 mode.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        self.__ensure_v1_client_mode()  # Lambda does not support Gaze, so we require v1 mode of client\n        result = await self._post_images_async(\n            inference_input=inference_input, endpoint=\"/gaze/gaze_detection\"\n        )\n        return combine_gaze_detections(detections=result)\n\n    @wrap_errors\n    def get_clip_image_embeddings(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        clip_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Get CLIP embeddings for input image(s).\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) to embed.\n            clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: CLIP embeddings for the input image(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        extra_payload = {}\n        if clip_version is not None:\n            extra_payload[\"clip_version_id\"] = clip_version\n        result = self._post_images(\n            inference_input=inference_input,\n            endpoint=\"/clip/embed_image\",\n            extra_payload=extra_payload,\n        )\n        result = combine_clip_embeddings(embeddings=result)\n        return unwrap_single_element_list(result)\n\n    @wrap_errors_async\n    async def get_clip_image_embeddings_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        clip_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Get CLIP embeddings for input image(s) asynchronously.\n\n        Args:\n            inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) to embed.\n            clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: CLIP embeddings for the input image(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        extra_payload = {}\n        if clip_version is not None:\n            extra_payload[\"clip_version_id\"] = clip_version\n        result = await self._post_images_async(\n            inference_input=inference_input,\n            endpoint=\"/clip/embed_image\",\n            extra_payload=extra_payload,\n        )\n        result = combine_clip_embeddings(embeddings=result)\n        return unwrap_single_element_list(result)\n\n    @wrap_errors\n    def get_clip_text_embeddings(\n        self,\n        text: Union[str, List[str]],\n        clip_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Get CLIP embeddings for input text(s).\n\n        Args:\n            text (Union[str, List[str]]): Input text(s) to embed.\n            clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: CLIP embeddings for the input text(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        payload = self.__initialise_payload()\n        payload[\"text\"] = text\n        if clip_version is not None:\n            payload[\"clip_version_id\"] = clip_version\n        headers = DEFAULT_HEADERS.copy()\n        execution_id_value = execution_id.get()\n        if execution_id_value is not None:\n            headers[EXECUTION_ID_HEADER] = execution_id_value\n\n        response = requests.post(\n            self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/embed_text\"),\n            json=payload,\n            headers=headers,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return unwrap_single_element_list(sequence=response.json())\n\n    @wrap_errors_async\n    async def get_clip_text_embeddings_async(\n        self,\n        text: Union[str, List[str]],\n        clip_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Get CLIP embeddings for input text(s) asynchronously.\n\n        Args:\n            text (Union[str, List[str]]): Input text(s) to embed.\n            clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: CLIP embeddings for the input text(s).\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        payload = self.__initialise_payload()\n        payload[\"text\"] = text\n        if clip_version is not None:\n            payload[\"clip_version_id\"] = clip_version\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/embed_text\"),\n                json=payload,\n                headers=DEFAULT_HEADERS,\n            ) as response:\n                response.raise_for_status()\n                response_payload = await response.json()\n        return unwrap_single_element_list(sequence=response_payload)\n\n    @wrap_errors\n    def clip_compare(\n        self,\n        subject: Union[str, ImagesReference],\n        prompt: Union[str, List[str], ImagesReference, List[ImagesReference]],\n        subject_type: str = \"image\",\n        prompt_type: str = \"text\",\n        clip_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Compare a subject against prompts using CLIP embeddings.\n\n        Args:\n            subject (Union[str, ImagesReference]): The subject to compare (image or text).\n            prompt (Union[str, List[str], ImagesReference, List[ImagesReference]]): The prompt(s) to compare against.\n            subject_type (str, optional): Type of subject ('image' or 'text'). Defaults to \"image\".\n            prompt_type (str, optional): Type of prompt(s) ('image' or 'text'). Defaults to \"text\".\n            clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: Comparison results between subject and prompt(s).\n\n        Raises:\n            InvalidParameterError: If subject_type or prompt_type is invalid.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        if (\n            subject_type not in CLIP_ARGUMENT_TYPES\n            or prompt_type not in CLIP_ARGUMENT_TYPES\n        ):\n            raise InvalidParameterError(\n                f\"Could not accept `subject_type` and `prompt_type` with values different than {CLIP_ARGUMENT_TYPES}\"\n            )\n        payload = self.__initialise_payload()\n        payload[\"subject_type\"] = subject_type\n        payload[\"prompt_type\"] = prompt_type\n        if clip_version is not None:\n            payload[\"clip_version_id\"] = clip_version\n        if subject_type == \"image\":\n            encoded_image = load_static_inference_input(\n                inference_input=subject,\n            )\n            payload = inject_images_into_payload(\n                payload=payload, encoded_images=encoded_image, key=\"subject\"\n            )\n        else:\n            payload[\"subject\"] = subject\n        if prompt_type == \"image\":\n            encoded_inference_inputs = load_static_inference_input(\n                inference_input=prompt,\n            )\n            payload = inject_images_into_payload(\n                payload=payload, encoded_images=encoded_inference_inputs, key=\"prompt\"\n            )\n        else:\n            payload[\"prompt\"] = prompt\n\n        headers = DEFAULT_HEADERS.copy()\n        execution_id_value = execution_id.get()\n        if execution_id_value is not None:\n            headers[EXECUTION_ID_HEADER] = execution_id_value\n\n        response = requests.post(\n            self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/compare\"),\n            json=payload,\n            headers=headers,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    @wrap_errors_async\n    async def clip_compare_async(\n        self,\n        subject: Union[str, ImagesReference],\n        prompt: Union[str, List[str], ImagesReference, List[ImagesReference]],\n        subject_type: str = \"image\",\n        prompt_type: str = \"text\",\n        clip_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Compare a subject against prompts using CLIP embeddings asynchronously.\n\n        Args:\n            subject (Union[str, ImagesReference]): The subject to compare (image or text).\n            prompt (Union[str, List[str], ImagesReference, List[ImagesReference]]): The prompt(s) to compare against.\n            subject_type (str, optional): Type of subject ('image' or 'text'). Defaults to \"image\".\n            prompt_type (str, optional): Type of prompt(s) ('image' or 'text'). Defaults to \"text\".\n            clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n        Returns:\n            Union[dict, List[dict]]: Comparison results between subject and prompt(s).\n\n        Raises:\n            InvalidParameterError: If subject_type or prompt_type is invalid.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        if (\n            subject_type not in CLIP_ARGUMENT_TYPES\n            or prompt_type not in CLIP_ARGUMENT_TYPES\n        ):\n            raise InvalidParameterError(\n                f\"Could not accept `subject_type` and `prompt_type` with values different than {CLIP_ARGUMENT_TYPES}\"\n            )\n        payload = self.__initialise_payload()\n        payload[\"subject_type\"] = subject_type\n        payload[\"prompt_type\"] = prompt_type\n        if clip_version is not None:\n            payload[\"clip_version_id\"] = clip_version\n        if subject_type == \"image\":\n            encoded_image = await load_static_inference_input_async(\n                inference_input=subject,\n            )\n            payload = inject_images_into_payload(\n                payload=payload, encoded_images=encoded_image, key=\"subject\"\n            )\n        else:\n            payload[\"subject\"] = subject\n        if prompt_type == \"image\":\n            encoded_inference_inputs = await load_static_inference_input_async(\n                inference_input=prompt,\n            )\n            payload = inject_images_into_payload(\n                payload=payload, encoded_images=encoded_inference_inputs, key=\"prompt\"\n            )\n        else:\n            payload[\"prompt\"] = prompt\n\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/compare\"),\n                json=payload,\n                headers=DEFAULT_HEADERS,\n            ) as response:\n                response.raise_for_status()\n                return await response.json()\n\n    @wrap_errors\n    def get_perception_encoder_image_embeddings(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        perception_encoder_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Get Perception Encoder embeddings for input image(s).\"\"\"\n        extra_payload = {}\n        if perception_encoder_version is not None:\n            extra_payload[\"perception_encoder_version_id\"] = perception_encoder_version\n        result = self._post_images(\n            inference_input=inference_input,\n            endpoint=\"/perception_encoder/embed_image\",\n            extra_payload=extra_payload,\n        )\n        return unwrap_single_element_list(result)\n\n    @wrap_errors\n    def get_perception_encoder_text_embeddings(\n        self,\n        text: Union[str, List[str]],\n        perception_encoder_version: Optional[str] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        \"\"\"Get Perception Encoder embeddings for input text(s).\"\"\"\n        payload = self.__initialise_payload()\n        payload[\"text\"] = text\n        if perception_encoder_version is not None:\n            payload[\"perception_encoder_version_id\"] = perception_encoder_version\n\n        headers = DEFAULT_HEADERS.copy()\n        execution_id_value = execution_id.get()\n        if execution_id_value is not None:\n            headers[EXECUTION_ID_HEADER] = execution_id_value\n\n        response = requests.post(\n            self.__wrap_url_with_api_key(\n                f\"{self.__api_url}/perception_encoder/embed_text\"\n            ),\n            json=payload,\n            headers=headers,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return unwrap_single_element_list(sequence=response.json())\n\n    @deprecated(\n        reason=\"Please use run_workflow(...) method. This method will be removed end of Q2 2024\"\n    )\n    @wrap_errors\n    def infer_from_workflow(\n        self,\n        workspace_name: Optional[str] = None,\n        workflow_name: Optional[str] = None,\n        specification: Optional[dict] = None,\n        images: Optional[Dict[str, Any]] = None,\n        parameters: Optional[Dict[str, Any]] = None,\n        excluded_fields: Optional[List[str]] = None,\n        use_cache: bool = True,\n        enable_profiling: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Run inference using a workflow specification.\n\n        Triggers inference from workflow specification at the inference HTTP\n        side. Either (`workspace_name` and `workflow_name`) or `workflow_specification` must be\n        provided. In the first case - definition of workflow will be fetched\n        from Roboflow API, in the latter - `workflow_specification` will be\n        used. `images` and `parameters` will be merged into workflow inputs,\n        the distinction is made to make sure the SDK can easily serialise\n        images and prepare a proper payload. Supported images are numpy arrays,\n        PIL.Image and base64 images, links to images and local paths.\n        `excluded_fields` will be added to request to filter out results\n        of workflow execution at the server side.\n\n        Args:\n            workspace_name (Optional[str], optional): Name of the workspace containing the workflow. Defaults to None.\n            workflow_name (Optional[str], optional): Name of the workflow. Defaults to None.\n            specification (Optional[dict], optional): Direct workflow specification. Defaults to None.\n            images (Optional[Dict[str, Any]], optional): Images to process. Defaults to None.\n            parameters (Optional[Dict[str, Any]], optional): Additional parameters for the workflow. Defaults to None.\n            excluded_fields (Optional[List[str]], optional): Fields to exclude from results. Defaults to None.\n            use_cache (bool, optional): Whether to use cached results. Defaults to True.\n            enable_profiling (bool, optional): Whether to enable profiling. Defaults to False.\n\n        Returns:\n            List[Dict[str, Any]]: Results of the workflow execution.\n\n        Raises:\n            InvalidParameterError: If neither workflow identifiers nor specification is provided.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        return self._run_workflow(\n            workspace_name=workspace_name,\n            workflow_id=workflow_name,\n            specification=specification,\n            images=images,\n            parameters=parameters,\n            excluded_fields=excluded_fields,\n            legacy_endpoints=True,\n            use_cache=use_cache,\n            enable_profiling=enable_profiling,\n        )\n\n    @wrap_errors\n    def run_workflow(\n        self,\n        workspace_name: Optional[str] = None,\n        workflow_id: Optional[str] = None,\n        specification: Optional[dict] = None,\n        images: Optional[Dict[str, Any]] = None,\n        parameters: Optional[Dict[str, Any]] = None,\n        excluded_fields: Optional[List[str]] = None,\n        use_cache: bool = True,\n        enable_profiling: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Run inference using a workflow specification.\n\n        Triggers inference from workflow specification at the inference HTTP\n        side. Either (`workspace_name` and `workflow_id`) or `workflow_specification` must be\n        provided. In the first case - definition of workflow will be fetched\n        from Roboflow API, in the latter - `workflow_specification` will be\n        used. `images` and `parameters` will be merged into workflow inputs,\n        the distinction is made to make sure the SDK can easily serialise\n        images and prepare a proper payload. Supported images are numpy arrays,\n        PIL.Image and base64 images, links to images and local paths.\n        `excluded_fields` will be added to request to filter out results\n        of workflow execution at the server side.\n\n        **Important!**\n        Method is not compatible with inference server &lt;=0.9.18. Please migrate to newer version of\n        the server before end of Q2 2024. Until that is done - use old method: infer_from_workflow(...).\n\n        Note:\n            Method is not compatible with inference server &lt;=0.9.18. Please migrate to newer version of\n            the server before end of Q2 2024. Until that is done - use old method: infer_from_workflow(...).\n\n        Args:\n            workspace_name (Optional[str], optional): Name of the workspace containing the workflow. Defaults to None.\n            workflow_id (Optional[str], optional): ID of the workflow. Defaults to None.\n            specification (Optional[dict], optional): Direct workflow specification. Defaults to None.\n            images (Optional[Dict[str, Any]], optional): Images to process. Defaults to None.\n            parameters (Optional[Dict[str, Any]], optional): Additional parameters for the workflow. Defaults to None.\n            excluded_fields (Optional[List[str]], optional): Fields to exclude from results. Defaults to None.\n            use_cache (bool, optional): Whether to use cached results. Defaults to True.\n            enable_profiling (bool, optional): Whether to enable profiling. Defaults to False.\n\n        Returns:\n            List[Dict[str, Any]]: Results of the workflow execution.\n\n        Raises:\n            InvalidParameterError: If neither workflow identifiers nor specification is provided.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        return self._run_workflow(\n            workspace_name=workspace_name,\n            workflow_id=workflow_id,\n            specification=specification,\n            images=images,\n            parameters=parameters,\n            excluded_fields=excluded_fields,\n            legacy_endpoints=False,\n            use_cache=use_cache,\n            enable_profiling=enable_profiling,\n        )\n\n    def _run_workflow(\n        self,\n        workspace_name: Optional[str] = None,\n        workflow_id: Optional[str] = None,\n        specification: Optional[dict] = None,\n        images: Optional[Dict[str, Any]] = None,\n        parameters: Optional[Dict[str, Any]] = None,\n        excluded_fields: Optional[List[str]] = None,\n        legacy_endpoints: bool = False,\n        use_cache: bool = True,\n        enable_profiling: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        response = self._execute_workflow_request(\n            workspace_name=workspace_name,\n            workflow_id=workflow_id,\n            specification=specification,\n            images=images,\n            parameters=parameters,\n            excluded_fields=excluded_fields,\n            legacy_endpoints=legacy_endpoints,\n            use_cache=use_cache,\n            enable_profiling=enable_profiling,\n        )\n        response_data = response.json()\n        workflow_outputs = response_data[\"outputs\"]\n        profiler_trace = response_data.get(\"profiler_trace\", [])\n        if enable_profiling:\n            save_workflows_profiler_trace(\n                directory=self.__inference_configuration.profiling_directory,\n                profiler_trace=profiler_trace,\n            )\n        return decode_workflow_outputs(\n            workflow_outputs=workflow_outputs,\n            expected_format=self.__inference_configuration.output_visualisation_format,\n        )\n\n    def _execute_workflow_request(\n        self,\n        workspace_name: Optional[str] = None,\n        workflow_id: Optional[str] = None,\n        specification: Optional[dict] = None,\n        images: Optional[Dict[str, Any]] = None,\n        parameters: Optional[Dict[str, Any]] = None,\n        excluded_fields: Optional[List[str]] = None,\n        legacy_endpoints: bool = False,\n        use_cache: bool = True,\n        enable_profiling: bool = False,\n    ) -&gt; Response:\n        named_workflow_specified = (workspace_name is not None) and (\n            workflow_id is not None\n        )\n        if not (named_workflow_specified != (specification is not None)):\n            raise InvalidParameterError(\n                \"Parameters (`workspace_name`, `workflow_id` / `workflow_name`) can be used mutually exclusive with \"\n                \"`specification`, but at least one must be set.\"\n            )\n        if images is None:\n            images = {}\n        if parameters is None:\n            parameters = {}\n        payload = {\n            \"api_key\": self.__api_key,\n            \"use_cache\": use_cache,\n            \"enable_profiling\": enable_profiling,\n        }\n        inputs = {}\n        for image_name, image in images.items():\n            loaded_image = load_nested_batches_of_inference_input(\n                inference_input=image,\n            )\n            inject_nested_batches_of_images_into_payload(\n                payload=inputs,\n                encoded_images=loaded_image,\n                key=image_name,\n            )\n        inputs.update(parameters)\n        payload[\"inputs\"] = inputs\n        if excluded_fields is not None:\n            payload[\"excluded_fields\"] = excluded_fields\n        if specification is not None:\n            payload[\"specification\"] = specification\n        if specification is not None:\n            if legacy_endpoints:\n                url = f\"{self.__api_url}/infer/workflows\"\n            else:\n                url = f\"{self.__api_url}/workflows/run\"\n        else:\n            if legacy_endpoints:\n                url = f\"{self.__api_url}/infer/workflows/{workspace_name}/{workflow_id}\"\n            else:\n                url = f\"{self.__api_url}/{workspace_name}/workflows/{workflow_id}\"\n        response = send_post_request(\n            url=url,\n            payload=payload,\n            headers=DEFAULT_HEADERS,\n            enable_retries=self.__inference_configuration.workflow_run_retries_enabled,\n        )\n        return response\n\n    @wrap_errors\n    def infer_from_yolo_world(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        class_names: List[str],\n        model_version: Optional[str] = None,\n        confidence: Optional[float] = None,\n    ) -&gt; List[dict]:\n        \"\"\"Run inference using YOLO-World model.\n\n        Args:\n            inference_input: Input image(s) to run inference on. Can be a single image\n                reference or a list of image references.\n            class_names: List of class names to detect in the image(s).\n            model_version: Optional version of YOLO-World model to use. If not specified,\n                uses the default version.\n            confidence: Optional confidence threshold for detections. If not specified,\n                uses the model's default threshold.\n\n        Returns:\n            List of dictionaries containing detection results for each input image.\n            Each dictionary contains bounding boxes, class labels, and confidence scores\n            for detected objects.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        encoded_inference_inputs = load_static_inference_input(\n            inference_input=inference_input,\n        )\n        payload = self.__initialise_payload()\n        payload[\"text\"] = class_names\n        if model_version is not None:\n            payload[\"yolo_world_version_id\"] = model_version\n        if confidence is not None:\n            payload[\"confidence\"] = confidence\n        url = self.__wrap_url_with_api_key(f\"{self.__api_url}/yolo_world/infer\")\n        requests_data = prepare_requests_data(\n            url=url,\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=1,\n            image_placement=ImagePlacement.JSON,\n        )\n        responses = execute_requests_packages(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        return [r.json() for r in responses]\n\n    @wrap_errors_async\n    async def infer_from_yolo_world_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        class_names: List[str],\n        model_version: Optional[str] = None,\n        confidence: Optional[float] = None,\n    ) -&gt; List[dict]:\n        \"\"\"Run inference using YOLO-World model asynchronously.\n\n        Args:\n            inference_input: Input image(s) to run inference on. Can be a single image\n                reference or a list of image references.\n            class_names: List of class names to detect in the image(s).\n            model_version: Optional version of YOLO-World model to use. If not specified,\n                uses the default version.\n            confidence: Optional confidence threshold for detections. If not specified,\n                uses the model's default threshold.\n\n        Returns:\n            List of dictionaries containing detection results for each input image.\n            Each dictionary contains bounding boxes, class labels, and confidence scores\n            for detected objects.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        encoded_inference_inputs = await load_static_inference_input_async(\n            inference_input=inference_input,\n        )\n        payload = self.__initialise_payload()\n        payload[\"text\"] = class_names\n        if model_version is not None:\n            payload[\"yolo_world_version_id\"] = model_version\n        if confidence is not None:\n            payload[\"confidence\"] = confidence\n        url = self.__wrap_url_with_api_key(f\"{self.__api_url}/yolo_world/infer\")\n        requests_data = prepare_requests_data(\n            url=url,\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=1,\n            image_placement=ImagePlacement.JSON,\n        )\n        return await execute_requests_packages_async(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def start_inference_pipeline_with_workflow(\n        self,\n        video_reference: Union[str, int, List[Union[str, int]]],\n        workflow_specification: Optional[dict] = None,\n        workspace_name: Optional[str] = None,\n        workflow_id: Optional[str] = None,\n        image_input_name: str = \"image\",\n        workflows_parameters: Optional[Dict[str, Any]] = None,\n        workflows_thread_pool_workers: int = 4,\n        cancel_thread_pool_tasks_on_exit: bool = True,\n        video_metadata_input_name: str = \"video_metadata\",\n        max_fps: Optional[Union[float, int]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = \"DROP_OLDEST\",\n        source_buffer_consumption_strategy: Optional[\n            BufferConsumptionStrategy\n        ] = \"EAGER\",\n        video_source_properties: Optional[Dict[str, float]] = None,\n        batch_collection_timeout: Optional[float] = None,\n        results_buffer_size: int = 64,\n    ) -&gt; dict:\n        \"\"\"Starts an inference pipeline using a workflow specification.\n\n        Args:\n            video_reference: Path to video file, camera index, or list of video sources.\n                Can be a string path, integer camera index, or list of either.\n            workflow_specification: Optional workflow specification dictionary. Mutually\n                exclusive with workspace_name/workflow_id.\n            workspace_name: Optional name of workspace containing workflow. Must be used\n                with workflow_id.\n            workflow_id: Optional ID of workflow to use. Must be used with workspace_name.\n            image_input_name: Name of the image input node in workflow. Defaults to \"image\".\n            workflows_parameters: Optional parameters to pass to workflow.\n            workflows_thread_pool_workers: Number of worker threads for workflow execution.\n                Defaults to 4.\n            cancel_thread_pool_tasks_on_exit: Whether to cancel pending tasks when exiting.\n                Defaults to True.\n            video_metadata_input_name: Name of video metadata input in workflow.\n                Defaults to \"video_metadata\".\n            max_fps: Optional maximum FPS to process video at.\n            source_buffer_filling_strategy: Strategy for filling source buffer when full.\n                One of: \"WAIT\", \"DROP_OLDEST\", \"ADAPTIVE_DROP_OLDEST\", \"DROP_LATEST\",\n                \"ADAPTIVE_DROP_LATEST\". Defaults to \"DROP_OLDEST\".\n            source_buffer_consumption_strategy: Strategy for consuming from source buffer.\n                One of: \"LAZY\", \"EAGER\". Defaults to \"EAGER\".\n            video_source_properties: Optional dictionary of video source properties.\n            batch_collection_timeout: Optional timeout for batch collection in seconds.\n            results_buffer_size: Size of results buffer. Defaults to 64.\n\n        Returns:\n            dict: Response containing pipeline initialization details.\n\n        Raises:\n            InvalidParameterError: If workflow specification parameters are invalid.\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        named_workflow_specified = (workspace_name is not None) and (\n            workflow_id is not None\n        )\n        if not (named_workflow_specified != (workflow_specification is not None)):\n            raise InvalidParameterError(\n                \"Parameters (`workspace_name`, `workflow_id`) can be used mutually exclusive with \"\n                \"`workflow_specification`, but at least one must be set.\"\n            )\n        payload = {\n            \"api_key\": self.__api_key,\n            \"video_configuration\": {\n                \"type\": \"VideoConfiguration\",\n                \"video_reference\": video_reference,\n                \"max_fps\": max_fps,\n                \"source_buffer_filling_strategy\": source_buffer_filling_strategy,\n                \"source_buffer_consumption_strategy\": source_buffer_consumption_strategy,\n                \"video_source_properties\": video_source_properties,\n                \"batch_collection_timeout\": batch_collection_timeout,\n            },\n            \"processing_configuration\": {\n                \"type\": \"WorkflowConfiguration\",\n                \"workflow_specification\": workflow_specification,\n                \"workspace_name\": workspace_name,\n                \"workflow_id\": workflow_id,\n                \"image_input_name\": image_input_name,\n                \"workflows_parameters\": workflows_parameters,\n                \"workflows_thread_pool_workers\": workflows_thread_pool_workers,\n                \"cancel_thread_pool_tasks_on_exit\": cancel_thread_pool_tasks_on_exit,\n                \"video_metadata_input_name\": video_metadata_input_name,\n            },\n            \"sink_configuration\": {\n                \"type\": \"MemorySinkConfiguration\",\n                \"results_buffer_size\": results_buffer_size,\n            },\n        }\n        response = requests.post(\n            f\"{self.__api_url}/inference_pipelines/initialise\",\n            json=payload,\n        )\n        response.raise_for_status()\n        return response.json()\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def list_inference_pipelines(self) -&gt; List[dict]:\n        \"\"\"Lists all active inference pipelines on the server.\n\n        This method retrieves information about all currently running inference pipelines\n        on the server, including their IDs and status.\n\n        Returns:\n            List[dict]: A list of dictionaries containing information about each active\n                inference pipeline.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n        \"\"\"\n        payload = {\"api_key\": self.__api_key}\n        response = requests.get(\n            f\"{self.__api_url}/inference_pipelines/list\",\n            json=payload,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def get_inference_pipeline_status(self, pipeline_id: str) -&gt; dict:\n        \"\"\"Gets the current status of a specific inference pipeline.\n\n        Args:\n            pipeline_id: The unique identifier of the inference pipeline to check.\n\n        Returns:\n            dict: A dictionary containing the current status and details of the pipeline.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n            ValueError: If pipeline_id is empty or None.\n        \"\"\"\n        self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n        payload = {\"api_key\": self.__api_key}\n        response = requests.get(\n            f\"{self.__api_url}/inference_pipelines/{pipeline_id}/status\",\n            json=payload,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def pause_inference_pipeline(self, pipeline_id: str) -&gt; dict:\n        \"\"\"Pauses a running inference pipeline.\n\n        Sends a request to pause the specified inference pipeline. The pipeline must be\n        currently running for this operation to succeed.\n\n        Args:\n            pipeline_id: The unique identifier of the inference pipeline to pause.\n\n        Returns:\n            dict: A dictionary containing the response from the server about the pause operation.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n            ValueError: If pipeline_id is empty or None.\n        \"\"\"\n        self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n        payload = {\"api_key\": self.__api_key}\n        response = requests.post(\n            f\"{self.__api_url}/inference_pipelines/{pipeline_id}/pause\",\n            json=payload,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def resume_inference_pipeline(self, pipeline_id: str) -&gt; dict:\n        \"\"\"Resumes a paused inference pipeline.\n\n        Sends a request to resume the specified inference pipeline. The pipeline must be\n        currently paused for this operation to succeed.\n\n        Args:\n            pipeline_id: The unique identifier of the inference pipeline to resume.\n\n        Returns:\n            dict: A dictionary containing the response from the server about the resume operation.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n            ValueError: If pipeline_id is empty or None.\n        \"\"\"\n        self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n        payload = {\"api_key\": self.__api_key}\n        response = requests.post(\n            f\"{self.__api_url}/inference_pipelines/{pipeline_id}/resume\",\n            json=payload,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def terminate_inference_pipeline(self, pipeline_id: str) -&gt; dict:\n        \"\"\"Terminates a running inference pipeline.\n\n        Sends a request to terminate the specified inference pipeline. This will stop all\n        processing and free up associated resources.\n\n        Args:\n            pipeline_id: The unique identifier of the inference pipeline to terminate.\n\n        Returns:\n            dict: A dictionary containing the response from the server about the termination operation.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n            ValueError: If pipeline_id is empty or None.\n        \"\"\"\n        self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n        payload = {\"api_key\": self.__api_key}\n        response = requests.post(\n            f\"{self.__api_url}/inference_pipelines/{pipeline_id}/terminate\",\n            json=payload,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    @experimental(\n        info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n    )\n    @wrap_errors\n    def consume_inference_pipeline_result(\n        self,\n        pipeline_id: str,\n        excluded_fields: Optional[List[str]] = None,\n    ) -&gt; dict:\n        \"\"\"Consumes and returns the next available result from an inference pipeline.\n\n        Args:\n            pipeline_id: The unique identifier of the inference pipeline to consume results from.\n            excluded_fields: Optional list of field names to exclude from the result. If None,\n                no fields will be excluded.\n\n        Returns:\n            dict: A dictionary containing the next available result from the pipeline.\n\n        Raises:\n            HTTPCallErrorError: If there is an error in the HTTP call.\n            HTTPClientError: If there is an error with the server connection.\n            InvalidParameterError: If pipeline_id is empty or None.\n        \"\"\"\n        self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n        if excluded_fields is None:\n            excluded_fields = []\n        payload = {\"api_key\": self.__api_key, \"excluded_fields\": excluded_fields}\n        response = requests.get(\n            f\"{self.__api_url}/inference_pipelines/{pipeline_id}/consume\",\n            json=payload,\n        )\n        api_key_safe_raise_for_status(response=response)\n        return response.json()\n\n    def _ensure_pipeline_id_not_empty(self, pipeline_id: str) -&gt; None:\n        if not pipeline_id:\n            raise InvalidParameterError(\"Empty `pipeline_id` parameter detected\")\n\n    def _post_images(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        endpoint: str,\n        model_id: Optional[str] = None,\n        extra_payload: Optional[Dict[str, Any]] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        encoded_inference_inputs = load_static_inference_input(\n            inference_input=inference_input,\n        )\n        payload = self.__initialise_payload()\n        if model_id is not None:\n            payload[\"model_id\"] = model_id\n        url = self.__wrap_url_with_api_key(f\"{self.__api_url}{endpoint}\")\n        if extra_payload is not None:\n            payload.update(extra_payload)\n        requests_data = prepare_requests_data(\n            url=url,\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=self.__inference_configuration.max_batch_size,\n            image_placement=ImagePlacement.JSON,\n        )\n        responses = execute_requests_packages(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        results = [r.json() for r in responses]\n        return unwrap_single_element_list(sequence=results)\n\n    async def _post_images_async(\n        self,\n        inference_input: Union[ImagesReference, List[ImagesReference]],\n        endpoint: str,\n        model_id: Optional[str] = None,\n        extra_payload: Optional[Dict[str, Any]] = None,\n    ) -&gt; Union[dict, List[dict]]:\n        encoded_inference_inputs = await load_static_inference_input_async(\n            inference_input=inference_input,\n        )\n        payload = self.__initialise_payload()\n        if model_id is not None:\n            payload[\"model_id\"] = model_id\n        url = self.__wrap_url_with_api_key(f\"{self.__api_url}{endpoint}\")\n        if extra_payload is not None:\n            payload.update(extra_payload)\n        requests_data = prepare_requests_data(\n            url=url,\n            encoded_inference_inputs=encoded_inference_inputs,\n            headers=DEFAULT_HEADERS,\n            parameters=None,\n            payload=payload,\n            max_batch_size=self.__inference_configuration.max_batch_size,\n            image_placement=ImagePlacement.JSON,\n        )\n        responses = await execute_requests_packages_async(\n            requests_data=requests_data,\n            request_method=RequestMethod.POST,\n            max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n        )\n        return unwrap_single_element_list(sequence=responses)\n\n    def __initialise_payload(self) -&gt; dict:\n        if self.__client_mode is not HTTPClientMode.V0:\n            return {\"api_key\": self.__api_key}\n        return {}\n\n    def __wrap_url_with_api_key(self, url: str) -&gt; str:\n        if self.__client_mode is not HTTPClientMode.V0:\n            return url\n        return f\"{url}?api_key={self.__api_key}\"\n\n    def __ensure_v1_client_mode(self) -&gt; None:\n        if self.__client_mode is not HTTPClientMode.V1:\n            raise WrongClientModeError(\"Use client mode `v1` to run this operation.\")\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.client_mode","title":"<code>client_mode</code>  <code>property</code>","text":"<p>Get the current client mode.</p> <p>Returns:</p> Name Type Description <code>HTTPClientMode</code> <code>HTTPClientMode</code> <p>The current API version mode (V0 or V1).</p>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.inference_configuration","title":"<code>inference_configuration</code>  <code>property</code>","text":"<p>Get the current inference configuration.</p> <p>Returns:</p> Name Type Description <code>InferenceConfiguration</code> <code>InferenceConfiguration</code> <p>The current inference configuration settings.</p>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.selected_model","title":"<code>selected_model</code>  <code>property</code>","text":"<p>Get the currently selected model identifier.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The identifier of the currently selected model, if any.</p>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.__init__","title":"<code>__init__(api_url, api_key=None)</code>","text":"<p>Initialize a new InferenceHTTPClient instance.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>The base URL for the inference API.</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def __init__(\n    self,\n    api_url: str,\n    api_key: Optional[str] = None,\n):\n    \"\"\"Initialize a new InferenceHTTPClient instance.\n\n    Args:\n        api_url (str): The base URL for the inference API.\n        api_key (Optional[str], optional): API key for authentication. Defaults to None.\n    \"\"\"\n    self.__api_url = api_url\n    self.__api_key = api_key\n    self.__inference_configuration = InferenceConfiguration.init_default()\n    self.__client_mode = _determine_client_mode(api_url=api_url)\n    self.__selected_model: Optional[str] = None\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.clip_compare","title":"<code>clip_compare(subject, prompt, subject_type='image', prompt_type='text', clip_version=None)</code>","text":"<p>Compare a subject against prompts using CLIP embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>Union[str, ImagesReference]</code> <p>The subject to compare (image or text).</p> required <code>prompt</code> <code>Union[str, List[str], ImagesReference, List[ImagesReference]]</code> <p>The prompt(s) to compare against.</p> required <code>subject_type</code> <code>str</code> <p>Type of subject ('image' or 'text'). Defaults to \"image\".</p> <code>'image'</code> <code>prompt_type</code> <code>str</code> <p>Type of prompt(s) ('image' or 'text'). Defaults to \"text\".</p> <code>'text'</code> <code>clip_version</code> <code>Optional[str]</code> <p>Version of CLIP model to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Comparison results between subject and prompt(s).</p> <p>Raises:</p> Type Description <code>InvalidParameterError</code> <p>If subject_type or prompt_type is invalid.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef clip_compare(\n    self,\n    subject: Union[str, ImagesReference],\n    prompt: Union[str, List[str], ImagesReference, List[ImagesReference]],\n    subject_type: str = \"image\",\n    prompt_type: str = \"text\",\n    clip_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Compare a subject against prompts using CLIP embeddings.\n\n    Args:\n        subject (Union[str, ImagesReference]): The subject to compare (image or text).\n        prompt (Union[str, List[str], ImagesReference, List[ImagesReference]]): The prompt(s) to compare against.\n        subject_type (str, optional): Type of subject ('image' or 'text'). Defaults to \"image\".\n        prompt_type (str, optional): Type of prompt(s) ('image' or 'text'). Defaults to \"text\".\n        clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: Comparison results between subject and prompt(s).\n\n    Raises:\n        InvalidParameterError: If subject_type or prompt_type is invalid.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    if (\n        subject_type not in CLIP_ARGUMENT_TYPES\n        or prompt_type not in CLIP_ARGUMENT_TYPES\n    ):\n        raise InvalidParameterError(\n            f\"Could not accept `subject_type` and `prompt_type` with values different than {CLIP_ARGUMENT_TYPES}\"\n        )\n    payload = self.__initialise_payload()\n    payload[\"subject_type\"] = subject_type\n    payload[\"prompt_type\"] = prompt_type\n    if clip_version is not None:\n        payload[\"clip_version_id\"] = clip_version\n    if subject_type == \"image\":\n        encoded_image = load_static_inference_input(\n            inference_input=subject,\n        )\n        payload = inject_images_into_payload(\n            payload=payload, encoded_images=encoded_image, key=\"subject\"\n        )\n    else:\n        payload[\"subject\"] = subject\n    if prompt_type == \"image\":\n        encoded_inference_inputs = load_static_inference_input(\n            inference_input=prompt,\n        )\n        payload = inject_images_into_payload(\n            payload=payload, encoded_images=encoded_inference_inputs, key=\"prompt\"\n        )\n    else:\n        payload[\"prompt\"] = prompt\n\n    headers = DEFAULT_HEADERS.copy()\n    execution_id_value = execution_id.get()\n    if execution_id_value is not None:\n        headers[EXECUTION_ID_HEADER] = execution_id_value\n\n    response = requests.post(\n        self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/compare\"),\n        json=payload,\n        headers=headers,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.clip_compare_async","title":"<code>clip_compare_async(subject, prompt, subject_type='image', prompt_type='text', clip_version=None)</code>  <code>async</code>","text":"<p>Compare a subject against prompts using CLIP embeddings asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>Union[str, ImagesReference]</code> <p>The subject to compare (image or text).</p> required <code>prompt</code> <code>Union[str, List[str], ImagesReference, List[ImagesReference]]</code> <p>The prompt(s) to compare against.</p> required <code>subject_type</code> <code>str</code> <p>Type of subject ('image' or 'text'). Defaults to \"image\".</p> <code>'image'</code> <code>prompt_type</code> <code>str</code> <p>Type of prompt(s) ('image' or 'text'). Defaults to \"text\".</p> <code>'text'</code> <code>clip_version</code> <code>Optional[str]</code> <p>Version of CLIP model to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Comparison results between subject and prompt(s).</p> <p>Raises:</p> Type Description <code>InvalidParameterError</code> <p>If subject_type or prompt_type is invalid.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def clip_compare_async(\n    self,\n    subject: Union[str, ImagesReference],\n    prompt: Union[str, List[str], ImagesReference, List[ImagesReference]],\n    subject_type: str = \"image\",\n    prompt_type: str = \"text\",\n    clip_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Compare a subject against prompts using CLIP embeddings asynchronously.\n\n    Args:\n        subject (Union[str, ImagesReference]): The subject to compare (image or text).\n        prompt (Union[str, List[str], ImagesReference, List[ImagesReference]]): The prompt(s) to compare against.\n        subject_type (str, optional): Type of subject ('image' or 'text'). Defaults to \"image\".\n        prompt_type (str, optional): Type of prompt(s) ('image' or 'text'). Defaults to \"text\".\n        clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: Comparison results between subject and prompt(s).\n\n    Raises:\n        InvalidParameterError: If subject_type or prompt_type is invalid.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    if (\n        subject_type not in CLIP_ARGUMENT_TYPES\n        or prompt_type not in CLIP_ARGUMENT_TYPES\n    ):\n        raise InvalidParameterError(\n            f\"Could not accept `subject_type` and `prompt_type` with values different than {CLIP_ARGUMENT_TYPES}\"\n        )\n    payload = self.__initialise_payload()\n    payload[\"subject_type\"] = subject_type\n    payload[\"prompt_type\"] = prompt_type\n    if clip_version is not None:\n        payload[\"clip_version_id\"] = clip_version\n    if subject_type == \"image\":\n        encoded_image = await load_static_inference_input_async(\n            inference_input=subject,\n        )\n        payload = inject_images_into_payload(\n            payload=payload, encoded_images=encoded_image, key=\"subject\"\n        )\n    else:\n        payload[\"subject\"] = subject\n    if prompt_type == \"image\":\n        encoded_inference_inputs = await load_static_inference_input_async(\n            inference_input=prompt,\n        )\n        payload = inject_images_into_payload(\n            payload=payload, encoded_images=encoded_inference_inputs, key=\"prompt\"\n        )\n    else:\n        payload[\"prompt\"] = prompt\n\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/compare\"),\n            json=payload,\n            headers=DEFAULT_HEADERS,\n        ) as response:\n            response.raise_for_status()\n            return await response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.configure","title":"<code>configure(inference_configuration)</code>","text":"<p>Configure the client with new inference settings.</p> <p>Parameters:</p> Name Type Description Default <code>inference_configuration</code> <code>InferenceConfiguration</code> <p>The new configuration to apply.</p> required <p>Returns:</p> Name Type Description <code>InferenceHTTPClient</code> <code>InferenceHTTPClient</code> <p>The client instance with updated configuration.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def configure(\n    self, inference_configuration: InferenceConfiguration\n) -&gt; \"InferenceHTTPClient\":\n    \"\"\"Configure the client with new inference settings.\n\n    Args:\n        inference_configuration (InferenceConfiguration): The new configuration to apply.\n\n    Returns:\n        InferenceHTTPClient: The client instance with updated configuration.\n    \"\"\"\n    self.__inference_configuration = inference_configuration\n    return self\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.consume_inference_pipeline_result","title":"<code>consume_inference_pipeline_result(pipeline_id, excluded_fields=None)</code>","text":"<p>Consumes and returns the next available result from an inference pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>str</code> <p>The unique identifier of the inference pipeline to consume results from.</p> required <code>excluded_fields</code> <code>Optional[List[str]]</code> <p>Optional list of field names to exclude from the result. If None, no fields will be excluded.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the next available result from the pipeline.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> <code>InvalidParameterError</code> <p>If pipeline_id is empty or None.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef consume_inference_pipeline_result(\n    self,\n    pipeline_id: str,\n    excluded_fields: Optional[List[str]] = None,\n) -&gt; dict:\n    \"\"\"Consumes and returns the next available result from an inference pipeline.\n\n    Args:\n        pipeline_id: The unique identifier of the inference pipeline to consume results from.\n        excluded_fields: Optional list of field names to exclude from the result. If None,\n            no fields will be excluded.\n\n    Returns:\n        dict: A dictionary containing the next available result from the pipeline.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n        InvalidParameterError: If pipeline_id is empty or None.\n    \"\"\"\n    self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n    if excluded_fields is None:\n        excluded_fields = []\n    payload = {\"api_key\": self.__api_key, \"excluded_fields\": excluded_fields}\n    response = requests.get(\n        f\"{self.__api_url}/inference_pipelines/{pipeline_id}/consume\",\n        json=payload,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.detect_gazes","title":"<code>detect_gazes(inference_input)</code>","text":"<p>Detect gazes in input image(s).</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for gaze detection.</p> required <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Gaze detection results for the input image(s).</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef detect_gazes(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Detect gazes in input image(s).\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for gaze detection.\n\n    Returns:\n        Union[dict, List[dict]]: Gaze detection results for the input image(s).\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()  # Lambda does not support Gaze, so we require v1 mode of client\n    result = self._post_images(\n        inference_input=inference_input, endpoint=\"/gaze/gaze_detection\"\n    )\n    return combine_gaze_detections(detections=result)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.detect_gazes_async","title":"<code>detect_gazes_async(inference_input)</code>  <code>async</code>","text":"<p>Detect gazes in input image(s) asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for gaze detection.</p> required <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Gaze detection results for the input image(s).</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def detect_gazes_async(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Detect gazes in input image(s) asynchronously.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for gaze detection.\n\n    Returns:\n        Union[dict, List[dict]]: Gaze detection results for the input image(s).\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()  # Lambda does not support Gaze, so we require v1 mode of client\n    result = await self._post_images_async(\n        inference_input=inference_input, endpoint=\"/gaze/gaze_detection\"\n    )\n    return combine_gaze_detections(detections=result)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_clip_image_embeddings","title":"<code>get_clip_image_embeddings(inference_input, clip_version=None)</code>","text":"<p>Get CLIP embeddings for input image(s).</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) to embed.</p> required <code>clip_version</code> <code>Optional[str]</code> <p>Version of CLIP model to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: CLIP embeddings for the input image(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef get_clip_image_embeddings(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    clip_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Get CLIP embeddings for input image(s).\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) to embed.\n        clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: CLIP embeddings for the input image(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    extra_payload = {}\n    if clip_version is not None:\n        extra_payload[\"clip_version_id\"] = clip_version\n    result = self._post_images(\n        inference_input=inference_input,\n        endpoint=\"/clip/embed_image\",\n        extra_payload=extra_payload,\n    )\n    result = combine_clip_embeddings(embeddings=result)\n    return unwrap_single_element_list(result)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_clip_image_embeddings_async","title":"<code>get_clip_image_embeddings_async(inference_input, clip_version=None)</code>  <code>async</code>","text":"<p>Get CLIP embeddings for input image(s) asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) to embed.</p> required <code>clip_version</code> <code>Optional[str]</code> <p>Version of CLIP model to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: CLIP embeddings for the input image(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def get_clip_image_embeddings_async(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    clip_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Get CLIP embeddings for input image(s) asynchronously.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) to embed.\n        clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: CLIP embeddings for the input image(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    extra_payload = {}\n    if clip_version is not None:\n        extra_payload[\"clip_version_id\"] = clip_version\n    result = await self._post_images_async(\n        inference_input=inference_input,\n        endpoint=\"/clip/embed_image\",\n        extra_payload=extra_payload,\n    )\n    result = combine_clip_embeddings(embeddings=result)\n    return unwrap_single_element_list(result)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_clip_text_embeddings","title":"<code>get_clip_text_embeddings(text, clip_version=None)</code>","text":"<p>Get CLIP embeddings for input text(s).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, List[str]]</code> <p>Input text(s) to embed.</p> required <code>clip_version</code> <code>Optional[str]</code> <p>Version of CLIP model to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: CLIP embeddings for the input text(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef get_clip_text_embeddings(\n    self,\n    text: Union[str, List[str]],\n    clip_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Get CLIP embeddings for input text(s).\n\n    Args:\n        text (Union[str, List[str]]): Input text(s) to embed.\n        clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: CLIP embeddings for the input text(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    payload = self.__initialise_payload()\n    payload[\"text\"] = text\n    if clip_version is not None:\n        payload[\"clip_version_id\"] = clip_version\n    headers = DEFAULT_HEADERS.copy()\n    execution_id_value = execution_id.get()\n    if execution_id_value is not None:\n        headers[EXECUTION_ID_HEADER] = execution_id_value\n\n    response = requests.post(\n        self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/embed_text\"),\n        json=payload,\n        headers=headers,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return unwrap_single_element_list(sequence=response.json())\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_clip_text_embeddings_async","title":"<code>get_clip_text_embeddings_async(text, clip_version=None)</code>  <code>async</code>","text":"<p>Get CLIP embeddings for input text(s) asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, List[str]]</code> <p>Input text(s) to embed.</p> required <code>clip_version</code> <code>Optional[str]</code> <p>Version of CLIP model to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: CLIP embeddings for the input text(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def get_clip_text_embeddings_async(\n    self,\n    text: Union[str, List[str]],\n    clip_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Get CLIP embeddings for input text(s) asynchronously.\n\n    Args:\n        text (Union[str, List[str]]): Input text(s) to embed.\n        clip_version (Optional[str], optional): Version of CLIP model to use. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: CLIP embeddings for the input text(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    payload = self.__initialise_payload()\n    payload[\"text\"] = text\n    if clip_version is not None:\n        payload[\"clip_version_id\"] = clip_version\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            self.__wrap_url_with_api_key(f\"{self.__api_url}/clip/embed_text\"),\n            json=payload,\n            headers=DEFAULT_HEADERS,\n        ) as response:\n            response.raise_for_status()\n            response_payload = await response.json()\n    return unwrap_single_element_list(sequence=response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_inference_pipeline_status","title":"<code>get_inference_pipeline_status(pipeline_id)</code>","text":"<p>Gets the current status of a specific inference pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>str</code> <p>The unique identifier of the inference pipeline to check.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the current status and details of the pipeline.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> <code>ValueError</code> <p>If pipeline_id is empty or None.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef get_inference_pipeline_status(self, pipeline_id: str) -&gt; dict:\n    \"\"\"Gets the current status of a specific inference pipeline.\n\n    Args:\n        pipeline_id: The unique identifier of the inference pipeline to check.\n\n    Returns:\n        dict: A dictionary containing the current status and details of the pipeline.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n        ValueError: If pipeline_id is empty or None.\n    \"\"\"\n    self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n    payload = {\"api_key\": self.__api_key}\n    response = requests.get(\n        f\"{self.__api_url}/inference_pipelines/{pipeline_id}/status\",\n        json=payload,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_model_description","title":"<code>get_model_description(model_id, allow_loading=True)</code>","text":"<p>Get the description of a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>allow_loading</code> <code>bool</code> <p>Whether to load the model if not already loaded. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ModelDescription</code> <code>ModelDescription</code> <p>Description of the model.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>ModelNotInitializedError</code> <p>If the model is not initialized and cannot be loaded.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def get_model_description(\n    self, model_id: str, allow_loading: bool = True\n) -&gt; ModelDescription:\n    \"\"\"Get the description of a model.\n\n    Args:\n        model_id (str): The identifier of the model.\n        allow_loading (bool, optional): Whether to load the model if not already loaded. Defaults to True.\n\n    Returns:\n        ModelDescription: Description of the model.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        ModelNotInitializedError: If the model is not initialized and cannot be loaded.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n    registered_models = self.list_loaded_models()\n    matching_model = filter_model_descriptions(\n        descriptions=registered_models.models,\n        model_id=de_aliased_model_id,\n    )\n    if matching_model is None and allow_loading is True:\n        registered_models = self.load_model(model_id=de_aliased_model_id)\n        matching_model = filter_model_descriptions(\n            descriptions=registered_models.models,\n            model_id=de_aliased_model_id,\n        )\n    if matching_model is not None:\n        return matching_model\n    raise ModelNotInitializedError(\n        f\"Model {model_id} (de-aliased: {de_aliased_model_id}) is not initialised and cannot \"\n        f\"retrieve its description.\"\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_model_description_async","title":"<code>get_model_description_async(model_id, allow_loading=True)</code>  <code>async</code>","text":"<p>Get the description of a model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>allow_loading</code> <code>bool</code> <p>Whether to load the model if not already loaded. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ModelDescription</code> <code>ModelDescription</code> <p>Description of the model.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>ModelNotInitializedError</code> <p>If the model is not initialized and cannot be loaded.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>async def get_model_description_async(\n    self, model_id: str, allow_loading: bool = True\n) -&gt; ModelDescription:\n    \"\"\"Get the description of a model asynchronously.\n\n    Args:\n        model_id (str): The identifier of the model.\n        allow_loading (bool, optional): Whether to load the model if not already loaded. Defaults to True.\n\n    Returns:\n        ModelDescription: Description of the model.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        ModelNotInitializedError: If the model is not initialized and cannot be loaded.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n    registered_models = await self.list_loaded_models_async()\n    matching_model = filter_model_descriptions(\n        descriptions=registered_models.models,\n        model_id=de_aliased_model_id,\n    )\n    if matching_model is None and allow_loading is True:\n        registered_models = await self.load_model_async(\n            model_id=de_aliased_model_id\n        )\n        matching_model = filter_model_descriptions(\n            descriptions=registered_models.models,\n            model_id=de_aliased_model_id,\n        )\n    if matching_model is not None:\n        return matching_model\n    raise ModelNotInitializedError(\n        f\"Model {model_id} (de-aliased: {de_aliased_model_id}) is not initialised and cannot \"\n        f\"retrieve its description.\"\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_perception_encoder_image_embeddings","title":"<code>get_perception_encoder_image_embeddings(inference_input, perception_encoder_version=None)</code>","text":"<p>Get Perception Encoder embeddings for input image(s).</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef get_perception_encoder_image_embeddings(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    perception_encoder_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Get Perception Encoder embeddings for input image(s).\"\"\"\n    extra_payload = {}\n    if perception_encoder_version is not None:\n        extra_payload[\"perception_encoder_version_id\"] = perception_encoder_version\n    result = self._post_images(\n        inference_input=inference_input,\n        endpoint=\"/perception_encoder/embed_image\",\n        extra_payload=extra_payload,\n    )\n    return unwrap_single_element_list(result)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_perception_encoder_text_embeddings","title":"<code>get_perception_encoder_text_embeddings(text, perception_encoder_version=None)</code>","text":"<p>Get Perception Encoder embeddings for input text(s).</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef get_perception_encoder_text_embeddings(\n    self,\n    text: Union[str, List[str]],\n    perception_encoder_version: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Get Perception Encoder embeddings for input text(s).\"\"\"\n    payload = self.__initialise_payload()\n    payload[\"text\"] = text\n    if perception_encoder_version is not None:\n        payload[\"perception_encoder_version_id\"] = perception_encoder_version\n\n    headers = DEFAULT_HEADERS.copy()\n    execution_id_value = execution_id.get()\n    if execution_id_value is not None:\n        headers[EXECUTION_ID_HEADER] = execution_id_value\n\n    response = requests.post(\n        self.__wrap_url_with_api_key(\n            f\"{self.__api_url}/perception_encoder/embed_text\"\n        ),\n        json=payload,\n        headers=headers,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return unwrap_single_element_list(sequence=response.json())\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.get_server_info","title":"<code>get_server_info()</code>","text":"<p>Get information about the inference server.</p> <p>Returns:</p> Name Type Description <code>ServerInfo</code> <code>ServerInfo</code> <p>Information about the server configuration and status.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef get_server_info(self) -&gt; ServerInfo:\n    \"\"\"Get information about the inference server.\n\n    Returns:\n        ServerInfo: Information about the server configuration and status.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    response = requests.get(f\"{self.__api_url}/info\")\n    response.raise_for_status()\n    response_payload = response.json()\n    return ServerInfo.from_dict(response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer","title":"<code>infer(inference_input, model_id=None)</code>","text":"<p>Run inference on one or more images.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for inference.</p> required <code>model_id</code> <code>Optional[str]</code> <p>Model identifier to use for inference. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Inference results for the input image(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef infer(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    model_id: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Run inference on one or more images.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n        model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: Inference results for the input image(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    if self.__client_mode is HTTPClientMode.V0:\n        return self.infer_from_api_v0(\n            inference_input=inference_input,\n            model_id=model_id,\n        )\n    return self.infer_from_api_v1(\n        inference_input=inference_input,\n        model_id=model_id,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_async","title":"<code>infer_async(inference_input, model_id=None)</code>  <code>async</code>","text":"<p>Run inference asynchronously on one or more images.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for inference.</p> required <code>model_id</code> <code>Optional[str]</code> <p>Model identifier to use for inference. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Inference results for the input image(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def infer_async(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    model_id: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Run inference asynchronously on one or more images.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n        model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: Inference results for the input image(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    if self.__client_mode is HTTPClientMode.V0:\n        return await self.infer_from_api_v0_async(\n            inference_input=inference_input,\n            model_id=model_id,\n        )\n    return await self.infer_from_api_v1_async(\n        inference_input=inference_input,\n        model_id=model_id,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_from_api_v0","title":"<code>infer_from_api_v0(inference_input, model_id=None)</code>","text":"<p>Run inference using API v0.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for inference.</p> required <code>model_id</code> <code>Optional[str]</code> <p>Model identifier to use for inference. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Inference results for the input image(s).</p> <p>Raises:</p> Type Description <code>ModelNotSelectedError</code> <p>If no model is selected.</p> <code>APIKeyNotProvided</code> <p>If API key is required but not provided.</p> <code>InvalidModelIdentifier</code> <p>If the model identifier format is invalid.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def infer_from_api_v0(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    model_id: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Run inference using API v0.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n        model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: Inference results for the input image(s).\n\n    Raises:\n        ModelNotSelectedError: If no model is selected.\n        APIKeyNotProvided: If API key is required but not provided.\n        InvalidModelIdentifier: If the model identifier format is invalid.\n    \"\"\"\n    requests_data = self._prepare_infer_from_api_v0_request_data(\n        inference_input=inference_input,\n        model_id=model_id,\n    )\n    responses = self._execute_infer_from_api_request(\n        requests_data=requests_data,\n    )\n    results = []\n    for request_data, response in zip(requests_data, responses):\n        if response_contains_jpeg_image(response=response):\n            visualisation = transform_visualisation_bytes(\n                visualisation=response.content,\n                expected_format=self.__inference_configuration.output_visualisation_format,\n            )\n            parsed_response = {\"visualization\": visualisation}\n        else:\n            parsed_response = response.json()\n            if parsed_response.get(\"visualization\") is not None:\n                parsed_response[\"visualization\"] = transform_base64_visualisation(\n                    visualisation=parsed_response[\"visualization\"],\n                    expected_format=self.__inference_configuration.output_visualisation_format,\n                )\n        parsed_response = adjust_prediction_to_client_scaling_factor(\n            prediction=parsed_response,\n            scaling_factor=request_data.image_scaling_factors[0],\n        )\n        results.append(parsed_response)\n    return unwrap_single_element_list(sequence=results)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_from_api_v0_async","title":"<code>infer_from_api_v0_async(inference_input, model_id=None)</code>  <code>async</code>","text":"<p>Run inference using API v0 asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for inference.</p> required <code>model_id</code> <code>Optional[str]</code> <p>Model identifier to use for inference. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, List[dict]]</code> <p>Union[dict, List[dict]]: Inference results for the input image(s).</p> <p>Raises:</p> Type Description <code>ModelNotSelectedError</code> <p>If no model is selected.</p> <code>APIKeyNotProvided</code> <p>If API key is required but not provided.</p> <code>InvalidModelIdentifier</code> <p>If the model identifier format is invalid.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>async def infer_from_api_v0_async(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    model_id: Optional[str] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Run inference using API v0 asynchronously.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for inference.\n        model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n    Returns:\n        Union[dict, List[dict]]: Inference results for the input image(s).\n\n    Raises:\n        ModelNotSelectedError: If no model is selected.\n        APIKeyNotProvided: If API key is required but not provided.\n        InvalidModelIdentifier: If the model identifier format is invalid.\n    \"\"\"\n    model_id_to_be_used = model_id or self.__selected_model\n    _ensure_model_is_selected(model_id=model_id_to_be_used)\n    _ensure_api_key_provided(api_key=self.__api_key)\n    model_id_to_be_used = resolve_roboflow_model_alias(model_id=model_id_to_be_used)\n    model_id_chunks = model_id_to_be_used.split(\"/\")\n    if len(model_id_chunks) != 2:\n        raise InvalidModelIdentifier(\n            f\"Invalid model id: {model_id}. Expected format: project_id/model_version_id.\"\n        )\n    max_height, max_width = _determine_client_downsizing_parameters(\n        client_downsizing_disabled=self.__inference_configuration.client_downsizing_disabled,\n        model_description=None,\n        default_max_input_size=self.__inference_configuration.default_max_input_size,\n    )\n    encoded_inference_inputs = await load_static_inference_input_async(\n        inference_input=inference_input,\n        max_height=max_height,\n        max_width=max_width,\n    )\n    params = {\n        \"api_key\": self.__api_key,\n    }\n    params.update(self.__inference_configuration.to_legacy_call_parameters())\n\n    execution_id_value = execution_id.get()\n    headers = DEFAULT_HEADERS\n    if execution_id_value:\n        headers = headers.copy()\n        headers[EXECUTION_ID_HEADER] = execution_id_value\n\n    requests_data = prepare_requests_data(\n        url=f\"{self.__api_url}/{model_id_chunks[0]}/{model_id_chunks[1]}\",\n        encoded_inference_inputs=encoded_inference_inputs,\n        headers=headers,\n        parameters=params,\n        payload=None,\n        max_batch_size=1,\n        image_placement=ImagePlacement.DATA,\n    )\n    responses = await execute_requests_packages_async(\n        requests_data=requests_data,\n        request_method=RequestMethod.POST,\n        max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n    )\n    results = []\n    for request_data, response in zip(requests_data, responses):\n        if not issubclass(type(response), dict):\n            visualisation = transform_visualisation_bytes(\n                visualisation=response,\n                expected_format=self.__inference_configuration.output_visualisation_format,\n            )\n            parsed_response = {\"visualization\": visualisation}\n        else:\n            parsed_response = response\n            if parsed_response.get(\"visualization\") is not None:\n                parsed_response[\"visualization\"] = transform_base64_visualisation(\n                    visualisation=parsed_response[\"visualization\"],\n                    expected_format=self.__inference_configuration.output_visualisation_format,\n                )\n        parsed_response = adjust_prediction_to_client_scaling_factor(\n            prediction=parsed_response,\n            scaling_factor=request_data.image_scaling_factors[0],\n        )\n        results.append(parsed_response)\n    return unwrap_single_element_list(sequence=results)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_from_workflow","title":"<code>infer_from_workflow(workspace_name=None, workflow_name=None, specification=None, images=None, parameters=None, excluded_fields=None, use_cache=True, enable_profiling=False)</code>","text":"<p>Run inference using a workflow specification.</p> <p>Triggers inference from workflow specification at the inference HTTP side. Either (<code>workspace_name</code> and <code>workflow_name</code>) or <code>workflow_specification</code> must be provided. In the first case - definition of workflow will be fetched from Roboflow API, in the latter - <code>workflow_specification</code> will be used. <code>images</code> and <code>parameters</code> will be merged into workflow inputs, the distinction is made to make sure the SDK can easily serialise images and prepare a proper payload. Supported images are numpy arrays, PIL.Image and base64 images, links to images and local paths. <code>excluded_fields</code> will be added to request to filter out results of workflow execution at the server side.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_name</code> <code>Optional[str]</code> <p>Name of the workspace containing the workflow. Defaults to None.</p> <code>None</code> <code>workflow_name</code> <code>Optional[str]</code> <p>Name of the workflow. Defaults to None.</p> <code>None</code> <code>specification</code> <code>Optional[dict]</code> <p>Direct workflow specification. Defaults to None.</p> <code>None</code> <code>images</code> <code>Optional[Dict[str, Any]]</code> <p>Images to process. Defaults to None.</p> <code>None</code> <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the workflow. Defaults to None.</p> <code>None</code> <code>excluded_fields</code> <code>Optional[List[str]]</code> <p>Fields to exclude from results. Defaults to None.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cached results. Defaults to True.</p> <code>True</code> <code>enable_profiling</code> <code>bool</code> <p>Whether to enable profiling. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Results of the workflow execution.</p> <p>Raises:</p> Type Description <code>InvalidParameterError</code> <p>If neither workflow identifiers nor specification is provided.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@deprecated(\n    reason=\"Please use run_workflow(...) method. This method will be removed end of Q2 2024\"\n)\n@wrap_errors\ndef infer_from_workflow(\n    self,\n    workspace_name: Optional[str] = None,\n    workflow_name: Optional[str] = None,\n    specification: Optional[dict] = None,\n    images: Optional[Dict[str, Any]] = None,\n    parameters: Optional[Dict[str, Any]] = None,\n    excluded_fields: Optional[List[str]] = None,\n    use_cache: bool = True,\n    enable_profiling: bool = False,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Run inference using a workflow specification.\n\n    Triggers inference from workflow specification at the inference HTTP\n    side. Either (`workspace_name` and `workflow_name`) or `workflow_specification` must be\n    provided. In the first case - definition of workflow will be fetched\n    from Roboflow API, in the latter - `workflow_specification` will be\n    used. `images` and `parameters` will be merged into workflow inputs,\n    the distinction is made to make sure the SDK can easily serialise\n    images and prepare a proper payload. Supported images are numpy arrays,\n    PIL.Image and base64 images, links to images and local paths.\n    `excluded_fields` will be added to request to filter out results\n    of workflow execution at the server side.\n\n    Args:\n        workspace_name (Optional[str], optional): Name of the workspace containing the workflow. Defaults to None.\n        workflow_name (Optional[str], optional): Name of the workflow. Defaults to None.\n        specification (Optional[dict], optional): Direct workflow specification. Defaults to None.\n        images (Optional[Dict[str, Any]], optional): Images to process. Defaults to None.\n        parameters (Optional[Dict[str, Any]], optional): Additional parameters for the workflow. Defaults to None.\n        excluded_fields (Optional[List[str]], optional): Fields to exclude from results. Defaults to None.\n        use_cache (bool, optional): Whether to use cached results. Defaults to True.\n        enable_profiling (bool, optional): Whether to enable profiling. Defaults to False.\n\n    Returns:\n        List[Dict[str, Any]]: Results of the workflow execution.\n\n    Raises:\n        InvalidParameterError: If neither workflow identifiers nor specification is provided.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    return self._run_workflow(\n        workspace_name=workspace_name,\n        workflow_id=workflow_name,\n        specification=specification,\n        images=images,\n        parameters=parameters,\n        excluded_fields=excluded_fields,\n        legacy_endpoints=True,\n        use_cache=use_cache,\n        enable_profiling=enable_profiling,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_from_yolo_world","title":"<code>infer_from_yolo_world(inference_input, class_names, model_version=None, confidence=None)</code>","text":"<p>Run inference using YOLO-World model.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) to run inference on. Can be a single image reference or a list of image references.</p> required <code>class_names</code> <code>List[str]</code> <p>List of class names to detect in the image(s).</p> required <code>model_version</code> <code>Optional[str]</code> <p>Optional version of YOLO-World model to use. If not specified, uses the default version.</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Optional confidence threshold for detections. If not specified, uses the model's default threshold.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of dictionaries containing detection results for each input image.</p> <code>List[dict]</code> <p>Each dictionary contains bounding boxes, class labels, and confidence scores</p> <code>List[dict]</code> <p>for detected objects.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef infer_from_yolo_world(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    class_names: List[str],\n    model_version: Optional[str] = None,\n    confidence: Optional[float] = None,\n) -&gt; List[dict]:\n    \"\"\"Run inference using YOLO-World model.\n\n    Args:\n        inference_input: Input image(s) to run inference on. Can be a single image\n            reference or a list of image references.\n        class_names: List of class names to detect in the image(s).\n        model_version: Optional version of YOLO-World model to use. If not specified,\n            uses the default version.\n        confidence: Optional confidence threshold for detections. If not specified,\n            uses the model's default threshold.\n\n    Returns:\n        List of dictionaries containing detection results for each input image.\n        Each dictionary contains bounding boxes, class labels, and confidence scores\n        for detected objects.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    encoded_inference_inputs = load_static_inference_input(\n        inference_input=inference_input,\n    )\n    payload = self.__initialise_payload()\n    payload[\"text\"] = class_names\n    if model_version is not None:\n        payload[\"yolo_world_version_id\"] = model_version\n    if confidence is not None:\n        payload[\"confidence\"] = confidence\n    url = self.__wrap_url_with_api_key(f\"{self.__api_url}/yolo_world/infer\")\n    requests_data = prepare_requests_data(\n        url=url,\n        encoded_inference_inputs=encoded_inference_inputs,\n        headers=DEFAULT_HEADERS,\n        parameters=None,\n        payload=payload,\n        max_batch_size=1,\n        image_placement=ImagePlacement.JSON,\n    )\n    responses = execute_requests_packages(\n        requests_data=requests_data,\n        request_method=RequestMethod.POST,\n        max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n    )\n    return [r.json() for r in responses]\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_from_yolo_world_async","title":"<code>infer_from_yolo_world_async(inference_input, class_names, model_version=None, confidence=None)</code>  <code>async</code>","text":"<p>Run inference using YOLO-World model asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) to run inference on. Can be a single image reference or a list of image references.</p> required <code>class_names</code> <code>List[str]</code> <p>List of class names to detect in the image(s).</p> required <code>model_version</code> <code>Optional[str]</code> <p>Optional version of YOLO-World model to use. If not specified, uses the default version.</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Optional confidence threshold for detections. If not specified, uses the model's default threshold.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of dictionaries containing detection results for each input image.</p> <code>List[dict]</code> <p>Each dictionary contains bounding boxes, class labels, and confidence scores</p> <code>List[dict]</code> <p>for detected objects.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def infer_from_yolo_world_async(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    class_names: List[str],\n    model_version: Optional[str] = None,\n    confidence: Optional[float] = None,\n) -&gt; List[dict]:\n    \"\"\"Run inference using YOLO-World model asynchronously.\n\n    Args:\n        inference_input: Input image(s) to run inference on. Can be a single image\n            reference or a list of image references.\n        class_names: List of class names to detect in the image(s).\n        model_version: Optional version of YOLO-World model to use. If not specified,\n            uses the default version.\n        confidence: Optional confidence threshold for detections. If not specified,\n            uses the model's default threshold.\n\n    Returns:\n        List of dictionaries containing detection results for each input image.\n        Each dictionary contains bounding boxes, class labels, and confidence scores\n        for detected objects.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    encoded_inference_inputs = await load_static_inference_input_async(\n        inference_input=inference_input,\n    )\n    payload = self.__initialise_payload()\n    payload[\"text\"] = class_names\n    if model_version is not None:\n        payload[\"yolo_world_version_id\"] = model_version\n    if confidence is not None:\n        payload[\"confidence\"] = confidence\n    url = self.__wrap_url_with_api_key(f\"{self.__api_url}/yolo_world/infer\")\n    requests_data = prepare_requests_data(\n        url=url,\n        encoded_inference_inputs=encoded_inference_inputs,\n        headers=DEFAULT_HEADERS,\n        parameters=None,\n        payload=payload,\n        max_batch_size=1,\n        image_placement=ImagePlacement.JSON,\n    )\n    return await execute_requests_packages_async(\n        requests_data=requests_data,\n        request_method=RequestMethod.POST,\n        max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.infer_on_stream","title":"<code>infer_on_stream(input_uri, model_id=None)</code>","text":"<p>Run inference on a video stream or sequence of images.</p> <p>Parameters:</p> Name Type Description Default <code>input_uri</code> <code>str</code> <p>URI of the input stream or directory.</p> required <code>model_id</code> <code>Optional[str]</code> <p>Model identifier to use for inference. Defaults to None.</p> <code>None</code> <p>Yields:</p> Type Description <code>Tuple[Union[str, int], ndarray, dict]</code> <p>Generator[Tuple[Union[str, int], np.ndarray, dict], None, None]: Tuples of (frame reference, frame data, prediction).</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def infer_on_stream(\n    self,\n    input_uri: str,\n    model_id: Optional[str] = None,\n) -&gt; Generator[Tuple[Union[str, int], np.ndarray, dict], None, None]:\n    \"\"\"Run inference on a video stream or sequence of images.\n\n    Args:\n        input_uri (str): URI of the input stream or directory.\n        model_id (Optional[str], optional): Model identifier to use for inference. Defaults to None.\n\n    Yields:\n        Generator[Tuple[Union[str, int], np.ndarray, dict], None, None]: Tuples of (frame reference, frame data, prediction).\n    \"\"\"\n    for reference, frame in load_stream_inference_input(\n        input_uri=input_uri,\n        image_extensions=self.__inference_configuration.image_extensions_for_directory_scan,\n    ):\n        prediction = self.infer(\n            inference_input=frame,\n            model_id=model_id,\n        )\n        yield reference, frame, prediction\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.init","title":"<code>init(api_url, api_key=None)</code>  <code>classmethod</code>","text":"<p>Initialize a new InferenceHTTPClient instance.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>The base URL for the inference API.</p> required <code>api_key</code> <code>Optional[str]</code> <p>API key for authentication. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InferenceHTTPClient</code> <code>InferenceHTTPClient</code> <p>A new instance of the InferenceHTTPClient.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    api_url: str,\n    api_key: Optional[str] = None,\n) -&gt; \"InferenceHTTPClient\":\n    \"\"\"Initialize a new InferenceHTTPClient instance.\n\n    Args:\n        api_url (str): The base URL for the inference API.\n        api_key (Optional[str], optional): API key for authentication. Defaults to None.\n\n    Returns:\n        InferenceHTTPClient: A new instance of the InferenceHTTPClient.\n    \"\"\"\n    return cls(api_url=api_url, api_key=api_key)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.list_inference_pipelines","title":"<code>list_inference_pipelines()</code>","text":"<p>Lists all active inference pipelines on the server.</p> <p>This method retrieves information about all currently running inference pipelines on the server, including their IDs and status.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: A list of dictionaries containing information about each active inference pipeline.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef list_inference_pipelines(self) -&gt; List[dict]:\n    \"\"\"Lists all active inference pipelines on the server.\n\n    This method retrieves information about all currently running inference pipelines\n    on the server, including their IDs and status.\n\n    Returns:\n        List[dict]: A list of dictionaries containing information about each active\n            inference pipeline.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    payload = {\"api_key\": self.__api_key}\n    response = requests.get(\n        f\"{self.__api_url}/inference_pipelines/list\",\n        json=payload,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.list_loaded_models","title":"<code>list_loaded_models()</code>","text":"<p>List all models currently loaded on the server.</p> <p>Returns:</p> Name Type Description <code>RegisteredModels</code> <code>RegisteredModels</code> <p>Information about registered models.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef list_loaded_models(self) -&gt; RegisteredModels:\n    \"\"\"List all models currently loaded on the server.\n\n    Returns:\n        RegisteredModels: Information about registered models.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    response = requests.get(\n        f\"{self.__api_url}/model/registry?api_key={self.__api_key}\"\n    )\n    response.raise_for_status()\n    response_payload = response.json()\n    return RegisteredModels.from_dict(response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.list_loaded_models_async","title":"<code>list_loaded_models_async()</code>  <code>async</code>","text":"<p>List all models currently loaded on the server asynchronously.</p> <p>Returns:</p> Name Type Description <code>RegisteredModels</code> <code>RegisteredModels</code> <p>Information about registered models.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def list_loaded_models_async(self) -&gt; RegisteredModels:\n    \"\"\"List all models currently loaded on the server asynchronously.\n\n    Returns:\n        RegisteredModels: Information about registered models.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    async with aiohttp.ClientSession() as session:\n        async with session.get(\n            f\"{self.__api_url}/model/registry?api_key={self.__api_key}\"\n        ) as response:\n            response.raise_for_status()\n            response_payload = await response.json()\n            return RegisteredModels.from_dict(response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.load_model","title":"<code>load_model(model_id, set_as_default=False)</code>","text":"<p>Load a model onto the server.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to load.</p> required <code>set_as_default</code> <code>bool</code> <p>Whether to set this model as the default. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RegisteredModels</code> <code>RegisteredModels</code> <p>Updated information about registered models.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef load_model(\n    self, model_id: str, set_as_default: bool = False\n) -&gt; RegisteredModels:\n    \"\"\"Load a model onto the server.\n\n    Args:\n        model_id (str): The identifier of the model to load.\n        set_as_default (bool, optional): Whether to set this model as the default. Defaults to False.\n\n    Returns:\n        RegisteredModels: Updated information about registered models.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n    response = requests.post(\n        f\"{self.__api_url}/model/add\",\n        json={\n            \"model_id\": de_aliased_model_id,\n            \"api_key\": self.__api_key,\n        },\n        headers=DEFAULT_HEADERS,\n    )\n    response.raise_for_status()\n    response_payload = response.json()\n    if set_as_default:\n        self.__selected_model = de_aliased_model_id\n    return RegisteredModels.from_dict(response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.load_model_async","title":"<code>load_model_async(model_id, set_as_default=False)</code>  <code>async</code>","text":"<p>Load a model onto the server asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to load.</p> required <code>set_as_default</code> <code>bool</code> <p>Whether to set this model as the default. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RegisteredModels</code> <code>RegisteredModels</code> <p>Updated information about registered models.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def load_model_async(\n    self, model_id: str, set_as_default: bool = False\n) -&gt; RegisteredModels:\n    \"\"\"Load a model onto the server asynchronously.\n\n    Args:\n        model_id (str): The identifier of the model to load.\n        set_as_default (bool, optional): Whether to set this model as the default. Defaults to False.\n\n    Returns:\n        RegisteredModels: Updated information about registered models.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n    payload = {\n        \"model_id\": de_aliased_model_id,\n        \"api_key\": self.__api_key,\n    }\n    async with aiohttp.ClientSession() as session:\n        async with session.post(\n            f\"{self.__api_url}/model/add\",\n            json=payload,\n            headers=DEFAULT_HEADERS,\n        ) as response:\n            response.raise_for_status()\n            response_payload = await response.json()\n    if set_as_default:\n        self.__selected_model = de_aliased_model_id\n    return RegisteredModels.from_dict(response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.ocr_image","title":"<code>ocr_image(inference_input, model='doctr', version=None, quantize=None, generate_bounding_boxes=None, language_codes=None)</code>","text":"<p>Run OCR on input image(s).</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for OCR.</p> required <code>model</code> <code>str</code> <p>OCR model to use ('doctr' or 'trocr'). Defaults to \"doctr\".</p> <code>'doctr'</code> <code>version</code> <code>Optional[str]</code> <p>Model version to use. Defaults to None. For trocr, supported versions are: 'trocr-small-printed', 'trocr-base-printed', 'trocr-large-printed'.</p> <code>None</code> <code>quantize</code> <code>Optional[bool]</code> <p>(Optional[bool]): flag of EasyOCR to decide which version of model to load</p> <code>None</code> <code>generate_bounding_boxes</code> <code>Optional[bool]</code> <p>(Optional[bool]): flag of some models (like DocTR) to decide if output variant with sv.Detections(...) compatible bounding boxes should be returned (due to historical reasons, some old implementations were flattening detected OCR structure into text and were only returning that as results).</p> <code>None</code> <code>language_codes</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): Parameter of EasyOCR that dictates the code of languages that model should recognise (leave blank for default for given OCR model version).</p> <code>None</code> <p>Returns:     Union[dict, List[dict]]: OCR results for the input image(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef ocr_image(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    model: str = \"doctr\",\n    version: Optional[str] = None,\n    quantize: Optional[bool] = None,\n    generate_bounding_boxes: Optional[bool] = None,\n    language_codes: Optional[List[str]] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Run OCR on input image(s).\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for OCR.\n        model (str, optional): OCR model to use ('doctr' or 'trocr'). Defaults to \"doctr\".\n        version (Optional[str], optional): Model version to use. Defaults to None.\n            For trocr, supported versions are: 'trocr-small-printed', 'trocr-base-printed', 'trocr-large-printed'.\n        quantize: (Optional[bool]): flag of EasyOCR to decide which version of model to load\n        generate_bounding_boxes: (Optional[bool]): flag of some models (like DocTR) to decide if output variant\n            with sv.Detections(...) compatible bounding boxes should be returned (due to historical reasons, some\n            old implementations were flattening detected OCR structure into text and were only returning that as\n            results).\n        language_codes: (Optional[List[str]]): Parameter of EasyOCR that dictates the code of languages that\n            model should recognise (leave blank for default for given OCR model version).\n    Returns:\n        Union[dict, List[dict]]: OCR results for the input image(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    encoded_inference_inputs = load_static_inference_input(\n        inference_input=inference_input,\n    )\n    payload = self.__initialise_payload()\n    if version:\n        key = f\"{model.lower()}_version_id\"\n        payload[key] = version\n    if quantize is not None:\n        payload[\"quantize\"] = quantize\n    if generate_bounding_boxes is not None:\n        payload[\"generate_bounding_boxes\"] = generate_bounding_boxes\n    if language_codes is not None:\n        payload[\"language_codes\"] = language_codes\n    model_path = resolve_ocr_path(model_name=model)\n    url = self.__wrap_url_with_api_key(f\"{self.__api_url}{model_path}\")\n    requests_data = prepare_requests_data(\n        url=url,\n        encoded_inference_inputs=encoded_inference_inputs,\n        headers=DEFAULT_HEADERS,\n        parameters=None,\n        payload=payload,\n        max_batch_size=1,\n        image_placement=ImagePlacement.JSON,\n    )\n    responses = execute_requests_packages(\n        requests_data=requests_data,\n        request_method=RequestMethod.POST,\n        max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n    )\n    results = [r.json() for r in responses]\n    return unwrap_single_element_list(sequence=results)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.ocr_image_async","title":"<code>ocr_image_async(inference_input, model='doctr', version=None, quantize=None, generate_bounding_boxes=None, language_codes=None)</code>  <code>async</code>","text":"<p>Run OCR on input image(s) asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>Input image(s) for OCR.</p> required <code>model</code> <code>str</code> <p>OCR model to use ('doctr' or 'trocr'). Defaults to \"doctr\".</p> <code>'doctr'</code> <code>version</code> <code>Optional[str]</code> <p>Model version to use. Defaults to None. For trocr, supported versions are: 'trocr-small-printed', 'trocr-base-printed', 'trocr-large-printed'.</p> <code>None</code> <code>quantize</code> <code>Optional[bool]</code> <p>(Optional[bool]): flag of EasyOCR to decide which version of model to load</p> <code>None</code> <code>generate_bounding_boxes</code> <code>Optional[bool]</code> <p>(Optional[bool]): flag of some models (like DocTR) to decide if output variant with sv.Detections(...) compatible bounding boxes should be returned (due to historical reasons, some old implementations were flattening detected OCR structure into text and were only returning that as results).</p> <code>None</code> <code>language_codes</code> <code>Optional[List[str]]</code> <p>(Optional[List[str]]): Parameter of EasyOCR that dictates the code of languages that model should recognise (leave blank for default for given OCR model version).</p> <code>None</code> <p>Returns:     Union[dict, List[dict]]: OCR results for the input image(s).</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors_async\nasync def ocr_image_async(\n    self,\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    model: str = \"doctr\",\n    version: Optional[str] = None,\n    quantize: Optional[bool] = None,\n    generate_bounding_boxes: Optional[bool] = None,\n    language_codes: Optional[List[str]] = None,\n) -&gt; Union[dict, List[dict]]:\n    \"\"\"Run OCR on input image(s) asynchronously.\n\n    Args:\n        inference_input (Union[ImagesReference, List[ImagesReference]]): Input image(s) for OCR.\n        model (str, optional): OCR model to use ('doctr' or 'trocr'). Defaults to \"doctr\".\n        version (Optional[str], optional): Model version to use. Defaults to None.\n            For trocr, supported versions are: 'trocr-small-printed', 'trocr-base-printed', 'trocr-large-printed'.\n        quantize: (Optional[bool]): flag of EasyOCR to decide which version of model to load\n        generate_bounding_boxes: (Optional[bool]): flag of some models (like DocTR) to decide if output variant\n            with sv.Detections(...) compatible bounding boxes should be returned (due to historical reasons, some\n            old implementations were flattening detected OCR structure into text and were only returning that as\n            results).\n        language_codes: (Optional[List[str]]): Parameter of EasyOCR that dictates the code of languages that\n            model should recognise (leave blank for default for given OCR model version).\n    Returns:\n        Union[dict, List[dict]]: OCR results for the input image(s).\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    encoded_inference_inputs = await load_static_inference_input_async(\n        inference_input=inference_input,\n    )\n    payload = self.__initialise_payload()\n    if version:\n        key = f\"{model.lower()}_version_id\"\n        payload[key] = version\n    if quantize is not None:\n        payload[\"quantize\"] = quantize\n    if generate_bounding_boxes is not None:\n        payload[\"generate_bounding_boxes\"] = generate_bounding_boxes\n    if language_codes is not None:\n        payload[\"language_codes\"] = language_codes\n    model_path = resolve_ocr_path(model_name=model)\n    url = self.__wrap_url_with_api_key(f\"{self.__api_url}{model_path}\")\n    requests_data = prepare_requests_data(\n        url=url,\n        encoded_inference_inputs=encoded_inference_inputs,\n        headers=DEFAULT_HEADERS,\n        parameters=None,\n        payload=payload,\n        max_batch_size=1,\n        image_placement=ImagePlacement.JSON,\n    )\n    responses = await execute_requests_packages_async(\n        requests_data=requests_data,\n        request_method=RequestMethod.POST,\n        max_concurrent_requests=self.__inference_configuration.max_concurrent_requests,\n    )\n    return unwrap_single_element_list(sequence=responses)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.pause_inference_pipeline","title":"<code>pause_inference_pipeline(pipeline_id)</code>","text":"<p>Pauses a running inference pipeline.</p> <p>Sends a request to pause the specified inference pipeline. The pipeline must be currently running for this operation to succeed.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>str</code> <p>The unique identifier of the inference pipeline to pause.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the response from the server about the pause operation.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> <code>ValueError</code> <p>If pipeline_id is empty or None.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef pause_inference_pipeline(self, pipeline_id: str) -&gt; dict:\n    \"\"\"Pauses a running inference pipeline.\n\n    Sends a request to pause the specified inference pipeline. The pipeline must be\n    currently running for this operation to succeed.\n\n    Args:\n        pipeline_id: The unique identifier of the inference pipeline to pause.\n\n    Returns:\n        dict: A dictionary containing the response from the server about the pause operation.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n        ValueError: If pipeline_id is empty or None.\n    \"\"\"\n    self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n    payload = {\"api_key\": self.__api_key}\n    response = requests.post(\n        f\"{self.__api_url}/inference_pipelines/{pipeline_id}/pause\",\n        json=payload,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.resume_inference_pipeline","title":"<code>resume_inference_pipeline(pipeline_id)</code>","text":"<p>Resumes a paused inference pipeline.</p> <p>Sends a request to resume the specified inference pipeline. The pipeline must be currently paused for this operation to succeed.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>str</code> <p>The unique identifier of the inference pipeline to resume.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the response from the server about the resume operation.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> <code>ValueError</code> <p>If pipeline_id is empty or None.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef resume_inference_pipeline(self, pipeline_id: str) -&gt; dict:\n    \"\"\"Resumes a paused inference pipeline.\n\n    Sends a request to resume the specified inference pipeline. The pipeline must be\n    currently paused for this operation to succeed.\n\n    Args:\n        pipeline_id: The unique identifier of the inference pipeline to resume.\n\n    Returns:\n        dict: A dictionary containing the response from the server about the resume operation.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n        ValueError: If pipeline_id is empty or None.\n    \"\"\"\n    self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n    payload = {\"api_key\": self.__api_key}\n    response = requests.post(\n        f\"{self.__api_url}/inference_pipelines/{pipeline_id}/resume\",\n        json=payload,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.run_workflow","title":"<code>run_workflow(workspace_name=None, workflow_id=None, specification=None, images=None, parameters=None, excluded_fields=None, use_cache=True, enable_profiling=False)</code>","text":"<p>Run inference using a workflow specification.</p> <p>Triggers inference from workflow specification at the inference HTTP side. Either (<code>workspace_name</code> and <code>workflow_id</code>) or <code>workflow_specification</code> must be provided. In the first case - definition of workflow will be fetched from Roboflow API, in the latter - <code>workflow_specification</code> will be used. <code>images</code> and <code>parameters</code> will be merged into workflow inputs, the distinction is made to make sure the SDK can easily serialise images and prepare a proper payload. Supported images are numpy arrays, PIL.Image and base64 images, links to images and local paths. <code>excluded_fields</code> will be added to request to filter out results of workflow execution at the server side.</p> <p>Important! Method is not compatible with inference server &lt;=0.9.18. Please migrate to newer version of the server before end of Q2 2024. Until that is done - use old method: infer_from_workflow(...).</p> Note <p>Method is not compatible with inference server &lt;=0.9.18. Please migrate to newer version of the server before end of Q2 2024. Until that is done - use old method: infer_from_workflow(...).</p> <p>Parameters:</p> Name Type Description Default <code>workspace_name</code> <code>Optional[str]</code> <p>Name of the workspace containing the workflow. Defaults to None.</p> <code>None</code> <code>workflow_id</code> <code>Optional[str]</code> <p>ID of the workflow. Defaults to None.</p> <code>None</code> <code>specification</code> <code>Optional[dict]</code> <p>Direct workflow specification. Defaults to None.</p> <code>None</code> <code>images</code> <code>Optional[Dict[str, Any]]</code> <p>Images to process. Defaults to None.</p> <code>None</code> <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Additional parameters for the workflow. Defaults to None.</p> <code>None</code> <code>excluded_fields</code> <code>Optional[List[str]]</code> <p>Fields to exclude from results. Defaults to None.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cached results. Defaults to True.</p> <code>True</code> <code>enable_profiling</code> <code>bool</code> <p>Whether to enable profiling. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Results of the workflow execution.</p> <p>Raises:</p> Type Description <code>InvalidParameterError</code> <p>If neither workflow identifiers nor specification is provided.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef run_workflow(\n    self,\n    workspace_name: Optional[str] = None,\n    workflow_id: Optional[str] = None,\n    specification: Optional[dict] = None,\n    images: Optional[Dict[str, Any]] = None,\n    parameters: Optional[Dict[str, Any]] = None,\n    excluded_fields: Optional[List[str]] = None,\n    use_cache: bool = True,\n    enable_profiling: bool = False,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Run inference using a workflow specification.\n\n    Triggers inference from workflow specification at the inference HTTP\n    side. Either (`workspace_name` and `workflow_id`) or `workflow_specification` must be\n    provided. In the first case - definition of workflow will be fetched\n    from Roboflow API, in the latter - `workflow_specification` will be\n    used. `images` and `parameters` will be merged into workflow inputs,\n    the distinction is made to make sure the SDK can easily serialise\n    images and prepare a proper payload. Supported images are numpy arrays,\n    PIL.Image and base64 images, links to images and local paths.\n    `excluded_fields` will be added to request to filter out results\n    of workflow execution at the server side.\n\n    **Important!**\n    Method is not compatible with inference server &lt;=0.9.18. Please migrate to newer version of\n    the server before end of Q2 2024. Until that is done - use old method: infer_from_workflow(...).\n\n    Note:\n        Method is not compatible with inference server &lt;=0.9.18. Please migrate to newer version of\n        the server before end of Q2 2024. Until that is done - use old method: infer_from_workflow(...).\n\n    Args:\n        workspace_name (Optional[str], optional): Name of the workspace containing the workflow. Defaults to None.\n        workflow_id (Optional[str], optional): ID of the workflow. Defaults to None.\n        specification (Optional[dict], optional): Direct workflow specification. Defaults to None.\n        images (Optional[Dict[str, Any]], optional): Images to process. Defaults to None.\n        parameters (Optional[Dict[str, Any]], optional): Additional parameters for the workflow. Defaults to None.\n        excluded_fields (Optional[List[str]], optional): Fields to exclude from results. Defaults to None.\n        use_cache (bool, optional): Whether to use cached results. Defaults to True.\n        enable_profiling (bool, optional): Whether to enable profiling. Defaults to False.\n\n    Returns:\n        List[Dict[str, Any]]: Results of the workflow execution.\n\n    Raises:\n        InvalidParameterError: If neither workflow identifiers nor specification is provided.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    return self._run_workflow(\n        workspace_name=workspace_name,\n        workflow_id=workflow_id,\n        specification=specification,\n        images=images,\n        parameters=parameters,\n        excluded_fields=excluded_fields,\n        legacy_endpoints=False,\n        use_cache=use_cache,\n        enable_profiling=enable_profiling,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.select_api_v0","title":"<code>select_api_v0()</code>","text":"<p>Select API version 0 for client operations.</p> <p>Returns:</p> Name Type Description <code>InferenceHTTPClient</code> <code>InferenceHTTPClient</code> <p>The client instance with API v0 selected.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def select_api_v0(self) -&gt; \"InferenceHTTPClient\":\n    \"\"\"Select API version 0 for client operations.\n\n    Returns:\n        InferenceHTTPClient: The client instance with API v0 selected.\n    \"\"\"\n    self.__client_mode = HTTPClientMode.V0\n    return self\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.select_api_v1","title":"<code>select_api_v1()</code>","text":"<p>Select API version 1 for client operations.</p> <p>Returns:</p> Name Type Description <code>InferenceHTTPClient</code> <code>InferenceHTTPClient</code> <p>The client instance with API v1 selected.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def select_api_v1(self) -&gt; \"InferenceHTTPClient\":\n    \"\"\"Select API version 1 for client operations.\n\n    Returns:\n        InferenceHTTPClient: The client instance with API v1 selected.\n    \"\"\"\n    self.__client_mode = HTTPClientMode.V1\n    return self\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.select_model","title":"<code>select_model(model_id)</code>","text":"<p>Select a model for inference operations.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to select.</p> required <p>Returns:</p> Name Type Description <code>InferenceHTTPClient</code> <code>InferenceHTTPClient</code> <p>The client instance with the selected model.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>def select_model(self, model_id: str) -&gt; \"InferenceHTTPClient\":\n    \"\"\"Select a model for inference operations.\n\n    Args:\n        model_id (str): The identifier of the model to select.\n\n    Returns:\n        InferenceHTTPClient: The client instance with the selected model.\n    \"\"\"\n    self.__selected_model = model_id\n    return self\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.start_inference_pipeline_with_workflow","title":"<code>start_inference_pipeline_with_workflow(video_reference, workflow_specification=None, workspace_name=None, workflow_id=None, image_input_name='image', workflows_parameters=None, workflows_thread_pool_workers=4, cancel_thread_pool_tasks_on_exit=True, video_metadata_input_name='video_metadata', max_fps=None, source_buffer_filling_strategy='DROP_OLDEST', source_buffer_consumption_strategy='EAGER', video_source_properties=None, batch_collection_timeout=None, results_buffer_size=64)</code>","text":"<p>Starts an inference pipeline using a workflow specification.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Path to video file, camera index, or list of video sources. Can be a string path, integer camera index, or list of either.</p> required <code>workflow_specification</code> <code>Optional[dict]</code> <p>Optional workflow specification dictionary. Mutually exclusive with workspace_name/workflow_id.</p> <code>None</code> <code>workspace_name</code> <code>Optional[str]</code> <p>Optional name of workspace containing workflow. Must be used with workflow_id.</p> <code>None</code> <code>workflow_id</code> <code>Optional[str]</code> <p>Optional ID of workflow to use. Must be used with workspace_name.</p> <code>None</code> <code>image_input_name</code> <code>str</code> <p>Name of the image input node in workflow. Defaults to \"image\".</p> <code>'image'</code> <code>workflows_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional parameters to pass to workflow.</p> <code>None</code> <code>workflows_thread_pool_workers</code> <code>int</code> <p>Number of worker threads for workflow execution. Defaults to 4.</p> <code>4</code> <code>cancel_thread_pool_tasks_on_exit</code> <code>bool</code> <p>Whether to cancel pending tasks when exiting. Defaults to True.</p> <code>True</code> <code>video_metadata_input_name</code> <code>str</code> <p>Name of video metadata input in workflow. Defaults to \"video_metadata\".</p> <code>'video_metadata'</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Optional maximum FPS to process video at.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Strategy for filling source buffer when full. One of: \"WAIT\", \"DROP_OLDEST\", \"ADAPTIVE_DROP_OLDEST\", \"DROP_LATEST\", \"ADAPTIVE_DROP_LATEST\". Defaults to \"DROP_OLDEST\".</p> <code>'DROP_OLDEST'</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Strategy for consuming from source buffer. One of: \"LAZY\", \"EAGER\". Defaults to \"EAGER\".</p> <code>'EAGER'</code> <code>video_source_properties</code> <code>Optional[Dict[str, float]]</code> <p>Optional dictionary of video source properties.</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Optional timeout for batch collection in seconds.</p> <code>None</code> <code>results_buffer_size</code> <code>int</code> <p>Size of results buffer. Defaults to 64.</p> <code>64</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Response containing pipeline initialization details.</p> <p>Raises:</p> Type Description <code>InvalidParameterError</code> <p>If workflow specification parameters are invalid.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef start_inference_pipeline_with_workflow(\n    self,\n    video_reference: Union[str, int, List[Union[str, int]]],\n    workflow_specification: Optional[dict] = None,\n    workspace_name: Optional[str] = None,\n    workflow_id: Optional[str] = None,\n    image_input_name: str = \"image\",\n    workflows_parameters: Optional[Dict[str, Any]] = None,\n    workflows_thread_pool_workers: int = 4,\n    cancel_thread_pool_tasks_on_exit: bool = True,\n    video_metadata_input_name: str = \"video_metadata\",\n    max_fps: Optional[Union[float, int]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = \"DROP_OLDEST\",\n    source_buffer_consumption_strategy: Optional[\n        BufferConsumptionStrategy\n    ] = \"EAGER\",\n    video_source_properties: Optional[Dict[str, float]] = None,\n    batch_collection_timeout: Optional[float] = None,\n    results_buffer_size: int = 64,\n) -&gt; dict:\n    \"\"\"Starts an inference pipeline using a workflow specification.\n\n    Args:\n        video_reference: Path to video file, camera index, or list of video sources.\n            Can be a string path, integer camera index, or list of either.\n        workflow_specification: Optional workflow specification dictionary. Mutually\n            exclusive with workspace_name/workflow_id.\n        workspace_name: Optional name of workspace containing workflow. Must be used\n            with workflow_id.\n        workflow_id: Optional ID of workflow to use. Must be used with workspace_name.\n        image_input_name: Name of the image input node in workflow. Defaults to \"image\".\n        workflows_parameters: Optional parameters to pass to workflow.\n        workflows_thread_pool_workers: Number of worker threads for workflow execution.\n            Defaults to 4.\n        cancel_thread_pool_tasks_on_exit: Whether to cancel pending tasks when exiting.\n            Defaults to True.\n        video_metadata_input_name: Name of video metadata input in workflow.\n            Defaults to \"video_metadata\".\n        max_fps: Optional maximum FPS to process video at.\n        source_buffer_filling_strategy: Strategy for filling source buffer when full.\n            One of: \"WAIT\", \"DROP_OLDEST\", \"ADAPTIVE_DROP_OLDEST\", \"DROP_LATEST\",\n            \"ADAPTIVE_DROP_LATEST\". Defaults to \"DROP_OLDEST\".\n        source_buffer_consumption_strategy: Strategy for consuming from source buffer.\n            One of: \"LAZY\", \"EAGER\". Defaults to \"EAGER\".\n        video_source_properties: Optional dictionary of video source properties.\n        batch_collection_timeout: Optional timeout for batch collection in seconds.\n        results_buffer_size: Size of results buffer. Defaults to 64.\n\n    Returns:\n        dict: Response containing pipeline initialization details.\n\n    Raises:\n        InvalidParameterError: If workflow specification parameters are invalid.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    named_workflow_specified = (workspace_name is not None) and (\n        workflow_id is not None\n    )\n    if not (named_workflow_specified != (workflow_specification is not None)):\n        raise InvalidParameterError(\n            \"Parameters (`workspace_name`, `workflow_id`) can be used mutually exclusive with \"\n            \"`workflow_specification`, but at least one must be set.\"\n        )\n    payload = {\n        \"api_key\": self.__api_key,\n        \"video_configuration\": {\n            \"type\": \"VideoConfiguration\",\n            \"video_reference\": video_reference,\n            \"max_fps\": max_fps,\n            \"source_buffer_filling_strategy\": source_buffer_filling_strategy,\n            \"source_buffer_consumption_strategy\": source_buffer_consumption_strategy,\n            \"video_source_properties\": video_source_properties,\n            \"batch_collection_timeout\": batch_collection_timeout,\n        },\n        \"processing_configuration\": {\n            \"type\": \"WorkflowConfiguration\",\n            \"workflow_specification\": workflow_specification,\n            \"workspace_name\": workspace_name,\n            \"workflow_id\": workflow_id,\n            \"image_input_name\": image_input_name,\n            \"workflows_parameters\": workflows_parameters,\n            \"workflows_thread_pool_workers\": workflows_thread_pool_workers,\n            \"cancel_thread_pool_tasks_on_exit\": cancel_thread_pool_tasks_on_exit,\n            \"video_metadata_input_name\": video_metadata_input_name,\n        },\n        \"sink_configuration\": {\n            \"type\": \"MemorySinkConfiguration\",\n            \"results_buffer_size\": results_buffer_size,\n        },\n    }\n    response = requests.post(\n        f\"{self.__api_url}/inference_pipelines/initialise\",\n        json=payload,\n    )\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.terminate_inference_pipeline","title":"<code>terminate_inference_pipeline(pipeline_id)</code>","text":"<p>Terminates a running inference pipeline.</p> <p>Sends a request to terminate the specified inference pipeline. This will stop all processing and free up associated resources.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_id</code> <code>str</code> <p>The unique identifier of the inference pipeline to terminate.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the response from the server about the termination operation.</p> <p>Raises:</p> Type Description <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> <code>ValueError</code> <p>If pipeline_id is empty or None.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@experimental(\n    info=\"Video processing in inference server is under development. Breaking changes are possible.\"\n)\n@wrap_errors\ndef terminate_inference_pipeline(self, pipeline_id: str) -&gt; dict:\n    \"\"\"Terminates a running inference pipeline.\n\n    Sends a request to terminate the specified inference pipeline. This will stop all\n    processing and free up associated resources.\n\n    Args:\n        pipeline_id: The unique identifier of the inference pipeline to terminate.\n\n    Returns:\n        dict: A dictionary containing the response from the server about the termination operation.\n\n    Raises:\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n        ValueError: If pipeline_id is empty or None.\n    \"\"\"\n    self._ensure_pipeline_id_not_empty(pipeline_id=pipeline_id)\n    payload = {\"api_key\": self.__api_key}\n    response = requests.post(\n        f\"{self.__api_url}/inference_pipelines/{pipeline_id}/terminate\",\n        json=payload,\n    )\n    api_key_safe_raise_for_status(response=response)\n    return response.json()\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.unload_model","title":"<code>unload_model(model_id)</code>","text":"<p>Unload a model from the server.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to unload.</p> required <p>Returns:</p> Name Type Description <code>RegisteredModels</code> <code>RegisteredModels</code> <p>Updated information about registered models.</p> <p>Raises:</p> Type Description <code>WrongClientModeError</code> <p>If not in API v1 mode.</p> <code>HTTPCallErrorError</code> <p>If there is an error in the HTTP call.</p> <code>HTTPClientError</code> <p>If there is an error with the server connection.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@wrap_errors\ndef unload_model(self, model_id: str) -&gt; RegisteredModels:\n    \"\"\"Unload a model from the server.\n\n    Args:\n        model_id (str): The identifier of the model to unload.\n\n    Returns:\n        RegisteredModels: Updated information about registered models.\n\n    Raises:\n        WrongClientModeError: If not in API v1 mode.\n        HTTPCallErrorError: If there is an error in the HTTP call.\n        HTTPClientError: If there is an error with the server connection.\n    \"\"\"\n    self.__ensure_v1_client_mode()\n    de_aliased_model_id = resolve_roboflow_model_alias(model_id=model_id)\n    response = requests.post(\n        f\"{self.__api_url}/model/remove\",\n        json={\n            \"model_id\": de_aliased_model_id,\n        },\n        headers=DEFAULT_HEADERS,\n    )\n    response.raise_for_status()\n    response_payload = response.json()\n    if (\n        de_aliased_model_id == self.__selected_model\n        or model_id == self.__selected_model\n    ):\n        self.__selected_model = None\n    return RegisteredModels.from_dict(response_payload)\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.use_api_v0","title":"<code>use_api_v0()</code>","text":"<p>Temporarily use API version 0 for client operations.</p> <p>Yields:</p> Type Description <code>InferenceHTTPClient</code> <p>Generator[InferenceHTTPClient, None, None]: The client instance temporarily using API v0.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@contextmanager\ndef use_api_v0(self) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n    \"\"\"Temporarily use API version 0 for client operations.\n\n    Yields:\n        Generator[InferenceHTTPClient, None, None]: The client instance temporarily using API v0.\n    \"\"\"\n    previous_client_mode = self.__client_mode\n    self.__client_mode = HTTPClientMode.V0\n    try:\n        yield self\n    finally:\n        self.__client_mode = previous_client_mode\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.use_api_v1","title":"<code>use_api_v1()</code>","text":"<p>Temporarily use API version 1 for client operations.</p> <p>Yields:</p> Type Description <code>InferenceHTTPClient</code> <p>Generator[InferenceHTTPClient, None, None]: The client instance temporarily using API v1.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@contextmanager\ndef use_api_v1(self) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n    \"\"\"Temporarily use API version 1 for client operations.\n\n    Yields:\n        Generator[InferenceHTTPClient, None, None]: The client instance temporarily using API v1.\n    \"\"\"\n    previous_client_mode = self.__client_mode\n    self.__client_mode = HTTPClientMode.V1\n    try:\n        yield self\n    finally:\n        self.__client_mode = previous_client_mode\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.use_configuration","title":"<code>use_configuration(inference_configuration)</code>","text":"<p>Temporarily use a different inference configuration.</p> <p>Parameters:</p> Name Type Description Default <code>inference_configuration</code> <code>InferenceConfiguration</code> <p>The temporary configuration to use.</p> required <p>Yields:</p> Type Description <code>InferenceHTTPClient</code> <p>Generator[InferenceHTTPClient, None, None]: The client instance with temporary configuration.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@contextmanager\ndef use_configuration(\n    self, inference_configuration: InferenceConfiguration\n) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n    \"\"\"Temporarily use a different inference configuration.\n\n    Args:\n        inference_configuration (InferenceConfiguration): The temporary configuration to use.\n\n    Yields:\n        Generator[InferenceHTTPClient, None, None]: The client instance with temporary configuration.\n    \"\"\"\n    previous_configuration = self.__inference_configuration\n    self.__inference_configuration = inference_configuration\n    try:\n        yield self\n    finally:\n        self.__inference_configuration = previous_configuration\n</code></pre>"},{"location":"reference/inference_sdk/http/client/#inference_sdk.http.client.InferenceHTTPClient.use_model","title":"<code>use_model(model_id)</code>","text":"<p>Temporarily use a specific model for inference operations.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to use.</p> required <p>Yields:</p> Type Description <code>InferenceHTTPClient</code> <p>Generator[InferenceHTTPClient, None, None]: The client instance temporarily using the specified model.</p> Source code in <code>inference_sdk/http/client.py</code> <pre><code>@contextmanager\ndef use_model(self, model_id: str) -&gt; Generator[\"InferenceHTTPClient\", None, None]:\n    \"\"\"Temporarily use a specific model for inference operations.\n\n    Args:\n        model_id (str): The identifier of the model to use.\n\n    Yields:\n        Generator[InferenceHTTPClient, None, None]: The client instance temporarily using the specified model.\n    \"\"\"\n    previous_model = self.__selected_model\n    self.__selected_model = model_id\n    try:\n        yield self\n    finally:\n        self.__selected_model = previous_model\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/","title":"Entities","text":""},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.HTTPClientMode","title":"<code>HTTPClientMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the HTTP client mode.</p> <p>Attributes:</p> Name Type Description <code>V0</code> <p>The version 0 of the HTTP client.</p> <code>V1</code> <p>The version 1 of the HTTP client.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>class HTTPClientMode(str, Enum):\n    \"\"\"Enum for the HTTP client mode.\n\n    Attributes:\n        V0: The version 0 of the HTTP client.\n        V1: The version 1 of the HTTP client.\n    \"\"\"\n\n    V0 = \"v0\"\n    V1 = \"v1\"\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration","title":"<code>InferenceConfiguration</code>  <code>dataclass</code>","text":"<p>Dataclass for inference configuration.</p> <p>Attributes:</p> Name Type Description <code>confidence_threshold</code> <code>Optional[float]</code> <p>The confidence threshold for the inference.</p> <code>keypoint_confidence_threshold</code> <code>Optional[float]</code> <p>The keypoint confidence threshold for the inference.</p> <code>format</code> <code>Optional[str]</code> <p>The format for the inference.</p> <code>mask_decode_mode</code> <code>Optional[str]</code> <p>The mask decode mode for the inference.</p> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>The tradeoff factor for the inference.</p> <code>max_candidates</code> <code>Optional[int]</code> <p>The maximum number of candidates for the inference.</p> <code>max_detections</code> <code>Optional[int]</code> <p>The maximum number of detections for the inference.</p> <code>iou_threshold</code> <code>Optional[float]</code> <p>The intersection over union threshold for the inference.</p> <code>stroke_width</code> <code>Optional[int]</code> <p>The stroke width for the inference.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass InferenceConfiguration:\n    \"\"\"Dataclass for inference configuration.\n\n    Attributes:\n        confidence_threshold: The confidence threshold for the inference.\n        keypoint_confidence_threshold: The keypoint confidence threshold for the inference.\n        format: The format for the inference.\n        mask_decode_mode: The mask decode mode for the inference.\n        tradeoff_factor: The tradeoff factor for the inference.\n        max_candidates: The maximum number of candidates for the inference.\n        max_detections: The maximum number of detections for the inference.\n        iou_threshold: The intersection over union threshold for the inference.\n        stroke_width: The stroke width for the inference.\n    \"\"\"\n\n    confidence_threshold: Optional[float] = None\n    keypoint_confidence_threshold: Optional[float] = None\n    format: Optional[str] = None\n    mask_decode_mode: Optional[str] = None\n    tradeoff_factor: Optional[float] = None\n    max_candidates: Optional[int] = None\n    max_detections: Optional[int] = None\n    iou_threshold: Optional[float] = None\n    stroke_width: Optional[int] = None\n    count_inference: Optional[bool] = None\n    service_secret: Optional[str] = None\n    disable_preproc_auto_orientation: Optional[bool] = None\n    disable_preproc_contrast: Optional[bool] = None\n    disable_preproc_grayscale: Optional[bool] = None\n    disable_preproc_static_crop: Optional[bool] = None\n    class_agnostic_nms: Optional[bool] = None\n    class_filter: Optional[List[str]] = None\n    fix_batch_size: Optional[bool] = None\n    visualize_predictions: bool = False\n    visualize_labels: Optional[bool] = None\n    output_visualisation_format: VisualisationResponseFormat = (\n        VisualisationResponseFormat.BASE64\n    )\n    image_extensions_for_directory_scan: Optional[List[str]] = field(\n        default_factory=lambda: DEFAULT_IMAGE_EXTENSIONS,\n    )\n    client_downsizing_disabled: bool = True\n    default_max_input_size: int = DEFAULT_MAX_INPUT_SIZE\n    disable_active_learning: bool = False\n    active_learning_target_dataset: Optional[str] = None\n    max_concurrent_requests: int = 1\n    max_batch_size: int = 1\n    source: Optional[str] = None\n    source_info: Optional[str] = None\n    profiling_directory: str = \"./inference_profiling\"\n    workflow_run_retries_enabled: bool = WORKFLOW_RUN_RETRIES_ENABLED\n\n    @classmethod\n    def init_default(cls) -&gt; \"InferenceConfiguration\":\n        return cls()\n\n    def to_api_call_parameters(\n        self, client_mode: HTTPClientMode, task_type: TaskType\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Convert the current configuration to API call parameters.\n\n        Args:\n            client_mode: The HTTP client mode.\n            task_type: The type of task the model is designed for.\n\n        Returns:\n            Dict[str, Any]: The API call parameters.\n        \"\"\"\n        if client_mode is HTTPClientMode.V0:\n            return self.to_legacy_call_parameters()\n        if task_type == OBJECT_DETECTION_TASK:\n            return self.to_object_detection_parameters()\n        if task_type == INSTANCE_SEGMENTATION_TASK:\n            return self.to_instance_segmentation_parameters()\n        if task_type == CLASSIFICATION_TASK:\n            return self.to_classification_parameters()\n        if task_type == KEYPOINTS_DETECTION_TASK:\n            return self.to_keypoints_detection_parameters()\n        raise ModelTaskTypeNotSupportedError(\n            f\"Model task {task_type} is not supported by API v1 client.\"\n        )\n\n    def to_object_detection_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert the current configuration to object detection parameters.\n\n        Returns:\n            Dict[str, Any]: The object detection parameters.\n        \"\"\"\n        parameters_specs = [\n            (\"disable_preproc_auto_orientation\", \"disable_preproc_auto_orient\"),\n            (\"disable_preproc_contrast\", \"disable_preproc_contrast\"),\n            (\"disable_preproc_grayscale\", \"disable_preproc_grayscale\"),\n            (\"disable_preproc_static_crop\", \"disable_preproc_static_crop\"),\n            (\"class_agnostic_nms\", \"class_agnostic_nms\"),\n            (\"class_filter\", \"class_filter\"),\n            (\"confidence_threshold\", \"confidence\"),\n            (\"fix_batch_size\", \"fix_batch_size\"),\n            (\"iou_threshold\", \"iou_threshold\"),\n            (\"max_detections\", \"max_detections\"),\n            (\"max_candidates\", \"max_candidates\"),\n            (\"visualize_labels\", \"visualization_labels\"),\n            (\"stroke_width\", \"visualization_stroke_width\"),\n            (\"visualize_predictions\", \"visualize_predictions\"),\n            (\"disable_active_learning\", \"disable_active_learning\"),\n            (\"active_learning_target_dataset\", \"active_learning_target_dataset\"),\n            (\"source\", \"source\"),\n            (\"source_info\", \"source_info\"),\n        ]\n        return get_non_empty_attributes(\n            source_object=self,\n            specification=parameters_specs,\n        )\n\n    def to_keypoints_detection_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert the current configuration to keypoints detection parameters.\n\n        Returns:\n            Dict[str, Any]: The keypoints detection parameters.\n        \"\"\"\n        parameters = self.to_object_detection_parameters()\n        parameters[\"keypoint_confidence\"] = self.keypoint_confidence_threshold\n        return remove_empty_values(dictionary=parameters)\n\n    def to_instance_segmentation_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert the current configuration to instance segmentation parameters.\n\n        Returns:\n            Dict[str, Any]: The instance segmentation parameters.\n        \"\"\"\n        parameters = self.to_object_detection_parameters()\n        parameters_specs = [\n            (\"mask_decode_mode\", \"mask_decode_mode\"),\n            (\"tradeoff_factor\", \"tradeoff_factor\"),\n        ]\n        for internal_name, external_name in parameters_specs:\n            parameters[external_name] = getattr(self, internal_name)\n        return remove_empty_values(dictionary=parameters)\n\n    def to_classification_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert the current configuration to classification parameters.\n\n        Returns:\n            Dict[str, Any]: The classification parameters.\n        \"\"\"\n        parameters_specs = [\n            (\"disable_preproc_auto_orientation\", \"disable_preproc_auto_orient\"),\n            (\"disable_preproc_contrast\", \"disable_preproc_contrast\"),\n            (\"disable_preproc_grayscale\", \"disable_preproc_grayscale\"),\n            (\"disable_preproc_static_crop\", \"disable_preproc_static_crop\"),\n            (\"confidence_threshold\", \"confidence\"),\n            (\"visualize_predictions\", \"visualize_predictions\"),\n            (\"stroke_width\", \"visualization_stroke_width\"),\n            (\"disable_active_learning\", \"disable_active_learning\"),\n            (\"source\", \"source\"),\n            (\"source_info\", \"source_info\"),\n            (\"active_learning_target_dataset\", \"active_learning_target_dataset\"),\n        ]\n        return get_non_empty_attributes(\n            source_object=self,\n            specification=parameters_specs,\n        )\n\n    def to_legacy_call_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert the current configuration to legacy call parameters.\n\n        Returns:\n            Dict[str, Any]: The legacy call parameters.\n        \"\"\"\n        parameters_specs = [\n            (\"confidence_threshold\", \"confidence\"),\n            (\"keypoint_confidence_threshold\", \"keypoint_confidence\"),\n            (\"format\", \"format\"),\n            (\"visualize_labels\", \"labels\"),\n            (\"mask_decode_mode\", \"mask_decode_mode\"),\n            (\"tradeoff_factor\", \"tradeoff_factor\"),\n            (\"max_detections\", \"max_detections\"),\n            (\"iou_threshold\", \"overlap\"),\n            (\"stroke_width\", \"stroke\"),\n            (\"count_inference\", \"countinference\"),\n            (\"service_secret\", \"service_secret\"),\n            (\"disable_preproc_auto_orientation\", \"disable_preproc_auto_orient\"),\n            (\"disable_preproc_contrast\", \"disable_preproc_contrast\"),\n            (\"disable_preproc_grayscale\", \"disable_preproc_grayscale\"),\n            (\"disable_preproc_static_crop\", \"disable_preproc_static_crop\"),\n            (\"disable_active_learning\", \"disable_active_learning\"),\n            (\"active_learning_target_dataset\", \"active_learning_target_dataset\"),\n            (\"source\", \"source\"),\n            (\"source_info\", \"source_info\"),\n        ]\n        return get_non_empty_attributes(\n            source_object=self,\n            specification=parameters_specs,\n        )\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration.to_api_call_parameters","title":"<code>to_api_call_parameters(client_mode, task_type)</code>","text":"<p>Convert the current configuration to API call parameters.</p> <p>Parameters:</p> Name Type Description Default <code>client_mode</code> <code>HTTPClientMode</code> <p>The HTTP client mode.</p> required <code>task_type</code> <code>TaskType</code> <p>The type of task the model is designed for.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The API call parameters.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def to_api_call_parameters(\n    self, client_mode: HTTPClientMode, task_type: TaskType\n) -&gt; Dict[str, Any]:\n    \"\"\"Convert the current configuration to API call parameters.\n\n    Args:\n        client_mode: The HTTP client mode.\n        task_type: The type of task the model is designed for.\n\n    Returns:\n        Dict[str, Any]: The API call parameters.\n    \"\"\"\n    if client_mode is HTTPClientMode.V0:\n        return self.to_legacy_call_parameters()\n    if task_type == OBJECT_DETECTION_TASK:\n        return self.to_object_detection_parameters()\n    if task_type == INSTANCE_SEGMENTATION_TASK:\n        return self.to_instance_segmentation_parameters()\n    if task_type == CLASSIFICATION_TASK:\n        return self.to_classification_parameters()\n    if task_type == KEYPOINTS_DETECTION_TASK:\n        return self.to_keypoints_detection_parameters()\n    raise ModelTaskTypeNotSupportedError(\n        f\"Model task {task_type} is not supported by API v1 client.\"\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration.to_classification_parameters","title":"<code>to_classification_parameters()</code>","text":"<p>Convert the current configuration to classification parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The classification parameters.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def to_classification_parameters(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the current configuration to classification parameters.\n\n    Returns:\n        Dict[str, Any]: The classification parameters.\n    \"\"\"\n    parameters_specs = [\n        (\"disable_preproc_auto_orientation\", \"disable_preproc_auto_orient\"),\n        (\"disable_preproc_contrast\", \"disable_preproc_contrast\"),\n        (\"disable_preproc_grayscale\", \"disable_preproc_grayscale\"),\n        (\"disable_preproc_static_crop\", \"disable_preproc_static_crop\"),\n        (\"confidence_threshold\", \"confidence\"),\n        (\"visualize_predictions\", \"visualize_predictions\"),\n        (\"stroke_width\", \"visualization_stroke_width\"),\n        (\"disable_active_learning\", \"disable_active_learning\"),\n        (\"source\", \"source\"),\n        (\"source_info\", \"source_info\"),\n        (\"active_learning_target_dataset\", \"active_learning_target_dataset\"),\n    ]\n    return get_non_empty_attributes(\n        source_object=self,\n        specification=parameters_specs,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration.to_instance_segmentation_parameters","title":"<code>to_instance_segmentation_parameters()</code>","text":"<p>Convert the current configuration to instance segmentation parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The instance segmentation parameters.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def to_instance_segmentation_parameters(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the current configuration to instance segmentation parameters.\n\n    Returns:\n        Dict[str, Any]: The instance segmentation parameters.\n    \"\"\"\n    parameters = self.to_object_detection_parameters()\n    parameters_specs = [\n        (\"mask_decode_mode\", \"mask_decode_mode\"),\n        (\"tradeoff_factor\", \"tradeoff_factor\"),\n    ]\n    for internal_name, external_name in parameters_specs:\n        parameters[external_name] = getattr(self, internal_name)\n    return remove_empty_values(dictionary=parameters)\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration.to_keypoints_detection_parameters","title":"<code>to_keypoints_detection_parameters()</code>","text":"<p>Convert the current configuration to keypoints detection parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The keypoints detection parameters.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def to_keypoints_detection_parameters(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the current configuration to keypoints detection parameters.\n\n    Returns:\n        Dict[str, Any]: The keypoints detection parameters.\n    \"\"\"\n    parameters = self.to_object_detection_parameters()\n    parameters[\"keypoint_confidence\"] = self.keypoint_confidence_threshold\n    return remove_empty_values(dictionary=parameters)\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration.to_legacy_call_parameters","title":"<code>to_legacy_call_parameters()</code>","text":"<p>Convert the current configuration to legacy call parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The legacy call parameters.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def to_legacy_call_parameters(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the current configuration to legacy call parameters.\n\n    Returns:\n        Dict[str, Any]: The legacy call parameters.\n    \"\"\"\n    parameters_specs = [\n        (\"confidence_threshold\", \"confidence\"),\n        (\"keypoint_confidence_threshold\", \"keypoint_confidence\"),\n        (\"format\", \"format\"),\n        (\"visualize_labels\", \"labels\"),\n        (\"mask_decode_mode\", \"mask_decode_mode\"),\n        (\"tradeoff_factor\", \"tradeoff_factor\"),\n        (\"max_detections\", \"max_detections\"),\n        (\"iou_threshold\", \"overlap\"),\n        (\"stroke_width\", \"stroke\"),\n        (\"count_inference\", \"countinference\"),\n        (\"service_secret\", \"service_secret\"),\n        (\"disable_preproc_auto_orientation\", \"disable_preproc_auto_orient\"),\n        (\"disable_preproc_contrast\", \"disable_preproc_contrast\"),\n        (\"disable_preproc_grayscale\", \"disable_preproc_grayscale\"),\n        (\"disable_preproc_static_crop\", \"disable_preproc_static_crop\"),\n        (\"disable_active_learning\", \"disable_active_learning\"),\n        (\"active_learning_target_dataset\", \"active_learning_target_dataset\"),\n        (\"source\", \"source\"),\n        (\"source_info\", \"source_info\"),\n    ]\n    return get_non_empty_attributes(\n        source_object=self,\n        specification=parameters_specs,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.InferenceConfiguration.to_object_detection_parameters","title":"<code>to_object_detection_parameters()</code>","text":"<p>Convert the current configuration to object detection parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The object detection parameters.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def to_object_detection_parameters(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert the current configuration to object detection parameters.\n\n    Returns:\n        Dict[str, Any]: The object detection parameters.\n    \"\"\"\n    parameters_specs = [\n        (\"disable_preproc_auto_orientation\", \"disable_preproc_auto_orient\"),\n        (\"disable_preproc_contrast\", \"disable_preproc_contrast\"),\n        (\"disable_preproc_grayscale\", \"disable_preproc_grayscale\"),\n        (\"disable_preproc_static_crop\", \"disable_preproc_static_crop\"),\n        (\"class_agnostic_nms\", \"class_agnostic_nms\"),\n        (\"class_filter\", \"class_filter\"),\n        (\"confidence_threshold\", \"confidence\"),\n        (\"fix_batch_size\", \"fix_batch_size\"),\n        (\"iou_threshold\", \"iou_threshold\"),\n        (\"max_detections\", \"max_detections\"),\n        (\"max_candidates\", \"max_candidates\"),\n        (\"visualize_labels\", \"visualization_labels\"),\n        (\"stroke_width\", \"visualization_stroke_width\"),\n        (\"visualize_predictions\", \"visualize_predictions\"),\n        (\"disable_active_learning\", \"disable_active_learning\"),\n        (\"active_learning_target_dataset\", \"active_learning_target_dataset\"),\n        (\"source\", \"source\"),\n        (\"source_info\", \"source_info\"),\n    ]\n    return get_non_empty_attributes(\n        source_object=self,\n        specification=parameters_specs,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.ModelDescription","title":"<code>ModelDescription</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataClassJsonMixin</code></p> <p>Dataclass for model description.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>The unique identifier of the model.</p> <code>task_type</code> <code>TaskType</code> <p>The type of task the model is designed for.</p> <code>batch_size</code> <code>Optional[Union[int, str]]</code> <p>The batch size for the model.</p> <code>input_height</code> <code>Optional[int]</code> <p>The height of the input image.</p> <code>input_width</code> <code>Optional[int]</code> <p>The width of the input image.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass ModelDescription(DataClassJsonMixin):\n    \"\"\"Dataclass for model description.\n\n    Attributes:\n        model_id: The unique identifier of the model.\n        task_type: The type of task the model is designed for.\n        batch_size: The batch size for the model.\n        input_height: The height of the input image.\n        input_width: The width of the input image.\n    \"\"\"\n\n    model_id: str\n    task_type: TaskType\n    batch_size: Optional[Union[int, str]] = None\n    input_height: Optional[int] = None\n    input_width: Optional[int] = None\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.RegisteredModels","title":"<code>RegisteredModels</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataClassJsonMixin</code></p> <p>Dataclass for registered models.</p> <p>Attributes:</p> Name Type Description <code>models</code> <code>List[ModelDescription]</code> <p>A list of model descriptions.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass RegisteredModels(DataClassJsonMixin):\n    \"\"\"Dataclass for registered models.\n\n    Attributes:\n        models: A list of model descriptions.\n    \"\"\"\n\n    models: List[ModelDescription]\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.ServerInfo","title":"<code>ServerInfo</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataClassJsonMixin</code></p> <p>Dataclass for Information about the inference server.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the inference server.</p> <code>version</code> <code>str</code> <p>The version of the inference server.</p> <code>uuid</code> <code>str</code> <p>The unique identifier of the inference server instance.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass ServerInfo(DataClassJsonMixin):\n    \"\"\"Dataclass for Information about the inference server.\n\n    Attributes:\n        name: The name of the inference server.\n        version: The version of the inference server.\n        uuid: The unique identifier of the inference server instance.\n    \"\"\"\n\n    name: str\n    version: str\n    uuid: str\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.VisualisationResponseFormat","title":"<code>VisualisationResponseFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the visualisation response format.</p> <p>Attributes:</p> Name Type Description <code>BASE64</code> <p>The base64 format.</p> <code>NUMPY</code> <p>The numpy format.</p> <code>PILLOW</code> <p>The pillow format.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>class VisualisationResponseFormat(str, Enum):\n    \"\"\"Enum for the visualisation response format.\n\n    Attributes:\n        BASE64: The base64 format.\n        NUMPY: The numpy format.\n        PILLOW: The pillow format.\n    \"\"\"\n\n    BASE64 = \"base64\"\n    NUMPY = \"numpy\"\n    PILLOW = \"pillow\"\n</code></pre>"},{"location":"reference/inference_sdk/http/entities/#inference_sdk.http.entities.get_non_empty_attributes","title":"<code>get_non_empty_attributes(source_object, specification)</code>","text":"<p>Get non-empty attributes from the source object.</p> <p>Parameters:</p> Name Type Description Default <code>source_object</code> <code>object</code> <p>The source object.</p> required <code>specification</code> <code>List[Tuple[str, str]]</code> <p>The specification of the attributes.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The non-empty attributes.</p> Source code in <code>inference_sdk/http/entities.py</code> <pre><code>def get_non_empty_attributes(\n    source_object: object, specification: List[Tuple[str, str]]\n) -&gt; Dict[str, Any]:\n    \"\"\"Get non-empty attributes from the source object.\n\n    Args:\n        source_object: The source object.\n        specification: The specification of the attributes.\n\n    Returns:\n        Dict[str, Any]: The non-empty attributes.\n    \"\"\"\n    attributes = {\n        external_name: getattr(source_object, internal_name)\n        for internal_name, external_name in specification\n    }\n    return remove_empty_values(dictionary=attributes)\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/","title":"Errors","text":""},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.APIKeyNotProvided","title":"<code>APIKeyNotProvided</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for API key not provided.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class APIKeyNotProvided(HTTPClientError):\n    \"\"\"Error for API key not provided.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.EncodingError","title":"<code>EncodingError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for encoding errors.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class EncodingError(HTTPClientError):\n    \"\"\"Error for encoding errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.HTTPCallErrorError","title":"<code>HTTPCallErrorError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for HTTP call errors.</p> <p>Attributes:</p> Name Type Description <code>description</code> <code>str</code> <p>The description of the error.</p> <code>status_code</code> <code>int</code> <p>The status code of the error.</p> <code>api_message</code> <code>str</code> <p>The API message of the error.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class HTTPCallErrorError(HTTPClientError):\n    \"\"\"Error for HTTP call errors.\n\n    Attributes:\n        description: The description of the error.\n        status_code: The status code of the error.\n        api_message: The API message of the error.\n    \"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        status_code: int,\n        api_message: Optional[str],\n    ):\n        super().__init__(description)\n        self.__description = description\n        self.__api_message = api_message\n        self.__status_code = status_code\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"The description of the error.\"\"\"\n        return self.__description\n\n    @property\n    def api_message(self) -&gt; str:\n        \"\"\"The API message of the error.\"\"\"\n        return self.__api_message\n\n    @property\n    def status_code(self) -&gt; int:\n        \"\"\"The status code of the error.\"\"\"\n        return self.__status_code\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"description='{self.description}', \"\n            f\"api_message='{self.api_message}',\"\n            f\"status_code={self.__status_code})\"\n        )\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.HTTPCallErrorError.api_message","title":"<code>api_message</code>  <code>property</code>","text":"<p>The API message of the error.</p>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.HTTPCallErrorError.description","title":"<code>description</code>  <code>property</code>","text":"<p>The description of the error.</p>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.HTTPCallErrorError.status_code","title":"<code>status_code</code>  <code>property</code>","text":"<p>The status code of the error.</p>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.HTTPClientError","title":"<code>HTTPClientError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for HTTP client errors.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class HTTPClientError(Exception):\n    \"\"\"Base class for HTTP client errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.InvalidInputFormatError","title":"<code>InvalidInputFormatError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for invalid input format.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class InvalidInputFormatError(HTTPClientError):\n    \"\"\"Error for invalid input format.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.InvalidModelIdentifier","title":"<code>InvalidModelIdentifier</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for invalid model identifier.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class InvalidModelIdentifier(HTTPClientError):\n    \"\"\"Error for invalid model identifier.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.InvalidParameterError","title":"<code>InvalidParameterError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for invalid parameter.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class InvalidParameterError(HTTPClientError):\n    \"\"\"Error for invalid parameter.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.ModelNotInitializedError","title":"<code>ModelNotInitializedError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for model not initialized.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class ModelNotInitializedError(HTTPClientError):\n    \"\"\"Error for model not initialized.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.ModelNotSelectedError","title":"<code>ModelNotSelectedError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for model not selected.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class ModelNotSelectedError(HTTPClientError):\n    \"\"\"Error for model not selected.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.ModelTaskTypeNotSupportedError","title":"<code>ModelTaskTypeNotSupportedError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for model task type not supported.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class ModelTaskTypeNotSupportedError(HTTPClientError):\n    \"\"\"Error for model task type not supported.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/errors/#inference_sdk.http.errors.WrongClientModeError","title":"<code>WrongClientModeError</code>","text":"<p>               Bases: <code>HTTPClientError</code></p> <p>Error for wrong client mode.</p> Source code in <code>inference_sdk/http/errors.py</code> <pre><code>class WrongClientModeError(HTTPClientError):\n    \"\"\"Error for wrong client mode.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/aliases/","title":"Aliases","text":""},{"location":"reference/inference_sdk/http/utils/aliases/#inference_sdk.http.utils.aliases.resolve_ocr_path","title":"<code>resolve_ocr_path(model_name)</code>","text":"<p>Resolve an OCR model name to its corresponding endpoint path.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the OCR model.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The endpoint path for the OCR model.</p> Source code in <code>inference_sdk/http/utils/aliases.py</code> <pre><code>def resolve_ocr_path(model_name: str) -&gt; str:\n    \"\"\"Resolve an OCR model name to its corresponding endpoint path.\n\n    Args:\n        model_name: The name of the OCR model.\n\n    Returns:\n        The endpoint path for the OCR model.\n    \"\"\"\n    model_name = model_name.lower()\n    if model_name not in OCR_ENDPOINTS:\n        raise ValueError(f\"OCR not supported: {model_name}\")\n    return OCR_ENDPOINTS[model_name]\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/aliases/#inference_sdk.http.utils.aliases.resolve_roboflow_model_alias","title":"<code>resolve_roboflow_model_alias(model_id)</code>","text":"<p>Resolve a Roboflow model alias to a registered model ID.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model alias to resolve.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The registered model ID.</p> Source code in <code>inference_sdk/http/utils/aliases.py</code> <pre><code>def resolve_roboflow_model_alias(model_id: str) -&gt; str:\n    \"\"\"Resolve a Roboflow model alias to a registered model ID.\n\n    Args:\n        model_id: The model alias to resolve.\n\n    Returns:\n        The registered model ID.\n    \"\"\"\n    return REGISTERED_ALIASES.get(model_id, model_id)\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/encoding/","title":"Encoding","text":""},{"location":"reference/inference_sdk/http/utils/encoding/#inference_sdk.http.utils.encoding.bytes_to_opencv_image","title":"<code>bytes_to_opencv_image(payload, array_type=np.uint8)</code>","text":"<p>Decode a bytes object to an OpenCV image.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>bytes</code> <p>The bytes object to decode.</p> required <code>array_type</code> <code>number</code> <p>The type of the array.</p> <code>uint8</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The OpenCV image.</p> Source code in <code>inference_sdk/http/utils/encoding.py</code> <pre><code>def bytes_to_opencv_image(\n    payload: bytes, array_type: np.number = np.uint8\n) -&gt; np.ndarray:\n    \"\"\"Decode a bytes object to an OpenCV image.\n\n    Args:\n        payload: The bytes object to decode.\n        array_type: The type of the array.\n\n    Returns:\n        The OpenCV image.\n    \"\"\"\n    bytes_array = np.frombuffer(payload, dtype=array_type)\n    decoding_result = cv2.imdecode(bytes_array, cv2.IMREAD_UNCHANGED)\n    if decoding_result is None:\n        raise EncodingError(\"Could not encode bytes to OpenCV image.\")\n    return decoding_result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/encoding/#inference_sdk.http.utils.encoding.bytes_to_pillow_image","title":"<code>bytes_to_pillow_image(payload)</code>","text":"<p>Decode a bytes object to a PIL image.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>bytes</code> <p>The bytes object to decode.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>The PIL image.</p> Source code in <code>inference_sdk/http/utils/encoding.py</code> <pre><code>def bytes_to_pillow_image(payload: bytes) -&gt; Image.Image:\n    \"\"\"Decode a bytes object to a PIL image.\n\n    Args:\n        payload: The bytes object to decode.\n\n    Returns:\n        The PIL image.\n    \"\"\"\n    buffer = BytesIO(payload)\n    try:\n        return Image.open(buffer)\n    except UnidentifiedImageError as error:\n        raise EncodingError(\"Could not encode bytes to PIL image.\") from error\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/encoding/#inference_sdk.http.utils.encoding.encode_base_64","title":"<code>encode_base_64(payload)</code>","text":"<p>Encode a bytes object to a base64 string.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>bytes</code> <p>The bytes object to encode.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The base64 string.</p> Source code in <code>inference_sdk/http/utils/encoding.py</code> <pre><code>def encode_base_64(payload: bytes) -&gt; str:\n    \"\"\"Encode a bytes object to a base64 string.\n\n    Args:\n        payload: The bytes object to encode.\n\n    Returns:\n        The base64 string.\n    \"\"\"\n    return base64.b64encode(payload).decode(\"utf-8\")\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/encoding/#inference_sdk.http.utils.encoding.numpy_array_to_base64_jpeg","title":"<code>numpy_array_to_base64_jpeg(image)</code>","text":"<p>Encode a numpy array to a base64 JPEG string.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array to encode.</p> required <p>Returns:</p> Type Description <code>Union[str]</code> <p>The base64 JPEG string.</p> Source code in <code>inference_sdk/http/utils/encoding.py</code> <pre><code>def numpy_array_to_base64_jpeg(\n    image: np.ndarray,\n) -&gt; Union[str]:\n    \"\"\"Encode a numpy array to a base64 JPEG string.\n\n    Args:\n        image: The numpy array to encode.\n\n    Returns:\n        The base64 JPEG string.\n    \"\"\"\n    _, img_encoded = cv2.imencode(\".jpg\", image)\n    image_bytes = np.array(img_encoded).tobytes()\n    return encode_base_64(payload=image_bytes)\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/encoding/#inference_sdk.http.utils.encoding.pillow_image_to_base64_jpeg","title":"<code>pillow_image_to_base64_jpeg(image)</code>","text":"<p>Encode a PIL image to a base64 JPEG string.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL image to encode.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The base64 JPEG string.</p> Source code in <code>inference_sdk/http/utils/encoding.py</code> <pre><code>def pillow_image_to_base64_jpeg(image: Image.Image) -&gt; str:\n    \"\"\"Encode a PIL image to a base64 JPEG string.\n\n    Args:\n        image: The PIL image to encode.\n\n    Returns:\n        The base64 JPEG string.\n    \"\"\"\n    with BytesIO() as buffer:\n        image.save(buffer, format=\"JPEG\")\n        return encode_base_64(payload=buffer.getvalue())\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/","title":"Executors","text":""},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.RequestMethod","title":"<code>RequestMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for the request method.</p> <p>Attributes:</p> Name Type Description <code>GET</code> <p>The GET method.</p> <code>POST</code> <p>The POST method.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>class RequestMethod(Enum):\n    \"\"\"Enum for the request method.\n\n    Attributes:\n        GET: The GET method.\n        POST: The POST method.\n    \"\"\"\n\n    GET = \"get\"\n    POST = \"post\"\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.execute_requests_packages","title":"<code>execute_requests_packages(requests_data, request_method, max_concurrent_requests)</code>","text":"<p>Execute a list of requests in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>requests_data</code> <code>List[RequestData]</code> <p>The list of requests to execute.</p> required <code>request_method</code> <code>RequestMethod</code> <p>The method to use for the requests.</p> required <code>max_concurrent_requests</code> <code>int</code> <p>The maximum number of concurrent requests.</p> required <p>Returns:</p> Type Description <code>List[Response]</code> <p>The list of responses.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>def execute_requests_packages(\n    requests_data: List[RequestData],\n    request_method: RequestMethod,\n    max_concurrent_requests: int,\n) -&gt; List[Response]:\n    \"\"\"Execute a list of requests in parallel.\n\n    Args:\n        requests_data: The list of requests to execute.\n        request_method: The method to use for the requests.\n        max_concurrent_requests: The maximum number of concurrent requests.\n\n    Returns:\n        The list of responses.\n    \"\"\"\n    requests_data_packages = make_batches(\n        iterable=requests_data,\n        batch_size=max_concurrent_requests,\n    )\n    results = []\n    for requests_data_package in requests_data_packages:\n        responses = make_parallel_requests(\n            requests_data=requests_data_package,\n            request_method=request_method,\n        )\n        results.extend(responses)\n    for response in results:\n        api_key_safe_raise_for_status(response=response)\n    return results\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.execute_requests_packages_async","title":"<code>execute_requests_packages_async(requests_data, request_method, max_concurrent_requests)</code>  <code>async</code>","text":"<p>Execute a list of requests in parallel asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>requests_data</code> <code>List[RequestData]</code> <p>The list of requests to execute.</p> required <code>request_method</code> <code>RequestMethod</code> <p>The method to use for the requests.</p> required <code>max_concurrent_requests</code> <code>int</code> <p>The maximum number of concurrent requests.</p> required <p>Returns:</p> Type Description <code>List[Union[dict, bytes]]</code> <p>The list of responses.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>async def execute_requests_packages_async(\n    requests_data: List[RequestData],\n    request_method: RequestMethod,\n    max_concurrent_requests: int,\n) -&gt; List[Union[dict, bytes]]:\n    \"\"\"Execute a list of requests in parallel asynchronously.\n\n    Args:\n        requests_data: The list of requests to execute.\n        request_method: The method to use for the requests.\n        max_concurrent_requests: The maximum number of concurrent requests.\n\n    Returns:\n        The list of responses.\n    \"\"\"\n    requests_data_packages = make_batches(\n        iterable=requests_data,\n        batch_size=max_concurrent_requests,\n    )\n    results = []\n    for requests_data_package in requests_data_packages:\n        responses = await make_parallel_requests_async(\n            requests_data=requests_data_package,\n            request_method=request_method,\n        )\n        results.extend(responses)\n    return results\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.make_parallel_requests","title":"<code>make_parallel_requests(requests_data, request_method)</code>","text":"<p>Execute a list of requests in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>requests_data</code> <code>List[RequestData]</code> <p>The list of requests to execute.</p> required <code>request_method</code> <code>RequestMethod</code> <p>The method to use for the requests.</p> required <p>Returns:</p> Type Description <code>List[Response]</code> <p>The list of responses.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>def make_parallel_requests(\n    requests_data: List[RequestData],\n    request_method: RequestMethod,\n) -&gt; List[Response]:\n    \"\"\"Execute a list of requests in parallel.\n\n    Args:\n        requests_data: The list of requests to execute.\n        request_method: The method to use for the requests.\n\n    Returns:\n        The list of responses.\n    \"\"\"\n    workers = len(requests_data)\n    make_request_closure = partial(make_request, request_method=request_method)\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        return list(executor.map(make_request_closure, requests_data))\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.make_parallel_requests_async","title":"<code>make_parallel_requests_async(requests_data, request_method)</code>  <code>async</code>","text":"<p>Execute a list of requests in parallel asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>requests_data</code> <code>List[RequestData]</code> <p>The list of requests to execute.</p> required <code>request_method</code> <code>RequestMethod</code> <p>The method to use for the requests.</p> required <p>Returns:</p> Type Description <code>List[Union[dict, bytes]]</code> <p>The list of responses.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>async def make_parallel_requests_async(\n    requests_data: List[RequestData],\n    request_method: RequestMethod,\n) -&gt; List[Union[dict, bytes]]:\n    \"\"\"Execute a list of requests in parallel asynchronously.\n\n    Args:\n        requests_data: The list of requests to execute.\n        request_method: The method to use for the requests.\n\n    Returns:\n        The list of responses.\n    \"\"\"\n    async with aiohttp.ClientSession() as session:\n        make_request_closure = partial(\n            make_request_async,\n            request_method=request_method,\n            session=session,\n        )\n        coroutines = [make_request_closure(data) for data in requests_data]\n        responses = list(await asyncio.gather(*coroutines))\n        return [r[1] for r in responses]\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.make_request","title":"<code>make_request(request_data, request_method)</code>","text":"<p>Make a request to the API.</p> <p>Parameters:</p> Name Type Description Default <code>request_data</code> <code>RequestData</code> <p>The request data.</p> required <code>request_method</code> <code>RequestMethod</code> <p>The method to use for the request.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>The response from the API.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>@backoff.on_predicate(\n    backoff.constant,\n    predicate=lambda r: r.status_code in RETRYABLE_STATUS_CODES,\n    max_tries=3,\n    interval=1,\n    backoff_log_level=logging.DEBUG,\n    giveup_log_level=logging.DEBUG,\n)\n@backoff.on_exception(\n    backoff.constant,\n    exception=ConnectionError,\n    max_tries=3,\n    interval=1,\n    backoff_log_level=logging.DEBUG,\n    giveup_log_level=logging.DEBUG,\n)\ndef make_request(request_data: RequestData, request_method: RequestMethod) -&gt; Response:\n    \"\"\"Make a request to the API.\n\n    Args:\n        request_data: The request data.\n        request_method: The method to use for the request.\n\n    Returns:\n        The response from the API.\n    \"\"\"\n    method = requests.get if request_method is RequestMethod.GET else requests.post\n    return method(\n        request_data.url,\n        headers=request_data.headers,\n        params=request_data.parameters,\n        data=request_data.data,\n        json=request_data.payload,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.make_request_async","title":"<code>make_request_async(request_data, request_method, session)</code>  <code>async</code>","text":"<p>Make a request to the API asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>request_data</code> <code>RequestData</code> <p>The request data.</p> required <code>request_method</code> <code>RequestMethod</code> <p>The method to use for the request.</p> required <code>session</code> <code>ClientSession</code> <p>The session to use for the request.</p> required <p>Returns:</p> Type Description <code>Tuple[int, Union[bytes, dict]]</code> <p>The response from the API.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>@backoff.on_predicate(\n    backoff.constant,\n    predicate=lambda r: r[0] in RETRYABLE_STATUS_CODES,\n    max_tries=3,\n    interval=1,\n    on_giveup=raise_client_error,\n    backoff_log_level=logging.DEBUG,\n    giveup_log_level=logging.DEBUG,\n)\n@backoff.on_exception(\n    backoff.constant,\n    exception=ClientConnectionError,\n    max_tries=3,\n    interval=1,\n    backoff_log_level=logging.DEBUG,\n    giveup_log_level=logging.DEBUG,\n)\nasync def make_request_async(\n    request_data: RequestData,\n    request_method: RequestMethod,\n    session: aiohttp.ClientSession,\n) -&gt; Tuple[int, Union[bytes, dict]]:\n    \"\"\"Make a request to the API asynchronously.\n\n    Args:\n        request_data: The request data.\n        request_method: The method to use for the request.\n        session: The session to use for the request.\n\n    Returns:\n        The response from the API.\n    \"\"\"\n    method = session.get if request_method is RequestMethod.GET else session.post\n    parameters_serialised = None\n    if request_data.parameters is not None:\n        parameters_serialised = {\n            name: (\n                str(value)\n                if not issubclass(type(value), list)\n                else [str(e) for e in value]\n            )\n            for name, value in request_data.parameters.items()\n        }\n    async with method(\n        request_data.url,\n        headers=request_data.headers,\n        params=parameters_serialised,\n        data=request_data.data,\n        json=request_data.payload,\n    ) as response:\n        try:\n            response_data = await response.json()\n        except:\n            response_data = await response.read()\n        if response_is_not_retryable_error(response=response):\n            response.raise_for_status()\n        return response.status, response_data\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.raise_client_error","title":"<code>raise_client_error(details)</code>","text":"<p>Raise a client error.</p> <p>Parameters:</p> Name Type Description Default <code>details</code> <code>dict</code> <p>The details of the error.</p> required Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>def raise_client_error(details: dict) -&gt; None:\n    \"\"\"Raise a client error.\n\n    Args:\n        details: The details of the error.\n    \"\"\"\n    status_code = details[\"value\"][0]\n    request_data = details[\"kwargs\"][\"request_data\"]\n    raise ClientResponseError(\n        request_info=RequestInfo(\n            url=request_data.url,\n            method=\"POST\",\n            headers={},\n        ),\n        history=(),\n        status=status_code,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/executors/#inference_sdk.http.utils.executors.response_is_not_retryable_error","title":"<code>response_is_not_retryable_error(response)</code>","text":"<p>Check if the response is not a retryable error.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ClientResponse</code> <p>The response to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the response is not a retryable error, False otherwise.</p> Source code in <code>inference_sdk/http/utils/executors.py</code> <pre><code>def response_is_not_retryable_error(response: ClientResponse) -&gt; bool:\n    \"\"\"Check if the response is not a retryable error.\n\n    Args:\n        response: The response to check.\n\n    Returns:\n        True if the response is not a retryable error, False otherwise.\n    \"\"\"\n    return response.status != 200 and response.status not in RETRYABLE_STATUS_CODES\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/iterables/","title":"Iterables","text":""},{"location":"reference/inference_sdk/http/utils/iterables/#inference_sdk.http.utils.iterables.make_batches","title":"<code>make_batches(iterable, batch_size)</code>","text":"<p>Make batches from an iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to make batches from.</p> required <code>batch_size</code> <code>int</code> <p>The size of the batches.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The batches.</p> Source code in <code>inference_sdk/http/utils/iterables.py</code> <pre><code>def make_batches(\n    iterable: Iterable[T], batch_size: int\n) -&gt; Generator[List[T], None, None]:\n    \"\"\"Make batches from an iterable.\n\n    Args:\n        iterable: The iterable to make batches from.\n        batch_size: The size of the batches.\n\n    Returns:\n        The batches.\n    \"\"\"\n    batch_size = max(batch_size, 1)\n    batch = []\n    for element in iterable:\n        batch.append(element)\n        if len(batch) &gt;= batch_size:\n            yield batch\n            batch = []\n    if len(batch) &gt; 0:\n        yield batch\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/iterables/#inference_sdk.http.utils.iterables.remove_empty_values","title":"<code>remove_empty_values(dictionary)</code>","text":"<p>Remove empty values from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>The dictionary to remove empty values from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary with empty values removed.</p> Source code in <code>inference_sdk/http/utils/iterables.py</code> <pre><code>def remove_empty_values(dictionary: dict) -&gt; dict:\n    \"\"\"Remove empty values from a dictionary.\n\n    Args:\n        dictionary: The dictionary to remove empty values from.\n\n    Returns:\n        The dictionary with empty values removed.\n    \"\"\"\n    return {k: v for k, v in dictionary.items() if v is not None}\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/iterables/#inference_sdk.http.utils.iterables.unwrap_single_element_list","title":"<code>unwrap_single_element_list(sequence)</code>","text":"<p>Unwrap a single element list.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>List[T]</code> <p>The list to unwrap.</p> required <p>Returns:</p> Type Description <code>Union[T, List[T]]</code> <p>The unwrapped list.</p> Source code in <code>inference_sdk/http/utils/iterables.py</code> <pre><code>def unwrap_single_element_list(sequence: List[T]) -&gt; Union[T, List[T]]:\n    \"\"\"Unwrap a single element list.\n\n    Args:\n        sequence: The list to unwrap.\n\n    Returns:\n        The unwrapped list.\n    \"\"\"\n    if len(sequence) == 1:\n        return sequence[0]\n    return sequence\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/","title":"Loaders","text":""},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_directory_inference_input","title":"<code>load_directory_inference_input(directory_path, image_extensions)</code>","text":"<p>Load an inference input from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>The path to the directory.</p> required <code>image_extensions</code> <code>Optional[List[str]]</code> <p>The extensions of the images.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The generator of the inference input.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def load_directory_inference_input(\n    directory_path: str,\n    image_extensions: Optional[List[str]],\n) -&gt; Generator[Tuple[Union[str, int], np.ndarray], None, None]:\n    \"\"\"Load an inference input from a directory.\n\n    Args:\n        directory_path: The path to the directory.\n        image_extensions: The extensions of the images.\n\n    Returns:\n        The generator of the inference input.\n    \"\"\"\n    paths = {\n        path.as_posix().lower()\n        for path in sv.list_files_with_extensions(\n            directory=directory_path,\n            extensions=image_extensions,\n        )\n    }\n    # making a set due to case-insensitive behaviour of Windows\n    # see: https://stackoverflow.com/questions/7199039/file-paths-in-windows-environment-not-case-sensitive\n    for path in paths:\n        yield path, cv2.imread(path)\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_image_from_string","title":"<code>load_image_from_string(reference, max_height=None, max_width=None)</code>","text":"<p>Load an image from a string.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>str</code> <p>The reference to the image.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, Optional[float]]</code> <p>The image and the scaling factor.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def load_image_from_string(\n    reference: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; Tuple[str, Optional[float]]:\n    \"\"\"Load an image from a string.\n\n    Args:\n        reference: The reference to the image.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The image and the scaling factor.\n    \"\"\"\n    if uri_is_http_link(uri=reference):\n        return load_image_from_url(\n            url=reference, max_height=max_height, max_width=max_width\n        )\n    if os.path.exists(reference):\n        if max_height is None or max_width is None:\n            with open(reference, \"rb\") as f:\n                img_bytes = f.read()\n            img_base64_str = encode_base_64(payload=img_bytes)\n            return img_base64_str, None\n        local_image = cv2.imread(reference)\n        if local_image is None:\n            raise EncodingError(f\"Could not load image from {reference}\")\n        local_image, scaling_factor = resize_opencv_image(\n            image=local_image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=local_image), scaling_factor\n    if max_height is not None and max_width is not None:\n        image_bytes = base64.b64decode(reference)\n        image = bytes_to_opencv_image(payload=image_bytes)\n        image, scaling_factor = resize_opencv_image(\n            image=image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=image), scaling_factor\n    return reference, None\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_image_from_string_async","title":"<code>load_image_from_string_async(reference, max_height=None, max_width=None)</code>  <code>async</code>","text":"<p>Load an image from a string asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>str</code> <p>The reference to the image.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, Optional[float]]</code> <p>The image and the scaling factor.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>async def load_image_from_string_async(\n    reference: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; Tuple[str, Optional[float]]:\n    \"\"\"Load an image from a string asynchronously.\n\n    Args:\n        reference: The reference to the image.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The image and the scaling factor.\n    \"\"\"\n    if uri_is_http_link(uri=reference):\n        return await load_image_from_url_async(\n            url=reference, max_height=max_height, max_width=max_width\n        )\n    if os.path.exists(reference):\n        local_image = cv2.imread(reference)\n        if local_image is None:\n            raise EncodingError(f\"Could not load image from {reference}\")\n        local_image, scaling_factor = resize_opencv_image(\n            image=local_image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=local_image), scaling_factor\n    if max_height is not None and max_width is not None:\n        image_bytes = base64.b64decode(reference)\n        image = bytes_to_opencv_image(payload=image_bytes)\n        image, scaling_factor = resize_opencv_image(\n            image=image,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return numpy_array_to_base64_jpeg(image=image), scaling_factor\n    return reference, None\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_image_from_url","title":"<code>load_image_from_url(url, max_height=None, max_width=None)</code>","text":"<p>Load an image from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, Optional[float]]</code> <p>The image and the scaling factor.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def load_image_from_url(\n    url: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; Tuple[str, Optional[float]]:\n    \"\"\"Load an image from a URL.\n\n    Args:\n        url: The URL of the image.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The image and the scaling factor.\n    \"\"\"\n    response = requests.get(url)\n    response.raise_for_status()\n    if max_height is None or max_width is None:\n        return encode_base_64(response.content), None\n    image = bytes_to_opencv_image(payload=response.content)\n    resized_image, scaling_factor = resize_opencv_image(\n        image=image,\n        max_height=max_height,\n        max_width=max_width,\n    )\n    serialised_image = numpy_array_to_base64_jpeg(image=resized_image)\n    return serialised_image, scaling_factor\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_image_from_url_async","title":"<code>load_image_from_url_async(url, max_height=None, max_width=None)</code>  <code>async</code>","text":"<p>Load an image from a URL asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, Optional[float]]</code> <p>The image and the scaling factor.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>async def load_image_from_url_async(\n    url: str,\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; Tuple[str, Optional[float]]:\n    \"\"\"Load an image from a URL asynchronously.\n\n    Args:\n        url: The URL of the image.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The image and the scaling factor.\n    \"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            response.raise_for_status()\n            response_payload = await response.read()\n    if max_height is None or max_width is None:\n        return encode_base_64(response_payload), None\n    image = bytes_to_opencv_image(payload=response_payload)\n    resized_image, scaling_factor = resize_opencv_image(\n        image=image,\n        max_height=max_height,\n        max_width=max_width,\n    )\n    serialised_image = numpy_array_to_base64_jpeg(image=resized_image)\n    return serialised_image, scaling_factor\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_nested_batches_of_inference_input","title":"<code>load_nested_batches_of_inference_input(inference_input, max_height=None, max_width=None)</code>","text":"<p>Load a nested batch of inference input.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[list, ImagesReference]</code> <p>The inference input.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[str, Optional[float]], list]</code> <p>The nested batch of inference input.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def load_nested_batches_of_inference_input(\n    inference_input: Union[list, ImagesReference],\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; Union[Tuple[str, Optional[float]], list]:\n    \"\"\"Load a nested batch of inference input.\n\n    Args:\n        inference_input: The inference input.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The nested batch of inference input.\n    \"\"\"\n    if not isinstance(inference_input, list):\n        return load_static_inference_input(\n            inference_input=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )[0]\n    result = []\n    for element in inference_input:\n        result.append(\n            load_nested_batches_of_inference_input(\n                inference_input=element,\n                max_height=max_height,\n                max_width=max_width,\n            )\n        )\n    return result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_static_inference_input","title":"<code>load_static_inference_input(inference_input, max_height=None, max_width=None)</code>","text":"<p>Load a static inference input.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>The inference input.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[str, Optional[float]]]</code> <p>The list of the inference input.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def load_static_inference_input(\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; List[Tuple[str, Optional[float]]]:\n    \"\"\"Load a static inference input.\n\n    Args:\n        inference_input: The inference input.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The list of the inference input.\n    \"\"\"\n    if issubclass(type(inference_input), list):\n        results = []\n        for element in inference_input:\n            results.extend(\n                load_static_inference_input(\n                    inference_input=element,\n                    max_height=max_height,\n                    max_width=max_width,\n                )\n            )\n        return results\n    if issubclass(type(inference_input), str):\n        return [\n            load_image_from_string(\n                reference=inference_input, max_height=max_height, max_width=max_width\n            )\n        ]\n    if issubclass(type(inference_input), np.ndarray):\n        image, scaling_factor = resize_opencv_image(\n            image=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return [(numpy_array_to_base64_jpeg(image=image), scaling_factor)]\n    if issubclass(type(inference_input), Image.Image):\n        image, scaling_factor = resize_pillow_image(\n            image=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return [(pillow_image_to_base64_jpeg(image=image), scaling_factor)]\n    raise InvalidInputFormatError(\n        f\"Unknown type of input ({inference_input.__class__.__name__}) submitted.\"\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_static_inference_input_async","title":"<code>load_static_inference_input_async(inference_input, max_height=None, max_width=None)</code>  <code>async</code>","text":"<p>Load a static inference input asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>inference_input</code> <code>Union[ImagesReference, List[ImagesReference]]</code> <p>The inference input.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> <code>None</code> <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[str, Optional[float]]]</code> <p>The list of the inference input.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>async def load_static_inference_input_async(\n    inference_input: Union[ImagesReference, List[ImagesReference]],\n    max_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n) -&gt; List[Tuple[str, Optional[float]]]:\n    \"\"\"Load a static inference input asynchronously.\n\n    Args:\n        inference_input: The inference input.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The list of the inference input.\n    \"\"\"\n    if issubclass(type(inference_input), list):\n        results = []\n        for element in inference_input:\n            results.extend(\n                await load_static_inference_input_async(\n                    inference_input=element,\n                    max_height=max_height,\n                    max_width=max_width,\n                )\n            )\n        return results\n    if issubclass(type(inference_input), str):\n        return [\n            await load_image_from_string_async(\n                reference=inference_input, max_height=max_height, max_width=max_width\n            )\n        ]\n    if issubclass(type(inference_input), np.ndarray):\n        image, scaling_factor = resize_opencv_image(\n            image=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return [(numpy_array_to_base64_jpeg(image=image), scaling_factor)]\n    if issubclass(type(inference_input), Image.Image):\n        image, scaling_factor = resize_pillow_image(\n            image=inference_input,\n            max_height=max_height,\n            max_width=max_width,\n        )\n        return [(pillow_image_to_base64_jpeg(image=image), scaling_factor)]\n    raise InvalidInputFormatError(\n        f\"Unknown type of input ({inference_input.__class__.__name__}) submitted.\"\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.load_stream_inference_input","title":"<code>load_stream_inference_input(input_uri, image_extensions)</code>","text":"<p>Load an inference input from a stream.</p> <p>Parameters:</p> Name Type Description Default <code>input_uri</code> <code>str</code> <p>The URI of the input.</p> required <code>image_extensions</code> <code>Optional[List[str]]</code> <p>The extensions of the images.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The generator of the inference input.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def load_stream_inference_input(\n    input_uri: str,\n    image_extensions: Optional[List[str]],\n) -&gt; Generator[Tuple[Union[str, int], np.ndarray], None, None]:\n    \"\"\"Load an inference input from a stream.\n\n    Args:\n        input_uri: The URI of the input.\n        image_extensions: The extensions of the images.\n\n    Returns:\n        The generator of the inference input.\n    \"\"\"\n    if os.path.isdir(input_uri):\n        yield from load_directory_inference_input(\n            directory_path=input_uri, image_extensions=image_extensions\n        )\n    else:\n        yield from enumerate(sv.get_video_frames_generator(source_path=input_uri))\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/loaders/#inference_sdk.http.utils.loaders.uri_is_http_link","title":"<code>uri_is_http_link(uri)</code>","text":"<p>Check if the URI is an HTTP link.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the URI is an HTTP link, False otherwise.</p> Source code in <code>inference_sdk/http/utils/loaders.py</code> <pre><code>def uri_is_http_link(uri: str) -&gt; bool:\n    \"\"\"Check if the URI is an HTTP link.\n\n    Args:\n        uri: The URI to check.\n\n    Returns:\n        True if the URI is an HTTP link, False otherwise.\n    \"\"\"\n    return uri.startswith(\"http://\") or uri.startswith(\"https://\")\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/","title":"Post processing","text":""},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.adjust_bbox_coordinates_to_client_scaling_factor","title":"<code>adjust_bbox_coordinates_to_client_scaling_factor(bbox, scaling_factor)</code>","text":"<p>Adjust a bbox coordinates to the client scaling factor.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>dict</code> <p>The bbox to adjust.</p> required <code>scaling_factor</code> <code>float</code> <p>The scaling factor.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The adjusted bbox.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def adjust_bbox_coordinates_to_client_scaling_factor(\n    bbox: dict,\n    scaling_factor: float,\n) -&gt; dict:\n    \"\"\"Adjust a bbox coordinates to the client scaling factor.\n\n    Args:\n        bbox: The bbox to adjust.\n        scaling_factor: The scaling factor.\n\n    Returns:\n        The adjusted bbox.\n    \"\"\"\n    bbox[\"x\"] = bbox[\"x\"] / scaling_factor\n    bbox[\"y\"] = bbox[\"y\"] / scaling_factor\n    bbox[\"width\"] = bbox[\"width\"] / scaling_factor\n    bbox[\"height\"] = bbox[\"height\"] / scaling_factor\n    return bbox\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.adjust_object_detection_predictions_to_client_scaling_factor","title":"<code>adjust_object_detection_predictions_to_client_scaling_factor(predictions, scaling_factor)</code>","text":"<p>Adjust a list of object detection predictions to the client scaling factor.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[dict]</code> <p>The list of object detection predictions.</p> required <code>scaling_factor</code> <code>float</code> <p>The scaling factor.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>The adjusted list of object detection predictions.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def adjust_object_detection_predictions_to_client_scaling_factor(\n    predictions: List[dict],\n    scaling_factor: float,\n) -&gt; List[dict]:\n    \"\"\"Adjust a list of object detection predictions to the client scaling factor.\n\n    Args:\n        predictions: The list of object detection predictions.\n        scaling_factor: The scaling factor.\n\n    Returns:\n        The adjusted list of object detection predictions.\n    \"\"\"\n    result = []\n    for prediction in predictions:\n        prediction = adjust_bbox_coordinates_to_client_scaling_factor(\n            bbox=prediction,\n            scaling_factor=scaling_factor,\n        )\n        result.append(prediction)\n    return result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.adjust_points_coordinates_to_client_scaling_factor","title":"<code>adjust_points_coordinates_to_client_scaling_factor(points, scaling_factor)</code>","text":"<p>Adjust a list of points coordinates to the client scaling factor.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>List[dict]</code> <p>The list of points.</p> required <code>scaling_factor</code> <code>float</code> <p>The scaling factor.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>The adjusted list of points.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def adjust_points_coordinates_to_client_scaling_factor(\n    points: List[dict],\n    scaling_factor: float,\n) -&gt; List[dict]:\n    \"\"\"Adjust a list of points coordinates to the client scaling factor.\n\n    Args:\n        points: The list of points.\n        scaling_factor: The scaling factor.\n\n    Returns:\n        The adjusted list of points.\n    \"\"\"\n    result = []\n    for point in points:\n        point[\"x\"] = point[\"x\"] / scaling_factor\n        point[\"y\"] = point[\"y\"] / scaling_factor\n        result.append(point)\n    return result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.adjust_prediction_to_client_scaling_factor","title":"<code>adjust_prediction_to_client_scaling_factor(prediction, scaling_factor)</code>","text":"<p>Adjust a prediction to the client scaling factor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>dict</code> <p>The prediction to adjust.</p> required <code>scaling_factor</code> <code>Optional[float]</code> <p>The scaling factor.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The adjusted prediction.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def adjust_prediction_to_client_scaling_factor(\n    prediction: dict,\n    scaling_factor: Optional[float],\n) -&gt; dict:\n    \"\"\"Adjust a prediction to the client scaling factor.\n\n    Args:\n        prediction: The prediction to adjust.\n        scaling_factor: The scaling factor.\n\n    Returns:\n        The adjusted prediction.\n    \"\"\"\n    if scaling_factor is None or prediction.get(\"is_stub\", False):\n        return prediction\n    if \"image\" in prediction:\n        prediction[\"image\"] = {\n            \"width\": round(prediction[\"image\"][\"width\"] / scaling_factor),\n            \"height\": round(prediction[\"image\"][\"height\"] / scaling_factor),\n        }\n    if predictions_should_not_be_post_processed(prediction=prediction):\n        return prediction\n    if \"points\" in prediction[\"predictions\"][0]:\n        prediction[\"predictions\"] = (\n            adjust_prediction_with_bbox_and_points_to_client_scaling_factor(\n                predictions=prediction[\"predictions\"],\n                scaling_factor=scaling_factor,\n                points_key=\"points\",\n            )\n        )\n    elif \"keypoints\" in prediction[\"predictions\"][0]:\n        prediction[\"predictions\"] = (\n            adjust_prediction_with_bbox_and_points_to_client_scaling_factor(\n                predictions=prediction[\"predictions\"],\n                scaling_factor=scaling_factor,\n                points_key=\"keypoints\",\n            )\n        )\n    elif \"x\" in prediction[\"predictions\"][0] and \"y\" in prediction[\"predictions\"][0]:\n        prediction[\"predictions\"] = (\n            adjust_object_detection_predictions_to_client_scaling_factor(\n                predictions=prediction[\"predictions\"],\n                scaling_factor=scaling_factor,\n            )\n        )\n    return prediction\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.adjust_prediction_with_bbox_and_points_to_client_scaling_factor","title":"<code>adjust_prediction_with_bbox_and_points_to_client_scaling_factor(predictions, scaling_factor, points_key)</code>","text":"<p>Adjust a list of predictions with bbox and points to the client scaling factor.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[dict]</code> <p>The list of predictions.</p> required <code>scaling_factor</code> <code>float</code> <p>The scaling factor.</p> required <code>points_key</code> <code>str</code> <p>The key of the points.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>The adjusted list of predictions.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def adjust_prediction_with_bbox_and_points_to_client_scaling_factor(\n    predictions: List[dict],\n    scaling_factor: float,\n    points_key: str,\n) -&gt; List[dict]:\n    \"\"\"Adjust a list of predictions with bbox and points to the client scaling factor.\n\n    Args:\n        predictions: The list of predictions.\n        scaling_factor: The scaling factor.\n        points_key: The key of the points.\n\n    Returns:\n        The adjusted list of predictions.\n    \"\"\"\n    result = []\n    for prediction in predictions:\n        prediction = adjust_bbox_coordinates_to_client_scaling_factor(\n            bbox=prediction,\n            scaling_factor=scaling_factor,\n        )\n        prediction[points_key] = adjust_points_coordinates_to_client_scaling_factor(\n            points=prediction[points_key],\n            scaling_factor=scaling_factor,\n        )\n        result.append(prediction)\n    return result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.combine_clip_embeddings","title":"<code>combine_clip_embeddings(embeddings)</code>","text":"<p>Combine clip embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Union[dict, List[dict]]</code> <p>The embeddings to combine.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>The combined embeddings.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def combine_clip_embeddings(embeddings: Union[dict, List[dict]]) -&gt; List[dict]:\n    \"\"\"Combine clip embeddings.\n\n    Args:\n        embeddings: The embeddings to combine.\n\n    Returns:\n        The combined embeddings.\n    \"\"\"\n    if issubclass(type(embeddings), list):\n        result = []\n        for e in embeddings:\n            result.extend(combine_clip_embeddings(embeddings=e))\n        return result\n    frame_id = embeddings[\"frame_id\"]\n    time = embeddings[\"time\"]\n    if len(embeddings[\"embeddings\"]) &gt; 1:\n        new_embeddings = [\n            {\"frame_id\": frame_id, \"time\": time, \"embeddings\": [e]}\n            for e in embeddings[\"embeddings\"]\n        ]\n    else:\n        new_embeddings = [embeddings]\n    return new_embeddings\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.combine_gaze_detections","title":"<code>combine_gaze_detections(detections)</code>","text":"<p>Combine gaze detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Union[dict, List[Union[dict, List[dict]]]]</code> <p>The detections to combine.</p> required <p>Returns:</p> Type Description <code>Union[dict, List[Dict]]</code> <p>The combined detections.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def combine_gaze_detections(\n    detections: Union[dict, List[Union[dict, List[dict]]]],\n) -&gt; Union[dict, List[Dict]]:\n    \"\"\"Combine gaze detections.\n\n    Args:\n        detections: The detections to combine.\n\n    Returns:\n        The combined detections.\n    \"\"\"\n    if not issubclass(type(detections), list):\n        return detections\n    detections = [e if issubclass(type(e), list) else [e] for e in detections]\n    return list(itertools.chain.from_iterable(detections))\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.decode_workflow_output","title":"<code>decode_workflow_output(workflow_output, expected_format)</code>","text":"<p>Decode a workflow output.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_output</code> <code>Dict[str, Any]</code> <p>The workflow output to decode.</p> required <code>expected_format</code> <code>VisualisationResponseFormat</code> <p>The expected format of the workflow output.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The decoded workflow output.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def decode_workflow_output(\n    workflow_output: Dict[str, Any],\n    expected_format: VisualisationResponseFormat,\n) -&gt; Dict[str, Any]:\n    \"\"\"Decode a workflow output.\n\n    Args:\n        workflow_output: The workflow output to decode.\n        expected_format: The expected format of the workflow output.\n\n    Returns:\n        The decoded workflow output.\n    \"\"\"\n    result = {}\n    for key, value in workflow_output.items():\n        if is_workflow_image(value=value):\n            value = decode_workflow_output_image(\n                value=value,\n                expected_format=expected_format,\n            )\n        elif issubclass(type(value), dict):\n            value = decode_workflow_output(\n                workflow_output=value, expected_format=expected_format\n            )\n        elif issubclass(type(value), list):\n            value = decode_workflow_output_list(\n                elements=value,\n                expected_format=expected_format,\n            )\n        result[key] = value\n    return result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.decode_workflow_output_image","title":"<code>decode_workflow_output_image(value, expected_format)</code>","text":"<p>Decode a workflow output image.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Dict[str, Any]</code> <p>The value to decode.</p> required <code>expected_format</code> <code>VisualisationResponseFormat</code> <p>The expected format of the value.</p> required <p>Returns:</p> Type Description <code>Union[str, ndarray, Image]</code> <p>The decoded value.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def decode_workflow_output_image(\n    value: Dict[str, Any],\n    expected_format: VisualisationResponseFormat,\n) -&gt; Union[str, np.ndarray, Image.Image]:\n    \"\"\"Decode a workflow output image.\n\n    Args:\n        value: The value to decode.\n        expected_format: The expected format of the value.\n\n    Returns:\n        The decoded value.\n    \"\"\"\n    if expected_format is VisualisationResponseFormat.BASE64:\n        return value[\"value\"]\n    return transform_base64_visualisation(\n        visualisation=value[\"value\"],\n        expected_format=expected_format,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.decode_workflow_output_list","title":"<code>decode_workflow_output_list(elements, expected_format)</code>","text":"<p>Decode a list of workflow outputs.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>List[Any]</code> <p>The list of elements to decode.</p> required <code>expected_format</code> <code>VisualisationResponseFormat</code> <p>The expected format of the elements.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>The decoded list of elements.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def decode_workflow_output_list(\n    elements: List[Any],\n    expected_format: VisualisationResponseFormat,\n) -&gt; List[Any]:\n    \"\"\"Decode a list of workflow outputs.\n\n    Args:\n        elements: The list of elements to decode.\n        expected_format: The expected format of the elements.\n\n    Returns:\n        The decoded list of elements.\n    \"\"\"\n    result = []\n    for element in elements:\n        if is_workflow_image(value=element):\n            element = decode_workflow_output_image(\n                value=element,\n                expected_format=expected_format,\n            )\n        elif issubclass(type(element), dict):\n            element = decode_workflow_output(\n                workflow_output=element, expected_format=expected_format\n            )\n        elif issubclass(type(element), list):\n            element = decode_workflow_output_list(\n                elements=element,\n                expected_format=expected_format,\n            )\n        result.append(element)\n    return result\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.decode_workflow_outputs","title":"<code>decode_workflow_outputs(workflow_outputs, expected_format)</code>","text":"<p>Decode a list of workflow outputs.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_outputs</code> <code>List[Dict[str, Any]]</code> <p>The list of workflow outputs.</p> required <code>expected_format</code> <code>VisualisationResponseFormat</code> <p>The expected format of the workflow outputs.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>The decoded list of workflow outputs.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def decode_workflow_outputs(\n    workflow_outputs: List[Dict[str, Any]],\n    expected_format: VisualisationResponseFormat,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Decode a list of workflow outputs.\n\n    Args:\n        workflow_outputs: The list of workflow outputs.\n        expected_format: The expected format of the workflow outputs.\n\n    Returns:\n        The decoded list of workflow outputs.\n    \"\"\"\n    return [\n        decode_workflow_output(\n            workflow_output=workflow_output,\n            expected_format=expected_format,\n        )\n        for workflow_output in workflow_outputs\n    ]\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.filter_model_descriptions","title":"<code>filter_model_descriptions(descriptions, model_id)</code>","text":"<p>Filter model descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>descriptions</code> <code>List[ModelDescription]</code> <p>The list of model descriptions.</p> required <code>model_id</code> <code>str</code> <p>The model ID.</p> required <p>Returns:</p> Type Description <code>Optional[ModelDescription]</code> <p>The filtered model description.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def filter_model_descriptions(\n    descriptions: List[ModelDescription],\n    model_id: str,\n) -&gt; Optional[ModelDescription]:\n    \"\"\"Filter model descriptions.\n\n    Args:\n        descriptions: The list of model descriptions.\n        model_id: The model ID.\n\n    Returns:\n        The filtered model description.\n    \"\"\"\n    matching_models = [d for d in descriptions if d.model_id == model_id]\n    if len(matching_models) &gt; 0:\n        return matching_models[0]\n    return None\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.is_workflow_image","title":"<code>is_workflow_image(value)</code>","text":"<p>Check if the value is a workflow image.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the value is a workflow image, False otherwise.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def is_workflow_image(value: Any) -&gt; bool:\n    \"\"\"Check if the value is a workflow image.\n\n    Args:\n        value: The value to check.\n\n    Returns:\n        True if the value is a workflow image, False otherwise.\n    \"\"\"\n    return issubclass(type(value), dict) and value.get(\"type\") == \"base64\"\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.predictions_should_not_be_post_processed","title":"<code>predictions_should_not_be_post_processed(prediction)</code>","text":"<p>Check if the predictions should not be post-processed.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>dict</code> <p>The prediction to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the predictions should not be post-processed, False otherwise.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def predictions_should_not_be_post_processed(prediction: dict) -&gt; bool:\n    \"\"\"Check if the predictions should not be post-processed.\n\n    Args:\n        prediction: The prediction to check.\n\n    Returns:\n        True if the predictions should not be post-processed, False otherwise.\n    \"\"\"\n    return (\n        \"predictions\" not in prediction\n        or not issubclass(type(prediction[\"predictions\"]), list)\n        or len(prediction[\"predictions\"]) == 0\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.response_contains_jpeg_image","title":"<code>response_contains_jpeg_image(response)</code>","text":"<p>Check if the response contains a JPEG image.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>The response to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the response contains a JPEG image, False otherwise.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def response_contains_jpeg_image(response: Response) -&gt; bool:\n    \"\"\"Check if the response contains a JPEG image.\n\n    Args:\n        response: The response to check.\n\n    Returns:\n        True if the response contains a JPEG image, False otherwise.\n    \"\"\"\n    content_type = None\n    for header_name in CONTENT_TYPE_HEADERS:\n        if header_name in response.headers:\n            content_type = response.headers[header_name]\n            break\n    if content_type is None:\n        return False\n    return \"image/jpeg\" in content_type\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.transform_base64_visualisation","title":"<code>transform_base64_visualisation(visualisation, expected_format)</code>","text":"<p>Transform a base64 visualisation.</p> <p>Parameters:</p> Name Type Description Default <code>visualisation</code> <code>str</code> <p>The visualisation to transform.</p> required <code>expected_format</code> <code>VisualisationResponseFormat</code> <p>The expected format of the visualisation.</p> required <p>Returns:</p> Type Description <code>Union[str, ndarray, Image]</code> <p>The transformed visualisation.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def transform_base64_visualisation(\n    visualisation: str,\n    expected_format: VisualisationResponseFormat,\n) -&gt; Union[str, np.ndarray, Image.Image]:\n    \"\"\"Transform a base64 visualisation.\n\n    Args:\n        visualisation: The visualisation to transform.\n        expected_format: The expected format of the visualisation.\n\n    Returns:\n        The transformed visualisation.\n    \"\"\"\n    visualisation_bytes = base64.b64decode(visualisation)\n    return transform_visualisation_bytes(\n        visualisation=visualisation_bytes, expected_format=expected_format\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/post_processing/#inference_sdk.http.utils.post_processing.transform_visualisation_bytes","title":"<code>transform_visualisation_bytes(visualisation, expected_format)</code>","text":"<p>Transform a visualisation bytes.</p> <p>Parameters:</p> Name Type Description Default <code>visualisation</code> <code>bytes</code> <p>The visualisation to transform.</p> required <code>expected_format</code> <code>VisualisationResponseFormat</code> <p>The expected format of the visualisation.</p> required <p>Returns:</p> Type Description <code>Union[str, ndarray, Image]</code> <p>The transformed visualisation.</p> Source code in <code>inference_sdk/http/utils/post_processing.py</code> <pre><code>def transform_visualisation_bytes(\n    visualisation: bytes,\n    expected_format: VisualisationResponseFormat,\n) -&gt; Union[str, np.ndarray, Image.Image]:\n    \"\"\"Transform a visualisation bytes.\n\n    Args:\n        visualisation: The visualisation to transform.\n        expected_format: The expected format of the visualisation.\n\n    Returns:\n        The transformed visualisation.\n    \"\"\"\n    if expected_format not in IMAGES_TRANSCODING_METHODS:\n        raise NotImplementedError(\n            f\"Expected format: {expected_format} is not supported in terms of visualisations transcoding.\"\n        )\n    transcoding_method = IMAGES_TRANSCODING_METHODS[expected_format]\n    return transcoding_method(visualisation)\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/pre_processing/","title":"Pre processing","text":""},{"location":"reference/inference_sdk/http/utils/pre_processing/#inference_sdk.http.utils.pre_processing.determine_scaling_aspect_ratio","title":"<code>determine_scaling_aspect_ratio(image_height, image_width, max_height, max_width)</code>","text":"<p>Determine the scaling aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>image_height</code> <code>int</code> <p>The height of the image.</p> required <code>image_width</code> <code>int</code> <p>The width of the image.</p> required <code>max_height</code> <code>int</code> <p>The maximum height of the image.</p> required <code>max_width</code> <code>int</code> <p>The maximum width of the image.</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The scaling aspect ratio.</p> Source code in <code>inference_sdk/http/utils/pre_processing.py</code> <pre><code>def determine_scaling_aspect_ratio(\n    image_height: int,\n    image_width: int,\n    max_height: int,\n    max_width: int,\n) -&gt; Optional[float]:\n    \"\"\"Determine the scaling aspect ratio.\n\n    Args:\n        image_height: The height of the image.\n        image_width: The width of the image.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The scaling aspect ratio.\n    \"\"\"\n    height_scaling_ratio = max_height / image_height\n    width_scaling_ratio = max_width / image_width\n    min_scaling_ratio = min(height_scaling_ratio, width_scaling_ratio)\n    return min_scaling_ratio if min_scaling_ratio &lt; 1.0 else None\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/pre_processing/#inference_sdk.http.utils.pre_processing.resize_opencv_image","title":"<code>resize_opencv_image(image, max_height, max_width)</code>","text":"<p>Resize an OpenCV image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image to resize.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> required <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, Optional[float]]</code> <p>The resized image and the scaling factor.</p> Source code in <code>inference_sdk/http/utils/pre_processing.py</code> <pre><code>def resize_opencv_image(\n    image: np.ndarray,\n    max_height: Optional[int],\n    max_width: Optional[int],\n) -&gt; Tuple[np.ndarray, Optional[float]]:\n    \"\"\"Resize an OpenCV image.\n\n    Args:\n        image: The image to resize.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The resized image and the scaling factor.\n    \"\"\"\n    if max_width is None or max_height is None:\n        return image, None\n    height, width = image.shape[:2]\n    scaling_ratio = determine_scaling_aspect_ratio(\n        image_height=height,\n        image_width=width,\n        max_height=max_height,\n        max_width=max_width,\n    )\n    if scaling_ratio is None:\n        return image, None\n    resized_image = cv2.resize(\n        src=image, dsize=None, fx=scaling_ratio, fy=scaling_ratio\n    )\n    return resized_image, scaling_ratio\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/pre_processing/#inference_sdk.http.utils.pre_processing.resize_pillow_image","title":"<code>resize_pillow_image(image, max_height, max_width)</code>","text":"<p>Resize a Pillow image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to resize.</p> required <code>max_height</code> <code>Optional[int]</code> <p>The maximum height of the image.</p> required <code>max_width</code> <code>Optional[int]</code> <p>The maximum width of the image.</p> required <p>Returns:</p> Type Description <code>Tuple[Image, Optional[float]]</code> <p>The resized image and the scaling factor.</p> Source code in <code>inference_sdk/http/utils/pre_processing.py</code> <pre><code>def resize_pillow_image(\n    image: Image.Image,\n    max_height: Optional[int],\n    max_width: Optional[int],\n) -&gt; Tuple[Image.Image, Optional[float]]:\n    \"\"\"Resize a Pillow image.\n\n    Args:\n        image: The image to resize.\n        max_height: The maximum height of the image.\n        max_width: The maximum width of the image.\n\n    Returns:\n        The resized image and the scaling factor.\n    \"\"\"\n    if max_width is None or max_height is None:\n        return image, None\n    width, height = image.size\n    scaling_ratio = determine_scaling_aspect_ratio(\n        image_height=height,\n        image_width=width,\n        max_height=max_height,\n        max_width=max_width,\n    )\n    if scaling_ratio is None:\n        return image, None\n    new_width = round(scaling_ratio * width)\n    new_height = round(scaling_ratio * height)\n    return image.resize(size=(new_width, new_height)), scaling_ratio\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/profilling/","title":"Profilling","text":""},{"location":"reference/inference_sdk/http/utils/profilling/#inference_sdk.http.utils.profilling.save_workflows_profiler_trace","title":"<code>save_workflows_profiler_trace(directory, profiler_trace)</code>","text":"<p>Save a workflow profiler trace.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to save the profiler trace.</p> required <code>profiler_trace</code> <code>List[dict]</code> <p>The profiler trace.</p> required Source code in <code>inference_sdk/http/utils/profilling.py</code> <pre><code>def save_workflows_profiler_trace(\n    directory: str,\n    profiler_trace: List[dict],\n) -&gt; None:\n    \"\"\"Save a workflow profiler trace.\n\n    Args:\n        directory: The directory to save the profiler trace.\n        profiler_trace: The profiler trace.\n    \"\"\"\n    directory = os.path.abspath(directory)\n    os.makedirs(directory, exist_ok=True)\n    formatted_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n    track_path = os.path.join(\n        directory, f\"workflow_execution_tack_{formatted_time}.json\"\n    )\n    with open(track_path, \"w\") as f:\n        json.dump(profiler_trace, f)\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/request_building/","title":"Request building","text":""},{"location":"reference/inference_sdk/http/utils/request_building/#inference_sdk.http.utils.request_building.RequestData","title":"<code>RequestData</code>  <code>dataclass</code>","text":"<p>Data class for request data.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL of the request.</p> <code>request_elements</code> <code>int</code> <p>The number of request elements.</p> <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The headers of the request.</p> <code>parameters</code> <code>Optional[Dict[str, Union[str, List[str]]]]</code> <p>The parameters of the request.</p> <code>data</code> <code>Optional[Union[str, bytes]]</code> <p>The data of the request.</p> <code>payload</code> <code>Optional[Dict[str, Any]]</code> <p>The payload of the request.</p> <code>image_scaling_factors</code> <code>List[Optional[float]]</code> <p>The scaling factors of the images.</p> Source code in <code>inference_sdk/http/utils/request_building.py</code> <pre><code>@dataclass(frozen=True)\nclass RequestData:\n    \"\"\"Data class for request data.\n\n    Attributes:\n        url: The URL of the request.\n        request_elements: The number of request elements.\n        headers: The headers of the request.\n        parameters: The parameters of the request.\n        data: The data of the request.\n        payload: The payload of the request.\n        image_scaling_factors: The scaling factors of the images.\n    \"\"\"\n\n    url: str\n    request_elements: int\n    headers: Optional[Dict[str, str]]\n    parameters: Optional[Dict[str, Union[str, List[str]]]]\n    data: Optional[Union[str, bytes]]\n    payload: Optional[Dict[str, Any]]\n    image_scaling_factors: List[Optional[float]]\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/request_building/#inference_sdk.http.utils.request_building.assembly_request_data","title":"<code>assembly_request_data(url, batch_inference_inputs, headers, parameters, payload, image_placement)</code>","text":"<p>Assemble request data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the request.</p> required <code>batch_inference_inputs</code> <code>List[Tuple[str, Optional[float]]]</code> <p>The batch inference inputs.</p> required <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The headers of the request.</p> required <code>parameters</code> <code>Optional[Dict[str, Union[str, List[str]]]]</code> <p>The parameters of the request.</p> required <code>payload</code> <code>Optional[Dict[str, Any]]</code> <p>The payload of the request.</p> required <code>image_placement</code> <code>ImagePlacement</code> <p>The image placement.</p> required <p>Returns:</p> Type Description <code>RequestData</code> <p>The request data.</p> Source code in <code>inference_sdk/http/utils/request_building.py</code> <pre><code>def assembly_request_data(\n    url: str,\n    batch_inference_inputs: List[Tuple[str, Optional[float]]],\n    headers: Optional[Dict[str, str]],\n    parameters: Optional[Dict[str, Union[str, List[str]]]],\n    payload: Optional[Dict[str, Any]],\n    image_placement: ImagePlacement,\n) -&gt; RequestData:\n    \"\"\"Assemble request data.\n\n    Args:\n        url: The URL of the request.\n        batch_inference_inputs: The batch inference inputs.\n        headers: The headers of the request.\n        parameters: The parameters of the request.\n        payload: The payload of the request.\n        image_placement: The image placement.\n\n    Returns:\n        The request data.\n    \"\"\"\n    data = None\n    if image_placement is ImagePlacement.DATA and len(batch_inference_inputs) != 1:\n        raise ValueError(\"Only single image can be placed in request `data`\")\n    if image_placement is ImagePlacement.JSON and payload is None:\n        payload = {}\n    if image_placement is ImagePlacement.JSON:\n        payload = deepcopy(payload)\n        payload = inject_images_into_payload(\n            payload=payload,\n            encoded_images=batch_inference_inputs,\n        )\n    elif image_placement is ImagePlacement.DATA:\n        data = batch_inference_inputs[0][0]\n    else:\n        raise NotImplemented(\n            f\"Not implemented request building method for {image_placement}\"\n        )\n    scaling_factors = [e[1] for e in batch_inference_inputs]\n\n    execution_id_value = execution_id.get()\n    if execution_id_value:\n        headers = headers.copy()\n        headers[EXECUTION_ID_HEADER] = execution_id_value\n\n    return RequestData(\n        url=url,\n        request_elements=len(batch_inference_inputs),\n        headers=headers,\n        parameters=parameters,\n        data=data,\n        payload=payload,\n        image_scaling_factors=scaling_factors,\n    )\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/request_building/#inference_sdk.http.utils.request_building.prepare_requests_data","title":"<code>prepare_requests_data(url, encoded_inference_inputs, headers, parameters, payload, max_batch_size, image_placement)</code>","text":"<p>Prepare requests data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the request.</p> required <code>encoded_inference_inputs</code> <code>List[Tuple[str, Optional[float]]]</code> <p>The encoded inference inputs.</p> required <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>The headers of the request.</p> required <code>parameters</code> <code>Optional[Dict[str, Union[str, List[str]]]]</code> <p>The parameters of the request.</p> required <code>payload</code> <code>Optional[Dict[str, Any]]</code> <p>The payload of the request.</p> required <code>max_batch_size</code> <code>int</code> <p>The maximum batch size.</p> required <code>image_placement</code> <code>ImagePlacement</code> <p>The image placement.</p> required <p>Returns:</p> Type Description <code>List[RequestData]</code> <p>The list of request data.</p> Source code in <code>inference_sdk/http/utils/request_building.py</code> <pre><code>def prepare_requests_data(\n    url: str,\n    encoded_inference_inputs: List[Tuple[str, Optional[float]]],\n    headers: Optional[Dict[str, str]],\n    parameters: Optional[Dict[str, Union[str, List[str]]]],\n    payload: Optional[Dict[str, Any]],\n    max_batch_size: int,\n    image_placement: ImagePlacement,\n) -&gt; List[RequestData]:\n    \"\"\"Prepare requests data.\n\n    Args:\n        url: The URL of the request.\n        encoded_inference_inputs: The encoded inference inputs.\n        headers: The headers of the request.\n        parameters: The parameters of the request.\n        payload: The payload of the request.\n        max_batch_size: The maximum batch size.\n        image_placement: The image placement.\n\n    Returns:\n        The list of request data.\n    \"\"\"\n    batches = list(\n        make_batches(\n            iterable=encoded_inference_inputs,\n            batch_size=max_batch_size,\n        )\n    )\n    requests_data = []\n    for batch_inference_inputs in batches:\n        request_data = assembly_request_data(\n            url=url,\n            batch_inference_inputs=batch_inference_inputs,\n            headers=headers,\n            parameters=parameters,\n            payload=payload,\n            image_placement=image_placement,\n        )\n        requests_data.append(request_data)\n    return requests_data\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/requests/","title":"Requests","text":""},{"location":"reference/inference_sdk/http/utils/requests/#inference_sdk.http.utils.requests.api_key_safe_raise_for_status","title":"<code>api_key_safe_raise_for_status(response)</code>","text":"<p>Raise an exception if the request is not successful.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>The response of the request.</p> required Source code in <code>inference_sdk/http/utils/requests.py</code> <pre><code>def api_key_safe_raise_for_status(response: Response) -&gt; None:\n    \"\"\"Raise an exception if the request is not successful.\n\n    Args:\n        response: The response of the request.\n    \"\"\"\n    request_is_successful = response.status_code &lt; 400\n    if request_is_successful:\n        return None\n    response.url = deduct_api_key_from_string(value=response.url)\n    response.raise_for_status()\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/requests/#inference_sdk.http.utils.requests.deduct_api_key","title":"<code>deduct_api_key(match)</code>","text":"<p>Deduct the API key from the string.</p> <p>Parameters:</p> Name Type Description Default <code>match</code> <code>Match</code> <p>The match of the API key.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string with the API key deducted.</p> Source code in <code>inference_sdk/http/utils/requests.py</code> <pre><code>def deduct_api_key(match: re.Match) -&gt; str:\n    \"\"\"Deduct the API key from the string.\n\n    Args:\n        match: The match of the API key.\n\n    Returns:\n        The string with the API key deducted.\n    \"\"\"\n    key_value = match.group(KEY_VALUE_GROUP)\n    if len(key_value) &lt; MIN_KEY_LENGTH_TO_REVEAL_PREFIX:\n        return f\"api_key=***\"\n    key_prefix = key_value[:2]\n    key_postfix = key_value[-2:]\n    return f\"api_key={key_prefix}***{key_postfix}\"\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/requests/#inference_sdk.http.utils.requests.deduct_api_key_from_string","title":"<code>deduct_api_key_from_string(value)</code>","text":"<p>Deduct the API key from the string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string to deduct the API key from.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string with the API key deducted.</p> Source code in <code>inference_sdk/http/utils/requests.py</code> <pre><code>def deduct_api_key_from_string(value: str) -&gt; str:\n    \"\"\"Deduct the API key from the string.\n\n    Args:\n        value: The string to deduct the API key from.\n\n    Returns:\n        The string with the API key deducted.\n    \"\"\"\n    return API_KEY_PATTERN.sub(deduct_api_key, value)\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/requests/#inference_sdk.http.utils.requests.inject_images_into_payload","title":"<code>inject_images_into_payload(payload, encoded_images, key='image')</code>","text":"<p>Inject images into the payload.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict</code> <p>The payload to inject the images into.</p> required <code>encoded_images</code> <code>List[Tuple[str, Optional[float]]]</code> <p>The encoded images.</p> required <code>key</code> <code>str</code> <p>The key of the images.</p> <code>'image'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The payload with the images injected.</p> Source code in <code>inference_sdk/http/utils/requests.py</code> <pre><code>def inject_images_into_payload(\n    payload: dict,\n    encoded_images: List[Tuple[str, Optional[float]]],\n    key: str = \"image\",\n) -&gt; dict:\n    \"\"\"Inject images into the payload.\n\n    Args:\n        payload: The payload to inject the images into.\n        encoded_images: The encoded images.\n        key: The key of the images.\n\n    Returns:\n        The payload with the images injected.\n    \"\"\"\n    if len(encoded_images) == 0:\n        return payload\n    if len(encoded_images) &gt; 1:\n        images_payload = [\n            {\"type\": \"base64\", \"value\": image} for image, _ in encoded_images\n        ]\n        payload[key] = images_payload\n    else:\n        payload[key] = {\"type\": \"base64\", \"value\": encoded_images[0][0]}\n    return payload\n</code></pre>"},{"location":"reference/inference_sdk/http/utils/requests/#inference_sdk.http.utils.requests.inject_nested_batches_of_images_into_payload","title":"<code>inject_nested_batches_of_images_into_payload(payload, encoded_images, key='image')</code>","text":"<p>Inject nested batches of images into the payload.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict</code> <p>The payload to inject the images into.</p> required <code>encoded_images</code> <code>Union[list, Tuple[str, Optional[float]]]</code> <p>The encoded images.</p> required <code>key</code> <code>str</code> <p>The key of the images.</p> <code>'image'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The payload with the images injected.</p> Source code in <code>inference_sdk/http/utils/requests.py</code> <pre><code>def inject_nested_batches_of_images_into_payload(\n    payload: dict,\n    encoded_images: Union[list, Tuple[str, Optional[float]]],\n    key: str = \"image\",\n) -&gt; dict:\n    \"\"\"Inject nested batches of images into the payload.\n\n    Args:\n        payload: The payload to inject the images into.\n        encoded_images: The encoded images.\n        key: The key of the images.\n\n    Returns:\n        The payload with the images injected.\n    \"\"\"\n    payload_value = _batch_of_images_into_inference_format(\n        encoded_images=encoded_images,\n    )\n    payload[key] = payload_value\n    return payload\n</code></pre>"},{"location":"reference/inference_sdk/utils/decorators/","title":"Decorators","text":""},{"location":"reference/inference_sdk/utils/decorators/#inference_sdk.utils.decorators.deprecated","title":"<code>deprecated(reason)</code>","text":"<p>Create a decorator that marks functions as deprecated.</p> <p>This decorator will emit a warning when the decorated function is called, indicating that the function is deprecated and providing a reason.</p> <p>Parameters:</p> Name Type Description Default <code>reason</code> <code>str</code> <p>The reason why the function is deprecated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>A decorator function that can be applied to mark functions as deprecated.</p> Source code in <code>inference_sdk/utils/decorators.py</code> <pre><code>def deprecated(reason: str):\n    \"\"\"Create a decorator that marks functions as deprecated.\n\n    This decorator will emit a warning when the decorated function is called,\n    indicating that the function is deprecated and providing a reason.\n\n    Args:\n        reason (str): The reason why the function is deprecated.\n\n    Returns:\n        callable: A decorator function that can be applied to mark functions as deprecated.\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {reason}\",\n                category=InferenceSDKDeprecationWarning,\n                stacklevel=2,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/inference_sdk/utils/decorators/#inference_sdk.utils.decorators.experimental","title":"<code>experimental(info)</code>","text":"<p>Create a decorator that marks functions as experimental.</p> <p>This decorator will emit a warning when the decorated function is called, indicating that the function is experimental and providing additional information.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>str</code> <p>Information about the experimental status of the function.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>A decorator function that can be applied to mark functions as experimental.</p> Source code in <code>inference_sdk/utils/decorators.py</code> <pre><code>def experimental(info: str):\n    \"\"\"Create a decorator that marks functions as experimental.\n\n    This decorator will emit a warning when the decorated function is called,\n    indicating that the function is experimental and providing additional information.\n\n    Args:\n        info (str): Information about the experimental status of the function.\n\n    Returns:\n        callable: A decorator function that can be applied to mark functions as experimental.\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is experimental: {info}\",\n                category=InferenceSDKDeprecationWarning,\n                stacklevel=2,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/inference_sdk/utils/environment/","title":"Environment","text":""},{"location":"reference/inference_sdk/utils/environment/#inference_sdk.utils.environment.str2bool","title":"<code>str2bool(value)</code>","text":"<p>Convert a string or boolean value to a boolean.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, bool]</code> <p>The value to convert. Can be either a string ('true'/'false') or a boolean value.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>The boolean value. Returns True for 'true' (case-insensitive) or True input, False for 'false' (case-insensitive) or False input.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is not 'true' or 'false' (case-insensitive).</p> Source code in <code>inference_sdk/utils/environment.py</code> <pre><code>def str2bool(value: Union[str, bool]) -&gt; bool:\n    \"\"\"Convert a string or boolean value to a boolean.\n\n    Args:\n        value (Union[str, bool]): The value to convert. Can be either a string ('true'/'false')\n            or a boolean value.\n\n    Returns:\n        bool: The boolean value. Returns True for 'true' (case-insensitive) or True input,\n            False for 'false' (case-insensitive) or False input.\n\n    Raises:\n        ValueError: If the input string is not 'true' or 'false' (case-insensitive).\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n    if value.lower() == \"true\":\n        return True\n    elif value.lower() == \"false\":\n        return False\n    else:\n        raise ValueError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n</code></pre>"},{"location":"scripts/gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages.\"\"\"\nimport ast\nimport os\nfrom pathlib import Path\nfrom typing import Union\n</pre> \"\"\"Generate the code reference pages.\"\"\" import ast import os from pathlib import Path from typing import Union In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>SKIP_MODULES = [\n    \"inference.enterprise.device_manager.command_handler\",\n    \"inference.enterprise.parallel.celeryconfig\",\n]\n</pre> SKIP_MODULES = [     \"inference.enterprise.device_manager.command_handler\",     \"inference.enterprise.parallel.celeryconfig\", ] In\u00a0[\u00a0]: Copied! <pre>def module_has_docstrings(path: str) -&gt; bool:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        try:\n            tree = ast.parse(f.read(), filename=path)\n        except SyntaxError:\n            return False  # skip broken files\n\n    if has_docstring(tree):\n        return True\n\n    for node in tree.body:\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            if has_docstring(node):\n                return True\n    return False\n</pre> def module_has_docstrings(path: str) -&gt; bool:     with open(path, \"r\", encoding=\"utf-8\") as f:         try:             tree = ast.parse(f.read(), filename=path)         except SyntaxError:             return False  # skip broken files      if has_docstring(tree):         return True      for node in tree.body:         if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):             if has_docstring(node):                 return True     return False In\u00a0[\u00a0]: Copied! <pre>def has_docstring(node: Union[ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module]):\n    return ast.get_docstring(node) is not None\n</pre> def has_docstring(node: Union[ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module]):     return ast.get_docstring(node) is not None In\u00a0[\u00a0]: Copied! <pre>if not os.environ.get(\"SKIP_CODEGEN\"):\n    for package in [\"inference\", \"inference_sdk\", \"inference_cli\"]:\n        nav = mkdocs_gen_files.Nav()\n        src = Path(__file__).parent.parent.parent / package\n\n        for path in sorted(p for p in src.rglob(\"*.py\") if \"landing\" not in p.parts):\n            if not module_has_docstrings(path=path.as_posix()):\n                continue\n            module_path = path.relative_to(src.parent).with_suffix(\"\")\n            doc_path = path.relative_to(src.parent).with_suffix(\".md\")\n            full_doc_path = Path(\"reference\", doc_path)\n\n            parts = list(module_path.parts)\n            identifier = \".\".join(parts)\n            if parts[-1] == \"__main__\" or parts[-1] == \"__init__\" or identifier in SKIP_MODULES:\n                # print(\"SKIPPING\", identifier)\n                continue\n\n            nav[parts] = f\"/reference/{module_path.as_posix()}.md\"\n\n            with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n                fd.write(f\"::: {identifier}\")\n\n            edit_path = f\"https://github.com/roboflow/inference/tree/main/{module_path.as_posix()}.py\"\n            # print(\"Edit path:\", edit_path)\n            mkdocs_gen_files.set_edit_path(full_doc_path, edit_path)\n\n        with mkdocs_gen_files.open(f\"reference/{package}/index.md\", \"w\") as nav_file:\n            generator = nav.build_literate_nav()\n            lines = list(generator)\n            # print(\"GENERATING NAVIGATION\")\n            # print(\"\\n\".join(lines))\n            nav_file.writelines(lines)\n</pre> if not os.environ.get(\"SKIP_CODEGEN\"):     for package in [\"inference\", \"inference_sdk\", \"inference_cli\"]:         nav = mkdocs_gen_files.Nav()         src = Path(__file__).parent.parent.parent / package          for path in sorted(p for p in src.rglob(\"*.py\") if \"landing\" not in p.parts):             if not module_has_docstrings(path=path.as_posix()):                 continue             module_path = path.relative_to(src.parent).with_suffix(\"\")             doc_path = path.relative_to(src.parent).with_suffix(\".md\")             full_doc_path = Path(\"reference\", doc_path)              parts = list(module_path.parts)             identifier = \".\".join(parts)             if parts[-1] == \"__main__\" or parts[-1] == \"__init__\" or identifier in SKIP_MODULES:                 # print(\"SKIPPING\", identifier)                 continue              nav[parts] = f\"/reference/{module_path.as_posix()}.md\"              with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:                 fd.write(f\"::: {identifier}\")              edit_path = f\"https://github.com/roboflow/inference/tree/main/{module_path.as_posix()}.py\"             # print(\"Edit path:\", edit_path)             mkdocs_gen_files.set_edit_path(full_doc_path, edit_path)          with mkdocs_gen_files.open(f\"reference/{package}/index.md\", \"w\") as nav_file:             generator = nav.build_literate_nav()             lines = list(generator)             # print(\"GENERATING NAVIGATION\")             # print(\"\\n\".join(lines))             nav_file.writelines(lines) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"scripts/macros/","title":"Macros","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\n</pre> import sys from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>def define_env(env):\n    \"\"\"Hook function to define macros for MkDocs.\"\"\"\n    \n    @env.macro\n    def get_version():\n        \"\"\"Read version from inference/core/version.py\"\"\"\n        # Find the root of the repository by iterating up parent directories\n        current_path = Path(__file__).resolve()\n        for parent in current_path.parents:\n            # Check if this directory contains the 'inference' subdirectory\n            if (parent / 'inference').is_dir():\n                repo_root = parent\n                break\n        else:\n            raise FileNotFoundError(\"Could not find repository root with 'inference' directory\")\n        \n        version_file_path = repo_root.joinpath('inference', 'core', 'version.py')\n        \n        try:\n            # Execute the version.py file and extract __version__\n            namespace = {}\n            with open(version_file_path, 'r') as f:\n                exec(f.read(), namespace)   \n            return namespace['__version__']\n        except Exception as e:\n            print(f\"Warning: Could not read version from {version_file_path}: {e}\")\n            return \"unknown\"\n    \n    # Make VERSION available globally to all templates\n    env.variables['VERSION'] = get_version()\n</pre> def define_env(env):     \"\"\"Hook function to define macros for MkDocs.\"\"\"          @env.macro     def get_version():         \"\"\"Read version from inference/core/version.py\"\"\"         # Find the root of the repository by iterating up parent directories         current_path = Path(__file__).resolve()         for parent in current_path.parents:             # Check if this directory contains the 'inference' subdirectory             if (parent / 'inference').is_dir():                 repo_root = parent                 break         else:             raise FileNotFoundError(\"Could not find repository root with 'inference' directory\")                  version_file_path = repo_root.joinpath('inference', 'core', 'version.py')                  try:             # Execute the version.py file and extract __version__             namespace = {}             with open(version_file_path, 'r') as f:                 exec(f.read(), namespace)                return namespace['__version__']         except Exception as e:             print(f\"Warning: Could not read version from {version_file_path}: {e}\")             return \"unknown\"          # Make VERSION available globally to all templates     env.variables['VERSION'] = get_version()"},{"location":"server_configuration/accepted_input_formats/","title":"Input formats accepted by <code>inference</code> server","text":""},{"location":"server_configuration/accepted_input_formats/#why-should-i-care","title":"Why should I care?","text":"<p>The Roboflow team has designed the inference server to be as user-friendly and straightforward to integrate as  possible. We understand that some users prioritize ease of use, which is why we did not restrict the use of  potentially less secure data loading methods. This approach caters to those who prefer a simple and accessible  serving mechanism without the need for rigorous security measures.</p> <p>However, we also recognize the importance of having a production-ready solution. Therefore, we offer configuration  options that allow users to disable potentially unsafe behaviors.</p> <p>In this document, we explain how to configure the server to either enhance security or enable more  flexible behaviors, depending on your needs.</p>"},{"location":"server_configuration/accepted_input_formats/#deserialization-of-pickled-numpy-objects","title":"Deserialization of pickled <code>numpy</code> objects","text":"<p>One of the ways to send requests to the inference server is via serialized numpy objects:</p> <pre><code>import cv2\nimport pickle\nimport requests\n\nimage = cv2.imread(\"...\")\nimg_str = pickle.dumps(image)\n\ninfer_payload = {\n    \"model_id\": \"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": img_str,\n    },\n    \"api_key\": \"YOUR-API-KEY\",\n}\n\nres = requests.post(\n    \"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n</code></pre> <p>Starting from version <code>v0.14.0</code>, deserialization of this type of payload is disabled by default. However, you can  enable it by setting an environmental variable, <code>ALLOW_NUMPY_INPUT=True</code>. Check inference cli docs to see how to run the server with that flag. This option is not available in Roboflow's Hosted Inference API.</p> <p>Warning</p> <p>Roboflow advises all users hosting the inference server in production environments not to enable this option if  the server is open to requests from the open Internet or is not locked down to accept only authenticated requests from your workspace's API key.</p>"},{"location":"server_configuration/accepted_input_formats/#sending-urls-to-inference-images","title":"Sending URLs to inference images","text":"<p>Making GET requests to obtain images from URLs can expose the server to  server-side request forgery (SSRF) attacks. However, it is also very convenient to simply provide an image URL  for requests: <pre><code>import requests\n\n\ninfer_payload = {\n    \"model_id\": \"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": \"https://some.com/image.jpg\",\n    },\n    \"api_key\": \"YOUR-API-KEY\",\n}\n\nres = requests.post(\n    \"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n</code></pre></p> <p>This option is enabled by default, but we recommend configuring the server to enhance security using one or more of the following environment variables: * <code>ALLOW_URL_INPUT</code> - Set to <code>False</code> disable image URLs of any kind to be accepted by server - default: <code>True</code>. * <code>ALLOW_NON_HTTPS_URL_INPUT</code> - set to <code>False</code> to only allow https protocol in URLs (useful to make sure domain names are not maliciously resolved) - default: <code>False</code> * <code>ALLOW_URL_INPUT_WITHOUT_FQDN</code> - set to <code>False</code> to enforce URLs with fully qualified domain names only - and reject URLs based on IPs - default: <code>False</code> * <code>WHITELISTED_DESTINATIONS_FOR_URL_INPUT</code> - Optionally, you can specify a comma-separated list of allowed destinations  for URL requests. For example: <code>WHITELISTED_DESTINATIONS_FOR_URL_INPUT=192.168.0.15,some.site.com</code>. URLs pointing to  other targets will be rejected. * <code>BLACKLISTED_DESTINATIONS_FOR_URL_INPUT</code> - Optionally, you can specify a comma-separated list of forbidden  destinations for URL requests. For example:  <code>BLACKLISTED_DESTINATIONS_FOR_URL_INPUT=192.168.0.15,some.site.com</code>. URLs pointing to these targets will be rejected. * <code>ALLOW_LOADING_IMAGES_FROM_LOCAL_FILESYSTEM</code> - Set to <code>False</code> to disable local filesystem access to images - default: <code>True</code>.</p> <p>Check inference cli docs to see how to run server with specific flags.</p>"},{"location":"server_configuration/environmental_variables/","title":"Environment Variables","text":"<p><code>Inference</code> behavior can be controlled by set of environmental variables. All environmental variables are listed in inference/core/env.py</p> <p>Below is a list of some environmental values that require more in-depth explanation.</p> Environmental variable Description Default <code>ONNXRUNTIME_EXECUTION_PROVIDERS</code> List of execution providers in priority order, warning message will be displayed if provider is not supported on user platform See here <code>SAM2_MAX_EMBEDDING_CACHE_SIZE</code> The number of sam2 embeddings that will be held in memory. The embeddings will be held in gpu memory. Each embedding takes 16777216 bytes. 100 <code>SAM2_MAX_LOGITS_CACHE_SIZE</code> The number of sam2 logits that will be held in memory. The the logits will be in cpu memory. Each logit takes 262144 bytes. 1000 <code>DISABLE_SAM2_LOGITS_CACHE</code> If set to True, disables the caching of SAM2 logits. This can be useful for debugging or in scenarios where memory usage needs to be minimized, but may result in slower performance for repeated similar requests. False <code>ENABLE_WORKFLOWS_PROFILING</code> If set to True, in <code>inference</code> server allows the server to output Workflows profiler traces the client, running in Python package with <code>InferencePipeline</code> it enables profiling. False <code>WORKFLOWS_PROFILER_BUFFER_SIZE</code> Size of profiler buffer (number of consecutive Wrofklows Execution Engine <code>run(...)</code> invocations to trace in buffer. 64 <code>RUNS_ON_JETSON</code> Boolean flag to tell if <code>inference</code> runs on Jetson device - set to <code>True</code> in all docker builds for Jetson architecture. False <code>WORKFLOWS_DEFINITION_CACHE_EXPIRY</code> Number of seconds to cache Workflows definitions as a result of <code>get_workflow_specification(...)</code> function call <code>15 * 60</code> - 15 minutes <code>DOCKER_SOCKET_PATH</code> Path to the local socket mounted to the container - by default empty, if provided - enables pooling docker container stats from the docker deamon socket. See more here Not Set <code>ENABLE_PROMETHEUS</code> Boolean flag to enable Prometheus <code>/metrics</code> enpoint. True for docker images in dockerhub <code>ENABLE_STREAM_API</code> Flag to enable Stream Management API in <code>inference</code> server - see more. False <code>STREAM_API_PRELOADED_PROCESSES</code> In context of Stream API - this environment variable controlls how many idle processes are warmed-up ready to be a worker for <code>InferencePipeline</code> - helps speeding up workers processes start on GPU 0 <code>TRANSIENT_ROBOFLOW_API_ERRORS</code> List of (comma separated) HTTP codes from RF API that should be retried (only applicable to GET endpoints) <code>None</code> <code>RETRY_CONNECTION_ERRORS_TO_ROBOFLOW_API</code> Fleg to decide if connection errors for RF API should be retried (only applicable to GET endpoints) <code>False</code> <code>ROBOFLOW_API_REQUEST_TIMEOUT</code> Timeout (in seconds given as integer) for requests to RF API <code>None</code> <code>TRANSIENT_ROBOFLOW_API_ERRORS_RETRIES</code> Number of times transient errors (connection errors and HTTP transient codes) to RF API will be retried (only applicable to GET endpoints) <code>3</code> <code>TRANSIENT_ROBOFLOW_API_ERRORS_RETRY_INTERVAL</code> Delay interval of retries (for connection errors and HTTP transient codes) of RF API requests (only applicable to GET endpoints) <code>3</code> <code>METRICS_ENABLED</code> Flag to control Roboflow Model Monitoring <code>True</code> <code>MODEL_VALIDATION_DISABLED</code> Flag that can make model loading faster by skipping trial inference <code>False</code> <code>DISABLE_VERSION_CHECK</code> Flag to disable <code>inference</code> version chack in background thread <code>False</code>"},{"location":"server_configuration/service_telemetry/","title":"Inference server telemetry","text":"<p>Service telemetry provides essential real-time data on system health, performance, and usage. It enables:</p> <ul> <li> <p>Monitoring and Diagnostics: Early detection of issues for quick resolution.</p> </li> <li> <p>Performance Optimization: Identifying bottlenecks to improve efficiency.</p> </li> <li> <p>Usage Insights: Understanding user behavior to guide improvements.</p> </li> <li> <p>Security: Detecting suspicious activities and ensuring compliance.</p> </li> <li> <p>Scalability: Predicting and managing resource demands.</p> </li> </ul> <p>In <code>inference</code> server, we enabled:</p> <ul> <li> <p><code>prometheus</code> metrics</p> </li> <li> <p>docker container metrics provided by Docker daemon</p> </li> </ul>"},{"location":"server_configuration/service_telemetry/#prometheus-in-inference-server","title":"\ud83d\udd25 <code>prometheus</code> in <code>inference</code> server","text":"<p>To enable metrics, set environmental variable <code>ENABLE_PROMETHEUS=True</code> in your docker container:</p> <pre><code>docker run -p 9001:9001 -e ENABLE_PROMETHEUS=True roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Then use <code>GET /metrics</code> endpoint to fetch the metrics in Python:</p> <pre><code>import requests\n\nresult = requests.get(\"http://127.0.0.1:9001/metrics\")\nresult.raise_for_status()\n\nprint(result.text)\n</code></pre> <p>or using curl: <pre><code>curl http://127.0.0.1:9001/metrics\n</code></pre></p>"},{"location":"server_configuration/service_telemetry/#docker-container-metrics","title":"Docker container metrics","text":"<p>Potential security issue</p> <p>This feature rely on docker daemon socket exposure inside container. This way we can expose docker container resource utilisation metrics without the need for \"supervisor\" service, but may be seen as security violation. We disable that ooption by default. Please acknowledge  potential security risks before enabling this option</p> <p>To expose container metrics you need to run docker container with <code>inference</code> server in a specific way:</p> <pre><code>docker run -p 9001:9001 \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -e DOCKER_SOCKET_PATH=/var/run/docker.sock\n  roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Explanation:</p> <ul> <li> <p>In line <code>2</code>, you mount a docker daemon socket from your host (typically <code>/var/run/docker.sock</code>,  but you need to verify your setup) into some location inside container (for convenience also <code>/var/run/docker.sock</code>)</p> </li> <li> <p>In line <code>3</code>, you expose environmental variable <code>DOCKER_SOCKET_PATH</code> with the location of the docker daemon socket  inside the container - in this case <code>/var/run/docker.sock</code>, as per specification in line <code>2</code></p> </li> </ul> <p>Then you would be able to reach <code>GET /device/stats</code> endpoint using cURL: <pre><code>curl http://127.0.0.1:9001/device/stats\n</code></pre></p> <p>or using Python: <pre><code>import requests\n\nresult = requests.get(\"http://127.0.0.1:9001/device/stats\")\nresult.raise_for_status()\n\nprint(result.json())\n</code></pre></p>"},{"location":"start/getting-started/","title":"Installation","text":"<p>You can install <code>inference</code> in a Python&gt;=3.9,&lt;3.13 environment.</p> <p>Installation Command</p> CPUNvidia GPU <pre><code>pip install inference\n</code></pre> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"start/getting-started/#quickstart","title":"Quickstart","text":"<p>With the following code snippet, we can load a model and then we used that model's <code>infer(...)</code> method to run an image through it.</p> <pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n\n# define the image url to use for inference\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n</code></pre> <p>Note</p> <p>For a more detailed example, please refer to the tutorial on running a model.</p>"},{"location":"start/getting-started/#choosing-a-deployment-method","title":"Choosing a Deployment Method","text":"<p>There are three primary ways to deploy Inference:</p> <ul> <li>Serverless Hosted API - for smaller image models.</li> <li>Dedicated Deployment - for bigger models and streaming video.</li> <li>Self Hosted - on your own edge device or server.</li> </ul> <p>Each has pros and cons and which one you should choose depends on your particular use-case and organizational constraints.</p> Serverless Dedicated Self-Hosted Workflows \u2705 \u2705 \u2705 Basic Logic Blocks \u2705 \u2705 \u2705 Pre-Trained Models \u2705 \u2705 \u2705 Fine-Tuned Models \u2705 \u2705 \u2705 Universe Models \u2705 \u2705 \u2705 Active Learning \u2705 \u2705 \u2705 Model Monitoring \u2705 \u2705 \u2705 Foundation Models \u2705 \u2705 Video Stream Management \u2705 \u2705 Dynamic Python Blocks \u2705 \u2705 Device Management \u2705 \u2705 Access Local Devices \u2705 Can Run Offline \u2705 Billing Per-Call Hourly See Below"},{"location":"start/getting-started/#cloud-hosting","title":"Cloud Hosting","text":"<p>By far the easiest way to get started is with Roboflow's managed services. You can jump straight to building without having to setup any infrastructure. It's often the front-door to using Inference even for those who know they will eventually want to self host.</p> <p>There are two cloud hosted offerings with different targeted use-cases, capabilities, and pricing models.</p>"},{"location":"start/getting-started/#serverless-hosted-api","title":"Serverless Hosted API","text":"<p>The Serverless Hosted API supports running Workflows on pre-trained &amp; fine-tuned models, chaining models, basic logic, visualizations, and external integrations.</p> <p>It supports cloud-hosted VLMs like ChatGPT and Anthropic Claude, but does not support running heavy models like Florence-2 or SAM 2. It also does not support streaming video.</p> <p>The Serverless API scales down to zero when you're not using it (and up to infinity under load) with quick (a couple of seconds) cold-start time. You pay per model inference with no minimums. Roboflow's free tier credits may be used.</p>"},{"location":"start/getting-started/#dedicated-deployments","title":"Dedicated Deployments","text":"<p>Dedicated Deployments are single-tenant virtual machines that are allocated for your exclusive use. They can optionally be configured with a GPU and used in development mode (where you may be evicted if capacity is needed for a higher priority task &amp; are limited to 3-hour sessions) or production mode (guaranteed capacity and no session time limit).</p> <p>On a Dedicated Deployment, you can stream video, run custom Python code, access heavy foundation models like SAM 2, Florence-2, and Paligemma (including your fine-tunes of those models), and install additional dependencies. They are much higher performance machines than the instances backing the Serverless Hosted API.</p> <p>Scale-up time is on the order of a minute or two.</p> <p>Dedicated Deployments Availability</p> <p>Dedicated Deployments are only available to Roboflow Workspaces with an active subscription (and are not available on the free trial). They are billed hourly.</p>"},{"location":"start/getting-started/#self-hosting","title":"Self Hosting","text":"<p>Running at the edge is a core priority and focus area of Inference. For many use-cases latency matters, bandwidth is limited, interfacing with local devices is key, and resiliency to Internet outages is mandatory.</p> <p>Running locally on a development machine, an AI computer, or an edge device is as simple as starting a Docker container.</p> <p>Self-Hosted Pricing</p> <p>Basic usage of self-hosted Inference Servers is completely free.</p> <p>Workflows and Models that require a Roboflow API Key to access Roboflow Cloud powered features (for example: the private model repository) are metered and consume credits (which cost money after a generous free tier is used up) based on the number of images or the hours of video processed.</p> <p>Detailed installation instructions and device-specific performance tips are here.</p>"},{"location":"start/getting-started/#bring-your-own-cloud","title":"Bring Your Own Cloud","text":"<p>Sometimes enterprise compliance policies regarding sensitive data requires running workloads on-premises. This is supported via self-hosting on your own cloud. Billing is the same as for self-hosting on an edge device.</p> <p></p>"},{"location":"start/getting-started/#next-steps","title":"Next Steps","text":"<p>Once you've decided on a deployment method and have a server running, interfacing with it is easy.</p>"},{"location":"start/next/","title":"Next Steps","text":""},{"location":"start/next/#using-your-new-server","title":"Using Your New Server","text":"<p>Once you have a server running, you can access it via its API or using the Python SDK. You can also use it to build Workflows using the Roboflow Platform UI.</p> Python SDKNode.jsHTTP / cURL <p>From a JavaScript app, hit your new server with an HTTP request.</p> <pre><code>const response = await fetch('http://localhost:9001/infer/workflows/roboflow-docs/model-comparison', {\n    method: 'POST',\n    headers: {\n        'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n        // api_key: \"&lt;YOUR API KEY&gt;\" // optional to access your private data and models\n        inputs: {\n            \"image\": {\n                \"type\": \"url\",\n                \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n            },\n            \"model1\": \"yolov8n-640\",\n            \"model2\": \"yolov11n-640\"\n        }\n    })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre> <p>Warning</p> <p>Be careful not to expose your API Key to external users (in other words: don't use this snippet in a public-facing front-end app).</p> <p>Using the server's API you can access it from any other client application. From the command line using cURL:</p> <pre><code>curl -X POST \"http://localhost:9001/infer/workflows/roboflow-docs/model-comparison\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"api_key\": \"&lt;YOUR API KEY -- REMOVE THIS LINE IF NOT FILLING&gt;\",\n    \"inputs\": {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n        },\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n}'\n</code></pre> <p>Tip</p> <p>ChatGPT is really good at converting snippets like this into other languages. If you need help, try pasting it in and asking it to translate it to your language of choice.</p> <p>The easiest way to build and test your Workflows is by using the Roboflow Platform's visual debugger but you can also build with our Python SDK or start by forking an Example project.</p> <p>Once you've built a Workflow you're happy with, you can set it up to run on a video stream.</p>"},{"location":"start/next/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install inference-sdk\n</code></pre>"},{"location":"start/next/#run-a-workflow","title":"Run a workflow","text":"<p>This code runs an example model comparison Workflow on an Inference Server running on your local machine:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"&lt;YOUR API KEY&gt;\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n</code></pre>"},{"location":"start/next/#tutorials","title":"Tutorials","text":"<p>You may also want to check out one of our Video Tutorials showing the end to end creation of a computer vision project with Inference:</p> Tutorial: Build an AI-Powered Self-Serve Checkout Created: 2 Feb 2025 <p>         Make a computer vision app that identifies different pieces of hardware, calculates       the total cost, and records the results to a database.       </p> Tutorial: Intro to Workflows Created: 6 Jan 2025 <p>         Learn how to build and deploy Workflows for common use-cases like detecting          vehicles, filtering detections, visualizing results, and calculating dwell          time on a live video stream.       </p> Tutorial: Build a Smart Parking System Created: 27 Nov 2024 <p>         Build a smart parking lot management system using Roboflow Workflows!         This tutorial covers license plate detection with YOLOv8, object tracking         with ByteTrack, and real-time notifications with a Telegram bot.       </p>"},{"location":"start/overview/","title":"Overview","text":"<p>Inference is the centralized hub that handles the core tasks of building computer vision applications. It implements the recurring heavy-lifting required for efficiently processing and managing video streams, deploying and monitoring models and pipelines, optimizing CPU and GPU resources, and managing dependencies.</p>"},{"location":"start/overview/#core-principles","title":"Core Principles","text":"<p>There are three three core principles of Inference:</p> <ol> <li>Easy to Use - You shouldn't need a PhD to use computer vision. Getting a project up and running should take hours, not weeks. Developers should spend their time on the things that are unique to their project, not reinventing the wheel. We try to maximize your speed of iteration and experimentation.</li> <li>Production Grade - Inference should be stable, thoroughly tested, scalable, secure, and fully-featured. It should be as fast as possible without degrading the developer experience.</li> <li>Extensible - Developers should never be constrained by what's included in the box. Adding additional models and functionality should be easy and seamless.</li> </ol>"},{"location":"start/overview/#use-at-any-scale","title":"Use at Any Scale","text":"<p>Inference is developed by Roboflow, a computer vision platform used by over a million developers and over half of the Fortune 100. Over the years, it has powered billions of inferences for mission critical applications like monitoring essential infrastructure, enforcing safety controls in warehouses, ensuring quality of life-saving products, and powering instant replay at some of the world's premier sporting events.</p>"},{"location":"start/overview/#open-source","title":"Open Source","text":"<p>The core functionality of Inference is open source on GitHub under the Apache 2.0 license. You may freely fork, extend, or contribute to its functionality.</p> <p>Models are subject to licensing which respects the underlying architecture. These licenses are listed in <code>inference/models</code>. Paid Roboflow accounts include a commercial license for some models (see roboflow.com/licensing for details).</p> <p>Cloud connected functionality (like our model and Workflows registries, dataset management, model monitoring, device management, and managed infrastructure) requires a Roboflow account and API key &amp; is metered based on usage according to Roboflow's platform pricing tiers.</p> Open Access With API Key Pre-Trained Models \u2705 \u2705 Foundation Models \u2705 \u2705 Video Stream Management \u2705 \u2705 Dynamic Python Blocks \u2705 \u2705 Public Workflows \u2705 \u2705 Private Workflows \u2705 Fine-Tuned Models \u2705 Universe Models \u2705 Active Learning \u2705 Serverless Hosted API \u2705 Dedicated Deployments \u2705 Commercial Model Licensing Paid Device Management Enterprise Model Monitoring Enterprise"},{"location":"start/overview/#managed-compute","title":"Managed Compute","text":"<p>If you don't want to manage your own infrastructure for self-hosting, Roboflow offers a hosted Inference Server via one-click Dedicated Deployments (CPU and GPU machines) billed hourly, or simple models and Workflows via our serverless Hosted API billed per API-call.</p> <p>Roboflow offers a generous free-tier to get started.</p>"},{"location":"start/overview/#local-installation","title":"Local Installation","text":"<p>Self-hosting for production or local development is easy. We recommend using Docker to manage your installation because machine learning dependencies are often fragile and finicky. On most systems, the easiest way to get started is to use our CLI to choose the right docker image with the <code>inference server start</code> command:</p> <pre><code>pip install inference-cli\ninference server start\n</code></pre> <p>For detailed instructions on various systems, see our installation guide then see next steps to connect to your new Inference Server.</p>"},{"location":"understand/alternatives/","title":"Inference vs Alternatives","text":"<p>With its wide aperature of functionality, Inference's features overlap with many other pieces of software. This guide aims to help readers understand when they should (and should not) choose Inference over other tools.</p>"},{"location":"understand/alternatives/#inference-servers","title":"Inference Servers","text":""},{"location":"understand/alternatives/#nvidia-triton-inference-server","title":"NVIDIA Triton Inference Server","text":"<p>Triton is a powerhouse tool for machine learning experts to deploy ML models at scale. Its primary focus is on extremely optimized pipelines that run efficiently on NVIDIA hardware. It can be tough to use, trading off simplicity and a quick development cycle for raw speed and is geared towards expert users. It can chain models together, but doing so is a rigid and manual process.</p> <p>By contrast, Inference tries to be as fast as possible while remaining developer friendly. It invests heavily in tooling to make it quick and easy to iterate on your project, and provides APIs for remotely updating your configuration. Workflows is more flexible and feature rich than Triton's model ensembles. Additionally, Inference leans heavily into computer vision specific features like visualization and stateful video functionality and integrations like notifications and data sinks.</p> <p>Choose Triton if: you're a machine learning expert working on a tightly defined project that values speed on NVIDIA GPUs above all else.</p>"},{"location":"understand/alternatives/#lightning-litserve","title":"Lightning LitServe","text":"<p>LitServe is a lightweight and customizable inference server focused on serving models with minimal overhead. It is fairly minimalistic but flexible and self-contained.</p> <p>Like Triton, LitServe is task-agnostic, meaning it is designed to balance the needs of vision models with NLP, audio, and tabular models. This means it's not as feature-rich for computer vision applications (for example, it doesn't have any built-in features for streaming video). It is also highly focused on model serving without an abstraction layer like Workflows for model chaining and integrations with other tools.</p> <p>Choose LitServe if: you are working on general-purpose machine learning tasks and were previously considering rolling your own server but want a more featureful starting point.</p>"},{"location":"understand/alternatives/#tensorflow-serving","title":"Tensorflow Serving","text":"<p>If you're deeply engrained in the Tensorflow ecosystem and want to deploy a variety of Tensorflow models in different modalities like NLP, recommender systems, and audio in addition to CV models, Tensorflow Serving may be a good choice.</p> <p>It can be complex to setup and maintain and lacks features many users would consider table stakes (like pre- and post-processing which in many cases will need to be custom coded). Like several of the other servers listed here, it lacks depth in vision-specific functionality.</p> <p>Choose Tensorflow Serving if: the Tensorflow ecosystem is very important to you and you're willing to put in the legwork to take advantage of its advanced feature set.</p>"},{"location":"understand/alternatives/#torchserve","title":"TorchServe","text":"<p>The PyTorch ecosystem's equivalent of Tensorflow Serving is TorchServe. It's optimized for serving PyTorch models across several domains including vision, NLP, tabular data, and audio.</p> <p>Like Tensorflow Serving, it is designed for large-scale cloud deployments and can require custom configuration for things like pre- and post-processing and deploying multiple models. Because of its wide mandate it lacks many vision-specific features (like video streaming).</p> <p>Chose TorchServe if: you're looking for a way to scale and customize the deployment of your PyTorch models and don't need vision-specific functionality.</p>"},{"location":"understand/alternatives/#fastapi-or-flask","title":"FastAPI or Flask","text":"<p>In the olden days, most people rolled their own servers to expose their ML models to client applications. In fact, Roboflow Inference's HTTP interface and REST API are built on FastAPI.</p> <p>In this day and age, it's certainly still possible to start from scratch, but you'll be reinventing the wheel and will run into a lot of footguns others have already solved along the way. It's usually better and faster to use one of the existing ML-focused servers.</p> <p>Choose FastAPI or Flask if: your main goal is learning the intricacies of making an inference server.</p>"},{"location":"understand/alternatives/#workflow-builders","title":"Workflow Builders","text":""},{"location":"understand/alternatives/#comfyui","title":"ComfyUI","text":"<p>ComfyUI is the closest comparable to Roboflow Workflows. It's a node-based pipeline builder with its roots in generative image and video models. It has a UI for chaining models and applying logic.</p> <p>While there is some overlap in functionality (for example, there is a Stable Diffusion Workflows Block and a YOLO ComfyUI Node), but the ecosystem of nodes and the community focus of Comfy is squarely centered around generative models while Inference is focused on interfacing with the real-world.</p> <p>Chose ComfyUI if: you're mainly interested in using generative image and video models like Flux and don't need to use custom fine-tuned models in your pipeline to do things like selectively replacing specific objects.</p>"},{"location":"understand/alternatives/#node-red","title":"Node-RED","text":"<p>Node-RED is a low-code development platform for connecting devices, APIs, and services. It is widely used in home-automation use-cases because of its friendly interface for non-technical users. It provides a graphical interface for designing workflows and supports custom nodes which allows integration with ML models (primarily via external servers).</p> <p>But because it's not designed for machine learning tasks it can struggle with high-performance, compute-heavy tasks and isn't well suited for computer vision use-cases.</p> <p>Choose Node-RED: (possibly in conjunction with Inference via a custom node) if its wide selection of integrations with IoT sensors and other tools is a big unlock for your project.</p>"},{"location":"understand/alternatives/#edge-deployment","title":"Edge Deployment","text":""},{"location":"understand/alternatives/#edge-impulse","title":"Edge Impulse","text":"<p>Edge Impulse is a platform focused on deploying ML models to very low-power edge devices and embedded systems. It supports both vision and other models like audio, time-series, and signal processing. Edge Impulse is uniquely good at working with microcontrollers and has SDKs for single-board computers and mobile devices.</p> <p>The design focus on TinyML makes it less suited for high-resource, general-purpose tasks like video processing and running modern, state-of-the-art ML models. It also requires some familiarity with embedded systems. It does not offer an equivalent to Workflows to create complex logic and integrate with other systems and typically requires custom coding your application logic to run on the embedded board.</p> <p>Chose Edge Impulse if: you're working on an IoT or wearable device that's not capable of running more powerful models, framework, and logic.</p>"},{"location":"understand/alternatives/#nvidia-deepstream","title":"NVIDIA DeepStream","text":"<p>DeepStream is NVIDIA's platform for building highly optimized video processing pipelines accelerated by NVIDIA's hardware, taking full advantage of TensorRT for accelerated inference and CUDA for parallel processing. It targets many of the same business problems as Inference, including monitoring security cameras, smart cities, and industrial IoT.</p> <p>DeepStream has a reputation for being difficult to use with a steep learning curve. It requires familiarity with NVIDIA tooling and while it is highly configurable, it's also highly complex. It's focused on video processing, without deep integrations with other tooling. DeepStream is not open source; ensure that the license is suitable for your project.</p> <p>Choose DeepStream if: you're an expert willing to invest a lot of time and effort into optimizing a single project and high throughput is your primary objective.</p>"},{"location":"understand/architecture/","title":"Architecture","text":"<p>Inference is best run in server mode (it also supports a native Python interface; though see why we recommend Docker). You interact with it via a REST API (most often through the Python SDK) or a web browser (for example, the Roboflow Platform's UI optionally provides a frontend for interfacing with a locally hosted Inference Server).</p> <p>It orchestrates getting predictions through a model (or series of models) and processing the results. Inference automatically does multithreading to parallelize workloads and efficiently utilize the available GPU and CPU cores. It also dynamically adapts to variable processing rates when consuming video streams to ensure its host machine is not overloaded.</p> <p>Inference talks to services in the cloud to retrieve model weights and Workflow definitions and keeps track of model results for later evaluation, but all of the computation is done locally which means it can run offline.</p> <p>One Inference Server can handle multiple clients and streams.</p>"},{"location":"understand/architecture/#inference-as-a-microservice","title":"Inference as a Microservice","text":"<p>The most common way to use Inference is as a small part of a larger system. It's producing a <code>response</code> that is consumed by downstream code. This response sometimes represents the prediction from a model (for example, a set of <code>Detections</code> containing objects' categorization, location, and size in an image) but it could also represent the result of post-processing logic (like the pass/fail state of an inspection), an aggregation (like the count of unique objects seen over the past hour), or a visualization.</p> <p>For image workloads, the input is passed in as a parameter and the response is returned synchronously.</p> Inference as a Microservice <p>In the case of video streams, a visual agent (called an <code>InferencePipeline</code>) is started and runs in a loop until terminated. Responses are polled or subscribed to by the client application for display or processing.</p> InferencePipeline Video Streaming <p>Example microservice use-cases:</p> <ul> <li>Tagging of user-uploaded images to a website</li> <li>Determining if a machine is setup correctly before allowing it to turn on</li> <li>Blurring faces in a video</li> <li>Detecting mismatched wiring in a finished circuit board</li> <li>Inspecting a manufactured good to ensure it matches the spec</li> <li>Validating that an object is defect and blemish free</li> <li>Counting the number of pills in an image</li> </ul>"},{"location":"understand/architecture/#inference-as-an-appliance","title":"Inference as an Appliance","text":"<p>Inference can also be treated as an autonomous agent that continuously consumes and processes a video stream and performs downstream actions (like updating a database, sending notifications, firing webhooks, or signaling hardware). In this paradigm, the full logic of the system is defined in a Workflow and the output is pushed to external systems.</p> Inference as an Appliance <p>Example appliance use-cases:</p> <ul> <li>Stopping a conveyor belt if a jam has occurred</li> <li>Collecting highway traffic analytics</li> <li>Flagging suspicious activity in a security camera feed</li> <li>Updating an inventory system as vehicles enter or leave a yard</li> <li>Sounding an alarm when a scrap heap overflows</li> <li>Cataloguing retail customers' wait time over the course of a day</li> </ul>"},{"location":"understand/architecture/#why-docker","title":"Why Docker","text":"<p>We highly recommend using our Docker container to interface with Inference. Machine learning dependencies are sensitive to minor changes to their environment; if they are not isolated into a deterministic environment, your system is likely to break when you update your operating system or drivers, update the dependencies of your application code, apply security patches, or setup a new machine. We ensure that library versions are compatible with each other, packages are compiled to take  advantage of the GPU, and security patches are applied.</p> <p>Additionally, using Docker gives you portability. You can decide later to serve multiple clients from a single large server or to upgrade from a CPU to a GPU without having to refactor your application code.</p>"},{"location":"understand/features/","title":"Inference Features","text":"<p>Inference aspires to be a one-stop shop for all of your computer vision needs (and where the feature you seek is not in stock, it's straightforward to extend the functionality to suit your needs). A non-exhaustive list of features is captured here.</p>"},{"location":"understand/features/#model-serving","title":"Model Serving","text":"<p>The core of Inference is centered around serving computer vision models. It implements architectures for tasks like Object Detection, Image Classification, Instance Segmentation, Keypoint Detection, Image Embedding, OCR, Visual Question Answering, Gaze Detection, and more.</p>"},{"location":"understand/features/#image-processing","title":"Image Processing","text":"<p>A model is only as good as the image it receives. As the old adage says, \"Garbage In, Garbage Out\", which is why Inference includes and applies the same image pre- and post-processing methods that models use during training. It also applies these steps efficiently to avoid unnecessary latency.</p>"},{"location":"understand/features/#video-stream-management","title":"Video Stream Management","text":"<p>To do video inference properly requires attention to the image ingestion pipeline. Inference spawns separate threads to process video streams so that your model is never left hanging and always gets the most recent frame possible.</p>"},{"location":"understand/features/#workflows","title":"Workflows","text":"<p>If models are the brains and cameras are the eyes, Workflows are the nervous system and Workflow Blocks are the body's other organs and tools. By declaring a computation graph, Workflows efficiently pipe and parallelize data through models, logic, integrations, and custom code. There are already over 100 different Workflow Blocks included with new ones contributed regularly.</p>"},{"location":"understand/features/#server","title":"Server","text":"<p>Inference exposes an HTTP server for interfacing with its functionality. This lets you use it as a micro-service in a larger system and programmatically command its operations.</p>"},{"location":"understand/features/#sdk","title":"SDK","text":"<p>The included Python Client makes it easy to interact with the server from your applications.</p>"},{"location":"understand/features/#cli","title":"CLI","text":"<p>The Command Line Interface provides convenience methods for starting the server, running benchmarks, cloud deployment, and testing.</p>"},{"location":"understand/features/#speed","title":"Speed","text":"<p>Built-in optimizations like automatic parallelization via multiprocessing, hardware acceleration, and dynamic batching help you get the most out of your hardware. With the optional TensorRT flag you can also take advantage of quantization and device-specific layer fusion optimizations on supported GPUs. This lets you run more streams and process with higher resolution and lower latency.</p>"},{"location":"understand/features/#offline-cache","title":"Offline Cache","text":"<p>By pulling down models and Workflow definitions and storing them locally, the Inference Server can operate in offline mode.</p>"},{"location":"understand/features/#insights","title":"Insights","text":"<p>Inference can connect with Roboflow's platform to help you monitor and improve your models by uploading outlier data, accessing stats and telemetry, and tying into downstream data sinks and business intelligence suites.</p>"},{"location":"understand/features/#roboflow-integration","title":"Roboflow Integration","text":"<p>With over 100k fine-tuned models available, Roboflow Universe is the largest repository of computer vision models in the world. It also creates the most popular platform for training custom computer vision models. Inference's optional integration with the Roboflow platform supercharges CV applications by providing access to the best and most relevant problems for solving computer vision problems.</p>"},{"location":"understand/features/#portability","title":"Portability","text":"<p>We support running Inference on a myriad of devices from MacOS development machines to beefy cloud servers to tiny edge devices and AI appliances. Simply swap out the Docker tag and the same code you're running locally can be deployed to another platform.</p>"},{"location":"understand/features/#extensibility","title":"Extensibility","text":"<p>As an open source project, Inference can be forked and extended to add new models, Workflow Blocks, backends, and more. We also support using Dynamic Python Blocks to seamlessly bridge the gaps between blocks.</p>"},{"location":"understand/features/#cutting-edge-updates","title":"Cutting Edge Updates","text":"<p>With over 60 releases last year, Inference is constantly being updated. It often adopts new state of the art models within days of their release and new functionality is added weekly.</p>"},{"location":"understand/features/#stability-and-scalability","title":"Stability and Scalability","text":"<p>Inference has a suite of over 350 tests that run on every Pull Request across each supported target. It's used in production by many of the world's largest companies and is backed by the creator of the industry standard computer vision platform. It's powered billions of inferences across hundreds of thousands of models. It also powers Roboflow's core model serving infrastructure and products.</p>"},{"location":"understand/features/#security","title":"Security","text":"<p>We undergo an annual pen test and SOC 2 Type II audit and have an infrastructure and security team working to mitigate issues and respond to vulnerabilities and issues flagged by automated scans and external parties.</p>"},{"location":"understand/features/#support","title":"Support","text":"<p>Roboflow has a fully staffed customer support and professional services organization available for enterprise customers.</p>"},{"location":"understand/vocabulary/","title":"Glossary","text":"<p>Inference uses some terms (and terms of art) that may be unfamiliar to some readers. This page aims to disambiguate and clarify their meaning, specifically in the context of the Roboflow and Inference ecosystem.</p>"},{"location":"understand/vocabulary/#api","title":"API","text":"<p>An interface that allows software applications to communicate with each other. In Roboflow, the API provides access to workflows and blocks, enabling users to configure and execute inference pipelines programmatically.</p>"},{"location":"understand/vocabulary/#block","title":"Block","text":"<p>The fundamental unit of a Roboflow Workflow. Blocks perform specific tasks, such running model inference, performing logic, or interfacing with external services.</p>"},{"location":"understand/vocabulary/#cli","title":"CLI","text":"<p>Command-Line Interface. A tool used to interact with Inference, such as starting the server, running benchmarks, or executing Workflows.</p>"},{"location":"understand/vocabulary/#client","title":"Client","text":"<p>A software application or library that interacts with the API. Our Python SDK is a client that abstracts the REST API into a more user-friendly interface.</p>"},{"location":"understand/vocabulary/#commercial-license","title":"Commercial License","text":"<p>A license permitting businesses to use models under terms suited for commercial applications, typically involving subscription plans or usage fees. While Inference uses a permissive Apache 2.0 license, some models have terms that may require downstream users to open source their own codebase. A commercial license removes that restriction.</p>"},{"location":"understand/vocabulary/#dedicated","title":"Dedicated","text":"<p>A configuration where cloud compute resources for running Inference are allocated exclusively to a single user or organization, ensuring optimal performance and enabling additional functionality. Billed based on running time &amp; usage.</p>"},{"location":"understand/vocabulary/#definition","title":"Definition","text":"<p>The structured JSON description of a Workflow, including its sequence of Blocks, input parameters, response format, and any custom logic.</p>"},{"location":"understand/vocabulary/#dynamic-block","title":"Dynamic Block","text":"<p>A type of Workflows Block using custom Python Code included in the JSON Definition of the Workflow allowing advanced customization at runtime.</p>"},{"location":"understand/vocabulary/#enterprise","title":"Enterprise","text":"<p>A service tier for large organizations, offering enhanced capabilities, scalability, and support. Source code for enterprise functionality is included in the <code>enterprise</code> folder of the repo but may only be used in conjunction with an active Enterprise license.</p>"},{"location":"understand/vocabulary/#execution-engine","title":"Execution Engine","text":"<p>The backend system responsible for executing Workflows, managing the execution of Blocks, and optimizing resource allocation.</p>"},{"location":"understand/vocabulary/#fine-tuned-model","title":"Fine-Tuned Model","text":"<p>A model that has been trained on a specific dataset for improved performance in a targeted application. For example, a \"Scratch Detection\" model tuned to find scratches on a specific automotive component.</p>"},{"location":"understand/vocabulary/#foundation-model","title":"Foundation Model","text":"<p>A general-purpose model that knows a lot about a lot and does not necessarily need to be fine-tuned on a specific dataset and can be used \"zero-shot\".</p>"},{"location":"understand/vocabulary/#kind","title":"Kind","text":"<p>A categorization of data types used in Workflows. Defining the input and output Kinds of a Workflow Block allows the Execution Engine to validate, serialize, and optimize data and connections. Example Kinds are <code>detection</code> (representing a prediction from an object detection model), <code>image</code> (containing pixels and their metadata), and <code>float_zero_to_one</code>.</p>"},{"location":"understand/vocabulary/#inference-pipeline","title":"Inference Pipeline","text":"<p>An asynchronous interface for video streaming that handles efficiently consuming and routing video frames from a camera source and through a Workflow while maintaining state.</p>"},{"location":"understand/vocabulary/#lmm","title":"LMM","text":"<p>Large Multimodal Model. A Block type that processes multiple data types, such as images and text. Examples are Florence-2 and OpenAI's GPT-4o.</p>"},{"location":"understand/vocabulary/#managed","title":"Managed","text":"<p>A deployment running in Roboflow's Cloud environment where the scaling and infrastructure are provided as a service. Contrast with \"Self-Hosted\" where the customer installs the software on their own infrastructure and is responsible for its setup and maintenance.</p>"},{"location":"understand/vocabulary/#metered","title":"Metered","text":"<p>A billing approach based on usage metrics, such as the number of Workflow executions or hours of video processed. Models and Workflows that require an API Key to access Roboflow's Cloud Services are metered.</p>"},{"location":"understand/vocabulary/#model","title":"Model","text":"<p>A trained machine learning artifact used for inference tasks such as object detection or classification. Consists of an architecture (like ResNet-32) and trained weights.</p>"},{"location":"understand/vocabulary/#parameter","title":"Parameter","text":"<p>An input to a Workflow. Can be an image or data like strings, numbers, arrays, or objects. Used as inputs to Blocks.</p>"},{"location":"understand/vocabulary/#platform","title":"Platform","text":"<p>The Roboflow ecosystem, which includes end-to-end tools for collecting and organizing data, annotating images, creating datasets, training models, building Workflows, and monitoring deployments.</p>"},{"location":"understand/vocabulary/#pre-trained-model","title":"Pre-Trained Model","text":"<p>A model architecture (like YOLOv11) loaded with weights that have been trained on a generic dataset (like Microsoft COCO). Contrast with a Fine-Tuned model that has been trained on a domain-specific dataset.</p>"},{"location":"understand/vocabulary/#public","title":"Public","text":"<p>A dataset, model, or Workflow that is accessible to all users within the Roboflow ecosystem. These can be found and distributed on Roboflow Universe.</p>"},{"location":"understand/vocabulary/#schema","title":"Schema","text":"<p>The structured format of input and output data for a Workflow, defining the properties and types a downstream application needs to pass and parse to integrate with the Workflow.</p>"},{"location":"understand/vocabulary/#sdk","title":"SDK","text":"<p>Software Development Kit. A set of tools and libraries provided for integrating with the API using user-friendly abstractions.</p>"},{"location":"understand/vocabulary/#server","title":"Server","text":"<p>A device running Inference's HTTP interface, usually through Docker on port 9001.</p>"},{"location":"understand/vocabulary/#serverless","title":"Serverless","text":"<p>An execution model for Inference where resources scale on demand, without requiring manual resource management. Contrast with Dedicated. Billed solely based on usage.</p>"},{"location":"understand/vocabulary/#traditional-cv","title":"Traditional CV","text":"<p>Blocks in Workflows that implement traditional computer vision techniques, such as filtering or edge detection, without relying on machine learning models.</p>"},{"location":"understand/vocabulary/#universe","title":"Universe","text":"<p>A large collection of datasets, pre-trained models, and other resources shared publicly by other users and available for use within the Roboflow ecosystem.</p>"},{"location":"understand/vocabulary/#weights","title":"Weights","text":"<p>The learned parameters of a machine learning model determining its predictions for a given input.</p>"},{"location":"understand/vocabulary/#workflow","title":"Workflow","text":"<p>A series of interconnected Blocks designed to process data and produce desired outcomes through a defined sequence of operations. Can be used for chaining models, maintaining state, performing custom logic, and integrating with external systems.</p>"},{"location":"understand/vocabulary/#workspace","title":"Workspace","text":"<p>A collaborative environment in Roboflow where users can create datasets, train models, and build Workflows. A Workspace is the container for data and arbiter of access via seats and API Keys.</p>"},{"location":"using_inference/http_api/","title":"Http api","text":"<p>The HTTP Inference API provides a standard API through which to run inference on computer vision models. The HTTP API is a helpful way to treat your machine learning models as their own microservice. With this interface, you will run a docker container and make requests over HTTP. The requests contain all of the information Inference needs to run a computer vision model including the model ID, the input image data, and any configurable parameters used during processing (e.g. confidence threshold).</p>"},{"location":"using_inference/http_api/#quickstart","title":"Quickstart","text":""},{"location":"using_inference/http_api/#install-the-inference-server","title":"Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Start the server using <code>inference server start</code>. After you have started the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"using_inference/http_api/#run-inference","title":"Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions. The easiest way to interact with the Roboflow Inference server is to sue the Inference SDK. To do this first install it with pip:</p> <pre><code>pip install inference-sdk\n</code></pre> <p>Next, instantiate a client and use the <code>infer(...)</code> method:</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = \"1\"\nmodel_id = project_id + \"/\" + model_version\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\n\nresults = client.infer(image_url, model_id=model_id)\n</code></pre> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>Hint</p> <p>See full docs for the Inference SDK.</p>"},{"location":"using_inference/http_api/#visualize-results","title":"Visualize Results","text":"<pre><code>import os\n\nimport cv2\nimport supervision as sv\nfrom inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nmodel_id = \"soccer-players-5fuqs/1\"\nimage_file = \"soccer.jpg\"\n\nimage = cv2.imread(image_file)\n\n#Configure client\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\", # route for local inference server\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"], # api key for your workspace\n)\n\n#Run inference\nresult = client.infer(image, model_id=model_id)\n\n#Load results into Supervision Detection API\ndetections = sv.Detections.from_inference(result)\n\n#Create Supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n#Extract labels array from inference results\nlabels = [p['class'] for p in result['predictions']]\n\n#Apply results to image using Supervision annotators\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\n#Write annotated image to file or display image\nwith sv.ImageSink(target_dir_path=\"./results/\", overwrite=True) as sink:\n    sink.save_image(annotated_image)\n#or sv.plot_image(annotated_image)\n</code></pre>"},{"location":"using_inference/http_api/#hosted-api","title":"Hosted API","text":"<p>Roboflow hosts a powerful and infinitely scalable version of the Roboflow Inference Server. This makes it even easier to integrate computer vision models into your software without adding any maintenance burden. And, since the Roboflow hosted APIs are running using the Inference package, it's easy to switch between using a hosted server and an on prem server without having to reinvent your client code. To use the hosted API, simply replace the <code>api_url</code> parameter passed to the Inference SDK client configuration. The hosted API base URL is <code>https://detect.roboflow.com</code>.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nproject_id = \"soccer-players-5fuqs/1\"\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nresults = client.infer(image_url, model_id=f\"{model_id}\")\n</code></pre>"},{"location":"using_inference/inference_pipeline/","title":"InferencePipeline","text":"<p>The Inference Pipeline interface is made for streaming and is likely the best route to go for real time use cases.  It is an asynchronous interface that can consume many different video sources including local devices (like webcams),  RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>Now, since version <code>v0.9.18</code> <code>InferencePipeline</code> supports multiple sources of video at the same time! </p>"},{"location":"using_inference/inference_pipeline/#quickstart","title":"Quickstart","text":"<p>First, install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Next, create an Inference Pipeline:</p> <pre><code># import the InferencePipeline interface\nfrom inference import InferencePipeline\n# import a built-in sink called render_boxes (sinks are the logic that happens after inference)\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\napi_key = \"YOUR_ROBOFLOW_API_KEY\"\n\n# create an inference pipeline object\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\", # set the model id to a yolov8x model with in put size 1280\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\", # set the video reference (source of video), it can be a link/path to a video file, an RTSP stream url, or an integer representing a device id (usually 0 for built in webcams)\n    on_prediction=render_boxes, # tell the pipeline object what to do with each set of inference by passing a function\n    api_key=api_key, # provide your roboflow api key for loading models from the roboflow api\n)\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>Let's break down the example line by line:</p> <p><code>pipeline = InferencePipeline.init(...)</code></p> <p>Here, we are calling a class method of InferencePipeline.</p> <p><code>model_id=\"yolov8x-1280\"</code></p> <p>We set the model ID to a YOLOv8x model pre-trained on COCO with input resolution <code>1280x1280</code>.</p> <p><code>video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\"</code></p> <p>We set the video reference to a URL. Later we will show the various values that can be used as a video reference.</p> <p><code>on_prediction=render_boxes</code></p> <p>The <code>on_prediction</code> argument defines our sink (or a list of sinks).</p> <p><code>pipeline.start(); pipeline.join()</code></p> <p>Here, we start and join the thread that processes the video stream.</p>"},{"location":"using_inference/inference_pipeline/#what-is-video-reference","title":"What is video reference?","text":"<p>Inference Pipelines can consume many different types of video streams.</p> <ul> <li>Device Id (integer): Providing an integer instructs a pipeline to stream video from a local device, like a webcam. Typically, built in webcams show up as device <code>0</code>.</li> <li>Video File (string): Providing the path to a video file will result in the pipeline reading every frame from the file, running inference with the specified model, then running the <code>on_prediction</code> method with each set of resulting predictions.</li> <li>Video URL (string): Providing the path to a video URL is equivalent to providing a video file path and voids needing to first download the video.</li> <li>RTSP URL (string): Providing an RTSP URL will result in the pipeline streaming frames from an RTSP stream as fast as possible, then running the <code>on_prediction</code> callback on the latest available frame.</li> <li>Since version <code>0.9.18</code> - list of elements that may be any of values described above.</li> </ul>"},{"location":"using_inference/inference_pipeline/#how-the-inferencepipeline-works","title":"How the <code>InferencePipeline</code> works?","text":"<p><code>InferencePipeline</code> spins a video source consumer thread for each provided video reference. Frames from videos are grabbed by video multiplexer that awaits <code>batch_collection_timeout</code> (if source will not provide frame, smaller batch  will be passed to <code>on_video_frame(...)</code>, but missing frames and predictions will be filled with <code>None</code> before passing to <code>on_prediction(...)</code>). <code>on_prediction(...)</code> may work in <code>SEQUENTIAL</code> mode (only one element at once), or <code>BATCH</code>  mode - all batch elements at a time and that can be controlled by <code>sink_mode</code> parameter.</p> <p>For static video files, <code>InferencePipeline</code> processes all frames by default, for streams - it is possible to drop frames from the buffers - in favour of always processing the most recent data (when model inference is slow, more frames can be accumulated in buffer - stream processing drop older frames and only processes the most recent one).</p> <p>To enhance stability, in case of streams processing - video sources will be automatically re-connected once  connectivity is lost during processing. That is meant to prevent failures in production environment when the pipeline can run long hours and need to gracefully handle sources downtimes.</p>"},{"location":"using_inference/inference_pipeline/#how-to-provide-a-custom-inference-logic-to-inferencepipeline","title":"How to provide a custom inference logic to <code>InferencePipeline</code>","text":"<p>As of <code>inference&gt;=0.9.16</code>, Inference Pipelines support running custom inference logic. This means, instead of passing  a model ID, you can pass a custom callable. This callable should accept and <code>VideoFrame</code> return a dictionary with  results from the processing (as <code>on_video_frame</code> handler). It can be model predictions or results of any other processing you wish to execute. It is important to note that the sink being used (<code>on_prediction</code> handler you use) - must be adjusted to the specific format of <code>on_video_frame(...)</code> response. This way, you can shape video processing in a way you want.</p> <pre><code># This is example, reference implementation - you need to adjust the code to your purposes\nimport os\nimport json\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference import InferencePipeline\nfrom typing import Any, List\n\nTARGET_DIR = \"./my_predictions\"\n\nclass MyModel:\n\n  def __init__(self, weights_path: str):\n    self._model = your_model_loader(weights_path)\n\n  # before v0.9.18  \n  def infer(self, video_frame: VideoFrame) -&gt; Any:\n    return self._model(video_frame.image)\n\n  # after v0.9.18  \n  def infer(self, video_frames: List[VideoFrame]) -&gt; List[Any]: \n    # result must be returned as list of elements representing model prediction for single frame\n    # with order unchanged.\n    return self._model([v.image for v in video_frames])\n\ndef save_prediction(prediction: dict, video_frame: VideoFrame) -&gt; None:\n  with open(os.path.join(TARGET_DIR, f\"{video_frame.frame_id}.json\")) as f:\n    json.dump(prediction, f)\n\nmy_model = MyModel(\"./my_model.pt\")\n\npipeline = InferencePipeline.init_with_custom_logic(\n  video_reference=\"./my_video.mp4\",\n  on_video_frame=my_model.infer,\n  on_prediction=save_prediction,\n)\n\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre>"},{"location":"using_inference/inference_pipeline/#inferencepipeline-and-roboflow-workflows","title":"<code>InferencePipeline</code> and Roboflow <code>workflows</code>","text":"<p>Info</p> <p>This is feature preview. Please refer to workflows docs.</p> <p>Feature preview do not support multiple videos input!</p> <p>We are working to make <code>workflows</code> compatible with <code>InferencePipeline</code>. Since version <code>0.9.16</code> we introduce  an initializer to be used with workflow definitions. Here is the example:</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\ndef workflows_sink(\n    predictions: dict,\n    video_frame: VideoFrame,\n) -&gt; None:\n    render_boxes(\n        predictions[\"predictions\"][0],\n        video_frame,\n        display_statistics=True,\n    )\n\n\n# here you may find very basic definition of workflow - with a single object detection model.\n# Please visit workflows docs: https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows to\n# find more examples.\nworkflow_specification = {\n    \"specification\": {\n        \"version\": \"1.0\",\n        \"inputs\": [\n            {\"type\": \"InferenceImage\", \"name\": \"image\"},\n        ],\n        \"steps\": [\n            {\n                \"type\": \"ObjectDetectionModel\",\n                \"name\": \"step_1\",\n                \"image\": \"$inputs.image\",\n                \"model_id\": \"yolov8n-640\",\n                \"confidence\": 0.5,\n            }\n        ],\n        \"outputs\": [\n            {\"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.step_1.*\"},\n        ],\n    }\n}\npipeline = InferencePipeline.init_with_workflow(\n    video_reference=\"./my_video.mp4\",\n    workflow_specification=workflow_specification,\n    on_prediction=workflows_sink,\n    image_input_name=\"image\",  # adjust according to name of WorkflowImage input you define\n    video_metadata_input_name=\"video_metadata\" # AVAILABLE from v0.17.0! adjust according to name of WorkflowVideoMetadata input you define\n)\n\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>Additionally, since <code>v0.9.21</code>, you can initialise <code>InferencePipeline</code> with <code>workflow</code> registered in Roboflow App - providing your <code>workspace_name</code> and <code>workflow_id</code>:</p> <pre><code>pipeline = InferencePipeline.init_with_workflow(\n    video_reference=\"./my_video.mp4\",\n    workspace_name=\"&lt;your_workspace&gt;\",\n    workflow_id=\"&lt;your_workflow_id_to_be_found_in_workflow_url&gt;\",\n    on_prediction=workflows_sink,\n)\n</code></pre> <p>Workflows profiling</p> <p>Since <code>inference v0.22.0</code>, you may profile your Workflow execution inside <code>InferencePipeline</code> when  you export environmental variable <code>ENABLE_WORKFLOWS_PROFILING=True</code>. Additionally, you can tune the  number of frames you keep in profiler buffer via another environmental variable <code>WORKFLOWS_PROFILER_BUFFER_SIZE</code>. <code>init_with_workflow(...)</code> was also given a new parameter <code>profiling_directory</code> which can be adjusted to  dictate where to save the trace. </p>"},{"location":"using_inference/inference_pipeline/#sinks","title":"Sinks","text":"<p>Sinks define what an Inference Pipeline should do with each prediction. A sink is a function with signature:</p>"},{"location":"using_inference/inference_pipeline/#before-v0918","title":"Before <code>v0.9.18</code>","text":"<pre><code>from inference.core.interfaces.camera.entities import VideoFrame\n\n\ndef on_prediction(\n    predictions: dict,\n    video_frame: VideoFrame,\n) -&gt; None:\n    pass\n</code></pre> <p>The arguments are:</p> <ul> <li><code>predictions</code>: A dictionary that is the response object resulting from a call to a model's <code>infer(...)</code> method.</li> <li><code>video_frame</code>: A VideoFrame object containing metadata and pixel data from the video frame.</li> </ul>"},{"location":"using_inference/inference_pipeline/#after-v0918","title":"After <code>v0.9.18</code>","text":"<pre><code>from typing import Union, List, Optional\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef on_prediction(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        # SOME PROCESSING\n</code></pre> <p>See more info in Custom Sink section on how to create sink.</p>"},{"location":"using_inference/inference_pipeline/#usage","title":"Usage","text":"<p>You can also make <code>on_prediction</code> accepting other parameters that configure its behaviour, but those needs to be  latched in function closure before injection into <code>InferencePipeline</code> init methods.</p> <pre><code>from functools import partial\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference import InferencePipeline\n\n\ndef on_prediction(\n    predictions: dict,\n    video_frame: VideoFrame,\n    my_parameter: int,\n) -&gt; None:\n    # you need to implement your logic here, with `my_parameter` used\n    pass\n\npipeline = InferencePipeline.init(\n  video_reference=\"./my_video.mp4\",\n  model_id=\"yolov8n-640\",\n  on_prediction=partial(on_prediction, my_parameter=42),\n)\n</code></pre>"},{"location":"using_inference/inference_pipeline/#custom-sinks","title":"Custom Sinks","text":"<p>To create a custom sink, define a new function with the appropriate signature.</p> <pre><code>from typing import Union, List, Optional, Any\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef on_prediction(\n    predictions: Union[Any, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    if not issubclass(type(predictions), list):\n      # this is required to support both sequential and batch processing with single code\n      # if you use only one mode - you may create function that handles with only one type\n      # of input\n      predictions = [predictions]\n      video_frame = [video_frame]\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        # SOME PROCESSING\n</code></pre> <p>In <code>v0.9.18</code> we introduced <code>InferencePipeline</code> parameter called <code>sink_mode</code> - here is how it works. With <code>SinkMode.SEQUENTIAL</code> - each frame and prediction triggers separate call for sink, in case of <code>SinkMode.BATCH</code> -  list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None  values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>.  <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single  video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos -  sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also  possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p>"},{"location":"using_inference/inference_pipeline/#why-there-is-optional-in-listoptionaldict-and-listoptionalvideoframe","title":"Why there is <code>Optional</code> in  <code>List[Optional[dict]]</code> and <code>List[Optional[VideoFrame]]</code>?","text":"<p>It may happen that it is not possible to collect video frames from all the video sources (for instance when one of the  source disconnected and re-connection is attempted). <code>predictions</code> and <code>video_frame</code> are ordered matching the order of <code>video_reference</code> list of <code>InferencePipeline</code> and <code>None</code> elements will appear in position of missing frames. We provide this information to sink, as some sinks may require all predictions and video frames from the batch to be provided (even if missing) - for example: <code>render_boxes(...)</code> sink needs that information to maintain the position of frames in tiles mosaic.</p> <p>Info</p> <p>See our tutorial on creating a custom Inference Pipeline sink!</p> <p>prediction</p> <p>Predictions are provided to the sink as a dictionary containing keys:</p> <ul> <li><code>predictions</code>: predictions - either for single frame or batch of frames. Content depends on which model runs behind  <code>InferencePipeline</code> - for Roboflow models - it will come as dict or list of dicts. The schema of elements is given  below.</li> </ul> <p>Depending on the model output, predictions look differently. You must adjust sink to the prediction format. For instance, Roboflow object-detection prediction contains the following keys:</p> <ul> <li><code>x</code>: The center x coordinate of the predicted bounding box in pixels</li> <li><code>y</code>: The center y coordinate of the predicted bounding box in pixels</li> <li><code>width</code>: The width of the predicted bounding box in pixels</li> <li><code>height</code>: The height of the predicted bounding box in pixels</li> <li><code>confidence</code>: The confidence value of the prediction (between 0 and 1)</li> <li><code>class</code>: The predicted class name</li> <li><code>class_id</code>: The predicted class ID</li> </ul>"},{"location":"using_inference/inference_pipeline/#other-pipeline-configuration","title":"Other Pipeline Configuration","text":"<p>Inference Pipelines are highly configurable. Configurations include:</p> <ul> <li><code>max_fps</code>: Used to set the maximum rate of frame processing.</li> <li><code>confidence</code>: Confidence threshold used for inference.</li> <li><code>iou_threshold</code>: IoU threshold used for inference.</li> <li><code>video_source_properties</code>: Optional dictionary of properties to configure the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. See the OpenCV Documentation for a list of all possible properties.</li> </ul> <pre><code>from inference import InferencePipeline\npipeline = InferencePipeline.init(\n    ...,\n    max_fps=10,\n    confidence=0.75,\n    iou_threshold=0.4,\n    video_source_properties={\n        \"frame_width\": 1920.0,\n        \"frame_height\": 1080.0,\n        \"fps\": 30.0,\n    },\n)\n</code></pre> <p>See the reference docs for the full list of Inference Pipeline parameters.</p> <p>Breaking change planned at the end of Q4 2024</p> <p>We've discovered that the behaviour of <code>max_fps</code> parameter is not in line with <code>inference</code> clients expectations regarding processing of video files. Current implementation for vides waits before processing the next  video frame, instead droping the frames to modulate video FPS. </p> <p>We have added a way to change this suboptimal behaviour in release <code>v0.26.0</code> - new behaviour of  <code>InferencePipeline</code> can be enabled setting environmental variable flag  <code>ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True</code>. </p> <p>Please note that the new behaviour will be the default one end of Q4 2024!</p>"},{"location":"using_inference/inference_pipeline/#performance","title":"Performance","text":"<p>We tested the performance of Inference on a variety of hardware devices.</p> <p>Below are the results of our benchmarking tests for Inference.</p>"},{"location":"using_inference/inference_pipeline/#macbook-m2","title":"MacBook M2","text":"Test FPS yolov8-n ~26 yolov8-s ~12 yolov8-m ~5 <p>Tested against the same 1080p 60fps RTSP stream emitted by localhost.</p>"},{"location":"using_inference/inference_pipeline/#jetson-orin-nano","title":"Jetson Orin Nano","text":"Test FPS yolov8-n ~25 yolov8-s ~18 yolov8-m ~8 <p>With old version reaching at max 6-7 fps. This test was executed against 4K@60fps stream, which is not possible to be decoded in native pace due to resource constrains. New implementation proved to run without stability issues for few hours straight.</p>"},{"location":"using_inference/inference_pipeline/#tesla-t4","title":"Tesla T4","text":"<p>GPU workstation with Tesla T4 was able to run 4 concurrent HD streams at 15FPS utilising ~80% GPU - reaching over 60FPS throughput per GPU (against <code>yolov8-n</code>).</p>"},{"location":"using_inference/inference_pipeline/#migrating-from-inferencestream-to-inferencepipeline","title":"Migrating from <code>inference.Stream</code> to <code>InferencePipeline</code>","text":"<p>Inference is deprecating support for <code>inference.Stream</code>, our video stream inference interface. <code>inference.Stream</code> is being replaced with <code>InferencePipeline</code>, which has feature parity and achieves better performance. There are also new, more advanced features available in <code>InferencePipeline</code>.</p>"},{"location":"using_inference/inference_pipeline/#new-features-in-inferencepipeline","title":"New Features in <code>InferencePipeline</code>","text":""},{"location":"using_inference/inference_pipeline/#stability","title":"Stability","text":"<p>New implementation allows <code>InferencePipeline</code> to re-connect to a video source, eliminating the need to create additional logic to run inference against streams for long hours in fault-tolerant mode.</p>"},{"location":"using_inference/inference_pipeline/#granularity-of-control","title":"Granularity of control","text":"<p>New implementation let you decide how to handle video sources - and provided automatic selection of mode. Your videos will be processed frame-by-frame with each frame being passed to model, and streams will be processed in a way to provide continuous, up-to-date predictions on the most fresh frames - and the system will automatically adjust to performance of the hardware to ensure best experience.</p>"},{"location":"using_inference/inference_pipeline/#observability","title":"Observability","text":"<p>New implementation allows to create reports about InferencePipeline state in runtime - providing an easy way to build monitoring on top of it.</p>"},{"location":"using_inference/inference_pipeline/#migrate-from-inferencestream-to-inferencepipeline","title":"Migrate from <code>inference.Stream</code> to <code>InferencePipeline</code>","text":"<p>Let's assume you used <code>inference.Stream(...)</code> with your custom handlers:</p> <pre><code>import numpy as np\n\n\ndef on_prediction(predictions: dict, image: np.ndarray) -&gt; None:\n    pass\n</code></pre> <p>Now, the structure of handlers has changed into:</p> <pre><code>import numpy as np\n\ndef on_prediction(predictions, video_frame) -&gt; None:\n    pass\n</code></pre> <p>With predictions being still dict (passed as second parameter) in the same, standard Roboflow format, but <code>video_frame</code> is a dataclass with the following property:</p> <ul> <li><code>image</code>: which is video frame (<code>np.ndarray</code>)</li> <li><code>frame_id</code>: int value representing the place of the frame in stream order</li> <li><code>frame_timestamp</code>: time of frame grabbing - the exact moment when frame appeared in the file/stream   on the receiver side (<code>datetime.datetime</code>)</li> </ul> <p>Additionally, it eliminates the need of grabbing <code>.frame_id</code> from <code>inference.Stream()</code>.</p> <p><code>InferencePipeline</code> exposes interface to manage its state (possibly from different thread) - including functions like <code>.start()</code>, <code>.pause()</code>, <code>.terminate()</code>.</p>"},{"location":"using_inference/inference_pipeline/#migrate-to-changes-introduced-in-v0918","title":"Migrate to changes introduced in <code>v0.9.18</code>","text":"<p>List of changes: 1. <code>VideoFrame</code> got new parameter: <code>source_id</code> - indicating which video source yielded the frame 2. <code>on_prediction</code> callable signature changed: <pre><code>from typing import Callable, Any, Optional, List, Union\nfrom inference.core.interfaces.camera.entities import VideoFrame\n# OLD\nSinkHandler = Callable[[Any, VideoFrame], None]\n\n# NEW\nSinkHandler = Optional[\n    Union[\n        Callable[[Any, VideoFrame], None],\n        Callable[[List[Optional[Any]], List[Optional[VideoFrame]]], None],\n    ]\n]\n</code></pre> this change is non-breaking, as there is new parameter of <code>InferencePipeline.init*()</code> functions - <code>sink_mode</code> with default  value on <code>ADAPTIVE</code> - which forces single video frame and prediction to be provided for sink invocation if one video  only is specified. Old sinks were adjusted to work in dual mode - for instance in the demo you see <code>render_boxes(...)</code>  displaying image tiles.</p> <p>Example: <pre><code>from typing import Union, List, Optional\nimport json\n\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef save_prediction(predictions: dict, file_name: str) -&gt; None:\n  with open(file_name, \"w\") as f:\n    json.dump(predictions, f)\n\ndef on_prediction_old(predictions: dict, video_frame: VideoFrame) -&gt; None:\n  save_prediction(\n    predictions=predictions,\n    file_name=f\"frame_{video_frame.frame_id}.json\"\n  )\n\ndef on_prediction_new(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        save_prediction(\n        predictions=prediction,\n        file_name=f\"source_{frame.source_id}_frame_{frame.frame_id}.json\"\n      )\n</code></pre></p> <ol> <li><code>on_video_frame</code> callable used in InferencePipeline.init_with_custom_logic(...)<code>changed: Previously:</code>InferenceHandler = Callable[[VideoFrame], Any]<code>Now:</code>InferenceHandler = Callable[ListVideoFrame, List[Any]]`</li> </ol> <p>Example: <pre><code>from inference.core.interfaces.camera.entities import VideoFrame\nfrom typing import Any, List\n\nMY_MODEL = ...\n\n# before v0.9.18  \ndef on_video_frame_old(video_frame: VideoFrame) -&gt; Any:\n  return MY_MODEL(video_frame.image)\n\n# after v0.9.18  \ndef on_video_frame_new(video_frames: List[VideoFrame]) -&gt; List[Any]: \n  # result must be returned as list of elements representing model prediction for single frame\n  # with order unchanged.\n  return MY_MODEL([v.image for v in video_frames])\n</code></pre></p> <ol> <li>The interface for <code>PipelineWatchdog</code> changed - and there is also a side effect change in form of pipeline state report  that is emitted being changed.</li> </ol> <p>Old watchdog: <pre><code>class PipelineWatchDog(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def register_video_source(self, video_source: VideoSource) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_inference_started(\n        self, frame_timestamp: datetime, frame_id: int\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_prediction_ready(\n        self, frame_timestamp: datetime, frame_id: int\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_report(self) -&gt; Optional[PipelineStateReport]:\n        pass\n</code></pre></p> <p>New watchdog: <pre><code>class PipelineWatchDog(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def register_video_sources(self, video_sources: List[VideoSource]) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_inference_started(\n        self,\n        frames: List[VideoFrame],\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_prediction_ready(\n        self,\n        frames: List[VideoFrame],\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_report(self) -&gt; Optional[PipelineStateReport]:\n        pass\n</code></pre></p> <p>Old report: <pre><code>@dataclass(frozen=True)\nclass PipelineStateReport:\n    video_source_status_updates: List[StatusUpdate]\n    latency_report: LatencyMonitorReport\n    inference_throughput: float\n    source_metadata: Optional[SourceMetadata]\n</code></pre></p> <p>New report: <pre><code>@dataclass(frozen=True)\nclass PipelineStateReport:\n    video_source_status_updates: List[StatusUpdate]\n    latency_reports: List[LatencyMonitorReport]  # now - one report for each source\n    inference_throughput: float\n    sources_metadata: List[SourceMetadata] # now - one metadata for each source\n</code></pre></p> <p>If there was custom watchdog created on your end - reimplementation should be easy, as all the data passed to methods previously for single video source / frame are now provided for all sources / frames.</p>"},{"location":"using_inference/native_python_api/","title":"Native python api","text":"<p>The native python API is the most simple and involves accessing the base package APIs directly. Going this route, you will import Inference modules directly into your python code. You will load models, run inference, and handle the results all within your own logic. You will also need to manage the dependencies within your python environment. If you are creating a simple app or just testing, the native Python API is a great place to start.</p> <p>Using the native python API centers on loading models, then calling their <code>infer(...)</code> method to get inference results.</p>"},{"location":"using_inference/native_python_api/#quickstart","title":"Quickstart","text":"<p>This example shows how to load a model, run inference, then display the results.</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Next, import a model:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n</code></pre> <p>The <code>get_model</code> method is a utility function which will help us load a computer vision model from Roboflow. We load a model by referencing its <code>model_id</code>. For Roboflow models, the model ID is a combination of a project name and a version number <code>f\"{project_name}/{version_number}\"</code>.</p> <p>Hint</p> <p>You can find your models project name and version number in the Roboflow App. You can also browse public models that are ready to use on Roboflow Universe. In this example, we are using a special model ID that is an alias of a COCO pretrained model on Roboflow Universe. You can see the list of model aliases here.</p> <p>Next, we can run inference with our model by providing an input image:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nresults = model.infer(\"people-walking.jpg\") # replace with path to your image\n</code></pre> <p>The results object is an inference response object. It contains some meta data (e.g. processing time) as well as an array of the predictions. The type of response and its attributes will depend on the type of model. See all of the Inference Response objects.</p> <p>Now, lets visualize the results using Supervision:</p> <pre><code>from inference import get_model\nimport supervision as sv\nimport cv2\n\n# Load model\nmodel = get_model(model_id=\"yolov8x-1280\")\n\n# Load image with cv2\nimage = cv2.imread(\"people-walking.jpg\")\n\n# Run inference\nresults = model.infer(image)[0]\n\n# Load results into Supervision Detection API\ndetections = sv.Detections.from_inference(results)\n\n# Create Supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# Extract labels array from inference results\nlabels = [p.class_name for p in results[0].predictions]\n\n\n\n# Apply results to image using Supervision annotators\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\n# Write annotated image to file or display image\nsv.plot_image(annotated_image)\n</code></pre> <p></p>"},{"location":"using_inference/native_python_api/#different-image-types","title":"Different Image Types","text":"<p>In the previous example, we saw that we can provide different image types to the <code>infer(...)</code> method. The <code>infer(...)</code> method accepts images in many forms including PIL images, OpenCV images (Numpy arrays), paths to local images, image URLs, and more. Under the hood, models use the <code>load_image(...)</code> method in the <code>image_utils</code> module.</p> <pre><code>from inference import get_model\n\nimport cv2\nfrom PIL import Image\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nimage_url = \"https://media.roboflow.com/inference/people-walking.jpg\"\nlocal_image_file = \"people-walking.jpg\"\npil_image = Image.open(local_image_file)\nnumpy_image = cv2.imread(local_image_file)\n\nresults = model.infer(image_url)\n#or     = model.infer(local_image_file)\n#or     = model.infer(pil_image)\n#or     = model.infer(numpy_image)\n</code></pre>"},{"location":"using_inference/native_python_api/#inference-parameters","title":"Inference Parameters","text":"<p>The <code>infer(...)</code> method accepts keyword arguments to set inference parameters. The example below shows setting the confidence threshold and the IoU threshold.</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nresults = model.infer(\"people-walking.jpg\", confidence=0.75, iou_threshold=0.5)\n</code></pre>"},{"location":"workflows/about/","title":"What is Roboflow Workflows?","text":"<p>Roboflow Workflows is an ecosystem that enables users to create machine learning applications using a wide range of pluggable and reusable blocks. These blocks are organized in a way that makes it easy for users to design and connect different components. The graphical interface allows you to visually construct workflows without needing extensive technical expertise. Once the workflow is designed, the Workflows engine runs the application, ensuring all the components work together seamlessly. This provides a rapid transition from prototype to production-ready solutions, allowing you to quickly iterate and deploy applications.</p> <p>Roboflow offers a growing selection of Workflow blocks, and the community can also create new blocks. This ensures that the ecosystem continuously expands and evolves. Moreover, Roboflow provides flexible deployment options, including on-premises and cloud-based solutions, allowing users to deploy their applications in the environment  that best suits their needs.</p> <p>With Workflows, you can:</p> <ul> <li> <p>Detect, classify, and segment objects in images using state-of-the-art models.</p> </li> <li> <p>Use Large Multimodal Models (LMMs) to make determinations at any stage in a workflow.</p> </li> <li> <p>Introduce elements of business logic to translate model predictions into your domain language</p> </li> </ul> Explore all Workflow blocks Begin building with Workflows <p></p> <p>In this section of the documentation, we'll walk through everything you need to know to create and run workflows. Let\u2019s get started!</p> <p>Next, create and run a workflow or browse example Workflows.</p>"},{"location":"workflows/blocks_bundling/","title":"Bundling Workflows blocks","text":"<p>To efficiently manage the Workflows ecosystem, a standardized method for building and distributing blocks is  essential. This allows users to create their own blocks and bundle them into Workflow plugins. A Workflow plugin  is essentially a Python library that implements a defined interface and can be structured in various ways. </p> <p>This page outlines the mandatory interface requirements and suggests a structure for blocks that aligns with  the Workflows versioning guidelines.</p>"},{"location":"workflows/blocks_bundling/#proposed-structure-of-plugin","title":"Proposed structure of plugin","text":"<p>We propose the following structure of plugin:</p> <pre><code>.\n\u251c\u2500\u2500 requirements.txt   # file with requirements\n\u251c\u2500\u2500 setup.py           # use different package creation method if you like\n\u251c\u2500\u2500 {plugin_name}\n\u2502   \u251c\u2500\u2500 __init__.py    # main module that contains loaders\n\u2502   \u251c\u2500\u2500 kinds.py       # optionally - definitions of custom kinds\n\u2502   \u251c\u2500\u2500 {block_name}   # package for your block\n\u2502   \u2502   \u251c\u2500\u2500 v1.py      # version 1 of your block\n\u2502   \u2502   \u251c\u2500\u2500 ...        # ... next versions\n\u2502   \u2502   \u2514\u2500\u2500 v5.py      # version 5 of your block\n\u2502   \u2514\u2500\u2500 {block_name}   # package for another block\n\u2514\u2500\u2500 tests              # tests for blocks\n</code></pre>"},{"location":"workflows/blocks_bundling/#required-interface","title":"Required interface","text":"<p>Plugin must only provide few extensions to <code>__init__.py</code> in main package compared to standard Python library:</p> <ul> <li> <p><code>load_blocks()</code> function to provide list of blocks' classes (required)</p> </li> <li> <p><code>load_kinds()</code> function to return all custom kinds the plugin defines (optional)</p> </li> <li> <p><code>REGISTERED_INITIALIZERS</code> module property which is a dict mapping name of block  init parameter into default value or parameter-free function constructing that value - optional </p> </li> </ul>"},{"location":"workflows/blocks_bundling/#load_blocks-function","title":"<code>load_blocks()</code> function","text":"<p>Function is supposed to enlist all blocks in the plugin - it is allowed to define  a block once.</p> <p>Example:</p> <pre><code>from typing import List, Type\nfrom inference.core.workflows.prototypes.block import WorkflowBlock\n\n# example assumes that your plugin name is `my_plugin` and\n# you defined the blocks that are imported here\nfrom my_plugin.block_1.v1 import Block1V1\nfrom my_plugin.block_2.v1 import Block2V1\n\ndef load_blocks() -&gt; List[Type[WorkflowBlock]]:\n    return [\n        Block1V1,\n        Block2V1,\n]\n</code></pre>"},{"location":"workflows/blocks_bundling/#load_kinds-function","title":"<code>load_kinds()</code> function","text":"<p><code>load_kinds()</code> function to return all custom kinds the plugin defines. It is optional as your blocks may not need custom kinds.</p> <p>Example:</p> <pre><code>from typing import List\nfrom inference.core.workflows.execution_engine.entities.types import Kind\n\n# example assumes that your plugin name is `my_plugin` and\n# you defined the imported kind\nfrom my_plugin.kinds import MY_KIND\n\n\ndef load_kinds() -&gt; List[Kind]:\n    return [MY_KIND]\n</code></pre>"},{"location":"workflows/blocks_bundling/#registered_initializers-dictionary","title":"<code>REGISTERED_INITIALIZERS</code> dictionary","text":"<p>As you know from the docs describing the Workflows Compiler  and the blocks development guide, Workflow blocs are dynamically initialized during compilation and may require constructor  parameters. Those parameters can default to values registered in the <code>REGISTERED_INITIALIZERS</code> dictionary. To expose default a value for an init parameter of your block -  simply register the name of the init param and its value (or a function generating a value) in the dictionary. This is optional part of the plugin interface, as not every block requires a constructor.</p> <p>Example:</p> <pre><code>import os\n\ndef init_my_param() -&gt; str:\n    # do init here\n    return \"some-value\"\n\nREGISTERED_INITIALIZERS = {\n    \"param_1\": 37,\n    \"param_2\": init_my_param,\n}\n</code></pre>"},{"location":"workflows/blocks_bundling/#serializers-and-deserializers-for-kinds","title":"Serializers and deserializers for Kinds","text":"<p>Support for custom serializers and deserializers was introduced in Execution Engine <code>v1.3.0</code>. From that version onward it is possible to point custom functions that  Execution Engine should use to serialize and deserialize any kind.</p> <p>Deserializers will determine how to decode inputs send through the wire  into internal data representation used by blocks. Serializers, on the other hand,  are useful when Workflow results are to be send through the wire.</p> <p>Below you may find example on how to add serializer and deserializer  for arbitrary kind. The code should be placed in main <code>__init__.py</code> of your plugin:</p> <pre><code>from typing import Any\n\ndef serialize_kind(value: Any) -&gt; Any:\n  # place here the code that will be used to\n  # transform internal Workflows data representation into \n  # the external one (that can be sent through the wire in JSON, using\n  # default JSON encoder for Python).\n  pass\n\n\ndef deserialize_kind(parameter_name: str, value: Any) -&gt; Any:\n  # place here the code that will be used to decode \n  # data sent through the wire into the Execution Engine\n  # and transform it into proper internal Workflows data representation\n  # which is understood by the blocks.\n  pass\n\n\nKINDS_SERIALIZERS = {\n    \"name_of_the_kind\": serialize_kind,\n}\nKINDS_DESERIALIZERS = {\n    \"name_of_the_kind\": deserialize_kind,\n}\n</code></pre>"},{"location":"workflows/blocks_bundling/#tips-and-tricks","title":"Tips And Tricks","text":"<ul> <li> <p>Each serializer must be a function taking the value to serialize and returning serialized value (accepted by default Python JSON encoder)</p> </li> <li> <p>Each deserializer must be a function accepting two parameters - name of  Workflow input to be deserialized and the value to be deserialized - the goal  of the function is to align input data with expected internal representation</p> </li> <li> <p>Kinds from <code>roboflow_core</code> plugin already have reasonable serializers and  deserializers</p> </li> <li> <p>If you do not like the way how data is serialized in <code>roboflow_core</code> plugin,  feel free to alter the serialization methods for kinds, simply registering the function in your plugin and loading it to the Execution Engine - the  serializer/deserializer defined as the last one will be in use. </p> </li> </ul>"},{"location":"workflows/blocks_bundling/#enabling-plugin-in-your-workflows-ecosystem","title":"Enabling plugin in your Workflows ecosystem","text":"<p>To load a plugin you must:</p> <ul> <li> <p>install the Python package with the plugin in the environment you run Workflows</p> </li> <li> <p>export an environment variable named <code>WORKFLOWS_PLUGINS</code> set to a comma-separated list of names of plugins you want to load. </p> </li> <li> <p>Example: to load two plugins <code>plugin_a</code> and <code>plugin_b</code>, you need to run    <code>export WORKFLOWS_PLUGINS=\"plugin_a,plugin_b\"</code></p> </li> </ul>"},{"location":"workflows/blocks_connections/","title":"Rules dictating which blocks can be connected","text":"<p>A natural question you might ask is: How do I know which blocks to connect to achieve my desired outcome?  This is a crucial question, which is why we've created auto-generated  documentation for all supported Workflow blocks. In this guide, we\u2019ll show you how to use  these docs effectively and explain key details that help you understand why certain connections between  blocks are possible, while others may not be.</p> <p>Note</p> <p>Using the Workflows UI in the Roboflow app, you may find compatible connections between steps automatically, without any input from you. This page briefly explains how to determine whether two blocks can be connected, making it possible to connect steps manually if needed. This page appears before the link to the block gallery because it explains how to use those docs effectively. It also introduces references to concepts covered in the User and Developer Guide. Please continue reading those sections if you find some concepts presented here need further explanation.</p>"},{"location":"workflows/blocks_connections/#navigating-the-blocks-documentation","title":"Navigating the blocks documentation","text":"<p>When you open the blocks documentation, you\u2019ll see a list of all blocks supported by Roboflow. Each block entry  includes a name, brief description, category, and license for the block. You can click on any block to see more  detailed information.</p> <p>On the block details page, you\u2019ll find documentation for all supported versions of that block,  starting with the latest version. For each version, you\u2019ll see:</p> <ul> <li> <p>detailed description of the block</p> </li> <li> <p>type identifier, which is required in Workflow definitions to identify the specific block used for a step</p> </li> <li> <p>table of configuration properties, listing the fields that can be specified in a Workflow definition,  including their types, descriptions, and whether they can accept a dynamic selector or just a fixed value.</p> </li> <li> <p>Available connections, showing which blocks can provide inputs to this block and which can use its outputs.</p> </li> <li> <p>A list of input and output bindings:</p> </li> <li> <p>input bindings are the names of step definition properties that can hold selectors, along with the type    (or <code>kind</code>) of data they pass.</p> </li> <li> <p>output bindings are names and kinds for block outputs that can be used as inputs by steps defined in    the Workflow definition</p> </li> <li> <p>An example of a Workflow step based on the documented block.</p> </li> </ul> <p>The <code>kind</code> mentioned above refers to the type of data flowing through the connection during execution,  and this is further explained in the developer guide.</p>"},{"location":"workflows/blocks_connections/#what-makes-connections-valid","title":"What makes connections valid?","text":"<p>Each block provides a manifest that lists the fields to be included in the Workflow definition when creating a step. The values of these fields in a Workflow definition may contain:</p> <ul> <li> <p>References (selectors) to data the block will process, such as step outputs or  batch-oriented workflow inputs</p> </li> <li> <p>Configuration values: Specific settings for the step or references (selectors) that  provide configuration parameters dynamically during execution.</p> </li> </ul> <p>The manifest also includes the block's outputs.</p> <p>For each step definition field (if it can hold a selector) and step output,  the expected kind is specified. A kind is a high-level definition  of the type of data that will be passed during workflow execution. Simply put, it describes the data that  will replace the selector during block execution.</p> <p>To ensure steps are correctly connected, the Workflow Compiler checks if the input and output kinds match. If they do, the connection is valid.</p> <p>Additionally, the <code>dimensionality level</code> of the data is considered when  validating connections. This ensures that data from multiple sources is compatible across the entire Workflow,  not just between two connected steps. More details on dimensionality levels can be found in the  user guide describing workflow execution.</p>"},{"location":"workflows/blocks_gallery_template/","title":"Blocks gallery template","text":"Blocks"},{"location":"workflows/create_and_run/","title":"How to Create and Run a Workflow","text":"<p>In this example, we are going to build a Workflow from scratch that detects dogs, classifies their breeds, and visualizes the results.</p>"},{"location":"workflows/create_and_run/#step-1-create-a-workflow","title":"Step 1: Create a Workflow","text":"<p>Open https://app.roboflow.com/ in your browser and navigate to the Workflows tab, then click the Create Workflow button. Select Custom Workflow to start the creation process.</p> <p></p>"},{"location":"workflows/create_and_run/#step-2-add-an-object-detection-model","title":"Step 2: Add an object detection model","text":"<p>We need to add a block with an object detection model to the existing workflow. We will use the <code>yolov8n-640</code> model.</p> <p></p>"},{"location":"workflows/create_and_run/#step-3-crop-each-detected-object-to-run-breed-classification","title":"Step 3: Crop each detected object to run breed classification","text":"<p>Next, we'll add a block to our Workflow that crops the objects detected by our first model.</p> <p></p>"},{"location":"workflows/create_and_run/#step-4-classify-dog-breeds-with-second-stage-model","title":"Step 4: Classify dog breeds with second stage model","text":"<p>We will then add a classification model that runs on each crop to classify its content. We will use the Roboflow Universe model <code>dog-breed-xpaq6/1</code>. Ensure that in the block configuration the <code>Image</code> property points to the <code>crops</code> output of the Dynamic Crop block.</p> <p></p>"},{"location":"workflows/create_and_run/#step-5-replace-bounding-box-classes-with-classification-model-predictions","title":"Step 5: Replace Bounding Box classes with classification model predictions","text":"<p>When each crop is classified, we want to assign the predicted class (dog breed) to the bounding boxes from the object detection model. To do this, we use the Detections Classes Replacement block, which accepts a reference to the object detection model predictions as well as a reference to the classification results on the crops.</p> <p></p>"},{"location":"workflows/create_and_run/#step-6-visualise-predictions","title":"Step 6: Visualise predictions","text":"<p>As a final step of the workflow, we want to visualize our predictions. We'll use two visualization blocks: Bounding Box Visualization and Label Visualization chained together. First, add Bounding Box Visualization and refer to <code>$inputs.image</code> for the Image property (that's the image sent as input to the workflow). The second step (Label Visualization) should point to the output of the Bounding Box Visualization step. Both visualization steps should refer to predictions from the Detections Classes Replacement step.</p> <p></p>"},{"location":"workflows/create_and_run/#step-7-construct-output","title":"Step 7: Construct output","text":"<p>You now have everything ready to construct your workflow output. You can use any intermediate step output that you need, but in this example we will only select bounding boxes with replaced classes (output from the Detections Classes Replacement step) and visualization (output from the Label Visualization step).</p>"},{"location":"workflows/create_and_run/#step-8-running-the-workflow","title":"Step 8: Running the workflow","text":"<p>Now your workflow is ready. You can click the <code>Save</code> button and move to the <code>Run Preview</code> panel.</p> <p>We will run our workflow against the following example image: <code>https://media.roboflow.com/inference/dog.jpeg</code>. Here are the results:</p> <p></p> <p>Click the <code>Show Visual</code> button to see the results of our visualization efforts.</p>"},{"location":"workflows/create_and_run/#different-ways-of-running-your-workflow","title":"Different ways of running your workflow","text":"<p>Your workflow is now saved on the Roboflow platform. This means you can run it in several ways, including:</p> <ul> <li> <p>HTTP request to Roboflow Hosted API</p> </li> <li> <p>an HTTP request to your local instance of the <code>inference</code> server</p> </li> <li> <p>on video</p> </li> </ul> <p>To see code snippets, click the <code>Deploy Workflow</code> button:</p>"},{"location":"workflows/create_and_run/#workflow-definition-for-quick-reproduction","title":"Workflow definition for quick reproduction","text":"<p>To make it easier to reproduce the workflow, below you can find a workflow definition you can copy-paste to UI editor.</p> Workflow definition <pre><code>{\n  \"version\": \"1.0\",\n  \"inputs\": [\n    {\n      \"type\": \"InferenceImage\",\n      \"name\": \"image\"\n    }\n  ],\n  \"steps\": [\n    {\n      \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n      \"name\": \"model\",\n      \"images\": \"$inputs.image\",\n      \"model_id\": \"yolov8n-640\"\n    },\n    {\n      \"type\": \"roboflow_core/dynamic_crop@v1\",\n      \"name\": \"dynamic_crop\",\n      \"images\": \"$inputs.image\",\n      \"predictions\": \"$steps.model.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n      \"name\": \"model_1\",\n      \"images\": \"$steps.dynamic_crop.crops\",\n      \"model_id\": \"dog-breed-xpaq6/1\"\n    },\n    {\n      \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n      \"name\": \"detections_classes_replacement\",\n      \"object_detection_predictions\": \"$steps.model.predictions\",\n      \"classification_predictions\": \"$steps.model_1.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n      \"name\": \"bounding_box_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$inputs.image\"\n    },\n    {\n      \"type\": \"roboflow_core/label_visualization@v1\",\n      \"name\": \"label_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$steps.bounding_box_visualization.image\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"detections\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.detections_classes_replacement.predictions\"\n    },\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"visualisation\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.label_visualization.image\"\n    }\n  ]\n}\n</code></pre>"},{"location":"workflows/create_and_run/#next-steps","title":"Next Steps","text":"<p>Now that you have created and run your first workflow, you can explore our other supported blocks and create a more complex workflow.</p> <p>Refer to our Supported Blocks documentation to learn more about what blocks are supported. We also recommend reading the Understanding workflows page.</p>"},{"location":"workflows/create_workflow_block/","title":"Creating Workflow blocks","text":"<p>Workflows blocks development requires an understanding of the Workflow Ecosystem. Before diving deeper into the details, let's summarize the  required knowledge:</p> <p>Understanding of Workflow execution, in particular:</p> <ul> <li> <p>what is the relation of Workflow blocks and steps in Workflow definition</p> </li> <li> <p>how Workflow blocks and their manifests are used by Workflows Compiler</p> </li> <li> <p>what is the <code>dimensionality level</code> of batch-oriented data passing through Workflow</p> </li> <li> <p>how Execution Engine interacts with step, regarding  its inputs and outputs</p> </li> <li> <p>what is the nature and role of Workflow <code>kinds</code></p> </li> <li> <p>understanding how <code>pydantic</code> works</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#environment-setup","title":"Environment setup","text":"<p>As you will soon see, creating a Workflow block is simply a matter of defining a Python class that implements  a specific interface. This design allows you to run the block using the Python interpreter, just like any  other Python code. However, you may encounter difficulties when assembling all the required inputs, which would  normally be provided by other blocks during Workflow execution. Therefore, it's important to set up the development  environment properly for a smooth workflow. We recommend following these steps as part of the standard development  process (initial steps can be skipped for subsequent contributions):</p> <ol> <li> <p>Set up the <code>conda</code> environment and install main dependencies of <code>inference</code>, as described in <code>inference</code> contributor guide.</p> </li> <li> <p>Familiarize yourself with the organization of the Workflows codebase.</p> Workflows codebase structure - cheatsheet <p>Below are the key packages and directories in the Workflows codebase, along with their descriptions:</p> <ul> <li> <p><code>inference/core/workflows</code> - the main package for Workflows.</p> </li> <li> <p><code>inference/core/workflows/core_steps</code> - contains Workflow blocks that are part of the Roboflow Core plugin. At the top levels, you'll find block categories, and as you go deeper, each block has its own package, with modules hosting different versions, starting from <code>v1.py</code></p> </li> <li> <p><code>inference/core/workflows/execution_engine</code> - contains the Execution Engine. You generally won\u2019t need to modify this package unless contributing to Execution Engine functionality.</p> </li> <li> <p><code>tests/workflows/</code> - the root directory for Workflow tests</p> </li> <li> <p><code>tests/workflows/unit_tests/</code> - suites of unit tests for the Workflows Execution Engine and core blocks. This is where you can test utility functions used in your blocks.</p> </li> <li> <p><code>tests/workflows/integration_tests/</code> - suites of integration tests for the Workflows Execution Engine and core blocks. You can run end-to-end (E2E) tests of your workflows in combination with other blocks here.</p> </li> </ul> </li> <li> <p>Create a minimalistic block \u2013 You\u2019ll learn how to do this in the following sections. Start by implementing a simple block manifest and basic logic to ensure the block runs as expected.</p> </li> <li> <p>Add the block to the plugin \u2013 Once your block is created, add it to the list of blocks exported from the plugin. If you're adding the block to the Roboflow Core plugin, make sure to add an entry for your block in the loader.py. If you forget this step, your block won\u2019t be visible!</p> </li> <li> <p>Iterate and refine your block \u2013 Continue developing and running your block until you\u2019re satisfied with the results. The sections below explain how to iterate on your block in various scenarios.</p> </li> </ol>"},{"location":"workflows/create_workflow_block/#running-your-blocks-using-workflows-ui","title":"Running your blocks using Workflows UI","text":"<p>We recommend running the inference server with a mounted volume (which is much faster than re-building <code>inference</code>  server on each change):</p> <p><pre><code>inference_repo$ docker run -p 9001:9001 \\\n   -v ./inference:/app/inference \\\n   roboflow/roboflow-inference-server-cpu:latest\n</code></pre> and connecting your local server to Roboflow UI:</p> <p>to quickly run previews:</p> My block requires extra dependencies - I cannot use pre-built <code>inference</code> server <p>It's natural that your blocks may sometimes require additional dependencies. To add a dependency, simply include it in one of the  requirements fileshat are installed in the relevant Docker image  (usually the CPU build  of the <code>inference</code> server).</p> <p>Afterward, run:</p> <pre><code>inference_repo$ docker build \\\n   -t roboflow/roboflow-inference-server-cpu:test \\ \n   -f docker/dockerfiles/Dockerfile.onnx.cpu .\n</code></pre> <p>You can then run your local build by specifying the test tag you just created:</p> <pre><code>inference_repo$ inference_repo$ docker run -p 9001:9001 \\\n   -v ./inference:/app/inference \\\n   roboflow/roboflow-inference-server-cpu:test\n</code></pre>"},{"location":"workflows/create_workflow_block/#running-your-blocks-without-workflows-ui","title":"Running your blocks without Workflows UI","text":"<p>For contributors without access to the Roboflow platform, we recommend running the server as mentioned in the  section above. However, instead of using the UI editor, you will need to create a simple Workflow definition and  send a request to the server.</p> Running your Workflow without UI <p>The following code snippet demonstrates how to send a request to the <code>inference</code> server to run a Workflow.  The <code>inference_sdk</code> is included with the <code>inference</code> package as a lightweight client library for our server.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nYOUR_WORKFLOW_DEFINITION = ...\n\nclient = InferenceHTTPClient(\n    api_url=object_detection_service_url,\n    api_key=\"XXX\",  # optional, only required if Workflow uses Roboflow Platform\n)\nresult = client.run_workflow(\n    specification=YOUR_WORKFLOW_DEFINITION,\n    images={\n        \"image\": your_image_np,   # this is example input, adjust it\n    },\n    parameters={\n        \"my_parameter\": 37,   # this is example input, adjust it\n    },\n)\n</code></pre>"},{"location":"workflows/create_workflow_block/#recommended-way-for-regular-contributors","title":"Recommended way for regular contributors","text":"<p>Creating integration tests in the <code>tests/workflows/integration_tests/execution</code> directory is a natural part of the  development iteration process. This approach allows you to develop and test simultaneously, providing valuable  feedback as you refine your code. Although it requires some experience, it significantly enhances  long-term code maintenance.</p> <p>The process is straightforward:</p> <ol> <li> <p>Create a New Test Module: For example, name it <code>test_workflows_with_my_custom_block.py</code>.</p> </li> <li> <p>Develop Example Workflows: Create one or more example Workflows. It would be best if your block cooperates  with other blocks from the ecosystem. </p> </li> <li> <p>Run Tests with Sample Data: Execute these Workflows in your tests using sample data  (you can explore our  fixtures to find example data we usually use).</p> </li> <li> <p>Assert Expected Results: Validate that the results match your expectations.</p> </li> </ol> <p>By incorporating testing into your development flow, you ensure that your block remains stable over time and  effectively interacts with existing blocks, enhancing the expressiveness of your work!</p> <p>You can run your test using the following command:</p> <p><pre><code>pytest tests/workflows/integration_tests/execution/test_workflows_with_my_custom_block\n</code></pre> Feel free to reference other tests for examples or use the following template:</p> Integration test template <pre><code>def test_detection_plus_classification_workflow_when_XXX(\n    model_manager: ModelManager,\n    dogs_image: np.ndarray, \n    roboflow_api_key: str,\n) -&gt; None:\n    # given\n    workflow_init_parameters = {\n        \"workflows_core.model_manager\": model_manager,\n        \"workflows_core.api_key\": roboflow_api_key,\n        \"workflows_core.step_execution_mode\": StepExecutionMode.LOCAL,\n    }\n    execution_engine = ExecutionEngine.init(\n        workflow_definition=&lt;YOUR-EXAMPLE-WORKLFOW&gt;,\n        init_parameters=workflow_init_parameters,\n        max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n    )\n\n    # when\n    result = execution_engine.run(\n        runtime_parameters={\n            \"image\": dogs_image,\n        }\n    )\n\n    # then\n    assert isinstance(result, list), \"Expected list to be delivered\"\n    assert len(result) == 1, \"Expected 1 element in the output for one input image\"\n    assert set(result[0].keys()) == {\n        \"predictions\",\n    }, \"Expected all declared outputs to be delivered\"\n    assert (\n        len(result[0][\"predictions\"]) == 2\n    ), \"Expected 2 dogs crops on input image, hence 2 nested classification results\"\n    assert [result[0][\"predictions\"][0][\"top\"], result[0][\"predictions\"][1][\"top\"]] == [\n        \"116.Parson_russell_terrier\",\n        \"131.Wirehaired_pointing_griffon\",\n    ], \"Expected predictions to be as measured in reference run\"\n</code></pre> <ul> <li> <p>In line <code>2</code>, you\u2019ll find the <code>model_manager</code> fixture, which is typically required by model blocks. This fixture provides the <code>ModelManager</code> abstraction from <code>inference</code>, used for loading and unloading models.</p> </li> <li> <p>Line <code>3</code> defines a fixture that includes an image of two dogs (explore other fixtures to find more example images).</p> </li> <li> <p>Line <code>4</code> is an optional fixture you may want to use if any of the blocks in your tested workflow require a Roboflow API key. If that\u2019s the case, export the <code>ROBOFLOW_API_KEY</code> environment variable with a valid key before running the test.</p> </li> <li> <p>Lines <code>7-11</code> provide the setup for the initialization parameters of the blocks that the Execution Engine will create at runtime, based on your Workflow definition.</p> </li> <li> <p>Lines <code>19-23</code> demonstrate how to run a Workflow by injecting input parameters. Please ensure that the keys in runtime_parameters match the inputs declared in your Workflow definition.</p> </li> <li> <p>Starting from line <code>26</code>, you\u2019ll find example assertions within the test.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#prototypes","title":"Prototypes","text":"<p>To create a Workflow block you need some amount of imports from the core of Workflows library. Here is the list of imports that you may find useful while creating a block:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import (\n    Batch,  # batches of data will come in Batch[X] containers\n    OutputDefinition,  # class used to declare outputs in your manifest\n    WorkflowImageData,  # internal representation of image\n    # - use whenever your input kind is image\n)\n\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,  # type alias for result of `run(...)` method\n    WorkflowBlock,  # base class for your block\n    WorkflowBlockManifest,  # base class for block manifest\n)\n\nfrom inference.core.workflows.execution_engine.entities.types import *  \n# module with `kinds` from the core library\n</code></pre> <p>The most important are:</p> <ul> <li> <p><code>WorkflowBlock</code> - base class for your block</p> </li> <li> <p><code>WorkflowBlockManifest</code> - base class for block manifest</p> </li> </ul> <p>Understanding internal data representation</p> <p>You may have noticed that we recommend importing the <code>Batch</code> and <code>WorkflowImageData</code> classes, which are    fundamental components used when constructing building blocks in our system. For a deeper understanding of    how these classes fit into the overall architecture, we encourage you to refer to the    Data Representations page for more detailed information. </p>"},{"location":"workflows/create_workflow_block/#block-manifest","title":"Block manifest","text":"<p>A manifest is a crucial component of a Workflow block that defines a prototype  for step declaration that can be placed in a Workflow definition to use the block.  In particular, it: </p> <ul> <li> <p>Uses <code>pydantic</code> to power syntax parsing of Workflows definitions:  It inherits from  <code>pydantic BaseModel</code> features to parse and  validate Workflow definitions. This schema can also be automatically exported to a format compatible with the  Workflows UI, thanks to <code>pydantic's</code> integration with the OpenAPI standard.</p> </li> <li> <p>Defines Data Bindings: It specifies which fields in the manifest are selectors for data flowing through  the workflow during execution and indicates their kinds.</p> </li> <li> <p>Describes Block Outputs: It outlines the outputs that the block will produce.</p> </li> <li> <p>Specifies Dimensionality: It details the properties related to input and output dimensionality.</p> </li> <li> <p>Indicates Batch Inputs and Empty Values: It informs the Execution Engine whether the step accepts batch  inputs and empty values.</p> </li> <li> <p>Ensures Compatibility: It dictates the compatibility with different Execution Engine versions to maintain  stability. For more details, see versioning.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#scaffolding-for-manifest","title":"Scaffolding for manifest","text":"<p>To understand how manifests work, let's define one step-by-step. The example block that we build here will be  calculating images similarity. We start from imports and class scaffolding:</p> <pre><code>from typing import Literal\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n</code></pre> <p>This is the minimal representation of a manifest. It defines two special fields that are important for  Compiler and Execution engine:</p> <ul> <li> <p><code>type</code> - required to parse syntax of Workflows definitions based on dynamic pool of blocks - this is the  <code>pydantic</code> type discriminator that lets the Compiler understand which block manifest is to be verified when  parsing specific steps in a Workflow definition</p> </li> <li> <p><code>name</code> - this property will be used to give the step a unique name and let other steps selects it via selectors</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#adding-inputs","title":"Adding inputs","text":"<p>We want our step to take two inputs with images to be compared.</p> Adding inputs <p>Let's see how to add definitions of those inputs to manifest: </p> <pre><code>from typing import Literal, Union\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n)\n\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    # all properties apart from `type` and `name` are treated as either \n    # hardcoded parameters or data selectors. Data selectors are strings \n    # that start from `$steps.` or `$inputs.` marking references for data \n    # available in runtime - in this case we usually specify kinds of data\n    # to let compiler know what we expect the data to look like.\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n</code></pre> <ul> <li> <p>in the lines <code>2-9</code>, we've added a couple of imports to ensure that we have everything needed</p> </li> <li> <p>line <code>20</code> defines <code>image_1</code> parameter - as manifest is prototype for Workflow Definition,  the only way to tell about image to be used by step is to provide selector - we have  a specialised type in core library that can be used - <code>Selector</code>. If you look deeper into codebase, you will discover this is type alias constructor function - telling <code>pydantic</code> to expect string matching <code>$inputs.{name}</code> and <code>$steps.{name}.*</code> patterns respectively, additionally providing  extra schema field metadata that tells Workflows ecosystem components that the <code>kind</code> of data behind selector is  image. important note: we denote kind as list - the list of specific kinds  is interpreted as union of kinds by Execution Engine.</p> </li> <li> <p>denoting <code>pydantic</code> <code>Field(...)</code> attribute in the last parts of line <code>20</code> is optional, yet appreciated,  especially for blocks intended to cooperate with Workflows UI </p> </li> <li> <p>starting in line <code>23</code>, you can find definition of <code>image_2</code> parameter which is very similar to <code>image_1</code>.</p> </li> </ul> <p>Such definition of manifest can handle the following step declaration in Workflow definition:</p> <pre><code>{\n  \"type\": \"my_plugin/images_similarity@v1\",\n  \"name\": \"my_step\",\n  \"image_1\": \"$inputs.my_image\",\n  \"image_2\": \"$steps.image_transformation.image\"\n}\n</code></pre> <p>This definition will make the Compiler and Execution Engine:</p> <ul> <li> <p>initialize the step from Workflow block declaring type <code>my_plugin/images_similarity@v1</code></p> </li> <li> <p>supply two parameters for the steps run method:</p> </li> <li> <p><code>input_1</code> of type <code>WorkflowImageData</code> which will be filled with image submitted as Workflow execution input    named <code>my_image</code>.</p> </li> <li> <p><code>imput_2</code> of type <code>WorkflowImageData</code> which will be generated at runtime, by another step called    <code>image_transformation</code></p> </li> </ul>"},{"location":"workflows/create_workflow_block/#adding-parameters-to-the-manifest","title":"Adding parameters to the manifest","text":"<p>Let's now add the parameter that will influence step execution.</p> Adding parameter to the manifest <pre><code>from typing import Literal, Union\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n    FLOAT_ZERO_TO_ONE_KIND,\n)\n\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    # all properties apart from `type` and `name` are treated as either \n    # hardcoded parameters or data selectors. Data selectors are strings \n    # that start from `$steps.` or `$inputs.` marking references for data \n    # available in runtime - in this case we usually specify kinds of data\n    # to let compiler know what we expect the data to look like.\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        float,\n        Selector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n</code></pre> <ul> <li> <p>line <code>9</code> imports <code>float_zero_to_one</code> <code>kind</code>    definition which will be used to define the parameter.</p> </li> <li> <p>in line <code>27</code> we start defining parameter called <code>similarity_threshold</code>. Manifest will accept  either float values or selector to workflow input of <code>kind</code> <code>float_zero_to_one</code>, imported in line <code>9</code>.</p> </li> </ul> <p>Such definition of manifest can handle the following step declaration in Workflow definition:</p> <pre><code>{\n  \"type\": \"my_plugin/images_similarity@v1\",\n  \"name\": \"my_step\",\n  \"image_1\": \"$inputs.my_image\",\n  \"image_2\": \"$steps.image_transformation.image\",\n  \"similarity_threshold\": \"$inputs.my_similarity_threshold\"\n}\n</code></pre> <p>or alternatively:</p> <pre><code>{\n  \"type\": \"my_plugin/images_similarity@v1\",\n  \"name\": \"my_step\",\n  \"image_1\": \"$inputs.my_image\",\n  \"image_2\": \"$steps.image_transformation.image\",\n  \"similarity_threshold\": 0.5\n}\n</code></pre>"},{"location":"workflows/create_workflow_block/#declaring-block-outputs","title":"Declaring block outputs","text":"<p>We have successfully defined inputs for our block, but we are still missing couple of elements required to  successfully run blocks. Let's define block outputs.</p> Declaring block outputs <p>Minimal set of information required is outputs description. Additionally,  to increase block stability, we advise to provide information about execution engine  compatibility.</p> <pre><code>from typing import Literal, Union\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        float,\n        Selector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          )\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n</code></pre> <ul> <li> <p>line <code>5</code> imports class that is used to describe step outputs</p> </li> <li> <p>line <code>11</code> imports <code>boolean</code> <code>kind</code> to be used  in outputs definitions</p> </li> <li> <p>lines <code>32-39</code> declare class method to specify outputs from the block -  each entry in list declare one return property for each batch element and its <code>kind</code>. Our block will return boolean flag <code>images_match</code> for each pair of images.</p> </li> <li> <p>lines <code>41-43</code> declare compatibility of the block with Execution Engine - see versioning page for more details</p> </li> </ul> <p>As a result of those changes:</p> <ul> <li> <p>Execution Engine would understand that steps created based on this block  are supposed to deliver specified outputs and other steps can refer to those outputs in their inputs</p> </li> <li> <p>the blocks loading mechanism will not load the block given that Execution Engine is not in version <code>v1</code></p> </li> </ul> LEARN MORE: Dynamic outputs <p>Some blocks may not be able to arbitrailry define their outputs using  classmethod - regardless of the content of step manifest that is available after  parsing. To support this we introduced the following convention:</p> <ul> <li> <p>classmethod <code>describe_outputs(...)</code> shall return list with one element of  name <code>*</code> and kind <code>*</code> (aka <code>WILDCARD_KIND</code>)</p> </li> <li> <p>additionally, block manifest should implement instance method <code>get_actual_outputs(...)</code> that provides list of actual outputs that can be generated based on filled manifest data </p> </li> </ul> <pre><code>from typing import Literal, Union, List, Optional\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n    FloatZeroToOne,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n    WILDCARD_KIND,\n)\n\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        float,\n        Selector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n    outputs: List[str]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"*\", \n            kind=[WILDCARD_KIND],\n          ),\n        ]\n\n    def get_actual_outputs(self) -&gt; List[OutputDefinition]:\n        # here you have access to `self`:\n        return [\n          OutputDefinition(name=e, kind=[BOOLEAN_KIND])\n          for e in self.outputs\n        ]\n</code></pre>"},{"location":"workflows/create_workflow_block/#definition-of-block-class","title":"Definition of block class","text":"<p>At this stage, the manifest of our simple block is ready, we will continue  with our example. You can check out the advanced topics section for more details that would just  be a distractions now.</p>"},{"location":"workflows/create_workflow_block/#base-implementation","title":"Base implementation","text":"<p>Having the manifest ready, we can prepare baseline implementation of the  block.</p> Block scaffolding <pre><code>from typing import Literal, Union, List, Optional, Type\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n    FloatZeroToOne,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        Selector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass ImagesSimilarityBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ImagesSimilarityManifest\n\n    def run(\n        self,\n        image_1: WorkflowImageData,\n        image_2: WorkflowImageData,\n        similarity_threshold: float,\n    ) -&gt; BlockResult:\n        pass\n</code></pre> <ul> <li> <p>lines <code>1</code>, <code>5-6</code> and <code>8-11</code> added changes into import surtucture to  provide additional symbols required to properly define block class and all of its methods signatures</p> </li> <li> <p>lines <code>53-55</code> defines class method <code>get_manifest(...)</code> to simply return  the manifest class we cretaed earlier</p> </li> <li> <p>lines <code>57-63</code> define <code>run(...)</code> function, which Execution Engine will invoke with data to get desired results. Please note that  manifest fields defining inputs of image kind are marked as <code>WorkflowImageData</code> - which is compliant with intenal data  representation of <code>image</code> kind described in kind documentation.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#providing-implementation-for-block-logic","title":"Providing implementation for block logic","text":"<p>Let's now add an example implementation of  the <code>run(...)</code> method to our block, such that it can produce meaningful results.</p> <p>Note</p> <p>The Content of this section is supposed to provide examples on how to interact  with the Workflow ecosystem as block creator, rather than providing robust  implementation of the block.</p> Implementation of <code>run(...)</code> method <pre><code>from typing import Literal, Union, List, Optional, Type\nfrom pydantic import Field\nimport cv2\n\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n    FloatZeroToOne,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        Selector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass ImagesSimilarityBlock(WorkflowBlock):\n\n    def __init__(self):\n        self._sift = cv2.SIFT_create()\n        self._matcher = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ImagesSimilarityManifest\n\n    def run(\n        self,\n        image_1: WorkflowImageData,\n        image_2: WorkflowImageData,\n        similarity_threshold: float,\n    ) -&gt; BlockResult:\n        image_1_gray = cv2.cvtColor(image_1.numpy_image, cv2.COLOR_BGR2GRAY)\n        image_2_gray = cv2.cvtColor(image_2.numpy_image, cv2.COLOR_BGR2GRAY)\n        kp_1, des_1 = self._sift.detectAndCompute(image_1_gray, None)\n        kp_2, des_2 = self._sift.detectAndCompute(image_2_gray, None)\n        matches = self._matcher.knnMatch(des_1, des_2, k=2)\n        good_matches = []\n        for m, n in matches:\n            if m.distance &lt; similarity_threshold * n.distance:\n                good_matches.append(m)\n        return {\n            \"images_match\": len(good_matches) &gt; 0,\n        }\n</code></pre> <ul> <li> <p>in line <code>3</code> we import OpenCV</p> </li> <li> <p>lines <code>55-57</code> defines block constructor, thanks to this - state of block  is initialised once and live through consecutive invocation of <code>run(...)</code> method - for  instance when Execution Engine runs on consecutive frames of video</p> </li> <li> <p>lines <code>69-80</code> provide implementation of block functionality - the details are trully not important regarding Workflows ecosystem, but there are few details you should focus:</p> <ul> <li> <p>lines <code>69</code> and <code>70</code> make use of <code>WorkflowImageData</code> abstraction, showcasing how  <code>numpy_image</code> property can be used to get <code>np.ndarray</code> from internal representation of images in Workflows. We advise to expole remaining properties of <code>WorkflowImageData</code> to discover more.</p> </li> <li> <p>result of workflow block execution, declared in lines <code>78-80</code> is in our case just a dictionary  with the keys being the names of outputs declared in manifest, in line <code>43</code>. Be sure to provide all declared outputs - otherwise Execution Engine will raise error.</p> </li> </ul> </li> </ul>"},{"location":"workflows/create_workflow_block/#exposing-block-in-plugin","title":"Exposing block in <code>plugin</code>","text":"<p>Now, your block is ready to be used, but Execution Engine is not aware of its existence. This is because no registered  plugin exports the block you just created. Details of blocks bundling are be covered in separate page,  but the remaining thing to do is to add block class into list returned from your plugins' <code>load_blocks(...)</code> function:</p> <pre><code># __init__.py of your plugin (or roboflow_core plugin if you contribute directly to `inference`)\n\nfrom my_plugin.images_similarity.v1 import  ImagesSimilarityBlock  \n# this is example import! requires adjustment\n\ndef load_blocks():\n    return [ImagesSimilarityBlock]\n</code></pre>"},{"location":"workflows/create_workflow_block/#advanced-topics","title":"Advanced topics","text":""},{"location":"workflows/create_workflow_block/#blocks-processing-batches-of-inputs","title":"Blocks processing batches of inputs","text":"<p>Sometimes, performance of your block may benefit if all input data is processed at once as batch. This may happen for models running on GPU. Such mode of operation is supported for Workflows blocks - here is the example on how to use it for your block.</p> Implementation of blocks accepting batches <pre><code>from typing import Literal, Union, List, Optional, Type\nfrom pydantic import Field\nimport cv2\n\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n    Batch,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    IMAGE_KIND,\n    FloatZeroToOne,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Selector(kind=[IMAGE_KIND]) = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        Selector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def get_parameters_accepting_batches(cls) -&gt; bool:\n        return [\"image_1\", \"image_2\"]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass ImagesSimilarityBlock(WorkflowBlock):\n\n    def __init__(self):\n        self._sift = cv2.SIFT_create()\n        self._matcher = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ImagesSimilarityManifest\n\n    def run(\n        self,\n        image_1: Batch[WorkflowImageData],\n        image_2: Batch[WorkflowImageData],\n        similarity_threshold: float,\n    ) -&gt; BlockResult:\n        results = []\n        for image_1_element, image_2_element in zip(image_1, image_2): \n          image_1_gray = cv2.cvtColor(image_1_element.numpy_image, cv2.COLOR_BGR2GRAY)\n          image_2_gray = cv2.cvtColor(image_2_element.numpy_image, cv2.COLOR_BGR2GRAY)\n          kp_1, des_1 = self._sift.detectAndCompute(image_1_gray, None)\n          kp_2, des_2 = self._sift.detectAndCompute(image_2_gray, None)\n          matches = self._matcher.knnMatch(des_1, des_2, k=2)\n          good_matches = []\n          for m, n in matches:\n              if m.distance &lt; similarity_threshold * n.distance:\n                  good_matches.append(m)\n          results.append({\"images_match\": len(good_matches) &gt; 0})\n        return results\n</code></pre> <ul> <li> <p>line <code>13</code> imports <code>Batch</code> from core of workflows library - this class represent container which is  veri similar to list (but read-only) to keep batch elements</p> </li> <li> <p>lines <code>40-42</code> define class method that changes default behaviour of the block and make it capable  to process batches - we are marking each parameter that the <code>run(...)</code> method recognizes as batch-oriented. </p> </li> <li> <p>changes introduced above made the signature of <code>run(...)</code> method to change, now <code>image_1</code> and <code>image_2</code> are not instances of <code>WorkflowImageData</code>, but rather batches of elements of this type. Important note:  having multiple batch-oriented parameters we expect that those batches would have the elements related to each other at corresponding positions - such that our block comparing <code>image_1[1]</code> into <code>image_2[1]</code> actually performs logically meaningful operation.</p> </li> <li> <p>lines <code>74-77</code>, <code>85-86</code> present changes that needed to be introduced to run processing across all batch  elements - showcasing how to iterate over batch elements if needed</p> </li> <li> <p>it is important to note how outputs are constructed in line <code>85</code> - each element of batch will be given its entry in the list which is returned from <code>run(...)</code> method. Order must be aligned with order of batch  elements. Each output dictionary must provide all keys declared in block outputs.</p> </li> </ul> Inputs that accept both batches and scalars <p>It is relatively unlikely, but may happen that your block would need to accept both batch-oriented data and scalars within a single input parameter. Execution Engine recognises that using  <code>get_parameters_accepting_batches_and_scalars(...)</code> method of block manifest. Take a look at the  example provided below:</p> <pre><code>from typing import Literal, Union, List, Optional, Type, Any, Dict\nfrom pydantic import Field\n\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    Batch,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n)\n\nclass ExampleManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/example@v1\"] \n    name: str\n    param_1: Selector()\n    param_2: List[Selector()]\n    param_3: Dict[str, Selector()]\n\n    @classmethod\n    def get_parameters_accepting_batches_and_scalars(cls) -&gt; bool:\n        return [\"param_1\", \"param_2\", \"param_3\"]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [OutputDefinition(name=\"dummy\")]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass ExampleBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ExampleManifest\n\n    def run(\n        self,\n        param_1: Any,\n        param_2: List[Any],\n        param_3: Dict[str, Any],\n    ) -&gt; BlockResult:\n        batch_size = None\n        if isinstance(param_1, Batch):\n            param_1_result = ...  # do something with batch-oriented param\n            batch_size = len(param_1)\n        else:\n            param_1_result = ... # do something with scalar param\n        for element in param_2:\n           if isinstance(element, Batch):\n              ...\n           else:\n              ...\n        for key, value in param_3.items():\n           if isinstance(element, value):\n              ...\n           else:\n              ...\n        if batch_size is None:\n           return {\"dummy\": \"some_result\"}\n        result = []\n        for _ in range(batch_size):\n           result.append({\"dummy\": \"some_result\"})\n        return result\n</code></pre> <ul> <li> <p>lines <code>20-22</code> specify manifest parameters that are expected to accept mixed (both scalar and batch-oriented) input data - point out that at this stage there is no difference in definition compared to previous examples.</p> </li> <li> <p>lines <code>24-26</code> specify <code>get_parameters_accepting_batches_and_scalars(...)</code> method to tell the Execution  Engine that block <code>run(...)</code> method can handle both scalar and batch-oriented inputs for the specified  parameters.</p> </li> <li> <p>lines <code>45-47</code> depict the parameters of mixed nature in <code>run(...)</code> method signature.</p> </li> <li> <p>line <code>49</code> reveals that we must keep track of the expected output size within the block logic. That's  why it is quite tricky to implement blocks with mixed inputs. Normally, when block <code>run(...)</code> method  operates on scalars - in majority of cases (exceptions will be described below) - the metod constructs  single output dictionary. Similairly, when batch-oriented inputs are accepted - those inputs  define expected output size. In this case, however, we must manually detect batches and catch their sizes.</p> </li> <li> <p>lines <code>50-54</code> showcase how we usually deal with mixed parameters - applying different logic when  batch-oriented data is detected</p> </li> <li> <p>as mentioned earlier, output construction must also be adjusted to the nature of mixed inputs - which  is illustrated in lines <code>65-70</code></p> </li> </ul>"},{"location":"workflows/create_workflow_block/#implementation-of-flow-control-block","title":"Implementation of flow-control block","text":"<p>Flow-control blocks differs quite substantially from other blocks that just process the data. Here we will show  how to create a flow control block, but first - a little bit of theory:</p> <ul> <li> <p>flow-control block is the block that declares compatibility with step selectors in their manifest (selector to step is defined as <code>$steps.{step_name}</code> - similar to step output selector, but without specification of output name)</p> </li> <li> <p>flow-control blocks cannot register outputs, they are meant to return <code>FlowControl</code> objects</p> </li> <li> <p><code>FlowControl</code> object specify next steps (from selectors provided in step manifest) that for given  batch element (SIMD flow-control) or whole workflow execution (non-SIMD flow-control) should pick up next</p> </li> </ul> Implementation of flow-control <p>Example provides and comments out implementation of random continue block</p> <pre><code>from typing import List, Literal, Optional, Type, Union\nimport random\n\nfrom pydantic import Field\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n  WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepSelector,\n    Selector,\n    IMAGE_KIND,\n)\nfrom inference.core.workflows.execution_engine.v1.entities import FlowControl\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/random_continue@v1\"]\n    name: str\n    image: Selector(kind=[IMAGE_KIND]) = ImageInputField\n    probability: float\n    next_steps: List[StepSelector] = Field(\n        description=\"Reference to step which shall be executed if expression evaluates to true\",\n        examples=[[\"$steps.on_true\"]],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return []\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.2.0,&lt;2.0.0\"\n\n\nclass RandomContinueBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: WorkflowImageData,\n        probability: float,\n        next_steps: List[str],\n    ) -&gt; BlockResult:\n        if not next_steps or random.random() &gt; probability:\n            return FlowControl()\n        return FlowControl(context=next_steps)\n</code></pre> <ul> <li> <p>line <code>10</code> imports type annotation for step selector which will be used to  notify Execution Engine that the block controls the flow</p> </li> <li> <p>line <code>14</code> imports <code>FlowControl</code> class which is the only viable response from flow-control block</p> </li> <li> <p>line <code>28</code> defines list of step selectors which effectively turns the block into flow-control one</p> </li> <li> <p>lines <code>55</code> and <code>56</code> show how to construct output - <code>FlowControl</code> object accept context being <code>None</code>, <code>string</code> or  <code>list of strings</code> - <code>None</code> represent flow termination for the batch element, strings are expected to be selectors  for next steps, passed in input.</p> </li> </ul> Implementation of flow-control - batch variant <p>Example provides and comments out implementation of random continue block</p> <pre><code>from typing import List, Literal, Optional, Type, Union\nimport random\n\nfrom pydantic import Field\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n  WorkflowImageData,\n  Batch,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepSelector,\n    Selector,\n    IMAGE_KIND,\n)\nfrom inference.core.workflows.execution_engine.v1.entities import FlowControl\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/random_continue@v1\"]\n    name: str\n    image: Selector(kind=[IMAGE_KIND]) = ImageInputField\n    probability: float\n    next_steps: List[StepSelector] = Field(\n        description=\"Reference to step which shall be executed if expression evaluates to true\",\n        examples=[[\"$steps.on_true\"]],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return []\n\n    @classmethod\n    def get_parameters_accepting_batches(cls) -&gt; List[str]:\n        return [\"image\"]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass RandomContinueBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: Batch[WorkflowImageData],\n        probability: float,\n        next_steps: List[str],\n    ) -&gt; BlockResult:\n        result = []\n        for _ in image:\n           if not next_steps or random.random() &gt; probability:\n               result.append(FlowControl())\n           result.append(FlowControl(context=next_steps))\n        return result\n</code></pre> <ul> <li> <p>line <code>11</code> imports type annotation for step selector which will be used to  notify Execution Engine that the block controls the flow</p> </li> <li> <p>line <code>15</code> imports <code>FlowControl</code> class which is the only viable response from flow-control block</p> </li> <li> <p>lines <code>29-32</code> defines list of step selectors which effectively turns the block into flow-control one</p> </li> <li> <p>lines <code>38-40</code> contain definition of <code>get_parameters_accepting_batches(...)</code> method telling Execution  Engine that block <code>run(...)</code> method expects batch-oriented <code>image</code> parameter.</p> </li> <li> <p>line <code>59</code> revels that we need to return flow-control guide for each and every element of <code>image</code> batch.</p> </li> <li> <p>to achieve that end, in line <code>60</code> we iterate over the contntent of batch.</p> </li> <li> <p>lines <code>61-63</code> show how to construct output - <code>FlowControl</code> object accept context being <code>None</code>, <code>string</code> or  <code>list of strings</code> - <code>None</code> represent flow termination for the batch element, strings are expected to be selectors  for next steps, passed in input.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#nested-selectors","title":"Nested selectors","text":"<p>Some block will require list of selectors or dictionary of selectors to be  provided in block manifest field. Version <code>v1</code> of Execution Engine supports only  one level of nesting - so list of lists of selectors or dictionary with list of selectors  will not be recognised properly.</p> <p>Practical use cases showcasing usage of nested selectors are presented below.</p>"},{"location":"workflows/create_workflow_block/#fusion-of-predictions-from-variable-number-of-models","title":"Fusion of predictions from variable number of models","text":"<p>Let's assume that you want to build a block to get majority vote on multiple classifiers predictions - then you would  like your run method to look like that:</p> <pre><code># pseud-code here\ndef run(self, predictions: List[dict]) -&gt; BlockResult:\n    predicted_classes = [p[\"class\"] for p in predictions]\n    counts = Counter(predicted_classes)\n    return {\"top_class\": counts.most_common(1)[0]}\n</code></pre> Nested selectors - models ensemble <pre><code>from typing import List, Literal, Optional, Type\n\nfrom pydantic import Field\nimport supervision as sv\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector,\n    OBJECT_DETECTION_PREDICTION_KIND,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/fusion_of_predictions@v1\"]\n    name: str\n    predictions: List[Selector(kind=[OBJECT_DETECTION_PREDICTION_KIND])] = Field(\n        description=\"Selectors to step outputs\",\n        examples=[[\"$steps.model_1.predictions\", \"$steps.model_2.predictions\"]],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"predictions\", \n            kind=[OBJECT_DETECTION_PREDICTION_KIND],\n          )\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass FusionBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        predictions: List[sv.Detections],\n    ) -&gt; BlockResult:\n        merged = sv.Detections.merge(predictions)\n        return {\"predictions\": merged}\n</code></pre> <ul> <li> <p>lines <code>23-26</code> depict how to define manifest field capable of accepting  list of selectors</p> </li> <li> <p>line <code>50</code> shows what to expect as input to block's <code>run(...)</code> method -  list of objects which are representation of specific kind. If the block accepted  batches, the input type of <code>predictions</code> field would be <code>List[Batch[sv.Detections]</code></p> </li> </ul> <p>Such block is compatible with the following step declaration:</p> <pre><code>{\n  \"type\": \"my_plugin/fusion_of_predictions@v1\",\n  \"name\": \"my_step\",\n  \"predictions\": [\n    \"$steps.model_1.predictions\",\n    \"$steps.model_2.predictions\"  \n  ]\n}\n</code></pre>"},{"location":"workflows/create_workflow_block/#block-with-data-transformations-allowing-dynamic-parameters","title":"Block with data transformations allowing dynamic parameters","text":"<p>Occasionally, blocks may need to accept group of \"named\" selectors,  which names and values are to be defined by creator of Workflow definition.  In such cases, block manifest shall accept dictionary of selectors, where keys serve as names for those selectors.</p> Nested selectors - named selectors <pre><code>from typing import List, Literal, Optional, Type, Any\n\nfrom pydantic import Field\nimport supervision as sv\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    Selector\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/named_selectors_example@v1\"]\n    name: str\n    data: Dict[str, Selector()] = Field(\n        description=\"Selectors to step outputs\",\n        examples=[{\"a\": $steps.model_1.predictions\", \"b\": \"$Inputs.data\"}],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(name=\"my_output\", kind=[]),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass BlockWithNamedSelectorsV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        data: Dict[str, Any],\n    ) -&gt; BlockResult:\n        ...\n        return {\"my_output\": ...}\n</code></pre> <ul> <li> <p>lines <code>22-25</code> depict how to define manifest field capable of accepting  dictionary of selectors - providing mapping between selector name and value</p> </li> <li> <p>line <code>46</code> shows what to expect as input to block's <code>run(...)</code> method -  dict of objects which are reffered with selectors. If the block accepted  batches, the input type of <code>data</code> field would be <code>Dict[str, Union[Batch[Any], Any]]</code>. In non-batch cases, non-batch-oriented data referenced by selector is automatically  broadcasted, whereas for blocks accepting batches - <code>Batch</code> container wraps only  batch-oriented inputs, with other inputs being passed as singular values.</p> </li> </ul> <p>Such block is compatible with the following step declaration:</p> <pre><code>{\n  \"type\": \"my_plugin/named_selectors_example@v1\",\n  \"name\": \"my_step\",\n  \"data\": {\n    \"a\": \"$steps.model_1.predictions\",\n    \"b\": \"$inputs.my_parameter\"  \n  }\n}\n</code></pre> <p>Practical implications will be the following:</p> <ul> <li> <p>under <code>data[\"a\"]</code> inside <code>run(...)</code> you will be able to find model's predictions -  like <code>sv.Detections</code> if <code>model_1</code> is object-detection model</p> </li> <li> <p>under <code>data[\"b\"]</code> inside <code>run(...)</code>, you will find value of input parameter named <code>my_parameter</code></p> </li> </ul>"},{"location":"workflows/create_workflow_block/#inputs-and-output-dimensionality-vs-run-method","title":"Inputs and output dimensionality vs <code>run(...)</code> method","text":"<p>The dimensionality of block inputs plays a crucial role in shaping the <code>run(...)</code> method\u2019s signature, and that's  why the system enforces strict bounds on the differences in dimensionality levels between inputs  (with the maximum allowed difference being <code>1</code>). This restriction is critical for ensuring consistency and  predictability when writing blocks.</p> <p>If dimensionality differences weren't controlled, it would be difficult to predict the structure of  the <code>run(...)</code> method, making development harder and less reliable. That\u2019s why validation of this property  is strictly enforced during the Workflow compilation process.</p> <p>Similarly, the output dimensionality also affects the method signature and the format of the expected output.  The ecosystem supports the following scenarios:</p> <ul> <li> <p>all inputs have the same dimensionality and outputs does not change dimensionality - baseline case</p> </li> <li> <p>all inputs have the same dimensionality and output decreases dimensionality</p> </li> <li> <p>all inputs have the same dimensionality and output increases dimensionality</p> </li> <li> <p>inputs have different dimensionality and output is allowed to keep the dimensionality of  reference input</p> </li> </ul> <p>Other combinations of input/output dimensionalities are not allowed to ensure consistency and to prevent ambiguity in  the method signatures.</p> Impact of dimensionality on <code>run(...)</code> method - batches disabled output dimensionality increaseoutput dimensionality decreasedifferent input dimensionalities <p>In this example, we perform dynamic crop of image based on predictions.</p> <pre><code>from typing import Dict, List, Literal, Optional, Type, Union\nfrom uuid import uuid4\n\nfrom inference.core.workflows.execution_engine.constants import DETECTION_ID_KEY\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n    ImageParentMetadata,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_block/dynamic_crop@v1\"]\n    image: Selector(kind=[IMAGE_KIND])\n    predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return 1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"crops\", kind=[IMAGE_KIND]),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\nclass DynamicCropBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: WorkflowImageData,\n        predictions: sv.Detections,\n    ) -&gt; BlockResult:\n        crops = []\n        for (x_min, y_min, x_max, y_max) in predictions.xyxy.round().astype(dtype=int):\n            cropped_image = image.numpy_image[y_min:y_max, x_min:x_max]\n            parent_metadata = ImageParentMetadata(parent_id=f\"{uuid4()}\")\n            if cropped_image.size:\n                result = WorkflowImageData(\n                    parent_metadata=parent_metadata,\n                    numpy_image=cropped_image,\n                )\n            else:\n                result = None\n            crops.append({\"crops\": result})\n        return crops\n</code></pre> <ul> <li> <p>in lines <code>28-30</code> manifest class declares output dimensionality  offset - value <code>1</code> should be understood as adding <code>1</code> to dimensionality level</p> </li> <li> <p>point out, that in line <code>63</code>, block eliminates empty images from further processing but  placing <code>None</code> instead of dictionatry with outputs. This would utilise the same  Execution Engine behaviour that is used for conditional execution - datapoint will be eliminated from downstream processing (unless steps requesting empty inputs  are present down the line).</p> </li> <li> <p>in lines <code>64-65</code> results for single input <code>image</code> and <code>predictions</code> are collected -  it is meant to be list of dictionares containing all registered outputs as keys. Execution engine will understand that the step returns batch of elements for each input element and create nested sturcures of indices to keep track of during execution of downstream steps.</p> </li> </ul> <p>In this example, the block visualises crops predictions and creates tiles presenting all crops predictions in single output image.</p> <pre><code>from typing import List, Literal, Type, Union\n\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/tile_detections@v1\"]\n    crops: Selector(kind=[IMAGE_KIND])\n    crops_predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND]\n    )\n    scalar_parameter: Union[float, Selector()]\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return -1\n\n    @classmethod\n    def get_parameters_enforcing_auto_batch_casting(cls) -&gt; List[str]:\n        return [\"crops\", \"crops_predictions\"]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"visualisations\", kind=[IMAGE_KIND]),\n        ]\n\n\nclass TileDetectionsBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        crops: Batch[WorkflowImageData],\n        crops_predictions: Batch[sv.Detections],\n        scalar_parameter: float,\n    ) -&gt; BlockResult:\n        annotator = sv.BoxAnnotator()\n        visualisations = []\n        for image, prediction in zip(crops, crops_predictions):\n            annotated_image = annotator.annotate(\n                image.numpy_image.copy(),\n                prediction,\n            )\n            visualisations.append(annotated_image)\n        tile = sv.create_tiles(visualisations)\n        return {\"visualisations\": tile}\n</code></pre> <ul> <li> <p>in lines <code>30-32</code> manifest class declares output dimensionality  offset - value <code>-1</code> should be understood as decreasing dimensionality level by <code>1</code></p> </li> <li> <p>in lines <code>34-36</code> manifest class declares <code>run(...)</code> method inputs that will be subject to auto-batch casting ensuring that the signature is always stable. Auto-batch casting was introduced in Execution Engine <code>v0.1.6.0</code> </p> </li> <li> <p>refer to changelog for more details.</p> </li> <li> <p>in lines <code>53-55</code> you can see the impact of output dimensionality decrease on the method signature. First two inputs (declared in line <code>36</code>) are artificially wrapped in <code>Batch[]</code> container, whereas <code>scalar_parameter</code> remains primitive type. This is done by Execution Engine automatically  on output dimensionality decrease when all inputs have the same dimensionality to enable access to  all elements occupying the last dimensionality level. Obviously, only elements related to the same element  from top-level batch will be grouped. For instance, if you had two input images that you  cropped - crops from those two different images will be grouped separately.</p> </li> <li> <p>lines <code>65-66</code> illustrate how output is constructed - single value is returned and that value  will be indexed by Execution Engine in output batch with reduced dimensionality</p> </li> </ul> <p>In this example, block merges detections which were predicted based on  crops of original image - result is to provide single detections with  all partial ones being merged.</p> <pre><code>from copy import deepcopy\nfrom typing import Dict, List, Literal, Optional, Type, Union\n\nimport numpy as np\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n    IMAGE_KIND,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/stitch@v1\"]\n    image: Selector(kind=[IMAGE_KIND])\n    image_predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def get_input_dimensionality_offsets(cls) -&gt; Dict[str, int]:\n        return {\n            \"image\": 0,\n            \"image_predictions\": 1,\n        }\n\n    @classmethod\n    def get_dimensionality_reference_property(cls) -&gt; Optional[str]:\n        return \"image\"\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(\n                name=\"predictions\",\n                kind=[\n                    OBJECT_DETECTION_PREDICTION_KIND,\n                ],\n            ),\n        ]\n\n\nclass StitchDetectionsNonBatchBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: WorkflowImageData,\n        image_predictions: Batch[sv.Detections],\n    ) -&gt; BlockResult:\n        image_predictions = [deepcopy(p) for p in image_predictions if len(p)]\n        for p in image_predictions:\n            coords = p[\"parent_coordinates\"][0]\n            p.xyxy += np.concatenate((coords, coords))\n        return {\"predictions\": sv.Detections.merge(image_predictions)}\n</code></pre> <ul> <li> <p>in lines <code>31-36</code> manifest class declares input dimensionalities offset, indicating <code>image</code> parameter being top-level and <code>image_predictions</code> being nested batch of predictions</p> </li> <li> <p>whenever different input dimensionalities are declared, dimensionality reference property must be pointed (see lines <code>38-40</code>) - this dimensionality level would be used to calculate  output dimensionality - in this particular case, we specify <code>image</code>. This choice  has an implication in the expected format of result - in the chosen scenario we are supposed to return single dictionary with all registered outputs keys. If our choice is <code>image_predictions</code>, we would return list of dictionaries (of size equal to length of <code>image_predictions</code> batch). In other worlds, <code>get_dimensionality_reference_property(...)</code> which dimensionality level should be associated to the output.</p> </li> <li> <p>lines <code>63-64</code> present impact of dimensionality offsets specified in lines <code>31-36</code>. It is clearly visible that <code>image_predictions</code> is a nested batch regarding <code>image</code>. Obviously, only nested predictions relevant for the specific <code>images</code> are grouped in batch and provided to the method in runtime.</p> </li> <li> <p>as mentioned earlier, line <code>69</code> construct output being single dictionary, as we register output  at dimensionality level of <code>image</code> (which was also shipped as single element)</p> </li> </ul> Impact of dimensionality on <code>run(...)</code> method - batches enabled output dimensionality increaseoutput dimensionality decreasedifferent input dimensionalities <p>In this example, we perform dynamic crop of image based on predictions.</p> <pre><code>from typing import Dict, List, Literal, Optional, Type, Union\nfrom uuid import uuid4\n\nfrom inference.core.workflows.execution_engine.constants import DETECTION_ID_KEY\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n    ImageParentMetadata,\n    Batch,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_block/dynamic_crop@v1\"]\n    image: Selector(kind=[IMAGE_KIND])\n    predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def get_parameters_accepting_batches(cls) -&gt; bool:\n        return [\"image\", \"predictions\"]\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return 1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"crops\", kind=[IMAGE_KIND]),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\nclass DynamicCropBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: Batch[WorkflowImageData],\n        predictions: Batch[sv.Detections],\n    ) -&gt; BlockResult:\n        results = []\n        for single_image, detections in zip(image, predictions):\n            crops = []\n            for (x_min, y_min, x_max, y_max) in detections.xyxy.round().astype(dtype=int):\n                cropped_image = single_image.numpy_image[y_min:y_max, x_min:x_max]\n                parent_metadata = ImageParentMetadata(parent_id=f\"{uuid4()}\")\n                if cropped_image.size:\n                    result = WorkflowImageData(\n                        parent_metadata=parent_metadata,\n                        numpy_image=cropped_image,\n                    )\n                else:\n                    result = None\n                crops.append({\"crops\": result})\n            results.append(crops)\n        return results\n</code></pre> <ul> <li> <p>in lines <code>29-31</code> manifest declares that block accepts batches of inputs</p> </li> <li> <p>in lines <code>33-35</code> manifest class declares output dimensionality  offset - value <code>1</code> should be understood as adding <code>1</code> to dimensionality level</p> </li> <li> <p>in lines <code>55-66</code>, signature of input parameters reflects that the <code>run(...)</code> method runs against inputs of the same dimensionality and those inputs are provided in batches</p> </li> <li> <p>point out, that in line <code>70</code>, block eliminates empty images from further processing but  placing <code>None</code> instead of dictionatry with outputs. This would utilise the same  Execution Engine behaviour that is used for conditional execution - datapoint will be eliminated from downstream processing (unless steps requesting empty inputs  are present down the line).</p> </li> <li> <p>construction of the output, presented in lines <code>71-73</code> indicates two levels of nesting. First of all, block operates on batches, so it is expected to return list of outputs, one  output for each input batch element. Additionally, this output element for each input batch  element turns out to be nested batch - hence for each input iage and prediction, block  generates list of outputs - elements of that list are dictionaries providing values  for each declared output.</p> </li> </ul> <p>In this example, the block visualises crops predictions and creates tiles presenting all crops predictions in single output image.</p> <pre><code>from typing import List, Literal, Type, Union\n\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/tile_detections@v1\"]\n    images_crops: Selector(kind=[IMAGE_KIND])\n    crops_predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND]\n    )\n\n    @classmethod\n    def get_parameters_accepting_batches(cls) -&gt; bool:\n        return [\"images_crops\", \"crops_predictions\"]\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return -1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"visualisations\", kind=[IMAGE_KIND]),\n        ]\n\n\nclass TileDetectionsBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        images_crops: Batch[Batch[WorkflowImageData]],\n        crops_predictions: Batch[Batch[sv.Detections]],\n    ) -&gt; BlockResult:\n        annotator = sv.BoxAnnotator()\n        visualisations = []\n        for image_crops, crop_predictions in zip(images_crops, crops_predictions):\n            visualisations_batch_element = []\n            for image, prediction in zip(image_crops, crop_predictions):\n                annotated_image = annotator.annotate(\n                    image.numpy_image.copy(),\n                    prediction,\n                )\n                visualisations_batch_element.append(annotated_image)\n            tile = sv.create_tiles(visualisations_batch_element)\n            visualisations.append({\"visualisations\": tile})\n        return visualisations\n</code></pre> <ul> <li> <p>lines <code>29-31</code> manifest that block is expected to take batches as input</p> </li> <li> <p>in lines <code>33-35</code> manifest class declares output dimensionality  offset - value <code>-1</code> should be understood as decreasing dimensionality level by <code>1</code></p> </li> <li> <p>in lines <code>52-53</code> you can see the impact of output dimensionality decrease and batch processing on the method signature. First \"layer\" of <code>Batch[]</code> is a side effect of the  fact that manifest declared that block accepts batches of inputs. The second \"layer\" comes  from output dimensionality decrease. Execution Engine wrapps up the dimension to be reduced into  additional <code>Batch[]</code> container porvided in inputs, such that programmer is able to collect all nested batches elements that belong to specific top-level batch element.</p> </li> <li> <p>lines <code>66-67</code> illustrate how output is constructed - for each top-level batch element, block aggregates all crops and predictions and creates a single tile. As block accepts batches of inputs, this procedure end up with one tile for each top-level batch element - hence list of dictionaries is expected to be returned.</p> </li> </ul> <p>In this example, block merges detections which were predicted based on  crops of original image - result is to provide single detections with  all partial ones being merged.</p> <pre><code>from copy import deepcopy\nfrom typing import Dict, List, Literal, Optional, Type, Union\n\nimport numpy as np\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n    IMAGE_KIND,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/stitch@v1\"]\n    images: Selector(kind=[IMAGE_KIND])\n    images_predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def get_parameters_accepting_batches(cls) -&gt; bool:\n        return [\"images\", \"images_predictions\"]\n\n    @classmethod\n    def get_input_dimensionality_offsets(cls) -&gt; Dict[str, int]:\n        return {\n            \"image\": 0,\n            \"image_predictions\": 1,\n        }\n\n    @classmethod\n    def get_dimensionality_reference_property(cls) -&gt; Optional[str]:\n        return \"image\"\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(\n                name=\"predictions\",\n                kind=[\n                    OBJECT_DETECTION_PREDICTION_KIND,\n                ],\n            ),\n        ]\n\n\nclass StitchDetectionsBatchBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        images: Batch[WorkflowImageData],\n        images_predictions: Batch[Batch[sv.Detections]],\n    ) -&gt; BlockResult:\n        result = []\n        for image, image_predictions in zip(images, images_predictions):\n            image_predictions = [deepcopy(p) for p in image_predictions if len(p)]\n            for p in image_predictions:\n                coords = p[\"parent_coordinates\"][0]\n                p.xyxy += np.concatenate((coords, coords))\n            merged_prediction = sv.Detections.merge(image_predictions)\n            result.append({\"predictions\": merged_prediction})\n        return result\n</code></pre> <ul> <li> <p>lines <code>31-33</code> manifest that block is expected to take batches as input</p> </li> <li> <p>in lines <code>35-40</code> manifest class declares input dimensionalities offset, indicating <code>image</code> parameter being top-level and <code>image_predictions</code> being nested batch of predictions</p> </li> <li> <p>whenever different input dimensionalities are declared, dimensionality reference property must be pointed (see lines <code>42-44</code>) - this dimensionality level would be used to calculate  output dimensionality - in this particular case, we specify <code>image</code>. This choice  has an implication in the expected format of result - in the chosen scenario we are supposed to return single dictionary for each element of <code>image</code> batch. If our choice is <code>image_predictions</code>, we would return list of dictionaries (of size equal to length of nested <code>image_predictions</code> batch) for each input <code>image</code> batch element.</p> </li> <li> <p>lines <code>66-67</code> present impact of dimensionality offsets specified in lines <code>35-40</code> as well as  the declararion of batch processing from lines <code>32-34</code>. First \"layer\" of <code>Batch[]</code> container comes  from the latter, nested <code>Batch[Batch[]]</code> for <code>images_predictions</code> comes from the definition of input  dimensionality offset. It is clearly visible that <code>image_predictions</code> holds batch of predictions relevant for specific elements of <code>image</code> batch.</p> </li> <li> <p>as mentioned earlier, lines <code>76-77</code> construct output being single dictionary for each element of <code>image</code>  batch</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#block-accepting-empty-inputs","title":"Block accepting empty inputs","text":"<p>As discussed earlier, some batch elements may become \"empty\" during the execution of a Workflow.  This can happen due to several factors:</p> <ul> <li> <p>Flow-control mechanisms: Certain branches of execution can mask specific batch elements, preventing them  from being processed in subsequent steps.</p> </li> <li> <p>In data-processing blocks: In some cases, a block may not be able to produce a meaningful output for  a specific data point. For example, a Dynamic Crop block cannot generate a cropped image if the bounding box  size is zero.</p> </li> </ul> <p>Some blocks are designed to handle these empty inputs, such as block that can replace missing outputs with default  values. This block can be particularly useful when constructing structured outputs in a Workflow, ensuring  that even if some elements are empty, the output lacks missing elements making it harder to parse.</p> Block accepting empty inputs <pre><code>from typing import Any, List, Literal, Optional, Type\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import Selector\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/first_non_empty_or_default@v1\"]\n    data: List[Selector()]\n    default: Any\n\n    @classmethod\n    def accepts_empty_values(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [OutputDefinition(name=\"output\")]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.3.0,&lt;2.0.0\"\n\n\nclass FirstNonEmptyOrDefaultBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        data: Batch[Optional[Any]],\n        default: Any,\n    ) -&gt; BlockResult:\n        result = default\n        for data_element in data:\n            if data_element is not None:\n                return {\"output\": data_element}\n        return {\"output\": result}\n</code></pre> <ul> <li> <p>in lines <code>20-22</code> you may find declaration stating that block acccepts empt inputs </p> </li> <li> <p>a consequence of lines <code>20-22</code> is visible in line <code>41</code>, when signature states that  input <code>Batch</code> may contain empty elements that needs to be handled. In fact - the block  generates \"artificial\" output substituting empty value, which makes it possible for  those outputs to be \"visible\" for blocks not accepting empty inputs that refer to the  output of this block. You should assume that each input that is substituted by Execution Engine with data generated in runtime may provide optional elements.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#block-with-custom-constructor-parameters","title":"Block with custom constructor parameters","text":"<p>Some blocks may require objects constructed by outside world to work. In such scenario, Workflows Execution Engine job is to transfer those entities to the block,  making it possible to be used. The mechanism is described in  the page presenting Workflows Compiler, as this is the  component responsible for dynamic construction of steps from blocks classes.</p> <p>Constructor parameters must be:</p> <ul> <li> <p>requested by block - using class method <code>WorkflowBlock.get_init_parameters(...)</code></p> </li> <li> <p>provided in the environment running Workflows Execution Engine:</p> <ul> <li> <p>directly, as shown in this example</p> </li> <li> <p>using defaults registered for Workflow plugin</p> </li> </ul> </li> </ul> <p>Let's see how to request init parameters while defining block.</p> Block requesting constructor parameters <pre><code>from typing import Any, List, Literal, Optional, Type\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import Selector\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/example@v1\"]\n    data: List[Selector()]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [OutputDefinition(name=\"output\")]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass ExampleBlock(WorkflowBlock):\n\n    def __init__(my_parameter: int):\n        self._my_parameter = my_parameter\n\n    @classmethod\n    def get_init_parameters(cls) -&gt; List[str]:\n        return [\"my_parameter\"]\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        data: Batch[Any],\n    ) -&gt; BlockResult:\n        pass\n</code></pre> <ul> <li> <p>lines <code>30-31</code> declare class constructor which is not parameter-free</p> </li> <li> <p>to inform Execution Engine that block requires custom initialisation,  <code>get_init_parameters(...)</code> method in lines <code>33-35</code> enlists names of all  parameters that must be provided</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/","title":"Dynamic Python blocks","text":"<p>When the syntax for Workflow definitions was outlined, one key aspect was not covered: the ability to define blocks directly within the Workflow definition itself. This section can include the manifest and Python code for blocks defined in place, which are dynamically interpreted by the Execution Engine. These in-place blocks function similarly to those statically defined in plugins yet provide much more flexibility.</p>"},{"location":"workflows/custom_python_code_blocks/#execution-modes","title":"Execution Modes","text":"<p>Dynamic Python blocks support two execution modes:</p>"},{"location":"workflows/custom_python_code_blocks/#local-execution","title":"Local Execution","text":"<p>When running inference locally on your own hardware, dynamic blocks execute directly in your environment. This provides the fastest performance for development and testing.</p> <p>Warning</p> <p>Local execution of dynamic blocks only works in your local deployment of <code>inference</code> and requires careful consideration of security implications when running untrusted code.</p> <p>If you wish to disable the functionality, <code>export ALLOW_CUSTOM_PYTHON_EXECUTION_IN_WORKFLOWS=False</code></p>"},{"location":"workflows/custom_python_code_blocks/#cloud-execution-roboflow-serverless-v2","title":"Cloud Execution (Roboflow Serverless v2)","text":"<p>When using Roboflow's cloud infrastructure with Serverless v2 API, dynamic blocks execute in secure, isolated containers. This ensures safe execution of custom code without compromising your infrastructure.</p> <p>Data Serialization Requirements</p> <p>When using cloud execution, all input and output data must be serializable through Inference's serialization system. This means:</p> <ul> <li>Use simple Python types (str, int, float, bool, list, dict)</li> <li>Numpy arrays and standard computer vision data structures are supported</li> <li>Complex custom objects may need to be converted to simpler representations</li> <li>Avoid returning functions, lambda expressions, or other non-serializable Python objects</li> </ul> <p>The cloud execution environment provides the same standard libraries and imports as local execution, ensuring your code works consistently across both modes.</p>"},{"location":"workflows/custom_python_code_blocks/#state-management-and-shared-data","title":"State Management and Shared Data","text":"<p>Variables defined at the module level (outside of your <code>run</code> function) in your block's code are scoped to instances of that block. These variables:</p> <ul> <li>Persist across invocations of the same block (as long as the code doesn't change)</li> <li>Reset when the block's code changes any modification to the block's code creates a new namespace</li> <li>Are lost when the server/container restarts</li> </ul> <p>Example:</p> <p>This block increments a counter each time the block is run and remembers the last result:</p> <pre><code># This variable is block-scoped\ncounter = 0\nlast_result = None\n\ndef run(self, input_value):\n    global counter, last_result\n\n    counter += 1\n\n    # Store the last result for comparison\n    previous = last_result\n    last_result = input_value * 2\n\n    return {\n        \"run_count\": counter,\n        \"current\": last_result,\n        \"previous\": previous\n    }\n</code></pre>"},{"location":"workflows/custom_python_code_blocks/#best-practices-for-state-management","title":"Best Practices for State Management","text":"<p>Custom Block state is meant for caching expensive computations and optimization of artifact and dependency loading.</p> <ul> <li>Do not rely on state for critical data persistence - use external storage for important data.</li> <li>State may be lost at any time due to server restarts or container scaling</li> <li>In cloud environments, subsequent requests may hit different servers with different state</li> <li>Initialize block-scoped variables with default values to handle fresh starts</li> <li>Keep state lightweight. Large objects consume memory and may impact performance.</li> </ul>"},{"location":"workflows/custom_python_code_blocks/#theory","title":"Theory","text":"<p>The high-level overview of Dynamic Python blocks functionality:</p> <ul> <li> <p>user provides definition of dynamic block in JSON</p> </li> <li> <p>definition contains information required by Execution Engine to construct <code>WorkflowBlockManifest</code> and <code>WorkflowBlock</code> out of the  document</p> </li> <li> <p>At runtime, the Compiler turns the definition into dynamically created Python classes\u2014exactly the same as statically defined blocks</p> </li> <li> <p>In the Workflow definition, you may declare steps that use dynamic blocks, as if dynamic blocks were standard static ones</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#example","title":"Example","text":"<p>Let's take a look and discuss example workflow with dynamic Python blocks.</p> Workflow with dynamic block <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"dynamic_blocks_definitions\": [\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"OverlapMeasurement\",\n                \"inputs\": {\n                    \"predictions\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    },\n                    \"class_x\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    },\n                    \"class_y\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"overlap\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, predictions: sv.Detections, class_x: str, class_y: str) -&gt; BlockResult:\\n    bboxes_class_x = predictions[predictions.data[\\\"class_name\\\"] == class_x]\\n    bboxes_class_y = predictions[predictions.data[\\\"class_name\\\"] == class_y]\\n    overlap = []\\n    for bbox_x in bboxes_class_x:\\n        bbox_x_coords = bbox_x[0]\\n        bbox_overlaps = []\\n        for bbox_y in bboxes_class_y:\\n            if bbox_y[-1][\\\"detection_id\\\"] == bbox_x[-1][\\\"detection_id\\\"]:\\n                continue\\n            bbox_y_coords = bbox_y[0]\\n            x_min = max(bbox_x_coords[0], bbox_y_coords[0])\\n            y_min = max(bbox_x_coords[1], bbox_y_coords[1])\\n            x_max = min(bbox_x_coords[2], bbox_y_coords[2])\\n            y_max = min(bbox_x_coords[3], bbox_y_coords[3])\\n            # compute the area of intersection rectangle\\n            intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\\n            box_x_area = (bbox_x_coords[2] - bbox_x_coords[0] + 1) * (bbox_x_coords[3] - bbox_x_coords[1] + 1)\\n            local_overlap = intersection_area / (box_x_area + 1e-5)\\n            bbox_overlaps.append(local_overlap)\\n        overlap.append(bbox_overlaps)\\n    return  {\\\"overlap\\\": overlap}\\n\"\n            }\n        },\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"MaximumOverlap\",\n                \"inputs\": {\n                    \"overlaps\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"max_value\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, overlaps: List[List[float]]) -&gt; BlockResult:\\n    max_value = -1\\n    for overlap in overlaps:\\n        for overlap_value in overlap:\\n            if not max_value:\\n                max_value = overlap_value\\n            else:\\n                max_value = max(max_value, overlap_value)\\n    return {\\\"max_value\\\": max_value}\\n\"\n            }\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"model\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"OverlapMeasurement\",\n            \"name\": \"overlap_measurement\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"class_x\": \"dog\",\n            \"class_y\": \"dog\"\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"overlaps\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) &gt;=\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"overlaps\": \"$steps.overlap_measurement.overlap\"\n            },\n            \"next_steps\": [\n                \"$steps.maximum_overlap\"\n            ]\n        },\n        {\n            \"type\": \"MaximumOverlap\",\n            \"name\": \"maximum_overlap\",\n            \"overlaps\": \"$steps.overlap_measurement.overlap\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"overlaps\",\n            \"selector\": \"$steps.overlap_measurement.overlap\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"max_overlap\",\n            \"selector\": \"$steps.maximum_overlap.max_value\"\n        }\n    ]\n}\n</code></pre> <p>Let's start the analysis from <code>dynamic_blocks_definitions</code> - this is the part of  Workflow Definition that provides a list of dynamic blocks. Each block contains two sections:</p> <ul> <li> <p><code>manifest</code> - providing JSON representation of <code>BlockManifest</code> - refer blocks development guide</p> </li> <li> <p><code>code</code> - shipping Python code</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#definition-of-block-manifest","title":"Definition of block manifest","text":"<p>Manifest definition contains several fields, including:</p> <ul> <li> <p><code>block_type</code> - equivalent of <code>type</code> field in block manifest - must provide unique block identifier</p> </li> <li> <p><code>inputs</code> - dictionary with names and definitions of dynamic inputs</p> </li> <li> <p><code>outputs</code> - dictionary with names and definitions of dynamic outputs</p> </li> <li> <p><code>output_dimensionality_offset</code> - field specifies output dimensionality</p> </li> <li> <p><code>accepts_batch_input</code> - field dictates if input data in runtime is to be provided in batches by Execution Engine</p> </li> <li> <p><code>accepts_empty_values</code> - field deciding if empty inputs will be ignored while  constructing step inputs</p> </li> </ul> <p>In any doubt, refer to blocks development guide, as the dynamic blocks replicates standard blocs capabilities.</p>"},{"location":"workflows/custom_python_code_blocks/#definition-of-dynamic-input","title":"Definition of dynamic input","text":"<p>Dynamic inputs define fields of dynamically created block manifest. In other words,  this is definition based on which <code>BlockManifest</code> class will be created in runtime.</p> <p>Each input may define the following properties:</p> <ul> <li> <p><code>has_default_value</code> - flag to decide if dynamic manifest field has default</p> </li> <li> <p><code>default_value</code> - default value (used only if <code>has_default_value=True</code></p> </li> <li> <p><code>is_optional</code> - flag to decide if dynamic manifest field is optional</p> </li> <li> <p><code>is_dimensionality_reference</code> - flag to decide if dynamic manifest field ship selector to be used in runtime as dimensionality reference</p> </li> <li> <p><code>dimensionality_offset</code> - dimensionality offset for configured input property  of dynamic manifest</p> </li> <li> <p><code>selector_types</code> - type of selectors that may be used by property (one of  <code>input_image</code>, <code>step_output_image</code>, <code>input_parameter</code>, <code>step_output</code>). Step may not hold selector, but then must provide definition of specific type.</p> </li> <li> <p><code>selector_data_kind</code> - dictionary with list of selector kinds specific for each selector type</p> </li> <li> <p><code>value_types</code> - definition of specific type that is to be placed in manifest -  this field specifies typing of dynamically created manifest fields w.r.t Python types. Selection of types: <code>any</code>, <code>integer</code>, <code>float</code>, <code>boolean</code>, <code>dict</code>, <code>list</code>, <code>strig</code></p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#definition-of-dynamic-output","title":"Definition of dynamic output","text":"<p>Definitions of outputs are quite simple, hold optional list of <code>kinds</code> declared for given output.</p>"},{"location":"workflows/custom_python_code_blocks/#definition-of-python-code","title":"Definition of Python code","text":"<p>Python code is shipped in JSON document with the following fields:</p> <ul> <li> <p><code>run_function_code</code> - code of <code>run(...)</code> method of your dynamic block </p> </li> <li> <p><code>run_function_name</code> - name of run function</p> </li> <li> <p><code>init_function_code</code> - optional code for your init function that will  assemble step state - it is expected to return dictionary, which will be available for <code>run()</code> function under <code>self._init_results</code></p> </li> <li> <p><code>init_function_name</code> - name of init function</p> </li> <li> <p><code>imports</code> - list of additional imports (you may only use libraries from your environment, no dependencies will be  automatically installed)</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#how-to-create-run-method","title":"How to create <code>run(...)</code> method?","text":"<p>You must know the following:</p> <ul> <li> <p><code>run(...)</code> function must be defined, as if that was class instance method - with  the first argument being <code>self</code> and remaining arguments compatible with dynamic block manifest declared in definition of dynamic block</p> </li> <li> <p>you should expect baseline symbols to be provided, including your import statements and the following:</p> </li> </ul> <pre><code>from typing import Any, List, Dict, Set, Optional\nimport supervision as sv\nimport numpy as np\nimport math\nimport time\nimport json\nimport os\nimport requests\nimport cv2\nimport shapely\nfrom inference.core.workflows.execution_engine.entities.base import Batch, WorkflowImageData\nfrom inference.core.workflows.prototypes.block import BlockResult\n</code></pre> <p>So example function may look like the following (for clarity, we provide here Python code formatted nicely, but you must stringify the code to place it in definition):</p> <pre><code>def run(self, predictions: sv.Detections, class_x: str, class_y: str) -&gt; BlockResult:\n    bboxes_class_x = predictions[predictions.data[\"class_name\"] == class_x]\n    bboxes_class_y = predictions[predictions.data[\"class_name\"] == class_y]\n    overlap = []\n    for bbox_x in bboxes_class_x:\n        bbox_x_coords = bbox_x[0]\n        bbox_overlaps = []\n        for bbox_y in bboxes_class_y:\n            if bbox_y[-1][\"detection_id\"] == bbox_x[-1][\"detection_id\"]:\n                continue\n            bbox_y_coords = bbox_y[0]\n            x_min = max(bbox_x_coords[0], bbox_y_coords[0])\n            y_min = max(bbox_x_coords[1], bbox_y_coords[1])\n            x_max = min(bbox_x_coords[2], bbox_y_coords[2])\n            y_max = min(bbox_x_coords[3], bbox_y_coords[3])\n            # compute the area of intersection rectangle\n            intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\n            box_x_area = (bbox_x_coords[2] - bbox_x_coords[0] + 1) * (bbox_x_coords[3] - bbox_x_coords[1] + 1)\n            local_overlap = intersection_area / (box_x_area + 1e-5)\n            bbox_overlaps.append(local_overlap)\n        overlap.append(bbox_overlaps)\n    return  {\"overlap\": overlap}\n</code></pre>"},{"location":"workflows/custom_python_code_blocks/#how-to-create-init-method","title":"How to create <code>init(...)</code> method?","text":"<p>Init function is supposed to build <code>self._init_results</code> dictionary.</p> <p>Example:</p> <pre><code>def my_init() -&gt; Dict[str, Any]:\n    return {\"some\": \"value\"}\n</code></pre>"},{"location":"workflows/custom_python_code_blocks/#usage-of-dynamic-python-block-as-step","title":"Usage of Dynamic Python block as step","text":"<p>As shown in example Workflow definition, you may simply use the block  as if that was normal block exposed through static plugin:</p> <pre><code>{\n    \"type\": \"OverlapMeasurement\",\n    \"name\": \"overlap_measurement\",\n    \"predictions\": \"$steps.model.predictions\",\n    \"class_x\": \"dog\",\n    \"class_y\": \"dog\"\n}\n</code></pre>"},{"location":"workflows/definitions/","title":"Understanding Workflows Definitions syntax","text":"<p>In Roboflow Workflows, the Workflow Definition is the internal \"programming language\". It provides a structured  way to define how different blocks interact, specifying the necessary inputs, outputs, and configurations.  By using this syntax, users can create workflows without UI.</p> <p>Let's start from examining the Workflow Definition created in this tutorial and analyse it step by step.</p> Workflow definition <pre><code>{\n  \"version\": \"1.0\",\n  \"inputs\": [\n    {\n      \"type\": \"WorkflowImage\",\n      \"name\": \"image\"\n    },\n    {\n      \"type\": \"WorkflowParameter\",\n      \"name\": \"model\",\n      \"default_value\": \"yolov8n-640\"\n    }\n  ],\n  \"steps\": [\n    {\n      \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n      \"name\": \"model\",\n      \"images\": \"$inputs.image\",\n      \"model_id\": \"$inputs.model\"\n    },\n    {\n      \"type\": \"roboflow_core/dynamic_crop@v1\",\n      \"name\": \"dynamic_crop\",\n      \"images\": \"$inputs.image\",\n      \"predictions\": \"$steps.model.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n      \"name\": \"model_1\",\n      \"images\": \"$steps.dynamic_crop.crops\",\n      \"model_id\": \"dog-breed-xpaq6/1\"\n    },\n    {\n      \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n      \"name\": \"detections_classes_replacement\",\n      \"object_detection_predictions\": \"$steps.model.predictions\",\n      \"classification_predictions\": \"$steps.model_1.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n      \"name\": \"bounding_box_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$inputs.image\"\n    },\n    {\n      \"type\": \"roboflow_core/label_visualization@v1\",\n      \"name\": \"label_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$steps.bounding_box_visualization.image\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"detections\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.detections_classes_replacement.predictions\"\n    },\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"visualisation\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.label_visualization.image\"\n    }\n  ]\n}\n</code></pre>"},{"location":"workflows/definitions/#version-marker","title":"Version marker","text":"<p>Every Workflow Definition begins with the version parameter, which specifies the compatible version of the  Workflows Execution Engine. Roboflow utilizes Semantic Versioning to manage these  versions and maintains one version from each major release to ensure backward compatibility.  This means that a workflow defined for Execution Engine version <code>1.0.0</code> will function with version <code>1.3.4</code> and other  newer versions, but workflows created for more recent versions may not be compatible with earlier ones.</p> <p>List of Execution Engine versions loaded on the Roboflow Hosted platform is available  here.</p>"},{"location":"workflows/definitions/#inputs","title":"Inputs","text":"<p>Our example workflow specifies two inputs: <pre><code>[\n    {\n      \"type\": \"WorkflowImage\", \"name\": \"image\"\n    },\n    {\n      \"type\": \"WorkflowParameter\", \"name\": \"model\", \"default_value\": \"yolov8n-640\"\n    }\n]\n</code></pre> This entry in definition creates two placeholders that can be filled with data while running workflow. </p> <p>The first placeholder is named <code>image</code> and is of type <code>WorkflowImage</code>. This special input type is batch-oriented,  meaning it can accept one or more images at runtime to be processed as a single batch. You can add multiple inputs  of the type <code>WorkflowImage</code>, and it is expected that the data provided to these placeholders will contain  the same number of elements. Alternatively, you can mix inputs of sizes <code>N</code> and 1, where <code>N</code> represents the number  of elements in the batch.</p> <p>The second placeholder is a straightforward <code>WorkflowParameter</code> called model. This type of input allows users to  inject hyperparameters \u2014 such as model variants, confidence thresholds, and reference values \u2014 at runtime. The value is not expected to be a batch of elements, so when you provide a list, it will be interpreted as list of  elements, rather than batch of elements, each to be processed individually.</p> <p>More details about the nature of batch-oriented data processing in workflows can be found  here.</p>"},{"location":"workflows/definitions/#generic-batch-oriented-inputs","title":"Generic batch-oriented inputs","text":"<p>Since Execution Engine <code>v1.3.0</code> (inference release <code>v0.27.0</code>), Workflows support batch oriented inputs of any kind and  dimensionality.  This inputs are not enforced for now, but we expect that as the ecosystem grows, they will  be more and more useful.</p> Defining generic batch-oriented inputs <p>If you wanted to replace the <code>WorkflowImage</code> input with generic batch-oriented input, use the following construction:</p> <pre><code>{\n  \"inputs\": [\n    {\n      \"type\": \"WorkflowBatchInput\",\n      \"name\": \"image\",\n      \"kind\": [\"image\"]\n    }\n  ]\n}\n</code></pre> <p>Additionally, if your image is supposed to sit at higher dimensionality level,  add <code>dimensionality</code> property:</p> <pre><code>{\n  \"inputs\": [\n    {\n      \"type\": \"WorkflowBatchInput\",\n      \"name\": \"image\",\n      \"kind\": [\"image\"],\n      \"dimensionality\": 2\n    }\n  ]\n}\n</code></pre> <p>This will alter the expected format of <code>image</code> data in Workflow run - <code>dimensionality=2</code> enforces <code>image</code> to be nested batch of images - namely list  of list of images.</p>"},{"location":"workflows/definitions/#steps","title":"Steps","text":"<p>As mentioned here, steps are instances of Workflow blocks connected with inputs and outputs  of other steps to dictate how data flows through the workflow. Let's see example step definition:</p> <pre><code>{\n  \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n  \"name\": \"model\",\n  \"images\": \"$inputs.image\",\n  \"model_id\": \"$inputs.model\"\n}\n</code></pre> <p>Two common properties for each step are <code>type</code> and <code>name</code>. Type tells which block to load and name gives the step  unique identifier, based on which other steps may refer to output of given step.</p> <p>Two remaining properties declare <code>selectors</code> (this is how we call references in Workflows) to inputs - <code>image</code> and <code>model</code>. While running the workflow, data passed into those placeholders will be provided for block to process.</p> <p>Our documentation showcases what is the structure of each block and provides examples of how each block can be  used as workflow step. Explore our blocks collection here where you can find what are  block data inputs, outputs and configuration properties.</p> <p>Input data bindings of blocks (like <code>images</code> property) can be filled with selectors to batch-oriented inputs and  step outputs. Configuration properties of blocks (like <code>model_id</code>) usually can be filled with either values hardcoded in workflow definition (they cannot be altered in runtime) or selectors to inputs of type <code>WorkflowParameter</code>. For instance, valid definition can be obtained when <code>model_id</code> is either <code>\"$inputs.image\"</code> or <code>yolov8n-640</code>.</p> <p>Let's see now how step outputs are referred as inputs of another step: <pre><code>{\n  \"type\": \"roboflow_core/dynamic_crop@v1\",\n  \"name\": \"dynamic_crop\",\n  \"images\": \"$inputs.image\",\n  \"predictions\": \"$steps.model.predictions\"\n}\n</code></pre> In this particular case, <code>predictions</code> property defines output of step named <code>model</code>. Construction of selector is the following: <code>$steps.{step_name}.{step_output_name}</code>. Thanks to this reference, <code>model</code> step is connected with  <code>dynamic_crop</code> and in runtime model predictions will be passed into dynamic crop and will be reference for image  cropping procedure.</p>"},{"location":"workflows/definitions/#outputs","title":"Outputs","text":"<p>This section of Workflow Definition specifies how response from workflow execution looks like. Definitions of  each response field looks like that:</p> <pre><code>{\n  \"type\": \"JsonField\",\n  \"name\": \"detections\",\n  \"selector\": \"$steps.detections_classes_replacement.predictions\"\n}\n</code></pre> <p>The <code>selector</code> can reference either an input or a step output. Additionally, you can specify the <code>\"coordinates_system\"</code>  property, which accepts two values: <code>\"own\"</code> or <code>\"parent\"</code>. This property is relevant for outputs that provide model  detections and determines the coordinate system used for the detections. This becomes crucial when applying a  secondary object detection model on image crops derived from predictions of a primary model. In such cases,  the secondary model\u2019s predictions are based on the coordinates of the crops, not the original input image.  To ensure these coordinates are not translated back to the parent coordinate system, set  <code>\"coordinates_system\": \"own\"</code> (<code>parent</code> is default option).</p> <p>Additionally, outputs selectors support wildcards (<code>$steps.step_nane.*\"</code>) to grab all outputs of specific step.</p> <p>To fully understand how output structure is created - read about  data processing in Workflows.</p>"},{"location":"workflows/execution_engine_changelog/","title":"Execution Engine Changelog","text":"<p>Below you can find the changelog for Execution Engine.</p>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v170-inference-v0590","title":"Execution Engine <code>v1.7.0</code> | inference <code>v0.59.0</code>","text":"<p>Breaking change regarding step errors in workflows</p> <p>To fix a bug related to invalid HTTP responses codes in <code>inference-server</code> handling Workflows execution requests  we needed to alter the default mechanism responsible for handling errors in Execution Engine. As a result of change, effective immediately on Roboflow Hosted Platform and in <code>inference&gt;=0.59.0</code>, Workflow blocks interacting with  Roboflow platform which fails due to client misconfiguration (invalid Roboflow API key, invalid model ID, etc.)  instead of raising <code>StepExecutionError</code> (and HTTP 500 response from the server) will raise  <code>ClientCausedStepExecutionError</code> (and relevant HTTP response codes, such as 400, 401, 403, 404).</p> <p>List of scenarios affected with the change:</p> <ul> <li> <p>Block using Roboflow model defines invalid model ID - now will raise <code>ClientCausedStepExecutionError</code> with status code 400</p> </li> <li> <p>Block using Roboflow model defines invalid API key - now will raise <code>ClientCausedStepExecutionError</code> with status code 401</p> </li> <li> <p>Block using Roboflow model defines invalid API key or missing valid key with scpe to access resource - now will raise  <code>ClientCausedStepExecutionError</code> with status code 403</p> </li> <li> <p>Block using Roboflow model defines model which does not exist - now will raise  <code>ClientCausedStepExecutionError</code> with status code 404</p> </li> </ul> <p>Bringing back <code>legacy</code> error handling</p> <p>It is possible to bring back the legacy behaviour of error handler if needed, which may be halpful in transition  period - all it takes is setting environmental variable <code>DEFAULT_WORKFLOWS_STEP_ERROR_HANDLER=legacy</code>.</p>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v160-inference-v0530","title":"Execution Engine <code>v1.6.0</code> | inference <code>v0.53.0</code>","text":"<p>Change may require attention</p> <p>This release introduces upgrades and new features with no changes required to existing workflows.  Some blocks may need to be upgraded to take advantage of the latest Execution Engine capabilities.</p> <p>Prior versions of the Execution Engine had significant limitations when interacting with certain types of  blocks - specifically those operating in Single Instruction, Multiple Data (SIMD) mode. These blocks are designed to  process batches of inputs at once, apply the same operation to each element, and return results for the entire batch.</p> <p>For example, the <code>run(...)</code> method of such a block might look like:</p> <pre><code>def run(self, image: Batch[WorkflowImageData], confidence: float):\n    pass\n</code></pre> <p>In the manifest, the <code>image</code> field is declared as accepting batches.</p> <p>The issue arose when the input image came from a block that did not operate on batches. In such cases, the  Execution Engine was unable to construct a batch from individual images, which often resulted in frustrating  compilation errors such as:</p> <pre><code>Detected invalid reference plugged into property `images` of step `$steps.model` - the step property \nstrictly requires batch-oriented inputs, yet the input selector holds non-batch oriented input - this indicates \nthe problem with construction of your Workflow - usually the problem occurs when non-batch oriented step inputs are \nfilled with outputs of non batch-oriented steps or non batch-oriented inputs.\n</code></pre> <p>In Execution Engine <code>v1.6.0</code>, this limitation has been removed, introducing the following behaviour:</p> <ul> <li> <p>When it is detected that a given input must be batch-oriented, a procedure called Auto Batch Casting is applied.  This automatically converts the input into a <code>Batch[T]</code>. Since all batch-mode inputs were already explicitly denoted in  manifests, most blocks (with exceptions noted below) benefit from this upgrade without requiring any internal changes.</p> </li> <li> <p>The dimensionality (level of nesting) of an auto-batch cast parameter is determined at compilation time, based on the  context of the specific block in the workflow as well as its manifest. If other batch-oriented inputs are present  (referred to as lineage supports), the Execution Engine uses them as references when constructing auto-casted  batches. This ensures that the number of elements in each batch dimension matches the other data fed into the step  (simulating what would have been asserted if an actual batch input had been provided). If there are no  lineage supports, or if the block manifest requires it (e.g. input dimensionality offset is set), the missing  dimensions are generated similarly to the <code>torch.unsqueeze(...)</code> operation.</p> </li> <li> <p>Step outputs are then evaluated against the presence of an Auto Batch Casting context. Based on the evaluation,  outputs are saved either as batches or as scalars, ensuring that the effect of casting remains local, with the only  exception being output dimensionality changes introduced by the block itself. As a side effect, it is now possible to:</p> <ul> <li> <p>create output batches from scalars (when the step increases dimensionality), and</p> </li> <li> <p>collapse batches into scalars (when the block decreases dimensionality).</p> </li> </ul> </li> <li> <p>The two potential friction point arises - first when a block that does not accept batches (and thus does not denote  batch-accepting inputs) decreases output dimensionality. In previous versions, the Execution Engine handled this by  applying dimensionality wrapping: all batch-oriented inputs were wrapped with an additional <code>Batch[T]</code> dimension,  allowing the block\u2019s <code>run(...)</code> method to perform reduce operations across the list dimension. With Auto Batch Casting,  however, such blocks no longer provide the Execution Engine with a clear signal about whether certain inputs are  scalars or batches, making casting nondeterministic. To address this, a new manifest method was introduced:  <code>get_parameters_enforcing_auto_batch_casting(...)</code>. This method must return the list of parameters for which batch  casting should be enforced when dimensionality is decreased. It is not expected to be used in any other context.</p> </li> </ul> <p>Impact of new method on existing blocks</p> <p>The requirement of defining <code>get_parameters_enforcing_auto_batch_casting(...)</code> method to fully use  Auto Batch Casting feature in the case described above is non-strict. If the block will not be changed, the only effect will be that workflows wchich were previously failing with compilation error may  work or fail with runtime error, dependent on the details of block <code>run(...)</code> method implementation.</p> <ul> <li> <p>The second friction point arises when there is a block declaring input fields supporting batches and scalars using  <code>get_parameters_accepting_batches_and_scalars(...)</code> - by default, Execution Engine will skip auto-casting for such  parameters, as the method was historically always a way to declare that block itself has ability to broadcast scalars  into batches - see  implementation of <code>roboflow_core/detections_transformation@v1</code>  block. In a way, Auto Batch Casting is redundant for those blocks - so we propose leaving them as is and  upgrade to use <code>get_parameters_enforcing_auto_batch_casting(...)</code> instead of  <code>get_parameters_accepting_batches_and_scalars(...)</code> in new versions of such blocks.</p> </li> <li> <p>In earlier versions, a hard constraint existed: dimensionality collapse could only occur at levels \u2265 2 (i.e. only  on nested batches). This limitation is now removed. Dimensionality collapse blocks may also operate on scalars, with  the output dimensionality \u201cbouncing off\u201d the zero ground.</p> </li> </ul> <p>There is one key change in how outputs are built. In earlier versions of Execution Error, a block was not allowed  to produce a <code>Batch[X]</code> directly at the first dimension level \u2014 that space was reserved for mapping onto input batches. Starting with version <code>v1.6.0</code>, this restriction has been removed. </p> <p>Previously, outputs were always returned as a list of elements:</p> <ul> <li> <p>aligned with the input batches, or</p> </li> <li> <p>a single-element list if only scalars were given as inputs.</p> </li> </ul> <p>This raised a question: what should happen if a block now produces a batch at the first dimension level? We cannot simply <code>zip(...)</code> it with input-based outputs, since the size of these newly generated batches might not  match the number of input elements \u2014 making the operation ambiguous.</p> <p>To resolve this, we adopted the following rule:</p> <ul> <li> <p>Treat the situation as if there were a \"dummy\" input batch of size 1.</p> </li> <li> <p>Consider all batches produced from scalar inputs as being one level deeper than they appear.</p> </li> <li> <p>This follows the principle of broadcasting, allowing such outputs to expand consistently across all elements.</p> </li> <li> <p>Input batch may vanish as a result of execution, but when this happens and new first-level dimension emerges, it  is still going to be virtually nested to ensure outputs consistency.</p> </li> </ul> <p>Example:</p> <pre><code>(NO INPUTS)    IMAGE FETCHER BLOCK --&gt; image --&gt; OD MODEL --&gt; predictons --&gt; CROPS --&gt; output will be: [\"crops\": [&lt;crop&gt;, &lt;crop&gt;, ...]] \n</code></pre> <p>It is important to note that results generated from previously created workflows valid will be the same and the  change will only affect new workflows created to utilise new functionalities.</p>"},{"location":"workflows/execution_engine_changelog/#migration-guide","title":"Migration guide","text":"Adding <code>get_parameters_enforcing_auto_batch_casting(...)</code> method <p>Blocks which decrease output dimensionality and do not define batch-oriented inputs needs to  declare all inputs which implementation expects to have wrapped in <code>Batch[T]</code> with the new class  method of block manifest called <code>get_parameters_enforcing_auto_batch_casting(...)</code></p> <pre><code>from typing import List, Literal, Type, Union\n\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    Selector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/tile_detections@v1\"]\n    crops: Selector(kind=[IMAGE_KIND])\n    crops_predictions: Selector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND]\n    )\n    scalar_parameter: Union[float, Selector()]\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return -1\n\n    @classmethod\n    def get_parameters_enforcing_auto_batch_casting(cls) -&gt; List[str]:\n        return [\"crops\", \"crops_predictions\"]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"visualisations\", kind=[IMAGE_KIND]),\n        ]\n\n\nclass TileDetectionsBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        crops: Batch[WorkflowImageData],\n        crops_predictions: Batch[sv.Detections],\n        scalar_parameter: float,\n    ) -&gt; BlockResult:\n        print(\"This is parameter which will not be auto-batch cast!\", scalar_parameter)\n        annotator = sv.BoxAnnotator()\n        visualisations = []\n        for image, prediction in zip(crops, crops_predictions):\n            annotated_image = annotator.annotate(\n                image.numpy_image.copy(),\n                prediction,\n            )\n            visualisations.append(annotated_image)\n        tile = sv.create_tiles(visualisations)\n        return {\"visualisations\": tile}\n</code></pre> <ul> <li> <p>in lines <code>34-36</code> one needs to add declaration of fields that will be subject to enforced auto-batch casting</p> </li> <li> <p>as a result of the above, input parameters of run method (lines <code>53-54</code>) will be wrapped into <code>Batch[T]</code> by  Execution Engine.</p> </li> </ul>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v150-inference-v0380","title":"Execution Engine <code>v1.5.0</code> | inference <code>v0.38.0</code>","text":"<p>Change does not require any action</p> <p>This change does not require any change from Workflows users. This is just performance optimisation.</p> <ul> <li> <p>Exposed new parameter in the init method of <code>BaseExecutionEngine</code> class - <code>executor</code> which can accept instance of  Python <code>ThreadPoolExecutor</code> to be used by execution engine. Thanks to this change, processing should be faster, as  each <code>BaseExecutionEngine.run(...)</code> will not require dedicated instance of <code>ThreadPoolExecutor</code> as it was so far. Additionally, we are significantly limiting threads spawning which may also be a benefit in some installations.</p> </li> <li> <p>Despite the change, Execution Engine maintains the limit of concurrently executed steps - by limiting the number of steps that run through the executor at a time (since  Execution Engine is no longer in control of <code>ThreadPoolExecutor</code>  creation, and it is possible for the pool to have more workers available).</p> </li> </ul> How to inject <code>ThreadPoolExecutor</code> to Execution Engine? <pre><code>from concurrent.futures import ThreadPoolExecutor\nworkflow_init_parameters = { ... }\nwith ThreadPoolExecutor(max_workers=...) as thread_pool_executor:\n    execution_engine = ExecutionEngine.init(\n        init_parameters=workflow_init_parameters,\n        max_concurrent_steps=4,\n        workflow_id=\"your-workflow-id\",\n        executor=thread_pool_executor,\n    )\n    runtime_parameters = {\n      \"image\": cv2.imread(\"your-image-path\")\n    }\n    results = execution_engine.run(runtime_parameters=runtime_parameters)\n</code></pre>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v140-inference-v0290","title":"Execution Engine <code>v1.4.0</code> | inference <code>v0.29.0</code>","text":"<ul> <li> <p>Added new kind - <code>secret</code> to represent credentials. No action needed for existing  blocks, yet it is expected that over time blocks developers should use this kind, whenever block is to accept secret  value as parameter.</p> </li> <li> <p>Fixed issue with results serialization introduced in <code>v1.3.0</code> - by mistake, Execution Engine was not serializing  non-batch oriented outputs.</p> </li> <li> <p>Fixed Execution Engine bug with preparing inputs for steps. For non-SIMD steps before, while collecting inputs  in runtime, <code>WorkflowBlockManifest.accepts_empty_input()</code> method result was being ignored - causing the bug when one non-SIMD step was feeding empty values to downstream blocks. Additionally, in the light of changes made in <code>v1.3.0</code>, thanks to which non-SIMD blocks can easily feed inputs for downstream SIMD steps - it is needed to check if  upstream non-SIMD block yielded non-empty results (as SIMD block may not accept empty results). This check was added. No action needed for existing blocks, but this fix may fix previously broken Workflows.</p> </li> </ul>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v130-inference-v0270","title":"Execution Engine <code>v1.3.0</code> | inference <code>v0.27.0</code>","text":"<ul> <li> <p>Introduced the change that let each kind have serializer and deserializer defined. The change decouples Workflows  plugins with Execution Engine and make it possible to integrate the ecosystem with external systems that  require data transfer through the wire. Blocks bundling page was updated to reflect  that change.</p> </li> <li> <p>Kinds defined in <code>roboflow_core</code> plugin were provided with suitable serializers and deserializers</p> </li> <li> <p>Workflows Compiler and Execution Engine were enhanced to support batch-oriented inputs of  any kind, contrary to versions prior <code>v1.3.0</code>, which could only take <code>image</code> and <code>video_metadata</code> kinds as batch-oriented inputs (as a result of unfortunate and not-needed coupling of kind to internal data  format introduced at the level of Execution Engine). As a result of the change:</p> <ul> <li> <p>new input type was introduced: <code>WorkflowBatchInput</code> should be used from now on to denote  batch-oriented inputs (and clearly separate them from <code>WorkflowParameters</code>). <code>WorkflowBatchInput</code>  let users define both kind of the data and it's  dimensionality. New input type is effectively a superset of all previous batch-oriented inputs: <code>WorkflowImage</code> and <code>WorkflowVideoMetadata</code>, which remain supported, but will be removed in Execution Engine <code>v2</code>.  We advise adjusting to the new input format, yet the requirement is not strict at the moment - as  Execution Engine requires now explicit definition of input data kind to select data deserializer properly. This may not be the case in the future, as in most cases batch-oriented data kind may be inferred by compiler (yet this feature is not implemented for now).</p> </li> <li> <p>new selector type annotation was introduced - named simply <code>Selector(...)</code>. <code>Selector(...)</code> is supposed to replace <code>StepOutputSelector</code>, <code>WorkflowImageSelector</code>, <code>StepOutputImageSelector</code>,  <code>WorkflowVideoMetadataSelector</code> and <code>WorkflowParameterSelector</code> in block manifests,  letting developers express that specific step manifest property is able to hold either selector of specific kind. Mentioned old annotation types should be assumed deprecated, we advise to migrate into <code>Selector(...)</code>. </p> </li> <li> <p>as a result of simplification in the selectors type annotations, the old selector will no  longer be providing the information on which parameter of blocks' <code>run(...)</code> method is  shipped by Execution Engine wrapped into <code>Batch[X]</code> container. Instead of old selectors type annotations and <code>block_manifest.accepts_batch_input()</code> method,  we propose the switch into two methods explicitly defining the parameters that are expected to  be fed with batch-oriented data (<code>block_manifest.get_parameters_accepting_batches()</code>) and  parameters capable of taking both batches and scalar values  (<code>block_manifest.get_parameters_accepting_batches_and_scalars()</code>). Return value of <code>block_manifest.accepts_batch_input()</code> is built upon the results of two new methods. The change is non-breaking, as any existing block which was capable of processing batches must have implemented <code>block_manifest.accepts_batch_input()</code> method returning <code>True</code> and use appropriate selector type annotation which indicated batch-oriented data.</p> </li> </ul> </li> <li> <p>As a result of the changes, it is now possible to split any arbitrary workflows into multiple ones executing  subsets of steps, enabling building such tools as debuggers.</p> </li> </ul> <p>Breaking change planned - Execution Engine <code>v2.0.0</code></p> <ul> <li> <p><code>WorkflowImage</code> and <code>WorkflowVideoMetadata</code> inputs will be removed from Workflows ecosystem.</p> </li> <li> <p><code>StepOutputSelector,</code>WorkflowImageSelector<code>,</code>StepOutputImageSelector<code>,</code>WorkflowVideoMetadataSelector<code>and</code>WorkflowParameterSelector` type annotations used in block manifests will be removed from Workflows ecosystem.</p> </li> </ul>"},{"location":"workflows/execution_engine_changelog/#migration-guide_1","title":"Migration guide","text":"Kinds' serializers and deserializers <p>Creating your Workflows plugin you may introduce custom serializers and deserializers for Workflows kinds. To achieve that end, simply place the following dictionaries in the main module of the plugin (the same where you place <code>load_blocks(...)</code> function):</p> <pre><code>from typing import Any\n\ndef serialize_kind(value: Any) -&gt; Any:\n  # place here the code that will be used to\n  # transform internal Workflows data representation into \n  # the external one (that can be sent through the wire in JSON, using\n  # default JSON encoder for Python).\n  pass\n\n\ndef deserialize_kind(parameter_name: str, value: Any) -&gt; Any:\n  # place here the code that will be used to decode \n  # data sent through the wire into the Execution Engine\n  # and transform it into proper internal Workflows data representation\n  # which is understood by the blocks.\n  pass\n\n\nKINDS_SERIALIZERS = {\n    \"name_of_the_kind\": serialize_kind,\n}\nKINDS_DESERIALIZERS = {\n    \"name_of_the_kind\": deserialize_kind,\n}\n</code></pre> New type annotation for selectors - blocks without <code>Batch[X]</code> inputs <p>Blocks manifest may  optionally be updated to use <code>Selector</code> in the following way:</p> <pre><code>from typing import Union\nfrom inference.core.workflows.prototypes.block import WorkflowBlockManifest\nfrom inference.core.workflows.execution_engine.entities.types import (\n    INSTANCE_SEGMENTATION_PREDICTION_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    FLOAT_KIND,\n    WorkflowImageSelector,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowParameterSelector,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n\n    reference_image: Union[WorkflowImageSelector, StepOutputImageSelector]\n    predictions: StepOutputSelector(\n        kind=[\n            OBJECT_DETECTION_PREDICTION_KIND,\n            INSTANCE_SEGMENTATION_PREDICTION_KIND,\n        ]\n    )\n    confidence: WorkflowParameterSelector(kind=[FLOAT_KIND]) \n</code></pre> <p>should just be changed into:</p> <pre><code>from inference.core.workflows.prototypes.block import WorkflowBlockManifest\nfrom inference.core.workflows.execution_engine.entities.types import (\n    INSTANCE_SEGMENTATION_PREDICTION_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    FLOAT_KIND,\n    IMAGE_KIND,\n    Selector,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    reference_image: Selector(kind=[IMAGE_KIND])\n    predictions: Selector(\n        kind=[\n            OBJECT_DETECTION_PREDICTION_KIND,\n            INSTANCE_SEGMENTATION_PREDICTION_KIND,\n        ]\n    )\n    confidence: Selector(kind=[FLOAT_KIND]) \n</code></pre> New type annotation for selectors - blocks with <code>Batch[X]</code> inputs <p>Blocks manifest may  optionally be updated to use <code>Selector</code> in the following way:</p> <pre><code>from typing import Union\nfrom inference.core.workflows.prototypes.block import WorkflowBlockManifest\nfrom inference.core.workflows.execution_engine.entities.types import (\n    INSTANCE_SEGMENTATION_PREDICTION_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    FLOAT_KIND,\n    WorkflowImageSelector,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowParameterSelector,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n\n    reference_image: Union[WorkflowImageSelector, StepOutputImageSelector]\n    predictions: StepOutputSelector(\n        kind=[\n            OBJECT_DETECTION_PREDICTION_KIND,\n            INSTANCE_SEGMENTATION_PREDICTION_KIND,\n        ]\n    )\n    data: Dict[str, Union[StepOutputSelector(), WorkflowParameterSelector()]]\n    confidence: WorkflowParameterSelector(kind=[FLOAT_KIND]) \n\n    @classmethod\n    def accepts_batch_input(cls) -&gt; bool:\n        return True\n</code></pre> <p>should be changed into:</p> <pre><code>from inference.core.workflows.prototypes.block import WorkflowBlockManifest\nfrom inference.core.workflows.execution_engine.entities.types import (\n    INSTANCE_SEGMENTATION_PREDICTION_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    FLOAT_KIND,\n    IMAGE_KIND,\n    Selector,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    reference_image: Selector(kind=[IMAGE_KIND])\n    predictions: Selector(\n        kind=[\n            OBJECT_DETECTION_PREDICTION_KIND,\n            INSTANCE_SEGMENTATION_PREDICTION_KIND,\n        ]\n    )\n    data: Dict[str, Selector()]\n    confidence: Selector(kind=[FLOAT_KIND]) \n\n    @classmethod\n    def get_parameters_accepting_batches(cls)W -&gt; List[str]:\n        return [\"predictions\"]\n\n    @classmethod\n    def get_parameters_accepting_batches_and_scalars(cls) -&gt; List[str]:\n        return [\"data\"]\n</code></pre> <p>Please point out that:</p> <ul> <li>the <code>data</code> property in the original example was able to accept both batches of data and scalar values due to selector of batch-orienetd data (<code>StepOutputSelector</code>) and  scalar data (<code>WorkflowParameterSelector</code>). Now the same is manifested by <code>Selector(...)</code> type  annotation and return value from <code>get_parameters_accepting_batches_and_scalars(...)</code> method.</li> </ul> New inputs in Workflows definitions <p>Anyone that used either <code>WorkflowImage</code> or <code>WorkflowVideoMetadata</code> inputs in their  Workflows definition may optionally migrate into <code>WorkflowBatchInput</code>. The transition is illustrated below:</p> <pre><code>{\n  \"inputs\": [\n    {\"type\": \"WorkflowImage\", \"name\": \"image\"},\n    {\"type\": \"WorkflowVideoMetadata\", \"name\": \"video_metadata\"}\n  ]\n}\n</code></pre> <p>should be changed into: <pre><code>{\n  \"inputs\": [\n    {\n      \"type\": \"WorkflowBatchInput\",\n      \"name\": \"image\",\n      \"kind\": [\"image\"]\n    },\n    {\n      \"type\": \"WorkflowBatchInput\",\n      \"name\": \"video_metadata\",\n      \"kind\": [\"video_metadata\"]\n    }\n  ]\n}\n</code></pre></p> <p>Leaving <code>kind</code> field empty may prevent some data - like images - from being deserialized properly.</p> <p>Note</p> <p>If you do not like the way how data is serialized in <code>roboflow_core</code> plugin,  feel free to alter the serialization methods for kinds, simply registering the function in your plugin and loading it to the Execution Engine - the  serializer/deserializer defined as the last one will be in use.</p>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v120-inference-v0230","title":"Execution Engine <code>v1.2.0</code> | inference <code>v0.23.0</code>","text":"<ul> <li>The <code>video_metadata</code> kind has been deprecated, and we strongly recommend discontinuing its use for building  blocks moving forward. As an alternative, the <code>image</code> kind has been extended to support the same metadata as  <code>video_metadata</code> kind, which can now be provided optionally. This update is  non-breaking for existing blocks, but some older blocks that produce images may become incompatible with  future video processing blocks.</li> </ul> Potential blocks incompatibility <p>As previously mentioned, adding <code>video_metadata</code> as an optional field to the internal representation of  <code>image</code> kind (<code>WorkflowImageData</code> class)  may introduce some friction between existing blocks that output the <code>image</code> kind and  future video processing blocks that rely on <code>video_metadata</code> being part of <code>image</code> representation. </p> <p>The issue arises because, while we can provide default values for <code>video_metadata</code> in <code>image</code> without  explicitly copying them from the input, any non-default metadata that was added upstream may be lost.  This can lead to downstream blocks that depend on the <code>video_metadata</code> not functioning as expected.</p> <p>We've updated all existing <code>roboflow_core</code> blocks to account for this, but blocks created before this change in external repositories may cause issues in workflows where their output images are used by video processing blocks.</p> <ul> <li>While the deprecated <code>video_metadata</code> kind is still available for use, it will be fully removed in  Execution Engine version <code>v2.0.0</code>.</li> </ul> <p>Breaking change planned - Execution Engine <code>v2.0.0</code></p> <p><code>video_metadata</code> kind got deprecated and will be removed in <code>v2.0.0</code></p> <ul> <li>As a result of the changes mentioned above, the internal representation of the <code>image</code> kind has been updated to  include a new <code>video_metadata</code> property. This property can be optionally set in the constructor; if not provided,  a default value with reasonable defaults will be used. To simplify metadata manipulation within blocks, we have  introduced two new class methods: <code>WorkflowImageData.copy_and_replace(...)</code> and <code>WorkflowImageData.create_crop(...)</code>.  For more details, refer to the updated <code>WoorkflowImageData</code> usage guide.</li> </ul>"},{"location":"workflows/gallery_index_template/","title":"Workflows gallery","text":"<p>The Workflows gallery offers example workflow definitions to help you understand what can be achieved with workflows.  Browse through the various categories to find inspiration and ideas for building your own workflows.</p>"},{"location":"workflows/internal_data_types/","title":"Data representations in Workflows","text":"<p>Many frameworks enforce standard data types for developers to work with, and the Workflows ecosystem is no  exception. While the kind in a Workflow represents a high-level abstraction of the data  being passed through, it's important to understand the specific data types that will be provided to the  <code>WorkflowBlock.run(...)</code> method when building Workflow blocks.</p> <p>And this is exactly what you will learn here.</p>"},{"location":"workflows/internal_data_types/#batch","title":"<code>Batch</code>","text":"<p>When a Workflow block declares batch processing, it uses a special container type called Batch.  All batch-oriented parameters are wrapped with <code>Batch[X]</code>, where X is the data type:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\nfrom inference.core.workflows.prototypes.block import BlockResult\n\n# run method of Workflow block\ndef run(self, x: Batch[int], y: Batch[float]) -&gt; BlockResult:\n   pass\n</code></pre> <p>The <code>Batch</code> type functions similarly to a Python list, with one key difference: it is read-only. You cannot modify  its elements, nor can you add or remove elements. However, several useful operations are available:</p>"},{"location":"workflows/internal_data_types/#iteration-through-elements","title":"Iteration through elements","text":"<pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef iterate(batch: Batch[int]) -&gt; None:\n    for element in batch:\n        print(element)\n</code></pre>"},{"location":"workflows/internal_data_types/#zipping-multiple-batches","title":"Zipping multiple batches","text":"<p>Do not worry about batches alignment</p> <p>The Execution Engine ensures that batches provided to the run method are of equal size, preventing any loss of  elements due to unequal batch sizes during iteration.</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef zip_batches(batch_1: Batch[int], batch_2: Batch[int]) -&gt; None:\n    for element_1, element_2 in zip(batch_1, batch_2):\n        print(element_1, element_2)\n</code></pre>"},{"location":"workflows/internal_data_types/#getting-batch-element-indices","title":"Getting batch element indices","text":"<p>This returns a list of tuples where each tuple represents the position of the batch element in  potentially nested batch structures.</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef discover_indices(batch: Batch[int]) -&gt; None:\n    for index in batch.indices:\n        print(index)  # e.g., (0,) for 1D batch, (1, 3) for 2D nested batch, etc.\n</code></pre>"},{"location":"workflows/internal_data_types/#iterating-while-retrieving-both-elements-and-their-indices","title":"Iterating while retrieving both elements and their indices","text":"<pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef iterate_with_indices(batch: Batch[int]) -&gt; None:\n    for index, element in batch.iter_with_indices():\n        print(index, element)\n</code></pre>"},{"location":"workflows/internal_data_types/#additional-methods-of-batch-container","title":"Additional methods of <code>Batch</code> container","text":"<p>There are other methods in the Batch interface, such as <code>remove_by_indices(...)</code> or <code>broadcast(...)</code>, but these  are not intended for use within Workflow blocks. These methods are primarily used  by the Execution Engine when providing data to the block.</p>"},{"location":"workflows/internal_data_types/#workflowimagedata","title":"<code>WorkflowImageData</code>","text":"<p><code>WorkflowImageData</code> is a dataclass that encapsulates an image along with its metadata, providing useful methods to  manipulate the image representation within a Workflow block.</p> <p>Some users may expect an <code>np.ndarray</code> to be provided directly by the Execution Engine when an <code>image</code> kind is declared.  While that could be a convenient and straightforward approach, it introduces limitations, such as:</p> <ul> <li> <p>Lack of metadata: With only an <code>np.ndarray</code>, there's no way to attach metadata such as data lineage or the  image's location within the original file (e.g., when working with cropped images).</p> </li> <li> <p>Inability to cache multiple representations: If multiple blocks need to serialize and send the image over  HTTP, WorkflowImageData allows caching of different image representations, such as base64-encoded versions,  improving efficiency.</p> </li> </ul> <p>Video Metadata</p> <p>Since Execution Enginge <code>v1.2.0</code>, we have added <code>video_metadata</code> into <code>WorkflowImageData</code>. This  object is supposed to hold the context of video processing and will only be relevant for video processing blocks. Other blocks may ignore it's existance if not creating output image (covered in the next section). </p> <p>Operating on <code>WorkflowImageData</code> is fairly simple once you understand its interface. Here are some of the key methods and properties:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\n\ndef operate_on_image(workflow_image: WorkflowImageData) -&gt; None:\n    # Getting an np.ndarray representing the image.\n    numpy_image = workflow_image.numpy_image  \n\n    # Getting base64-encoded JPEG representation of the image, ideal for API transmission.\n    base64_image: str = workflow_image.base64_image  \n\n    # Converting the image into a format compatible with inference models.\n    inference_format = workflow_image.to_inference_format()  \n\n    # Accesses metadata related to the parent image\n    parent_metadata = workflow_image.parent_metadata\n    print(parent_metadata.parent_id)  # parent identifier\n    origin_coordinates = parent_metadata.origin_coordinates  # optional object with coordinates\n    print(\n        origin_coordinates.left_top_x, origin_coordinates.left_top_y, \n        origin_coordinates.origin_width, origin_coordinates.origin_height,\n    )\n\n    # or the same for root metadata (the oldest ancestor of the image - Workflow input image)\n    root_metadata = workflow_image.workflow_root_ancestor_metadata\n\n    # retrieving `VideoMetadata` object - see the usage guide section below\n    # if `workflow_image` is not provided with `VideoMetadata` - default metadata object will \n    # be created on accessing the property\n    video_metadata = workflow_image.video_metadata \n</code></pre> <p>Below you can find an example showcasing how to preserve metadata, while transforming image</p> <pre><code>import numpy as np\n\nfrom inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\n\ndef transform_image(image: WorkflowImageData) -&gt; WorkflowImageData:\n    transformed_image = some_transformation(image.numpy_image)\n    # `WorkflowImageData` exposes helper method to return a new object with\n    # updated image, but with preserved metadata. Metadata preservation\n    # should only be used when the output image is compatible regarding\n    # data lineage (the predecessor-successor relation for images).\n    # Lineage is not preserved for cropping and merging images (without common predecessor)\n    # - below you may find implementation tips.\n    return WorkflowImageData.copy_and_replace(\n        origin_image_data=image,\n        numpy_image=transformed_image,\n    )\n\n\ndef some_transformation(image: np.ndarray) -&gt; np.ndarray:\n    ...\n</code></pre> Images cropping <p>When your block increases dimensionality and provides output with <code>image</code> kind - usually that means cropping the  image. In such cases input image <code>video_metadata</code> is to be removed (as usually it does not make sense to keep them, as underlying video processing blocks will not work correctly when for dynamically created blocks).</p> <p>Below you can find scratch of implementation for that operation:</p> <pre><code>from typing import List, Tuple\n\nfrom dataclasses import replace\nfrom inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\ndef crop_images(\n    image: WorkflowImageData, \n    crops: List[Tuple[str, int, int, int, int]],\n) -&gt; List[WorkflowImageData]:\n    crops = []\n    original_image = image.numpy_image\n    for crop_id, x_min, y_min, x_max, y_max in crops:\n        cropped_image = original_image[y_min:y_max, x_min:x_max]\n        if not cropped_image.size:\n            # discarding empty crops\n            continue\n        result_crop = WorkflowImageData.create_crop(\n            origin_image_data=image, \n            crop_identifier=crop_id,\n            cropped_image=cropped_image,\n            offset_x=x_min,\n            offset_y=y_min,\n        )\n        crops.append(result_crop)\n    return crops\n</code></pre> <p>In some cases you may want to preserve <code>video_metadata</code>. Example of such situation is when  your block produces crops based on fixed coordinates (like video single footage with multiple fixed Regions of  Interest to be applied individual trackers) - then you want result crops to be processed in context of video, as if they were produced by separate cameras. To adjust behaviour of <code>create_crop(...)</code> method, simply add  <code>preserve_video_metadata=True</code>:</p> <pre><code>def crop_images(\n    image: WorkflowImageData, \n    crops: List[Tuple[str, int, int, int, int]],\n) -&gt; List[WorkflowImageData]:\n    # [...]\n    result_crop = WorkflowImageData.create_crop(\n        origin_image_data=image, \n        crop_identifier=crop_id,\n        cropped_image=cropped_image,\n        offset_x=x_min,\n        offset_y=y_min,\n        preserve_video_metadata=True\n    )\n    # [...]\n</code></pre> Merging images without common predecessor <p>If common <code>parent_metadata</code> cannot be pointed for multiple images you try to merge, you should denote that \"a new\" image appears in the Workflow. To do it simply:</p> <pre><code>from typing import List, Tuple\n\nfrom dataclasses import replace\nfrom inference.core.workflows.execution_engine.entities.base import \\\n    WorkflowImageData, ImageParentMetadata\n\ndef merge_images(image_1: WorkflowImageData, image_2: WorkflowImageData) -&gt; WorkflowImageData:\n    merged_image = some_mergin_operation(\n        image_1=image_1.numpy_image,\n        image_2=image_2.numpy_image\n    )\n    new_parent_metadata = ImageParentMetadata(\n        # this is just one of the option for creating id, yet sensible one\n        parent_id=f\"{image_1.parent_metadata.parent_id} + {image_2.parent_metadata.parent_id}\"\n    )\n    return WorkflowImageData(\n        parent_metadata=new_parent_metadata,\n        numpy_image=merged_imagem\n    )\n</code></pre>"},{"location":"workflows/internal_data_types/#videometadata","title":"<code>VideoMetadata</code>","text":"<p>Deprecation</p> <p><code>video_metadata</code> kind is deprecated - we advise not using that kind in new  blocks. <code>VideoMetadata</code> data representation became a member of <code>WorkflowImageData</code> in Execution Engine <code>v1.2.0</code>  (<code>inference</code> release <code>v0.23.0</code>)</p> <p><code>VideoMetadata</code> is a dataclass that provides the following metadata about video frame and video source:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import VideoMetadata\n\n\ndef inspect_vide_metadata(video_metadata: VideoMetadata) -&gt; None:\n    # Identifier string for video. To be treated as opaque.\n    print(video_metadata.video_identifier)\n\n    # Sequential number of the frame\n    print(video_metadata.frame_number)\n\n    # The timestamp of video frame. When processing video it is suggested that \"\n    # \"blocks rely on `fps` and `frame_number`, as real-world time-elapse will not \"\n    # \"match time-elapse in video file\n    print(video_metadata.frame_timestamp)\n\n    # Field represents FPS value (if possible to be retrieved) (optional)\n    print(video_metadata.fps)\n\n    # Field represents measured FPS of live stream (optional)\n    print(video_metadata.measured_fps)\n\n    # Field is a flag telling if frame comes from video file or stream.\n    # If not possible to be determined - None\n    print(video_metadata.comes_from_video_file)\n</code></pre>"},{"location":"workflows/kinds_template/","title":"Kinds","text":"<p>In Workflows, some values can\u2019t be set in advance and are only determined during execution.  This is similar to writing a function where you don\u2019t know the exact input values upfront \u2014 they\u2019re only  provided at runtime, either from user inputs or from other function outputs. </p> <p>To manage this, Workflows use selectors, which act like references, pointing to data without containing it directly. </p> <p>selectors</p> <p>Selectors might refer to a named input - for example input image - like <code>$inputs.image</code>  or predictions generated by a previous step - like <code>$steps.my_model.predictions</code></p> <p>In the Workflows ecosystem, users focus on data purpose (e.g., \u201cimage\u201d) without worrying about its exact format.  Meanwhile, developers building workflow blocks need precise data formats. Kinds serve both needs - they simplify data handling for users while ensuring developers work with the correct data structure.</p>"},{"location":"workflows/kinds_template/#what-are-the-kinds","title":"What are the Kinds?","text":"<p>Kinds is Workflows type system with each kind defining:</p> <ul> <li> <p>name - expressing semantic meaning of the underlying data - like <code>image</code> or <code>point</code>;</p> </li> <li> <p>Python data representation - the data type and format that blocks creators should expect when handling  the data within blocks;</p> </li> <li> <p>optional serialized data representation - defining what is the format of the kind that  external systems should use to integrate with Workflows ecosystem - when needed, custom kinds serializers and deserializers are provided to ensure seamless translation; </p> </li> </ul> <p>Using kinds streamlines compatibility: when a step outputs data of a certain kind and another step requires that  same kind, the workflow engine assumes they\u2019ll be compatible, reducing the need for compatibility checks and  providing compile-time verification of Workflows definitions.</p> <p>Note</p> <p>As for now, <code>kinds</code> are such simplistic that do not support types polymorphism - and developers are asked to use unions of kinds to solve that problem. As defining extensive unions of kinds may be  problematic, this problem will probably be addressed in Execution Engine <code>v2</code>.</p> <p>Warning</p> <p>In <code>inference</code> release <code>0.18.0</code> we decided to make drastic move to heal the ecosystem  from the problem with ambiguous kinds names (<code>Batch[X]</code> vs <code>X</code> - see more  here). </p> <p>The change is breaking only if there is remote Workflow plugin depending on imports from <code>inference.core.workflows.execution_engine.entities.types</code> module, which is not the case to the best of our knowledge. We removed problematic kinds as if they never existed in the ecosystem and fixed all blocks from <code>roboflow_core</code> plugin. If there is anyone impacted by the change - here is the  migration guide.</p> <p>This warning will be removed end of Q1 2025.</p> <p>Warning</p> <p>Support for proper serialization and deserialization of any arbitrary kind was  introduced in Execution Engine <code>v1.3.0</code> (released with inference <code>0.26.0</code>). Workflows plugins created prior that change may be updated - see refreshed  Blocks Bundling page.</p> <p>This warning will be removed end of Q1 2025.</p>"},{"location":"workflows/kinds_template/#kinds-declared-in-roboflow-plugins","title":"Kinds declared in Roboflow plugins","text":""},{"location":"workflows/modes_of_running/","title":"What are the options for running workflows?","text":"<p>There are several ways to run a Workflow, including:</p> <ul> <li> <p>Request to the HTTP API (Roboflow Hosted API or a self-hosted <code>inference</code> server) running the Workflows Execution Engine</p> </li> <li> <p>Video processing using InferencePipeline</p> </li> <li> <p>the <code>inference</code> Python package, where you can use the Workflows Execution Engine directly in your Python app</p> </li> </ul>"},{"location":"workflows/modes_of_running/#http-api-request","title":"HTTP API request","text":"<p>This way of running Workflows is ideal for clients who:</p> <ul> <li> <p>Want to use Workflows as a stand-alone, independent part of their systems.</p> </li> <li> <p>Maintain their main applications in languages other than Python.</p> </li> <li> <p>Prefer to offload compute-heavy tasks to dedicated servers.</p> </li> </ul> <p>Roboflow offers a hosted HTTP API that clients can use without needing their own infrastructure.  Alternatively, the <code>inference</code> server (which can run Workflows) can be set up on-site if needed.</p> <p>Running Workflows with the Roboflow Hosted API has several limitations:</p> <ul> <li> <p>Workflow runtime is limited to 20 seconds.</p> </li> <li> <p>The response payload is limited to 6 MB. This means that some blocks\u2014especially visualization blocks\u2014may fail if you use too many of them or if the input images are too large.</p> </li> </ul> <p>Integrating via HTTP is simple: just send a request to the server. You can do this using a HTTP client library in your preferred programming language,  leverage our Inference SDK in Python, or even use cURL. Explore the examples below to see how it\u2019s done.</p> <p>HTTP integration</p> cURLInference SDK in Python (Roboflow Hosted API)Inference SDK in Python (on-prem) <p>To run your workflow created in Roboflow APP with <code>cURL</code>, use the following command:</p> <pre><code>curl --location 'https://detect.roboflow.com/infer/workflows/&lt;your-workspace-name&gt;/&lt;your-workflow-id&gt;' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n    \"api_key\": \"&lt;YOUR-API-KEY&gt;\",\n    \"inputs\": {\n        \"image\": {\"type\": \"url\", \"value\": \"https://your-image-url\"},\n        \"parameter\": \"some-value\"\n    }\n}'\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>keys of <code>inputs</code> dictionary are dictated by your Workflow, names may differ dependent on  parameters you define</p> </li> <li> <p>values of <code>inputs</code> dictionary are also dependent on your Workflow definition - inputs declared as <code>WorkflowImage</code> have special structure - dictionary with <code>type</code> and <code>value</code> keys - using cURL your  options are <code>url</code> and <code>base64</code> as <code>type</code> - and value adjusted accordingly</p> </li> </ul> <p>To run your workflow created in Roboflow APP with <code>InferenceClient</code>:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n)\n\nresult = client.run_workflow(\n    workspace_name=\"&lt;your-workspace-name&gt;\",\n    workflow_id=\"&lt;your-workflow-id&gt;\",\n    images={\n        \"image\": [\"https://your-image-url\", \"https://your-other-image-url\"]\n    },\n    parameters={\n        \"parameter\": \"some-value\"\n    },\n)\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>method parameter named <code>images</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs declared as <code>WorkflowImage</code>. Names must match your Workflow definition, as value you can pass either <code>np.array</code>, <code>PIL.Image</code>, URL to your image, local path to your image or image in <code>base64</code> string. It is optional if Workflow does not define images as inputs. </p> </li> <li> <p>Batch input for images is supported - simply pass list of images under given input name.</p> </li> <li> <p>method parameter named <code>parameters</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs of type <code>WorkflowParameter</code>. It's optional and must be filled according to Workflow definition.</p> </li> </ul> <p>Note</p> <p>Please make sure you have <code>inference-sdk</code> package installed in your environment</p> <p>To run your workflow created in Roboflow APP with <code>InferenceClient</code>:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",  # please modify that value according to URL of your server\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n)\n\nresult = client.run_workflow(\n    workspace_name=\"&lt;your-workspace-name&gt;\",\n    workflow_id=\"&lt;your-workflow-id&gt;\",\n    images={\n        \"image\": [\"https://your-image-url\", \"https://your-other-image-url\"]\n    },\n    parameters={\n        \"parameter\": \"some-value\"\n    }    \n)\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>method parameter named <code>images</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs declared as <code>WorkflowImage</code>. Names must match your Workflow definition, as value you can pass either <code>np.array</code>, <code>PIL.Image</code>, URL to your image, local path to your image or image in <code>base64</code> string. It is optional if Workflow does not define images as inputs.</p> </li> <li> <p>Batch input for images is supported - simply pass list of images under given input name.</p> </li> <li> <p>method parameter named <code>parameters</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs of type <code>WorkflowParameter</code>. It's optional and must be filled according to Workflow definition.</p> </li> </ul> <p>Note</p> <ul> <li> <p>Please make sure you have <code>inference-sdk</code> package installed in your environment.</p> </li> <li> <p>Easiest way to run <code>inference</code> server on-prem is to use <code>inference-cli</code> package command: <pre><code>inference server start\n</code></pre></p> </li> </ul> <p>The above examples present how to run Workflow created and saved in Roboflow APP. It is also possible to create and run workflow that is created from scratch and may not contain API-KEY gated blocks (for instance  your own blocks). Then you should use the  following endpoint or Inference SDK as showcased in docs.</p>"},{"location":"workflows/modes_of_running/#video-processing-using-inferencepipeline","title":"Video processing using <code>InferencePipeline</code>","text":"<p>For use cases involving video files or streams, we recommend using InferencePipeline, which can run  Workflows on each video frame.</p> <p>This option is ideal for clients who:</p> <ul> <li> <p>Need low-latency, high-throughput video processing.</p> </li> <li> <p>Design workflows with single-frame processing times that meet real-time requirements (though complex workflows  might not be suitable for real-time processing)</p> </li> </ul> <p>Explore the example below to see how to combine <code>InferencePipeline</code> with Workflows.</p> <p>Integration with InferencePipeline</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef my_sink(result: dict, video_frame: VideoFrame):\n    print(result) # here you can find dictionary with outputs from your workflow\n\n\n# initialize a pipeline object\npipeline = InferencePipeline.init_with_workflow(\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n    workspace_name=\"&lt;your-workspace-name&gt;\",\n    workflow_id=\"&lt;your-workflow-id&gt;\",\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=my_sink,\n    image_input_name=\"image\",  # this parameter holds the name of Workflow input that represents \n    # image to be processed - please ADJUST it to your Workflow Definition \n)\npipeline.start() #start the pipeline\npipeline.join() #wait for the pipeline thread to finish\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>your Workflow must accept video frames under <code>image</code> parameter - when multiple video streams are  given for processing, all collected video frames will be submitted in batch under <code>image</code> parameter for workflow run. <code>image</code> parameter must be single batch oriented input of your workflow</p> </li> <li> <p>additional (non-batch oriented) inputs for your workflow can be passed as parameter to <code>init_with_workflow(...)</code>  method see docs</p> </li> </ul> <p>Note</p> <p>Make sure you have <code>inference</code> or <code>inference-gpu</code> package installed in your Python environment</p>"},{"location":"workflows/modes_of_running/#batch-processing-using-inference-cli","title":"Batch processing using <code>inference-cli</code>","text":"<p><code>inference-cli</code> is command-line wrapper library around <code>inference</code>. You can use it to process your data using Workflows without writing a single line of code. You simply point the data to be processed, select your Workflow and specify where results should be saved. Thanks to <code>inference-cli</code> you can process:</p> <ul> <li> <p>individual images</p> </li> <li> <p>directories of images</p> </li> <li> <p>video files</p> </li> </ul> <p>Processing directory of images</p> <p>You can start the processing using the following command:</p> <pre><code>inference workflows process-images-directory \\\n    -i {your_input_directory} \\\n    -o {your_output_directory} \\[workflows.py](..%2F..%2Finference_cli%2Fworkflows.py)\n    --workspace_name {your-roboflow-workspace-url} \\\n    --workflow_id {your-workflow-id} \\\n    --api-key {your_roboflow_api_key}\n</code></pre> <p>As a result, in the directory specified in <code>-o</code> option you should be able to find:</p> <ul> <li> <p>sub-directories named after files in your original directory with <code>results.json</code> file that contain Worklfow  results and optionally additional <code>*.jpg</code> files with images created during Workflow execution</p> </li> <li> <p><code>aggregated_results.csv</code> file that contain concatenated results of Workflow execution for all input image file</p> </li> </ul> <p>Note</p> <p>Make sure you have <code>inference</code> or <code>inference-cli</code> package installed in your Python environment</p>"},{"location":"workflows/modes_of_running/#workflows-in-python-package","title":"Workflows in Python package","text":"<p>Workflows Compiler and Execution Engine are bundled with <code>inference</code> package. Running Workflow directly may be ideal for clients who:</p> <ul> <li> <p>maintain their applications in Python</p> </li> <li> <p>agree for resource-heavy computations directly in their app</p> </li> <li> <p>want to avoid additional latency and errors related to sending HTTP requests</p> </li> <li> <p>expect full control over Workflow execution</p> </li> </ul> <p>In this scenario, you are supposed to provide all required initialisation values for blocks used in your Workflow, what makes this mode most technologically challenging, requiring you to understand handful of topics that we cover in  developer guide.</p> <p>Here you can find example on how to run simple workflow in Python code.</p> <p>Integration in Python</p> <pre><code>from inference.core.env import WORKFLOWS_MAX_CONCURRENT_STEPS\nfrom inference.core.managers.base import ModelManager\nfrom inference.core.workflows.core_steps.common.entities import StepExecutionMode\nfrom inference.core.env import MAX_ACTIVE_MODELS\nfrom inference.core.managers.base import ModelManager\nfrom inference.core.managers.decorators.fixed_size_cache import WithFixedSizeCache\nfrom inference.core.registries.roboflow import RoboflowModelRegistry\nfrom inference.models.utils import ROBOFLOW_MODEL_TYPES\n\n# initialisation of Model registry to manage models load into memory \n# (required by core blocks exposing Roboflow models)\nmodel_registry = RoboflowModelRegistry(ROBOFLOW_MODEL_TYPES)\nmodel_manager = ModelManager(model_registry=model_registry)\nmodel_manager = WithFixedSizeCache(model_manager, max_size=MAX_ACTIVE_MODELS)\n\n# workflow definition\nOBJECT_DETECTION_WORKFLOW = {\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\"type\": \"WorkflowImage\", \"name\": \"image\"},\n        {\"type\": \"WorkflowParameter\", \"name\": \"model_id\"},\n        {\"type\": \"WorkflowParameter\", \"name\": \"confidence\", \"default_value\": 0.3},\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\",\n        }\n    ],\n    \"outputs\": [\n        {\"type\": \"JsonField\", \"name\": \"result\", \"selector\": \"$steps.detection.*\"}\n    ],\n}\n\n# example init parameters for blocks - dependent on set of blocks\n# used in your workflow\nworkflow_init_parameters = {\n    \"workflows_core.model_manager\": model_manager,\n    \"workflows_core.api_key\": \"&lt;YOUR-API-KEY&gt;,\n    \"workflows_core.step_execution_mode\": StepExecutionMode.LOCAL,\n}\n\n# instance of Execution Engine - init(...) method invocation triggers\n# the compilation process\nexecution_engine = ExecutionEngine.init(\n    workflow_definition=OBJECT_DETECTION_WORKFLOW,\n    init_parameters=workflow_init_parameters,\n    max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n)\n\n# runing the workflow\nresult = execution_engine.run(\n    runtime_parameters={\n        \"image\": [&lt;your-image&gt;],\n        \"model_id\": \"yolov8n-640\",\n    }\n)\n</code></pre>"},{"location":"workflows/schema_api/","title":"Workflows Schema API","text":"<p>The Workflows Schema API provides developers with a clear, programmatic understanding of a workflow's structure, inputs, and outputs. It addresses the challenge of programmatically determining a workflow's input requirements and output types.</p>"},{"location":"workflows/schema_api/#purpose-and-benefits","title":"Purpose and Benefits","text":"<p>The API offers structured access to:</p> <ol> <li>Input Parameters: Required workflow inputs.</li> <li>Output Structure: Details of the returned data.</li> <li>Type Hints: Expected data types for inputs and outputs.</li> <li>Schemas of Kinds: Detailed schemas for complex data types.</li> </ol> <p>This enables developers to:</p> <ul> <li>Validate inputs programmatically</li> <li>Understand output data structures</li> <li>Integrate workflows into larger systems</li> <li>Generate documentation or UIs based on workflow requirements</li> </ul>"},{"location":"workflows/schema_api/#using-the-api","title":"Using the API","text":"<pre><code>import requests\n\nWORKSPACE_NAME = \"workspace-name\"\nWORKFLOW_ID = \"workflow-id\"\nINFERENCE_SERVER_URL = \"https://detect.roboflow.com\"\n\nWORKFLOW_SCHEMA_ENDPOINT = f\"{INFERENCE_SERVER_URL}/{WORKSPACE_NAME}/workflows/{WORKFLOW_ID}/describe_interface\"\nROBOFLOW_API_KEY = \"Your Roboflow API Key\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"api_key\": ROBOFLOW_API_KEY,\n}\n\nres = requests.post(WORKFLOW_SCHEMA_ENDPOINT, headers=headers, json=data)\n\nschema = res.json()\n\ninputs = schema[\"inputs\"]\noutputs = schema[\"outputs\"]\nkinds_schemas = schema[\"kinds_schemas\"]\ntyping_hints = schema[\"typing_hints\"]\n</code></pre>"},{"location":"workflows/schema_api/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>The <code>inputs</code> and <code>outputs</code> keys show all of the inputs and outputs the workflow expects to run and return.</p>"},{"location":"workflows/schema_api/#typing-hints","title":"Typing Hints","text":"<p>The <code>typing_hints</code> key shows the data types of the inputs and outputs.</p>"},{"location":"workflows/schema_api/#kinds-schemas","title":"Kinds Schemas","text":"<p>The <code>kinds_schemas</code> key returns an OpenAPI specification with more detailed information about the data  types being returned and how to parse them. For example, the <code>object_detection_prediction</code> contains  information about the nested data that will be present.</p>"},{"location":"workflows/testing/","title":"Testing in Workflows","text":"<p>Testing can be challenging when not done properly, which is why we recommend a practical approach  for testing blocks that you create. Since a block is not a standalone element in the ecosystem, testing  might seem complex, but with the right methodology, it becomes manageable.</p> <p>We suggest the following approach when adding a new block:</p> <ul> <li> <p>Unit tests should cover:</p> <ul> <li> <p>Parsing of the manifest, especially when aliases are in use. </p> </li> <li> <p>Utility functions within the block module. If written correctly, these functions should simply  transform input data into output data, making them easy to test.</p> </li> <li> <p>The <code>run(...)</code> method should be tested unit-wise only if assembling the test is straightforward. Otherwise, we recommend focusing on integration tests for Workflow definitions that include the block.</p> </li> <li> <p>Examples can be found here</p> </li> </ul> </li> <li> <p>Integration tests should contain:</p> <ul> <li> <p>practical use cases where the block is used in collaboration with others</p> </li> <li> <p>assertions for results, particularly for model predictions. These assertions should be based on empirical  verification, such as by visualizing and inspecting predictions to ensure they are accurate.</p> </li> <li> <p>When adopting models or inference techniques from external sources (e.g., open-source models),  assertions should confirm that the results are consistent with what you would get outside the Workflows ecosystem,  ensuring compatibility and correctness.</p> </li> <li> <p>Examples can be found here</p> </li> </ul> </li> </ul>"},{"location":"workflows/understanding/","title":"The pillars of Workflows","text":"<p>In the Roboflow Workflows ecosystem, various components work together seamlessly to power your applications.  Some of these elements will be part of your daily interactions, while others operate behind the scenes to ensure  smooth and efficient application performance. The core pillars of the ecosystem include:</p> <ul> <li> <p>Workflows UI: The intuitive interface where you design and manage your workflows.</p> </li> <li> <p>Workflow Definition: An interchangeable format that serves as a program written in the \"workflows\" language,  defining how your workflows operate.</p> </li> <li> <p>Workflows Blocks: Modular components that perform specific tasks within your workflows, organized in plugins which can be easily created and injected into the ecosystem.</p> </li> <li> <p>Workflows Compiler and Execution Engine: The systems that compile your workflow definitions and execute them,  ensuring everything runs smoothly in your environment of choice.</p> </li> </ul> <p>We will explore each of these components, providing you with a foundational understanding to help you navigate and  utilize the full potential of Roboflow Workflows effectively.</p>"},{"location":"workflows/understanding/#workflows-ui","title":"Workflows UI","text":"<p>Traditionally, building machine learning applications involves complex coding and deep technical expertise.  Roboflow Workflows simplifies this process in two key ways: providing pre-built blocks (which will be described later),  and delivering user-friendly GUI. </p> <p>The interface allows you to design applications without needing to write code. You can easily connect components  together and achieve your goals without a deep understanding of Python or the underlying workflow language.</p> <p>Thanks to the UI, creating powerful machine learning solutions is straightforward and accessible, allowing you to focus on innovation rather than intricate programming.</p> <p>While not essential, the UI is a highly valuable component of the Roboflow Workflows ecosystem. At the end of the workflow creation process, it generates the workflow definition required for the Compiler and Execution Engine to run the workflow.</p> <p></p>"},{"location":"workflows/understanding/#workflow-definition","title":"Workflow definition","text":"<p>A workflow definition is essentially a document written in the internal programming language of Roboflow Workflows.  It allows you to separate the design of your workflow from its execution. You can create a workflow definition once  and run it in various environments using the Workflows Compiler and Execution Engine.</p> <p>You have two options for creating a workflow definition: use the UI to design it visually, or write it from scratch if you\u2019re comfortable with the Workflows language. More details on writing definitions manually are available here. For now, it's important to grasp the role of the definition within the ecosystem.</p> <p>A workflow definition is, in fact, a JSON document that outlines:</p> <ul> <li> <p>Inputs: These are either images or configuration parameters that influence how the workflow operates.  Instead of hardcoding values, inputs are placeholders that will be replaced with actual data during execution.</p> </li> <li> <p>Steps: These are instances of workflow blocks. Each step takes inputs from either the workflow inputs or the  outputs of previous steps. The sequence and connections between steps determine the execution order.</p> </li> <li> <p>Outputs: Specify the field names in the execution result and reference step outputs. During runtime, the referenced values are dynamically provided based on the results of workflow execution.</p> </li> </ul>"},{"location":"workflows/understanding/#workflow-blocks","title":"Workflow blocks","text":"<p>For users of Roboflow Workflows, blocks are essentially black boxes engineered to perform specific operations. They act as templates for the steps executed within a workflow, each defining its own set of inputs, configuration properties, and outputs.</p> <p>When adding a block to your workflow, you need to provide its inputs by referencing either the workflow\u2019s input  or the output of another step. You also specify the values for any required parameters. Once the step is incorporated,  its outputs can be referenced by subsequent steps, allowing for seamless integration and chaining of operations.</p> <p>The creation of blocks is a more advanced topic, which you can explore here.  It\u2019s essential to understand that blocks are grouped in workflow plugins, which are standard Python libraries.  Roboflow offers its own set of plugins, and community members are encouraged to create their own.  The process of importing a plugin into your environment is detailed here.</p> <p>Feel free to explore Workflow blocks prepared by Roboflow.</p>"},{"location":"workflows/understanding/#workflows-compiler-and-execution-engine","title":"Workflows Compiler and Execution Engine","text":"<p>The Compiler and Execution Engine are essential components of the Roboflow Workflows ecosystem, doing the heavy  lifting so you don't have to.</p> <p>Much like a traditional programming compiler or interpreter, these components translate your workflow definition \u2014  a program you create using reusable blocks \u2014 into a format that can be executed by a computer. The workflow definition  acts as a blueprint, with blocks functioning like functions in programming, connected to produce the desired outcomes.</p> <p>Roboflow provides these tools as part of their Inference Server (which can be deployed locally or  accessed via the Roboflow Hosted platform), video processing component,  and Python package, making it easy to run your workflows in various environments.</p> <p>For a deeper dive into the Compiler and Execution Engine, please refer to our detailed documentation.</p>"},{"location":"workflows/versioning/","title":"Workflows versioning","text":"<p>Understanding the life-cycle of Workflows ecosystem is an important topic,  especially from blocks developer perspective. Those are rules that apply:</p> <ul> <li> <p>Workflows is part of <code>inference</code> - the package itself has a release whenever  any of its component changes and those changes are ready to be published</p> </li> <li> <p>Workflows Execution Engine declares it's version. The game plan is the following:</p> <ul> <li> <p>core of workflows is capable to host multiple versions of Execution Engine -  for instance current stable version and development version</p> </li> <li> <p>stable version is maintained and new features are added until there is  need for new version and the new version is accepted</p> </li> <li> <p>since new version is fully operational, previously stable version starts  being deprecated - there is grace period when old version will be patched  with bug fixes (but new features will not be added), after that it will be left as is. During grace period we will call blocks creators to upgrade  their plugins according to requirements of new version</p> </li> <li> <p>core library only maintains single Execution Engine version for each major - making a promise that features within major will be non-breaking and Workflow  created under version <code>1.0.0</code> will be fully functional under version <code>1.4.3</code> of  Execution Engine</p> </li> </ul> </li> <li> <p>to ensure stability of the ecosystem over time:</p> <ul> <li> <p>Each Workflow Definition declares Execution Engine version it is compatible with.  Since the core library only maintains single version for Execution Engine,  <code>version: 1.1.0</code> in Workflow Definition actually request Execution Engine in version <code>&gt;=1.1.0,&lt;2.0.0</code></p> </li> <li> <p>Each block, in its manifest should provide reasonable Execution Engine compatibility - for instance - if block rely on Execution Engine feature introduced in <code>1.3.7</code> it should  specify <code>&gt;=1.3.7,&lt;2.0.0</code> as compatible versions of Engine</p> </li> </ul> </li> <li> <p>Workflows blocks may be optionally versioned (which we recommend and apply for Roboflow plugins).</p> <ul> <li> <p>we propose the following naming convention for blocks' type identifiers:  <code>{plugin_name}/{block_family_name}@v{X}</code> to ensure good utilisation of blocks identifier  namespace</p> </li> <li> <p>we suggest to only modify specific version of the block if bug-fix is needed,  all other changes to block should yield new version</p> </li> <li> <p>each version of the block is to be submitted into new module (as suggested here) - even for the price of code duplication as we think stability is more important than DRY in this particular case</p> </li> <li> <p>on the similar note, we suggest each block to be as independent as possible,  as code which is shared across blocks, may unintentionally modify other blocks  destroying the stability of your Workflows</p> </li> </ul> </li> </ul>"},{"location":"workflows/workflow_execution/","title":"How Workflow execution looks like?","text":"<p>Workflow execution is a complex subject, but you don\u2019t need to understand every detail to get started effectively.  Grasping some basic concepts can significantly speed up your learning process with the Workflows ecosystem.  This document provides a clear and straightforward overview, designed to help you quickly understand the  fundamentals and build more powerful applications.</p> <p>For those interested in a deeper technical understanding, we invite you to explore the developer guide  for more detailed information.</p>"},{"location":"workflows/workflow_execution/#compilation","title":"Compilation","text":"<p>Workflow execution begins with compiling the Workflow definition. As you know, a Workflow definition is a  JSON document that outlines inputs, steps, outputs, and connections between elements. To turn this document  into an executable format, it must be compiled.</p> <p>From the Execution Engine\u2019s perspective, this process involves creating a computation graph and checking its  integrity and correctness. This verification step is crucial because it helps identify and alert you to errors  early on, making it easier and faster to debug issues. For instance, if you connect incompatible blocks, use an invalid selector, or create a loop in your workflow, the compiler will notify you with error messages. </p> <p>Once the compilation is complete, it means your Workflow is ready to run. This confirms that:</p> <ul> <li> <p>Your Workflow is compatible with the version of the Execution Engine in your environment.</p> </li> <li> <p>All blocks in your Workflow were successfully loaded and initialized.</p> </li> <li> <p>The connections between blocks are valid.</p> </li> <li> <p>The input data you provided for the Workflow has been validated.</p> </li> </ul> <p>At this point, the Execution Engine can begin execution of the Workflow.</p>"},{"location":"workflows/workflow_execution/#data-in-workflow-execution","title":"Data in Workflow execution","text":"<p>When you run a Workflow, you provide input data each time. Just like a function in programming that  can handle different input values, a Workflow can process different pieces of data each time you run it.  Let's see what happens with the data once you trigger Workflow execution. </p> <p>You provide input data substituting inputs' placeholders defined in the Workflow. These placeholders are  referenced by steps of your Workflow using selectors. When a step runs, the actual piece of data you  provided at that moment is used to make the computation. Its outputs can be later used by other steps, based on steps outputs selectors declared in Workflow definition, continuing this process until the Workflow  completes and all outputs are generated.</p> <p>Apart from parameters with fixed values in the Workflow definition, the definition itself does not include  actual data values. It simply tells the Execution Engine how to direct and handle the data you provide as input.</p>"},{"location":"workflows/workflow_execution/#what-is-the-data","title":"What is the data?","text":"<p>Input data in a Workflow can be divided into two types:</p> <ul> <li> <p>Batch-Oriented Data to be processed: Main data to be processed, which you expect to derive results  from (for instance: making inference with your model)</p> </li> <li> <p>Scalars: These are single values used for specific settings or configurations.</p> </li> </ul> <p>Thinking about standard data processing, like the one presented below, you may find the distinction  between scalars and batch-oriented data artificial. </p> <pre><code>def is_even(number: int) -&gt; bool:\n    return number % 2 == 0\n</code></pre> <p>You can easily submit different values as <code>number</code> parameter and do not bother associating the  parameter into one of the two categories.</p> <pre><code>is_even(number=1)\nis_even(number=2)\nis_even(number=3)\n</code></pre> <p>The situation becomes more complicated with machine learning models. Unlike a simple function like <code>is_even(...)</code>,  which processes one number at a time, ML models often handle multiple pieces of data at once. For example,  instead of providing just one image to a classification model, you can usually submit a list of images and  receive predictions for all of them at once performing the same operation for each image. </p> <p>This is different from our <code>is_even(...)</code> function, which would need to be called separately  for each number to get a list of results. The difference comes from how ML models work, especially how  GPUs process data - applying the same operation to many pieces of data simultaneously, executing  Single Instruction Multiple Data operations.</p> <p>The <code>is_even(...)</code> function can be adapted to handle batches of data by using a loop, like this:</p> <pre><code>results = []\nfor number in [1, 2, 3, 4]:\n    results.append(is_even(number))\n</code></pre> <p>In Workflows, usually you do not need to worry about broadcasting the operations into batches of data -  Execution Engine is doing that for you behind the scenes, but once you understood the role of batch-oriented data, let's think if all data can be represented as batches.</p> <p>Standard way of making predictions from classification model is be illustrated with the following  pseudo-code: <pre><code>images = [PIL.Image(...), PIL.Image(...), PIL.Image(...), PIL.Image(...)]\nmodel = MyClassificationModel()\n\npredictions = model.infer(images=images, confidence_threshold=0.5)\n</code></pre></p> <p>You can probably spot the difference between <code>images</code> and <code>confidence_threshold</code>.  Former is batch of data to apply single operation (prediction from a model) and the latter is parameter  influencing the processing for all elements in the batch and this type of data we call scalars.</p> <p>Nature of batches and scalars</p> <p>What we call scalar in Workflows ecosystem is not 100% equivalent to the mathematical  term which is usually associated to \"a single value\", but in Workflows we prefer slightly different  definition.</p> <p>In the Workflows ecosystem, a scalar is a piece of data that stays constant, regardless of how many  elements are processed. There is nothing that prevents from having a list of objects as a scalar value. For example, if you have a list of input images and a fixed list of reference images,  the reference images remain unchanged as you process each input. Thus, the reference images are considered  scalar data, while the list of input images is batch-oriented.</p> <p>Great news!</p> <p>Since Execution Engine <code>v1.6.0</code>, the practical aspects of dealing with scalars and batches are offloaded to  the Execution Engine (refer to changelog for more details). As a block  developer, it is still important to understand the difference, but when building blocks you are not forced to  think about the nuances that much.</p> <p>To illustrate the distinction, Workflow definitions hold inputs of the two categories:</p> <ul> <li> <p>Scalar inputs - like <code>WorkflowParameter</code></p> </li> <li> <p>Batch inputs - like <code>WorkflowImage</code>, <code>WorkflowVideoMetadata</code> or <code>WorkflowBatchInput</code></p> </li> </ul> <p>When you provide a single image as a <code>WorkflowImage</code> input, it is automatically expanded to form a batch.  If your Workflow definition includes multiple <code>WorkflowImage</code> placeholders, the actual data you provide for  execution must have the same batch size for all these inputs. The only exception is when you submit a  single image; it will be broadcast to fit the batch size requirements of other inputs.</p>"},{"location":"workflows/workflow_execution/#steps-interactions-with-data","title":"Steps interactions with data","text":"<p>If we asked you about the nature of step outputs in these scenarios:</p> <ul> <li> <p>A: The step receives only scalar parameters as input.</p> </li> <li> <p>B: The step receives batch-oriented data as input.</p> </li> <li> <p>C: The step receives both scalar parameters and batch-oriented data as input.</p> </li> </ul> <p>You would likely say:</p> <ul> <li> <p>In option A, the output will be non-batch.</p> </li> <li> <p>In options B and C, the output will be a batch. In option C, the non-batch-oriented parameters will be  broadcast to match the batch size of the data.</p> </li> </ul> <p>And you\u2019d be correct. Knowing that, you only have two more concepts to understand to become Workflows expert.</p> <p>Let\u2019s say you want to create a Workflow with these steps:</p> <ol> <li> <p>Detect objects in a batch of input images.</p> </li> <li> <p>Crop each detected object from the images.</p> </li> <li> <p>Classify each cropped object with a second model to add detailed labels.</p> </li> </ol> <p>Here\u2019s what happens with the data in the cropping step:</p> <ol> <li> <p>You start with a batch of images, let\u2019s say you have <code>n</code> images.</p> </li> <li> <p>The object detection model finds a different number of objects in each image.</p> </li> <li> <p>The cropping step then creates new image for each detected object, resulting in a new batch of images  for each original image.</p> </li> </ol> <p>So, you end up with a nested list of images, with sizes like <code>[(k[1], ), (k[2], ), ... (k[n])]</code>, where each <code>k[i]</code>  is a batch of images with a variable size based on the number of detections. The second model (classifier) will process these nested batches of cropped images. There is also nothing that stops you from going deeper  in nested batches world.</p> <p>Here\u2019s where it gets tricky, but Execution Engine simplifies this complexity. It manages the nesting of  data virtually, so blocks always receive data in a flattened, non-nested format. This makes it easier to apply  the same block, like an object detection model or classifier, regardless of how deeply nested your data is. But  there is a price - the notion of <code>dimensionality level</code> which dictates which steps may be connected, which not.</p> <p><code>dimensionality level</code> concept refers to the level of nesting of batch. Batch oriented Workflow inputs  have <code>dimensionality level 1</code>, crops that we described in our example have <code>dimensionality level 2</code> and so on. What matters from the perspective of plugging inputs to specific step is:</p> <ul> <li> <p>the difference in <code>dimensionality level</code> across step inputs</p> </li> <li> <p>the impact of step on <code>dimensionality level</code> of output (step may decrease, keep the same or increase dimensionality)</p> </li> </ul> <p>Majority of blocks are designed to work with inputs at the same dimensionality level, not changing dimensionality of its outputs, with some being exceptions to that rule. In our example, predictions from object-detection model occupy <code>dimensionality level 1</code>, while classification results are at <code>dimensionality level 2</code>, due to the fact that cropping step introduced new, dynamic level of dimensionality.</p> <p>Now, if you can find a block that accepts both object detection predictions and classification predictions, you could use our predictions together only if block specifies explicitly it accepts such combination of <code>dimensionality levels</code>,  otherwise you would end up seeing compilation error. Hopefully, there is a block you could use in this context. </p> <p></p> <p>Detections Classes Replacement block is designed to substitute bounding boxes classes labels with predictions from classification model performed at crops of original image with respect to bounding boxes predicted by first model.</p> <p>Warning</p> <p>We are working hard to change it, but so far the Workflow UI in Roboflow APP is not capable of displaying the  concept of <code>dimensionality level</code>. We know that it is suboptimal from UX perspective and very confusing but we must ask for patience until this situation gets better.</p> <p>Note</p> <p>Workflows Compiler keeps track of <code>data lineage</code> in Workflow definition, making it impossible to mix  together data at higher <code>dimensionality levels</code> that do not come from the same origin. This concept is  described in details in developer guide. From the user perspective it is important to understand that if image is cropped based on predictions from different models (or even the same model, using cropping step twice),  cropping outputs despite being at the same dimensionality level cannot be used as inputs to the same step.</p>"},{"location":"workflows/workflow_execution/#conditional-execution","title":"Conditional execution","text":"<p>Let\u2019s be honest\u2014programmers love branching, and for good reason. It\u2019s a common and useful construct in  programming languages.</p> <p>For example, it\u2019s easy to understand what\u2019s happening in this code:</p> <pre><code>def is_string_lower_cased(my_string: str) -&gt; str:\n    if my_string.lower() == my_string:\n        return \"String was lower-cased\"\n    return \"String was not lower-cased\"\n</code></pre> <p>But what about this code?</p> <pre><code>def is_string_lower_cased_batched(my_string: Batch[str]) -&gt; str:\n    pass\n</code></pre> <p>In this case, it\u2019s not immediately clear how branching would work with a batch of strings.  The concept of handling decisions for a single item is straightforward, but when working with batches,  the logic needs to account for multiple inputs at once. The problem arises due to the fact that independent decision must be made for each element of batch - which may lead to different execution branches for  different elements of a batch. In such simplistic example as provided it can be easily addressed:</p> <pre><code>def is_string_lower_cased_batched(my_string: Batch[str]) -&gt; Batch[str]:\n    result = []\n    for element in my_string:\n        if element.lower() == my_string:\n            result.append(\"String was lower-cased\")\n        else:\n            result.append(\"String was not lower-cased\") \n    return result\n</code></pre> <p>In Workflows, however we want blocks to decide where execution goes, not implement conditional statements inside block body and return merged results. This is why whole mechanism of conditional execution  emerged in Workflows Execution engine. This concept is important and has its own technical depth, but from  user perspective there are few things important to understand:</p> <ul> <li> <p>some Workflows blocks can impact execution flow - steps made out of those blocks will be specified a bunch  of step selectors, dictating possible next steps to be decided for each element of batch (non-batch oriented steps work as traditional if-else statements in programming)</p> </li> <li> <p>once data element is discarded from batch by conditional execution, it will be hidden from all  affected steps down the processing path and denoted in outputs as <code>None</code></p> </li> <li> <p>multiple flow-control steps may affect single next step, union of conditional execution masks will be created and dynamically applied</p> </li> <li> <p>step may be not executed if there is no inputs to the step left after conditional execution logic evaluation</p> </li> <li> <p>there are special blocks capable of merging alternative execution branches, such that data from that branches can be referred by single selector (for instance to build outputs). Example of such block is  <code>First Non Empty Or Default</code> - which collapses execution branches taking first value encountered or defaulting to specified value if no value spotted</p> </li> <li> <p>conditional execution usually impacts Workflow outputs - all values that are affected by branching are in  fact optional (if special blocks filling empty values are not used) and nested results may not be filled with data,  leaving empty (potentially nested) lists in results - see details  in section describing output construction.</p> </li> </ul>"},{"location":"workflows/workflow_execution/#output-construction","title":"Output construction","text":"<p>The most important thing to understand is that a Workflow's output is aligned with its input regarding  batch elements order. This means the output will always be a list of dictionaries, with each dictionary  corresponding to an item in the input batch. This structure makes it easier to parse results and handle  them iteratively, matching the outputs to the inputs.</p> <pre><code>input_images = [...]\nworkflow_results = execution_engine.run(\n    runtime_parameters={\"images\": input_images}\n)\n\nfor image, result in zip(input_images, workflow_results):\n    pass\n</code></pre> <p>Each element of the list is a dictionary with keys specified in Workflow definition via declaration like:</p> <pre><code>{\"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.detection.predictions\"}\n</code></pre> <p>what you may expect as a value under those keys, however, is dependent on the structure of the workflow.  All non-batch results got broadcast and placed in each and every output dictionary with the same value.  Elements at <code>dimensionality level 1</code> will be distributed evenly, with values in each dictionary corresponding  to the alignment of input data (predictions for input image 3, will be placed in third dictionary). Elements at  higher <code>dimensionality levels</code> will be embedded into lists of objects of types specific to the step output  being referred. </p> <p>For example, let's consider again our example with object-detection model, crops and secondary classification model. Assuming that predictions from object detection model are registered in the output under the name  <code>\"object_detection_predictions\"</code> and results of classifier are registered as <code>\"classifier_predictions\"</code>, you  may expect following output once three images are submitted as input for Workflow execution:</p> <pre><code>[\n  {\n    \"object_detection_predictions\": \"here sv.Detections object with 2 bounding boxes\",\n    \"classifier_predictions\": [\n      {\"classifier_prediction\":  \"for first crop\"},\n      {\"classifier_prediction\":  \"for second crop\"}\n    ]\n  },\n  {\n    \"object_detection_predictions\": \"empty sv.Detections\",\n    \"classifier_predictions\": []\n  },\n  {\n    \"object_detection_predictions\": \"here sv.Detections object with 3 bounding boxes\",\n    \"classifier_predictions\": [\n      {\"classifier_prediction\":  \"for first crop\"},\n      {\"classifier_prediction\":  \"for second crop\"},\n      {\"classifier_prediction\":  \"for third crop\"}\n    ]\n  }\n]\n</code></pre> <p>As you can see, <code>\"classifier_predictions\"</code> field is populated with list of results, of size equivalent to number  of bounding boxes for <code>\"object_detection_predictions\"</code>. </p> <p>Interestingly, if our workflows has ContinueIf block that only runs cropping and classifier if number of bounding boxes is different from two - it will turn <code>classifier_predictions</code> in first dictionary into empty list. If conditional  execution excludes steps at higher <code>dimensionality levels</code> from producing outputs as a side effect of execution -  output field selecting that values will be presented as nested list of empty lists, with depth matching <code>dimensionality level - 1</code> of referred output.</p> <p>Since Execution Engine <code>v1.6.0</code>, blocks within a workflow may collapse batches into scalars, as well as create new  batches from scalar inputs. The first scenario is pretty easy to understand - each dictionary in the output list will  simply be populated with the same scalar value. The case of emergent batch is slightly more complicated.  In such case we can find batch at dimensionality level 1, which has shape or elements order not compliant  with input batches. To prevent semantic ambiguity, we treat such batch as if it's dimensionality is one level higher (as if there is additional batch-oriented input of size one attached to the input of the block creating batch  dynamically). Such virtually nested outputs are broadcast, such that each dictionary in the output list will be given  new key with the same nested output. This nesting property is preserved even if there is no input-derived outputs  for given workflow - in such case, output is a list of size 1 which contains dictionary with nested output.</p> <p>Some outputs would require serialisation when Workflows Execution Engine runs behind HTTP API. We use the following serialisation strategies:</p> <ul> <li> <p>images got serialised into <code>base64</code></p> </li> <li> <p>numpy arrays are serialised into lists</p> </li> <li> <p>sv.Detections are serialised into <code>inference</code> format which can be decoded on the other end of the wire using  <code>sv.Detections.from_inference(...)</code></p> </li> </ul> <p>Note</p> <p>sv.Detections, which is our standard representation of detection-based predictions is treated specially  by output constructor. <code>JsonField</code> output definition can specify optionally <code>coordinates_system</code> property, which may enforce translation of detection coordinates into coordinates system of parent image in workflow. See more in docs page describing outputs definitions</p>"},{"location":"workflows/workflows_compiler/","title":"Compilation of Workflow Definition","text":"<p>Compilation is a process that takes a document written in a programming language, checks its correctness,  and transforms it into a format that the execution environment can understand.</p> <p>A similar process happens in the Workflows ecosystem whenever you want to run a Workflow Definition.  The Workflows Compiler performs several steps to transform a JSON document into a computation graph, which  is then executed by the Workflows Execution Engine. While this process can be complex, understanding it can  be helpful for developers contributing to the ecosystem. In this document, we outline key details of the  compilation process to assist in building Workflow blocks and encourage contributions to the core Execution Engine.</p> <p>Note</p> <p>This document covers the design of Execution Engine <code>v1</code> (which is current stable version). Please  acknowledge information about versioning to understand Execution Engine  development cycle.</p>"},{"location":"workflows/workflows_compiler/#stages-of-compilation","title":"Stages of compilation","text":"<p>Workflow compilation involves several stages, including:</p> <ol> <li> <p>Loading available blocks: Gathering all the blocks that can be used in the workflow based on  configuration of execution environment</p> </li> <li> <p>Compiling dynamic blocks: Turning dynamic blocks definitions into  standard Workflow Blocks</p> </li> <li> <p>Parsing the Workflow Definition: Reading and interpreting the JSON document that defines the workflow, detecting  syntax errors</p> </li> <li> <p>Building Workflow Execution Graph: Creating a graph that defines how data will flow through the workflow  during execution and verifying Workflow integrity</p> </li> <li> <p>Initializing Workflow steps from blocks: Setting up the individual workflow steps based on the available blocks,  steps definitions and configuration of execution environment.</p> </li> </ol> <p>Let's take a closer look at each of the workflow compilation steps.</p>"},{"location":"workflows/workflows_compiler/#workflows-blocks-loading","title":"Workflows blocks loading","text":"<p>As described in the blocks bundling guide, a group of Workflow blocks can be packaged  into a workflow plugin. A plugin is essentially a standard Python library that, in its main module, exposes specific  functions allowing Workflow Blocks to be dynamically loaded.</p> <p>The Workflows Compiler and Execution Engine are designed to be independent of specific Workflow Blocks, and the  Compiler has the ability to discover and load blocks from plugins.</p> <p>Roboflow provides the <code>roboflow_core</code> plugin, which includes a set of basic Workflow Blocks that are always  loaded by the Compiler, as both the Compiler and these blocks are bundled in the <code>inference</code> package.</p> <p>For custom plugins, once they are installed in the Python environment, they need to be referenced using an environment  variable called <code>WORKFLOWS_PLUGINS</code>. This variable should contain the names of the Python packages that contain the  plugins, separated by commas. </p> <p>For example, if you have two custom plugins, <code>numpy_plugin</code> and <code>pandas_plugin</code>, you can enable them in  your Workflows environment by setting: <pre><code>export WORKFLOWS_PLUGINS=\"numpy_plugin,pandas_plugin\"\n</code></pre></p> <p>Both <code>numpy_plugin</code> and <code>pandas_plugin</code> are not paths to library repositories, but rather names of the main modules of libraries shipping plugins (<code>import numpy_plugin</code> must work in your  Python environment for the plugin to be possible to be loaded).</p> <p>Once Compiler loads all plugins it is ready for the next stage of compilation.</p>"},{"location":"workflows/workflows_compiler/#compilation-of-dynamic-blocks","title":"Compilation of dynamic blocks","text":"<p>Note</p> <p>The topic of dynamic Python blocks is covered in separate docs page. To unerstand the content of this section you only need to know that there is a way to define Workflow Blocks in-place in Workflow Definition - specifying both block manifest and Python code in JSON document. This functionality only works if you run Workflows Execution Engine on your hardware and is disabled ad Roboflow hosted platform.</p> <p>The Workflows Compiler can transform Dynamic Python Blocks, defined directly in a Workflow Definition, into  full-fledged Workflow Blocks at runtime. The Compiler generates these block classes dynamically based on the  block's definition, eliminating the need for developers to manually create them as they would in a plugin.</p> <p>Once this process is complete, the dynamic blocks are added to the pool of available Workflow Blocks. These blocks  can then be used in the <code>steps</code> section of your Workflow Definition, just like any other standard block.</p>"},{"location":"workflows/workflows_compiler/#parsing-workflow-definition","title":"Parsing Workflow Definition","text":"<p>Once all Workflow Blocks are loaded, the Compiler retrieves the manifest classes for each block.  These manifests are <code>pydantic</code> data classes that define the structure of step entries in definition. At parsing stage, errors with Workflows Definition are alerted, for example:</p> <ul> <li> <p>usage of non-existing blocks</p> </li> <li> <p>invalid configuration of steps</p> </li> <li> <p>lack of required parameters for steps</p> </li> </ul> <p>Thanks to <code>pydantic</code>, the Workflows Compiler doesn't need its own parser. Additionally, blocks creators use standard  Python library to define block manifests.</p>"},{"location":"workflows/workflows_compiler/#building-workflow-execution-graph","title":"Building Workflow Execution Graph","text":"<p>Building the Workflow Execution graph is the most critical stage of Workflow compilation.  Here's how it works:</p>"},{"location":"workflows/workflows_compiler/#adding-vertices","title":"Adding Vertices","text":"<p>First, each input, step and output are added as vertices in the graph, with each vertex given a special label  for future identification. These vertices also include metadata, like marking input vertices with seeds for data  lineage tracking (more on this later).</p>"},{"location":"workflows/workflows_compiler/#adding-edges","title":"Adding Edges","text":"<p>After placing the vertices, the next step is to create edges between them based on the selectors defined in  the Workflow. The Compiler examines the block manifests to determine which properties can accept selectors  and the expected \"kind\" of those selectors. This enables the Compiler to detect errors in the Workflow  definition, such as:</p> <ul> <li> <p>Providing an output kind from one step that doesn't match the expected input kind of the next step.</p> </li> <li> <p>Referring to non-existent steps or inputs.</p> </li> </ul> <p>Each edge also contains metadata indicating which input property is being fed by the output data, which is  helpful at later stages of compilation and during execution</p> <p>Note</p> <p>Normally, step inputs \"request\" data from step outputs, forming an edge from Step A's output to Step B's input  during Step B's processing. However, control-flow blocks are an exception,  as they both accept data and declare other steps in the manifest, creating a special flow-control edge in the graph.</p>"},{"location":"workflows/workflows_compiler/#structural-validation","title":"Structural Validation","text":"<p>Once the graph is constructed, the Compiler checks for structural issues like cycles to ensure the graph can be  executed properly.</p>"},{"location":"workflows/workflows_compiler/#data-lineage-verification","title":"Data Lineage verification","text":"<p>Finally, data lineage properties are populated from input nodes and carried through the graph. So, what is  data lineage? Lineage is a list of identifiers that track the creation and nesting of batches through the steps,  determining:</p> <ul> <li> <p>the source path of data</p> </li> <li> <p><code>dimensionality level</code> of data</p> </li> <li> <p>compatibility of different pieces of data that may be referred by a step - ensuring that step will only  take corresponding batches elements from multiple sources (such that batch element index <code>example: (1, 2)</code> refers to the exact same piece of data when two batch-oriented inputs are connected into the step and not to some randomly  provided batches with different lineage that does not make sense to process together)</p> </li> </ul> <p>Each time a new nested batch is created by a step, a unique identifier is added to the lineage of the output.  This allows the Compiler to track and verify if the inputs across steps are compatible.</p> <p>Note</p> <p>Fundamental assumption of data lineage is that all batch-oriented inputs are granted the same lineage identifier - so implicitly it enforces all input batches to be fed with  data that has corresponding data-points at corresponding positions in batches. For instance,  if your Workflow compares <code>image_1</code> to <code>image_2</code> (and you declare those two inputs in Wofklow Definition), the Compiler assumes the elements of <code>image_1[3]</code> to correspond with <code>image_2[3]</code>.</p> <p>Thanks to lineage tracking, the Compiler can detect potential mistakes. For example, if you attempt to connect two  dynamic crop outputs to a single step's inputs, the Compiler will notice that the number of crops in each  output may not match. This would result in nested batch elements with mismatched indices, which could lead to  unpredictable results during execution if the situation is not prevented.</p> <p>Example of lineage missmatch</p> <p>Imagine the following scenario:</p> <ul> <li> <p>you declare single image input in your Workflow</p> </li> <li> <p>at first you perform object detection using two different models</p> </li> <li> <p>you use two dynamic crop steps - to crop based on first and second model predictions  respectivelly</p> </li> <li> <p>now you want to use block to compare two images features (using classical Compute Vision methods)</p> </li> </ul> <p>What would you expect to happen when you plug inputs from those two crop steps into comparison block?</p> <ul> <li> <p>Without tracing the lineage you would \"flatten\" and \"zip\" those two batches and pass pairs of images to comparison block - the problem is that in this case you cannot  determine if the comparisons between those elements actually makes sense - probably do not!</p> </li> <li> <p>With lineage tracing - Compiler knows that you attempt to feed two batches with lineages that do not match regarding last nesting level and raises compilation error.</p> </li> </ul> <p>One may ask - \"ok, but maybe I would like to apply secondary classifier on both crops and  merge results at the end to get all results in single output - is that possible?\". The answer is yes - as mentioned above, nested batches differ only at the last lineage level - so when we use  some blocks from \"dimensionality collapse\" category - we will align the results of secondary classifiers  into batches at <code>dimensionality level</code> 1 with matching lineage.</p> <p>As outlined in the section dedicated to blocks development, each block can define  the expected dimensionality of its inputs and outputs. This refers to how the data should be structured.  For example, if a block needs an <code>image</code> input that's one level above a batch of <code>predictions</code>, the Compiler will  check that this requirement is met when verifying the Workflow step. If the connections between steps don\u2019t match  the expected dimensionality, an error will occur. Additionally, each input is also verified to ensure it is compatible  based on data lineage. Once the step passes validation, the output dimensionality is determined and will be used to  check compatibility with subsequent steps.</p> <p>It\u2019s important to note that blocks define dimensionality requirements in relative terms, not absolute. This means  a block specifies the difference (or offset) in dimensionality between its inputs and outputs. This approach allows  blocks to work flexibly at any dimensionality level.</p> <p>Note</p> <p>In version 1, the Workflows Compiler only supports blocks that work across two different <code>dimensionality levels</code>.  This was done to keep the design straightforward. If there's a need for blocks that handle more  <code>dimensionality levels</code> in the future, we will consider expanding this support.</p>"},{"location":"workflows/workflows_compiler/#denoting-flow-control","title":"Denoting flow-control","text":"<p>The Workflows Compiler helps the Execution Engine manage flow-control structures in workflows. It marks specific  attributes that allow the system to understand how flow-control impacts building inputs for certain steps and the  execution of the workflow graph (for more details, see the  Execution Engine docs).</p> <p>To ensure the workflow structure is correct, the Compiler checks data lineage for flow-control steps in a  similar way as described in the section on data-lineage verification.</p> <p>The Compiler assumes flow-control steps can affect other steps if:</p> <ul> <li> <p>The flow-control step operates on non-batch-oriented inputs - in this case, the flow-control step can  either allow or prevent the connected step (and related steps) from running entirely, even if the input  is a batch of data - all batch elements are affected.</p> </li> <li> <p>The flow-control step operates on batch-oriented inputs with compatible lineage - here, the flow-control step  can decide separately for each element in the batch which ones will proceed and which ones will be stopped.</p> </li> </ul>"},{"location":"workflows/workflows_compiler/#batch-orientation-compatibility","title":"Batch-orientation compatibility","text":"<p>As it was outlined, Workflows define batch-oriented data and scalars. From the description of the nature of data in Workflows,  you can also conclude that operations which are executed against batch-oriented data have two almost equivalent ways of running:</p> <ul> <li> <p>all-at-once: taking whole batches of data and processing them</p> </li> <li> <p>one-by-one: looping over batch elements and getting results sequentially</p> </li> </ul> <p>Since the default way for Workflow blocks to deal with the batches is to consume them element-by-element,  there is no real difference between batch-oriented data and scalars  in such case. Execution Engine simply unpacks scalars from batches and pass them to each step.</p> <p>The process may complicate when block accepts batch input. You will learn the  details in blocks development guide, but  block is required to denote each input that must be provided batch-wise and all inputs which can be fed with both batch-oriented data and scalars at the same time (which is much  less common case). In such cases, lineage is used to deduce if the actual data fed into  every step input is batch or scalar. When violation is detected (for instance scalar is provided for input  that requires batches or vice versa) - the error is raised.</p> <p>Potential future improvements</p> <p>At this moment, we are not sure if the behaviour described above is limiting the potential of Workflows ecosystem. If you see that your Workflows cannot run due to the errors  being result of described mechanism - please let us know in  GitHub issues. </p>"},{"location":"workflows/workflows_compiler/#initializing-workflow-steps-from-blocks","title":"Initializing Workflow steps from blocks","text":"<p>The documentation often refers to a Workflow Step as an instance of a Workflow Block, which serves as its prototype.  To put it simply, a Workflow Block is a class that implements specific behavior, which can be customized by  configuration\u2014whether it's set by the environment running the Execution Engine, the Workflow definition,  or inputs at runtime.</p> <p>In programming, we create an instance of a class using a constructor, usually requiring initialization parameters.  One the same note, Workflow Blocks are initialized by the Workflows Compiler whenever a step in the Workflow  references that block. Some blocks may need specific initialization parameters, while others won't.</p> <p>When a block requires initialization parameters:</p> <ul> <li> <p>The block must declare the parameters it needs, as explained in detail in  the blocks development guide</p> </li> <li> <p>The values for these parameters must be provided from the environment where the Workflow is being executed.</p> </li> <li> <p>The values for these parameters must be provided from the environment where the Workflow is being executed.</p> </li> </ul> <p>This second part might seem tricky, so let\u2019s look at an example. In the in user guide, under the section showing how to integrate with Workflows using the <code>inference</code> Python package, you might come  across code like this:</p> <pre><code>[...]\n# example init parameters for blocks - dependent on set of blocks\n# used in your workflow\nworkflow_init_parameters = {\n    \"workflows_core.model_manager\": model_manager,\n    \"workflows_core.api_key\": \"&lt;YOUR-API-KEY&gt;,\n    \"workflows_core.step_execution_mode\": StepExecutionMode.LOCAL,\n}\n\n# instance of Execution Engine - init(...) method invocation triggers\n# the compilation process\nexecution_engine = ExecutionEngine.init(\n    ...,\n    init_parameters=workflow_init_parameters,\n    ...,\n)\n[...]\n</code></pre> <p>In this example, <code>workflow_init_parameters contains</code> values that the Compiler uses when  initializing Workflow steps based on block requests.</p> <p>Initialization parameters (often called \"init parameters\") can be passed to the Compiler in two ways:</p> <ul> <li> <p>Explicitly: You provide specific values (numbers, strings, objects, etc.).</p> </li> <li> <p>Implicitly: Default values are defined within the Workflows plugin, which can either be specific values or  functions (taking no parameters) that generate values dynamically, such as from environmental variables.</p> </li> </ul> <p>The dictionary <code>workflow_init_parameters</code> shows explicitly passed init parameters. The structure of the keys  is important: <code>{plugin_name}.{init_parameter_name}</code>. You can also specify just <code>{init_parameter_name}</code>, but this  changes how parameters are resolved.</p>"},{"location":"workflows/workflows_compiler/#how-parameters-are-resolved","title":"How Parameters Are Resolved?","text":"<p>When the Compiler looks for a block\u2019s required init parameter, it follows this process:</p> <ol> <li> <p>Exact Match: It first checks the explicitly provided parameters for an exact match to  <code>{plugin_name}.{init_parameter_name}</code>.</p> </li> <li> <p>Default Parameters: If no match is found, it checks the plugin\u2019s default parameters.</p> </li> <li> <p>General Match: Finally, it looks for a general match with just <code>{init_parameter_name}</code> in the explicitly  provided parameters.</p> </li> </ol> <p>This mechanism allows flexibility, as some block parameters can have default values while others must be  provided explicitly. Additionally, it lets certain parameters be shared across different plugins.</p>"},{"location":"workflows/workflows_execution_engine/","title":"Workflows Execution Engine in details","text":"<p>The compilation process creates a Workflow Execution graph, which  holds all the necessary details to run a Workflow definition. In this section, we'll explain the details  of the execution process.</p> <p>At a high level, the process does the following:</p> <ol> <li> <p>Validates runtime input: it checks that all required placeholders from the Workflow definition are filled  with data and ensures the data types are correct.</p> </li> <li> <p>Determines execution order: it defines the order in which the steps are executed.</p> </li> <li> <p>Prepares step inputs and caches outputs: it organizes the inputs for each step and saves the outputs  for future use.</p> </li> <li> <p>Builds the final Workflow outputs: it assembles the overall result of the Workflow.</p> </li> </ol>"},{"location":"workflows/workflows_execution_engine/#validation-of-runtime-input","title":"Validation of runtime input","text":"<p>The Workflow definition specifies the expected inputs for Workflow execution. As discussed  earlier, inputs can be either batch-oriented data to be processed by steps or parameters that  configure the step execution. This distinction is crucial to how the Workflow runs and will be explored throughout  this page.</p> <p>Starting with input validation, the Execution Engine has a dedicated component that parses and prepares the input  for use. It recognizes batch-oriented inputs from the Workflow definition and converts them into an internal  representation (e.g., <code>WorkflowImage</code> becomes <code>Batch[WorkflowImageData]</code>). This allows block developers to easily  work with the data. Non-batch-oriented parameters are checked for type consistency against the block manifests  used to create steps that require those parameters. This ensures that type errors are caught early in the  execution process.</p> <p>Note</p> <p>All batch-oriented inputs must have a size of either 1 or n. When a batch contains only a single element, it is  automatically broadcasted across the entire batch.</p>"},{"location":"workflows/workflows_execution_engine/#determining-execution-order","title":"Determining execution order","text":"<p>The Workflow Execution Graph is a directed acyclic graph (DAG),  which allows us to determine the topological order. Topological order refers to a sequence in which Workflow steps are  executed, ensuring that each step's dependencies are met before it runs. In other words, if a step relies on the output  of another step, the Workflow Engine ensures that the dependency step is executed first.</p> <p>Additionally, the topological structure allows us to identify which steps can be executed in parallel without causing  race conditions. Parallel execution is the default mode in the Workflows Execution Engine. This means that multiple  independent steps, such as those used in model ensembling can run simultaneously, resulting in significant  improvements in execution speed compared to sequential processing.</p> <p>Warning</p> <p>Due to the parallel execution mode in the Execution Engine (and to avoid unnecessary data copying when passing  it to each step), we strongly urge all block developers to avoid mutating any data passed to the block's <code>run(...)</code>  method. If modifications are necessary, always make a copy of the input object before making changes!</p>"},{"location":"workflows/workflows_execution_engine/#handling-step-inputs-and-outputs","title":"Handling step inputs and outputs","text":"<p>Handling step inputs and outputs is a complex task for the Execution Engine. This involves:</p> <ul> <li> <p>Differentiating between SIMD (Single Instruction, Multiple Data) and non-SIMD blocks in relation to their inputs.</p> </li> <li> <p>Preparing step inputs while considering conditional execution and the expected input dimensionality.</p> </li> <li> <p>Managing outputs from steps that control the flow of execution.</p> </li> <li> <p>Registering outputs from data-processing steps, ensuring they match the output dimensionality declared by the blocks.</p> </li> </ul> <p>Let\u2019s explore each of these topics in detail.</p>"},{"location":"workflows/workflows_execution_engine/#simd-vs-non-simd-steps","title":"SIMD vs non-SIMD steps","text":"<p>As the definition suggests, a SIMD (Single Instruction, Multiple Data) step processes batch-oriented data, where the  same operation is applied to each data point, potentially using non-batch-oriented parameters for configuration.  The output from such a step is expected to be a batch of elements, preserving the order of the input batch elements.  This applies to both regular processing steps and flow-control steps (see  blocks development guide), where flow-control decisions  affect each batch element individually.</p> <p>In essence, the type of data fed into the step determines whether it's SIMD or non-SIMD. If a step requests any  batch-oriented input, it will be treated as a SIMD step.</p> <p>Non-SIMD steps, by contrast, are expected to deliver a single result for the input data. In the case of non-SIMD  flow-control steps, they affect all downstream steps as a whole, rather than individually for each element in a batch.</p> <p>Historically, Execution Engine could not handle well all scenarios when non-SIMD steps' outputs were fed into SIMD steps inputs - causing compilation error due to lack of ability to automatically cast such outputs into batches when feeding into SIMD seps. Starting with Execution Engine <code>v1.6.0</code>, the handling of SIMD and non-SIMD blocks has been improved  through the introduction of Auto Batch Casting:</p> <ul> <li> <p>When a SIMD input is detected but receives scalar data, the Execution Engine automatically casts it into a batch.</p> </li> <li> <p>The dimensionality of the batch is determined at compile time, using lineage information from other  batch-oriented inputs when available. Missing dimensions are generated in a manner similar to <code>torch.unsqueeze(...)</code>.</p> </li> <li> <p>Outputs are evaluated against the casting context - leaving them as scalars when block keeps or decreases output  dimensionality or creating new batches when increase of dimensionality is expected.</p> </li> </ul>"},{"location":"workflows/workflows_execution_engine/#preparing-step-inputs","title":"Preparing step inputs","text":"<p>Each requested input element may be batch-oriented or not. Non-batch inputs are relatively easy,  they do not require special treatment. With batch-oriented ones, there is a lot more of a hustle. Execution Engine maintains indices for each batch-oriented datapoints, for instance:</p> <ul> <li> <p>if there is input images batch, each element will achieve their own unique index - let's say there is four input images, the batch indices will be <code>[(0, ), (1, ), (2, ), (3, )]</code>.</p> </li> <li> <p>step output being not-nested batch will also be indexed, for instance predictions from a model for each of image mentioned above will also be indexed <code>[(0, ), (1, ), (2, ), (3, )]</code>.</p> </li> <li> <p>having a block that increases <code>dimensionality level</code> - let's say a Dynamic Crop based on predictions from object-detection model - having 2 crops for first image, 1 for second and three for fourth - output of such step will be indexed in the following way: <code>[(0, 0), (0, 1), (1, 0), (3, 0), (3, 1), (3, 2)]</code>.</p> </li> </ul> <p>Indexing of elements is important while gathering inputs for steps execution. Thanks to them, all batch oriented  inputs may be aligned - such that Execution Engine will always ship prediction <code>(3, )</code> with image <code>(3, )</code> and  crops batch of crops <code>[(3, 0), (3, 1), (3, 2)]</code> when any step requests it. </p> <p>Each requested input element can either be batch-oriented or non-batch. Non-batch inputs are straightforward and don't  require special handling. However, batch-oriented inputs involve more complexity. The Execution Engine tracks indices  for each batch-oriented data point. For example:</p> <ul> <li> <p>If there's a batch of input images, each element receives its own unique index. For a batch of four images, the  indices would be <code>[(0,), (1,), (2,), (3,)]</code>.</p> </li> <li> <p>A step output which do not increase dimensionality will also be indexed similarly. For example, model predictions  for each of the four images would have indices <code>[(0,), (1,), (2,), (3,)]</code>.</p> </li> <li> <p>If a block increases the <code>dimensionality_level</code> (e.g., a dynamic crop based on predictions from an object  detection model), the output will be indexed differently. Suppose there are 2 crops for the first image,  1 for the second, and 3 for the fourth. The indices for this output would be  <code>[(0, 0), (0, 1), (1, 0), (3, 0), (3, 1), (3, 2)]</code>.</p> </li> </ul> <p>Indexing is crucial for aligning inputs during step execution. The Execution Engine ensures that all batch-oriented  inputs are synchronized. For example, it will match prediction <code>(3,)</code> with image <code>(3,)</code> and the corresponding batch  of crops <code>[(3, 0), (3, 1), (3, 2)]</code> when a step requests them.</p> <p>Note</p> <p>Keeping data lineage in order during compilation simplifies execution. The Execution Engine doesn't need to  verify if dynamically created nested batches come from the same source. Its job is to align indices when  preparing step inputs.</p>"},{"location":"workflows/workflows_execution_engine/#additional-considerations","title":"Additional Considerations","text":""},{"location":"workflows/workflows_execution_engine/#input-dimensionality-offsets","title":"Input Dimensionality Offsets","text":"<p>Workflow blocks define how input dimensionality is handled.  If the Execution Engine detects a difference in input dimensionality, it will wrap the larger dimension into a batch.  For example, if a block processes both input images and dynamically cropped images, the latter will be wrapped into a  batch so that each top-level image is processed with its corresponding batch of crops.</p>"},{"location":"workflows/workflows_execution_engine/#deeply-nested-batches-are-flattened-before-step-execution","title":"Deeply nested batches are flattened before step execution","text":"<p>Given the block defines all input at the same dimensionality level, no matter how deep the nesting of input batches is,  step input will be flattened to a single batch and indices in the outputs will be automagically maintained by  Execution Engine.</p>"},{"location":"workflows/workflows_execution_engine/#conditional-execution","title":"Conditional Execution","text":"<p>Flow-control blocks manage which steps should be executed based on certain conditions. During compilation,  steps affected by these conditions are flagged. When constructing their inputs, a mask for flow-control exclusion  (both SIMD- and non-SIMD-oriented) is applied. Based on this mask, specific input elements will be replaced with  <code>None</code>, representing an empty value.</p> <p>By default, blocks don't accept empty values, so any <code>None</code> at index <code>(i,)</code> in a batch will cause that index to be  excluded from processing. This is how flow control is managed within the Execution Engine. Some blocks, however,  are designed to handle empty inputs. In such cases, while the flow-control mask will be applied, empty inputs  won't be eliminated from the input batch.</p>"},{"location":"workflows/workflows_execution_engine/#batch-processing-mode","title":"Batch Processing Mode","text":"<p>Blocks may either process batch inputs all at once or, by default, require the Execution Engine to loop over each  input and repeatedly invoke the block's <code>run(...)</code> method.</p>"},{"location":"workflows/workflows_execution_engine/#managing-flow-control-steps-outputs","title":"Managing flow-control steps outputs","text":"<p>The outputs of flow-control steps are unique because these steps determine which data points should be  passed to subsequent steps which is roughly similar to outcome of this pseudocode:</p> <pre><code>if condition(A):\n    step_1(A)\n    step_2(A)\nelse:\n    step_3(A)\n</code></pre> <p>The Workflows Execution Engine parses the outputs from flow-control steps and creates execution branches.  Each branch has an associated mask:</p> <ul> <li> <p>For SIMD branches, the mask contains a set of indices that will remain active for processing.</p> </li> <li> <p>For non-SIMD branches, the mask is a simple <code>True</code> / <code>False</code> value that determines whether the entire  branch is active.</p> </li> </ul> <p>After a flow-control step executes, this mask is registered and applied to any steps affected by the decision.  This allows the Engine to filter out specific data points from processing in the downstream branch.  If a data point is excluded from the first step in a branch (due to the masking), that data point is automatically  eliminated from the entire branch (as a result of exclusion of empty inputs by default).</p>"},{"location":"workflows/workflows_execution_engine/#caching-steps-outputs","title":"Caching steps outputs","text":"<p>It's not just the outcomes of flow-control steps that need to be managed carefully\u2014data processing steps also require  attention to ensure their results are correctly passed to other steps. The key aspect here is properly indexing  the outputs.</p> <p>In simple cases where all inputs share the same <code>dimensionality level</code> and the output maintains that same  dimensionality, the Execution Engine's main task is to preserve the order of input indices. However, when input  dimensionalities differ, the Workflow block used to create the step determines how indexing should be handled.</p> <p>If the dimensionality changes during processing, the Execution Engine either uses the high-level index or creates  nested dimensions dynamically based on the length of element lists in the output. This ensures proper alignment and  tracking of data across steps.</p>"},{"location":"workflows/workflows_execution_engine/#building-workflow-outputs","title":"Building Workflow outputs","text":"<p>For details on how outputs are constructed, please refer to the information provided on the  Workflows Definitions page and the  Output Construction section of the Workflow Execution  documentation.</p>"},{"location":"workflows/batch_processing/about/","title":"Roboflow Batch Processing","text":"<p>Roboflow Batch Processing is a fully managed solution powered by Workflows that allows you to process large  volumes of videos and images without writing code. It offers an easy-to-use UI for quick tasks and a comprehensive  API for automating data processing\u2014fitting both small and large workloads.</p> <p>With configurable processing workflows, real-time monitoring, and event-based notifications, Roboflow Batch Processing  helps you efficiently manage data processing, track progress, and integrate with other systems\u2014making it easy to  achieve your goals.</p> <p>What is the nature of <code>batch</code> processing?</p> <p>Batch processing involves accepting large volumes of data to be processed in jobs that run in the background - without external orchestration and with weak guarantees on when the job will start or  when results will be available.</p> <p>When the service is not busy, jobs typically start within 3\u20138 minutes after being scheduled, but this time may  be longer under high load.</p> <p>This service is suitable for use cases that do not require real-time responses, such as:</p> <ul> <li> <p>Analyzing pre-recorded video files</p> </li> <li> <p>Making predictions from a large pool of images stored in your storage</p> </li> <li> <p>Automatic data labeling</p> </li> </ul>"},{"location":"workflows/batch_processing/about/#quick-overview","title":"Quick overview","text":""},{"location":"workflows/batch_processing/about/#getting-started","title":"Getting started","text":"<p>To get started with Roboflow Batch Processing, first build and test your Workflow. Once it's ready, select the data you  want to process and upload it using the UI or CLI tool. Then, initiate the processing and let Batch Processing handle  the rest. Once the job is completed, retrieve the results and use them as needed. That\u2019s it \u2014 you no longer need to  write code or run processing on your machine.</p>"},{"location":"workflows/batch_processing/about/#ui-interface","title":"UI Interface","text":"<p>The Roboflow platform provides a UI interface to interact with Roboflow Batch Processing, making it easy and accessible  to try out the feature or process a  small to moderate amount of data. </p> <p></p> <p>When creating a job, you can choose between image and video processing, select the appropriate Workflow, and adjust  settings to fine-tune the system\u2019s behavior. Key options include:</p> <ul> <li> <p>Machine type \u2013 Choose between GPU and CPU based on processing needs. For Workflows using multiple or large  models, a GPU is recommended, while smaller models or external API-based tasks can run efficiently on a CPU.</p> </li> <li> <p>Predictions visualization &amp; video outputs \u2013 Enable this option if you want to generate and retain visual  outputs of your Workflow.</p> </li> <li> <p>Video frame rate sub-sampling \u2013 Skip frames for faster, more cost-effective video processing.</p> </li> <li> <p>Maximum job runtime \u2013 Set a limit to help control costs and prevent excessive processing times.</p> </li> </ul>"},{"location":"workflows/batch_processing/about/#cli","title":"CLI","text":"<p>By installing <code>inference-cli</code> you gain access to the <code>inference rf-cloud</code> command, which allows you to interact with  managed components of the Roboflow Platform \u2014 including <code>batch-processing</code> and <code>data-staging</code>, the core components of  the Roboflow Batch Processing offering.</p> <p>Inference CLI setup</p> <p>To follow the tutorial you must install <code>inference-cli</code> and export your Roboflow API key.</p> <pre><code>pip install inference-cli\nexport ROBOFLOW_API_KEY=\"YOUR-API-KEY-GOES-HERE\"\n</code></pre> <p>If you struggle to find the API key, check our guide. </p> <p>The typical flow of interaction with the CLI is as follows:</p> <p>First, ingest data into the platform. For images, use the following command:</p> <pre><code>inference rf-cloud data-staging create-batch-of-images --images-dir &lt;your-images-dir-path&gt; --batch-id &lt;your-batch-id&gt;\n</code></pre> <p>for videos: <pre><code>inference rf-cloud data-staging create-batch-of-videos --videos-dir &lt;your-images-dir-path&gt; --batch-id &lt;your-batch-id&gt;\n</code></pre></p> <p>Format of <code>&lt;your-batch-id&gt;</code></p> <p>Batch ID must be lower-cased string without special caraters, with letters and digits allowed.</p> <p>Then, you can inspect the details of staged batch of data:</p> <pre><code>inference rf-cloud data-staging show-batch-details --batch-id &lt;your-batch-id&gt;\n</code></pre> <p>Once the data is ingested - you can trigger batch job.</p> <p>For images, use the following command:</p> <pre><code>inference rf-cloud batch-processing process-images-with-workflow \\\n  --workflow-id &lt;workflow-id&gt; \\\n  --batch-id &lt;batch-id&gt; \\\n  --machine-type gpu\n</code></pre> <p>For videos:</p> <pre><code>inference rf-cloud batch-processing process-videos-with-workflow \\\n  --workflow-id &lt;workflow-id&gt; \\\n  --batch-id &lt;batch-id&gt; \\\n  --machine-type gpu \\\n  --max-video-fps &lt;your-desired-fps&gt;\n</code></pre> <p>How would I know <code>&lt;workflow-id&gt;</code>?</p> <p>Workflow ID can be found in Roboflow App - open Workflow Editor of selected Workflow, hit \"Deploy\" button  and find identifier in code snippet.</p> <p>GPU vs CPU</p> <p>By default, processing run on CPU device, but if you require extra compute power - use <code>--machine-type gpu</code>  option of the above commands.</p> <p>Command will display the ID of the job, which can be used to check the job status: </p> <pre><code>inference rf-cloud batch-processing show-job-details --job-id &lt;your-job-id&gt;\n</code></pre> <p>This allows you to track the progress of your job. Additionally, the command will provide the ID of the output  batch, which can be used to export results.</p> <pre><code>inference rf-cloud data-staging export-batch --target-dir &lt;dir-to-export-result&gt; --batch-id &lt;output-batch-of-a-job&gt;\n</code></pre> <p>That's it - you should be able to see your processing results now.</p> <p>All configuration options</p> <p>To discover all configuration options use the following help commands:</p> <pre><code>inference rf-cloud --help\ninference rf-cloud data-staging --help \ninference rf-cloud batch-processing --help \n</code></pre>"},{"location":"workflows/batch_processing/about/#service-pricing","title":"Service Pricing","text":"<p>The service charges usage based on the runtime of the underlying compute machines, starting at 4 credits per GPU hour  and 1 credit per CPU hour. You can find the specific rates for your workspace on our  pricing page.</p> <p>We cannot provide an exact cost estimate for processing 1,000 images or 1 hour of video, as this depends entirely on  the complexity of the chosen Workflow. However, we offer benchmark results to help you better understand  potential costs.</p> Workflow Description Dataset Size Machine Type Charge Single Model - YOLOv8 Nano <code>(image size = 640)</code> - Object Detection 100k images GPU 0.04  credit / 1k images Single Model - YOLOv8 Nano <code>(image size = 1280)</code>- Object Detection 100k images GPU 0.06  credit / 1k images Single Model - YOLOv8 Medium <code>(image size = 640)</code> - Object Detection 100k images GPU 0.06  credit / 1k images Single Model - YOLOv8 Medium <code>(image size = 1280)</code> - Object Detection 100k images GPU 0.1  credit / 1k images Single Model - YOLOv8 Large <code>(image size = 640)</code> - Object Detection 100k images GPU 0.08 credit / 1k images Single Model - YOLOv8 Large <code>(image size = 1280)</code> - Object Detection 100k images GPU 0.18 credit / 1k images Single Model - Roboflow Instant - Object Detection 30k images GPU 0.33  credit / 1k images Single Model - Florence-2 - Object Detection + Region Captioning 30k images GPU 0.5  credit / 1k images Two stage - YoloV8 Nano + crop + YoloV8 Nano <code>(image size = 640)</code> - OD 10k images GPU 0.25  credit / 1k images Two stage - YoloV8 Nano + crop + YoloV8 Large <code>(image size = 640)</code> - OD + OD 10k images GPU 0.30  credit / 1k images Two stage - YoloV8 Nano + crop + CLIP <code>(image size = 640)</code> - OD + Embeddings 10k images GPU 0.25  credit / 1k images Two stage - YoloV8 Nano + crop + Classifier <code>(image size = 640)</code> - OD + CLS 10k images GPU 0.20  credit / 1k images Two stage - YoloV8 Nano + crop + SAM 2 <code>(image size = 640)</code> - OD + Segmentation 10k images GPU 0.40  credit / 1k images Single Model - YOLOv8 Nano <code>(image size = 640)</code> - Object Detection 4 videos, each 1h @ 30 fps 480p GPU 1  credit / video hour, 0.01 credit / 1k frames Single Model - YOLOv8 Nano <code>(image size = 640)</code> - Object Detection + tracking 32 videos, each 1m @ 10 fps HD CPU 1.8 credit / video hour, 0.05 credit / 1k frames Two stage - YoloV8 Nano + crop + Classifier <code>(image size = 640)</code> - OD + CLS 2 videos, each 1h @ 30 fps 480p GPU 4.6  credits / video hour, 0.046 credit / 1k frames <p>Cost estimation in practice</p> <p>Please consider the results above as reference values only\u2014we advise checking the cost of smaller data batches  before running large processing jobs. Reported values can be reproduced once optimal settings for machine type  and machine concurrency are configured.</p> <p>Please take into account the technical nuances of the service (described below) to better understand the pricing.  In particular, since the service shards data under the hood and executes parallel processing on multiple machines simultaneously, wall clock execution time usually does not equal the billed time. For instance, if a job uses  four GPU machines for one hour, the billed amount would be 4 GPU-hours (16 credits).</p>"},{"location":"workflows/batch_processing/about/#known-limitations","title":"Known limitations","text":"<ul> <li> <p>Batch Processing service cannot run Custom Python blocks.</p> </li> <li> <p>Certain Workflow blocks requiring access to env variables and local storage (like File Sink and Environment  Secret Store) are blacklisted and will not execute.</p> </li> <li> <p>Service only works with Workflows that define single input image parameter.</p> </li> </ul>"},{"location":"workflows/batch_processing/about/#technical-details-of-batch-processing","title":"Technical Details of Batch Processing","text":"<ul> <li> <p>Data is stored in the <code>data-staging</code> component of the system \u2014 both your input images / videos and the processing  results. The expiry time for any piece of data submitted to data staging is 7 days.</p> </li> <li> <p>When you upload data, you create a data batch in data staging. Batch processing jobs accept input batches marked  for processing (each batch is processed by a single job).</p> </li> <li> <p>A single batch processing job contains multiple stages (typically <code>processing</code> and <code>export</code>). Each stage creates an  output batch that you can retrieve later. We advise using <code>export</code> stage outputs, as they are optimized for  network transfer (content is compressed / packed into an archive).</p> </li> <li> <p>A running job in the  <code>processing</code> stage can be aborted using both the UI and CLI.</p> </li> <li> <p>An aborted or failed job can be restarted using the mentioned tools.</p> </li> <li> <p>The service automatically shards the data and processes it in parallel under the hood.</p> </li> <li> <p>Parallelism is applied at two levels:</p> <ul> <li> <p>The service automatically adjusts the number of machines running the job based on data volume, ensuring  sufficient throughput (values reaching 500k\u20131M images per hour should be achievable for certain workloads).</p> </li> <li> <p>A single machine runs multiple workers processing tasks (chunks of data) that belong to the job. This option can be configured by the user and should be adjusted to balance processing speed and costs.</p> </li> </ul> </li> </ul>"},{"location":"workflows/batch_processing/api_reference/","title":"API Reference","text":"<p>Below you can find API reference for Batch Processing and Data Staging services. Specifications can be used to build API integrations.</p> <p></p> <p></p>"},{"location":"workflows/batch_processing/api_reference/#data-staging-batches-explained","title":"Data Staging batches explained","text":"<p>The Data Staging service is responsible for creating and maintaining batches of data\u2014logically organized groups of  files that are either queued for processing or represent the output of a completed processing job.</p> <p>There are three types of batches:</p> <ul> <li> <p>Simple batches (recognized by <code>type: simple-batch</code>) are created when users ingest data one item at a time using  endpoints for individual images or videos. There is no strict size limit, but for best performance, it's recommended to  keep these batches relatively small\u2014ideally up to 5,000\u201310,000 items.</p> </li> <li> <p>Sharded batches (recognized by <code>type: sharded-batch</code>) are created when users leverage bulk ingestion options and  are currently supported only for images. These batches are designed for large-scale workloads, capable of handling  millions of data points. Data is automatically sharded to support efficient parallel processing.</p> </li> <li> <p>Multipart batches (recognized by <code>type: multipart-batch</code>) are created internally by the system and cannot be  created directly by users. Each multipart batch is a logical grouping of sub-batches (each behaving like a simple or  sharded batch), managed as a single entity. Operations such as listing or exporting are typically performed on  individual parts, which must be listed beforehand.</p> </li> </ul>"},{"location":"workflows/batch_processing/integration/","title":"Integration with Batch Processing","text":"<p>Batch processing is well-suited for task automation, especially when processes need to run on a  recurring basis. A common question is how to integrate Roboflow Batch Processing with external  systems on the client\u2019s end.</p> <p>This documentation provides a detailed guide on integration strategies, covering best practices  and implementation approaches.</p>"},{"location":"workflows/batch_processing/integration/#overview","title":"Overview","text":"<p>The following bullet-points illustrate the interaction with Roboflow Batch Processing:</p> <ul> <li> <p>Workflow Creation: A workflow is created to process the data.</p> </li> <li> <p>Data Ingestion: The data is ingested, creating a batch in Data Staging, which serves as ephemeral storage  for both input and output batch processing data.</p> </li> <li> <p>Processing: The data undergoes processing, which involves multiple (typically two) stages. Usually <code>processing</code>  stage is responsible for running the Workflow against ingested data producing predictions. This stage results in  CSV / JSONL files being created. This stage is usually followed by <code>export</code>, which is responsible for creating archives  with processing results for convenient extraction from Roboflow platform.</p> </li> <li> <p>Data Export: The data is exported from one of the output batches created during processing. Data is exposed  through download links which can be used to pull the data. </p> </li> </ul>"},{"location":"workflows/batch_processing/integration/#basic-interactions-with-batch-processing","title":"Basic interactions with Batch Processing","text":"<p>We will begin by outlining the basic interactions with this feature, providing a foundational understanding.  Building on this, we will explore more advanced concepts later in the documentation.</p> <p>For demonstration, we will use <code>inference-cli</code> commands and cURL, though the same functionality is available through the UI.</p> <p>The first step in batch processing is ingesting the data. Roboflow supports both video and image ingestion,  with options for individual uploads and optimized bulk ingestion.</p> <p><code>inference-cli</code> installation</p> <p>If you plan to use <code>inference-cli</code> snippets - please make sure that the package is installed in your environment</p> <pre><code>pip install inference-cli\n</code></pre> <p>For convenience, we recommend exporting Roboflow API key as env variable:</p> <pre><code>export ROBOFLOW_API_KEY=YOUR_API_KEY\n</code></pre>"},{"location":"workflows/batch_processing/integration/#video-ingestion","title":"Video ingestion","text":"<p>The simplest approach is uploading videos one by one. Once the request succeeds, the videos are ready for use. Use the  following command to ingest videos:</p> inference-clicURL <pre><code>inference rf-cloud data-staging create-batch-of-videos --videos-dir &lt;your-images-dir-path&gt; --batch-id &lt;your-batch-id&gt;\n</code></pre> <p>Ingesting video file is in fact requesting upload URL from Roboflow API and uploading the data  to the pointed location.</p> <pre><code>curl -X POST \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/upload/video\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\" \\\n  --data-urlencode \"fileName=your_video.mp4\"\n</code></pre> <p>Response contains <code>\"signedURLDetails\"</code> key with the following details:</p> <ul> <li> <p><code>\"uploadURL\"</code> - the URL to PUT the video</p> </li> <li> <p><code>\"extensionHeaders\"</code> - additional headers to include</p> </li> </ul> <p>To upload the video, send the following request:</p> <pre><code>curl -X PUT &lt;url-from-the-response&gt; -H \"Name: value\" --upload-file &lt;path-to-your-video&gt;\n</code></pre> <p>with all headers from <code>\"extensionHeaders\"</code> response field.</p> <p>You can find full API reference here.</p> <p><code>batch_id</code> constraints</p> <p>Currently, the client is in controll of <code>batch_id</code> and must meet the following  constraints:</p> <ul> <li> <p>only lower-cased ASCII letters, digits, hypen (<code>-</code>) and underscore (<code>_</code>) characters are allowed</p> </li> <li> <p><code>batch_id</code> must be at most 64 characters long</p> </li> </ul>"},{"location":"workflows/batch_processing/integration/#image-ingestion","title":"Image Ingestion","text":"<p>Images can also be uploaded individually. However, for larger batches, a batch upload API is available for better  performance. Clients can bundle up to 500 images into a single <code>*.tar</code> archive and send it to the API.</p> <p>Asynchronous indexing of batches</p> <p>When performing bulk ingestion, Roboflow indexes the data in the background. This means that after the command  completes, there may be a short delay before the data is fully available, depending on batch size.</p> inference-clicURL (single image)cURL (bulk upload) <pre><code>inference rf-cloud data-staging create-batch-of-images --images-dir &lt;your-images-dir-path&gt; --batch-id &lt;your-batch-id&gt;\n</code></pre> <p>The approach presented below is simplified version of bulk images ingest which let clients upload  images one-by-one. Due to speed limitations, we recommend using this method for batches up to 5000 images.  It is also important to note that this method cannot be used along with bulk ingest for the same batch.</p> <pre><code>curl -X POST \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/upload/image\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\" \\\n  --data-urlencode \"fileName=your_image.jpg\" \\\n  -F \"your_image.jpg=@/path/to/your/image.jpg\"\n</code></pre> <p>You can find full API reference here.</p> <p>To optimise ingest speed, we recommend using bulk ingests for batches of size exceeding 5000 images.  The procedure contains three steps:</p> <ul> <li> <p>requesting upload URL from Roboflow API</p> </li> <li> <p>packing images to <code>*.tar</code> archive according to limits of size and files number dictated by API</p> </li> <li> <p>uploading the archive to URL obtained from Roboflow API</p> </li> </ul> <pre><code>curl -X POST \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/bulk-upload/image-files\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\"\n</code></pre> <p>Response contains <code>\"signedURLDetails\"</code> key with the following details:</p> <ul> <li> <p><code>\"uploadURL\"</code> - the URL to PUT the video</p> </li> <li> <p><code>\"extensionHeaders\"</code> - additional headers to include</p> </li> </ul> <p>To upload the archive, send the following request:</p> <pre><code>curl -X PUT &lt;url-from-the-response&gt; -H \"Name: value\" --upload-file &lt;path-to-your-video&gt;\n</code></pre> <p>with all headers from <code>\"extensionHeaders\"</code> response field.</p> <p>Please remember, that bach created with bulk upload cannot be further filled with data ingested with simple upload  due to internal data organisation enforced by the upload method.</p> <p>You can find full API reference here.</p> <p><code>batch_id</code> constraints</p> <p>Currently, the client is in controll of <code>batch_id</code> and must meet the following  constraints:</p> <ul> <li> <p>only lower-cased ASCII letters, digits, hypen (<code>-</code>) and underscore (<code>_</code>) characters are allowed</p> </li> <li> <p><code>batch_id</code> must be at most 64 characters long</p> </li> </ul>"},{"location":"workflows/batch_processing/integration/#before-starting-the-job","title":"Before starting the job","text":"<p>Once data ingestion is complete, the next step is to start a job. However, since some background processing occurs,  it's important to determine when the data is fully ready.</p> <p>The simplest approach is to check the batch status by polling the system. This allows you to verify how many files  have been successfully ingested before proceeding. Use the following command:</p> inference-clicURL <pre><code>inference rf-cloud data-staging show-batch-details --batch-id &lt;your-batch-id&gt;\n</code></pre> <pre><code>curl -X GET \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/count\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\"\n</code></pre> <p>You can find full API reference here.</p> <p>To identify potential issues during data ingestion, you can fetch status updates using the following command:</p> inference-clicURL <pre><code>inference rf-cloud data-staging list-ingest-details --batch-id &lt;your-batch-id&gt;\n</code></pre> <p>Endpoint to fetch shard upload details supports pagination, as the number of shards may be huge. Use  <code>nextPageToken</code> in consecutive requests, based on previous responses (first request do not need to have this  parameter attached).</p> <pre><code>curl -X GET \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/shards\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\" \\\n  --data-urlencode \"nextPageToken=OptionalNextPageToken\"\n</code></pre> <p>You can find full API reference here.</p> <p>Only after confirming that the expected number of files is available can the data be considered ready for processing.</p>"},{"location":"workflows/batch_processing/integration/#job-kick-off","title":"Job kick off","text":"<p>Once data ingestion is complete, you can start processing the batch. For image processing, use the following command:</p> inference-clicURL <pre><code>inference rf-cloud batch-processing process-images-with-workflow --workflow-id &lt;workflow-id&gt; --batch-id &lt;batch-id&gt;\n</code></pre> <pre><code>curl -X POST \"https://api.roboflow.com/batch-processing/v1/external/{workspace}/jobs/{job_id}\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"type\": \"simple-image-processing-v1\",\n    \"jobInput\": {\n        \"type\": \"staging-batch-input-v1\",\n        \"batchId\": \"{batch_id}\"\n    },\n    \"computeConfiguration\": {\n        \"type\": \"compute-configuration-v2\",\n        \"machineType\": \"cpu\",\n        \"workersPerMachine\": 4\n    },\n    \"processingTimeoutSeconds\": 3600,\n    \"processingSpecification\": {\n        \"type\": \"workflows-processing-specification-v1\",\n        \"workspace\": \"{workspace}\",\n        \"workflowId\": \"{workflow_id}\",\n        \"aggregationFormat\": \"jsonl\"\n    }\n}'\n</code></pre> <p>This example does not cover all of the parameters that can be used.  You can find full API reference here.</p> <p><code>job_id</code> constraints</p> <p>Currently, the client is in controll of <code>job_id</code> and must meet the following  constraints:</p> <ul> <li> <p>only lower-cased ASCII letters, digits, hypen (<code>-</code>) and underscore (<code>_</code>) characters are allowed</p> </li> <li> <p><code>job_id</code> must be at most 20 characters long</p> </li> </ul> <p>Batch processing jobs can take time to complete. To determine when results are ready for export, you need to  periodically check the job status:</p> inference-clicURL <pre><code>inference rf-cloud batch-processing show-job-details --job-id &lt;your-job-id&gt;\n</code></pre> <p>Since job contains several stages, checking job status may be performed at different  level of granularity.</p> <p>To check the general status of a job: <pre><code>curl -X GET \"https://api.roboflow.com/batch-processing/v1/external/{workspace}/jobs/{job_id}\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\"\n</code></pre></p> <p>To list job stages: <pre><code>curl -X GET \"https://api.roboflow.com/batch-processing/v1/external/{workspace}/jobs/{job_id}/stages\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\"\n</code></pre></p> <p>To list job stage tasks (fundamental units of processing): <pre><code>curl -X GET \"https://api.roboflow.com/batch-processing/v1/external/{workspace}/jobs/{job_id}/stages/{stage_id}/tasks\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\" \\\n  --data-urlencode \"nextPageToken={next_page_token}\"\n</code></pre> The endpoint supports pagination - use <code>nextPageToken</code> in consecutive requests, based on previous responses  (first request do not need to have this parameter attached).</p> <p>You can find full API reference here.</p>"},{"location":"workflows/batch_processing/integration/#data-export","title":"Data export","text":"<p>Once the batch processing job is complete, you can download the results using the following command.  Figuring out <code>batch-id</code> would require listing job stages and choosing which stage output to export.  Typically <code>export</code> stage output is exported, as it contains archives that are easy to be transferred, but  one may choose to export results of other stages if needed.</p> inference-clicURL <pre><code>inference rf-cloud data-staging export-batch --target-dir &lt;dir-to-export-result&gt; --batch-id &lt;output-batch-of-a-job&gt;\n</code></pre> <p>The Batch Processing service creates \"multipart\" batches to store results. The term \"multipart\" refers to  nested nature of batch - namely it contains multiple \"parts\" of data, each being nested batch. To export such data batch, you may want to list its parts first:</p> <pre><code>curl -X GET \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/parts\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\"\n</code></pre> <p>Then, for each part, list operation can be performed. As a result - download URL for each batch element will be  exposed:</p> <p><pre><code>curl -X GET \"https://api.roboflow.com/data-staging/v1/external/{workspace}/batches/{batch_id}/list\" \\\n  -G \\\n  --data-urlencode \"api_key=YOUR_API_KEY\" \\\n  --data-urlencode \"nextPageToken=YOUR_NEXT_PAGE_TOKEN\" \\\n  --data-urlencode \"partName=YOUR_PART_NAME\"\n</code></pre> The endpoint supports pagination - use <code>nextPageToken</code> in consecutive requests, based on previous responses  (first request do not need to have this parameter attached).</p> <p>Having download URL for each batch element, the following <code>curl</code> command can be used to pull the data.</p> <pre><code>curl &lt;download-url&gt; -o &lt;downlaod-file-location&gt;\n</code></pre> <p>You can find full API reference here.</p> <p>As seen in this workflow, multiple manual interactions are required to complete the process. To efficiently handle  large volumes of data on a recurring basis, automation is essential. The next section will explore strategies for  automating batch processing.</p>"},{"location":"workflows/batch_processing/integration/#automation-on-top-of-batch-processing","title":"Automation on top of Batch Processing","text":"<p>The approach outlined above presents a few challenges:</p> <ul> <li> <p>Local Data Dependency: The process assumes that data is locally available, which is often not the case in  enterprise environments where data is stored in the cloud.</p> </li> <li> <p>Active Polling: The process requires constant polling to verify statuses, which is inefficient and not scalable.</p> </li> </ul> <p>To address these issues, the system supports webhook notifications for both data staging and batch processing. Webhooks allow external systems to automatically react to status changes, eliminating the need for manual polling and enabling seamless automation. Additionally, system allows for data ingestion through signed URLs which should streamline integrations with cloud storage.</p> <p>API integration</p> <p>In the document, we are presenting snippets from <code>inefrence-cli</code>, but under the hood there is  fully functional API that clients may integrate with (or use Python client embedded in CLI).</p>"},{"location":"workflows/batch_processing/integration/#data-ingestion","title":"Data ingestion","text":"<p>Both <code>inference rf-cloud data-staging create-batch-of-videos</code> and <code>inference rf-cloud data-staging create-batch-of-images</code> commands support additional parameters to enable webhook notifications and ingest data directly from cloud storage.</p> <ul> <li> <p><code>--data-source reference-file</code> instructs the CLI to process files referenced via signed URLs instead of requiring local data.</p> </li> <li> <p><code>--references &lt;path_or_url&gt;</code> specifies either a local path to a JSONL document containing file URLs or a signed URL pointing to such a file.</p> </li> <li> <p><code>--notifications-url &lt;webhook_url&gt;</code> defines the webhook URL where notifications about the ingestion process will be sent.</p> </li> <li> <p><code>--notification-category &lt;value&gt;</code> - optionally allows filtering which notifications are sent. Options include:</p> <ul> <li> <p><code>ingest-status</code> (default) - notifications about the overall ingestion process.</p> </li> <li> <p><code>files-status</code> - notifications for individual file processing.`</p> </li> </ul> </li> </ul> <p>API reference</p> <p>Use API reference document to find out how to enforce  Batch Processing service notifications when integrating directly with API.</p> <p>Limited access to the feature</p> <p>Currently, only Growth Plan customers and Enterprise customers can ingest data through signed URLs.</p>"},{"location":"workflows/batch_processing/integration/#structure-of-references-file","title":"Structure of references file","text":"<p>JSONL (JSON Lines)</p> <p>The JSONL (JSON Lines) format consists of multiple JSON objects, each on a separate line within a single text file.  This format is commonly used to store large datasets and allows for efficient line-by-line processing.</p> <p>Each entry in the references file contains two key attributes:</p> <ul> <li><code>name</code>: A unique identifier for the file.</li> <li><code>url</code>: A signed URL pointing to the file in cloud storage.</li> </ul> <p>Here\u2019s an example of the JSONL format: <pre><code>{\"name\": \"&lt;your-unique-name-of-file-1&gt;\", \"url\": \"https://&lt;signed-url&gt;\"}\n{\"name\": \"&lt;your-unique-name-of-file-2&gt;\", \"url\": \"https://&lt;signed-url&gt;\"}\n</code></pre></p> <p>You can store this references file locally or in cloud storage. If the file is stored in the cloud, simply provide the signed URL to the file when running the ingestion command.</p>"},{"location":"workflows/batch_processing/integration/#cloud-storage","title":"Cloud Storage","text":""},{"location":"workflows/batch_processing/integration/#aws-s3-datasource","title":"AWS S3 Datasource","text":"<p>Using AWS S3 for ingesting data for batch processing can be achieved easily with this example script. This also supports S3-compatible datasources such as Backblaze B2, Cloudflare R2, Oracle Cloud Infrastructure Object Storage and many more by providing <code>--endpoint-url</code> for <code>aws</code> command.</p> <p>AWS CLI Installation Required</p> <p>This script requires the AWS CLI to be installed and configured with appropriate credentials.</p> <pre><code>#!/bin/bash\n\n# Script to generate S3 signed URLs for image files in JSONL format\n# Usage: ./generateS3SignedUrls.sh &lt;s3-path&gt; [output-file] [expiration-seconds] [parallel-jobs]\n# Or with curl:\n# curl -fsSL https://raw.githubusercontent.com/roboflow/roboflow-python/main/scripts/generateS3SignedUrls.sh | bash -s --  s3://bucket/path output.jsonl\n\nset -e\n\n# Check if S3 path is provided\nif [ -z \"$1\" ]; then\n    echo \"Error: S3 path is required\"\n    echo \"Usage: $0 &lt;s3-path&gt; [output-file] [expiration-seconds] [parallel-jobs]\"\n    echo \"Example: $0 s3://my-bucket/images/ output.jsonl 3600 8\"\n    exit 1\nfi\n\nS3_PATH=\"$1\"\nOUTPUT_FILE=\"${2:-signed_urls.jsonl}\"\nEXPIRATION=\"${3:-21600}\"  # Default: 6 hours\nPARALLEL_JOBS=\"${4:-20}\"  # Default: 20 parallel jobs\n\n# Remove trailing slash from S3 path if present\nS3_PATH=\"${S3_PATH%/}\"\n\n# Extract bucket name from S3_PATH\nBUCKET=$(echo \"$S3_PATH\" | sed 's|s3://||' | cut -d'/' -f1)\n\n# Image file extensions to include (regex pattern for grep)\nIMAGE_PATTERN='\\.(jpg|jpeg|png|gif|bmp|webp|tiff|tif|svg)$'\n\n# Function to process a single file\nprocess_file() {\n    local file_path=\"$1\"\n    local bucket=\"$2\"\n    local expiration=\"$3\"\n\n    # Construct full S3 URI\n    local s3_uri=\"s3://${bucket}/${file_path}\"\n\n    # Generate signed URL\n    local signed_url=$(aws s3 presign \"$s3_uri\" --expires-in \"$expiration\" 2&gt;/dev/null)\n\n    if [ $? -eq 0 ]; then\n        # Create name with full path using double underscores instead of slashes\n        local name_with_path=$(echo \"$file_path\" | sed 's|/|__|g')\n\n        # Output JSONL\n        echo \"{\\\"name\\\": \\\"$name_with_path\\\", \\\"url\\\": \\\"$signed_url\\\"}\"\n    fi\n}\n\n# Export function and variables for xargs\nexport -f process_file\nexport BUCKET\nexport EXPIRATION\n\necho \"Listing files from $S3_PATH...\"\n\n# Get list of all files, filter for images, and process in parallel\naws s3 ls \"$S3_PATH/\" --recursive | \\\n    awk '{print $4}' | \\\n    grep -iE \"$IMAGE_PATTERN\" | \\\n    xargs -I {} -P \"$PARALLEL_JOBS\" bash -c 'process_file \"$@\"' _ {} \"$BUCKET\" \"$EXPIRATION\" | \\\n    tee \"$OUTPUT_FILE\"\n\necho \"\"\necho \"Done! Signed URLs written to $OUTPUT_FILE\"\necho \"Total images processed: $(wc -l &lt; \"$OUTPUT_FILE\")\"\n</code></pre>"},{"location":"workflows/batch_processing/integration/#gcs-datasource","title":"GCS Datasource","text":"<p>Using google cloud storage (GCS) for ingesting data for batch processing can be easily achieved with this example script.</p> <p>gcloud CLI Installation Required</p> <p>This script requires the gcloud CLI to be installed and configured with appropriate credentials.</p> <pre><code>#!/bin/bash\n\n# Script to generate GCS signed URLs for image files in JSONL format\n# Usage: ./listgcs.sh &lt;gcs-path&gt; [output-file] [expiration-seconds] [parallel-jobs]\n\nset -e\n\n# Check if GCS path is provided\nif [ -z \"$1\" ]; then\n    echo \"Error: GCS path is required\"\n    echo \"Usage: $0 &lt;gcs-path&gt; [output-file] [expiration-seconds] [parallel-jobs]\"\n    echo \"Example: $0 gs://my-bucket/images/ output.jsonl 21600 8\"\n    exit 1\nfi\n\nGCS_PATH=\"$1\"\nOUTPUT_FILE=\"${2:-signed_urls.jsonl}\"\nEXPIRATION_SECONDS=\"${3:-21600}\"  # Default: 6 hours\nPARALLEL_JOBS=\"${4:-20}\"  # Default: 20 parallel jobs\n\n# Remove trailing slash from GCS path if present\nGCS_PATH=\"${GCS_PATH%/}\"\n\n# Convert seconds to duration format for gcloud (e.g., 21600s)\nEXPIRATION=\"${EXPIRATION_SECONDS}s\"\n\n# Image file extensions to include (regex pattern for grep)\nIMAGE_PATTERN='\\.(jpg|jpeg|png|gif|bmp|webp|tiff|tif|svg)$'\n\n# Function to find an appropriate service account\nfind_service_account() {\n    # First, try to get the default compute service account for the current project\n    local project_id=$(gcloud config get-value project 2&gt;/dev/null)\n    if [ -n \"$project_id\" ]; then\n        local compute_sa=\"${project_id}-compute@developer.gserviceaccount.com\"\n        if gcloud iam service-accounts describe \"$compute_sa\" &gt;/dev/null 2&gt;&amp;1; then\n            echo \"$compute_sa\"\n            return 0\n        fi\n    fi\n\n    # If that doesn't work, try to find any service account in the project\n    local sa_list=$(gcloud iam service-accounts list --format=\"value(email)\" --limit=1 2&gt;/dev/null)\n    if [ -n \"$sa_list\" ]; then\n        echo \"$sa_list\" | head -n 1\n        return 0\n    fi\n\n    return 1\n}\n\n# Try to find a service account to use\nSERVICE_ACCOUNT=$(find_service_account)\nif [ -z \"$SERVICE_ACCOUNT\" ]; then\n    echo \"Warning: No service account found. Attempting to sign URLs without impersonation.\"\n    echo \"If this fails, you may need to:\"\n    echo \"1. Authenticate with a service account: gcloud auth activate-service-account --key-file=key.json\"\n    echo \"2. Or ensure you have appropriate service accounts in your project\"\n    echo \"\"\nfi\n\n# Function to process a single file\nprocess_file() {\n    local object=\"$1\"\n    local service_account=\"$2\"\n    local expiration=\"$3\"\n\n    # Create signed URL using gcloud storage sign-url\n    local signed_url_output\n    if [ -n \"$service_account\" ]; then\n        signed_url_output=$(gcloud storage sign-url --http-verb=GET --duration=\"$expiration\" --impersonate-service-account=\"$service_account\" \"$object\" 2&gt;/dev/null)\n    else\n        signed_url_output=$(gcloud storage sign-url --http-verb=GET --duration=\"$expiration\" \"$object\" 2&gt;/dev/null)\n    fi\n\n    if [ $? -eq 0 ] &amp;&amp; [ -n \"$signed_url_output\" ]; then\n        # Extract just the signed_url from the YAML output\n        local signed_url=$(echo \"$signed_url_output\" | grep \"signed_url:\" | sed 's/signed_url: //')\n\n        if [ -n \"$signed_url\" ]; then\n            # Extract the path after the bucket name and convert slashes to double underscores\n            local path_part=$(echo \"$object\" | sed 's|gs://[^/]*/||')\n            local name_with_path=$(echo \"$path_part\" | sed 's|/|__|g')\n\n            # Output JSONL\n            echo \"{\\\"name\\\": \\\"$name_with_path\\\", \\\"url\\\": \\\"$signed_url\\\"}\"\n        fi\n    fi\n}\n\n# Export function and variables for xargs\nexport -f process_file\nexport SERVICE_ACCOUNT\nexport EXPIRATION\n\necho \"Listing files from $GCS_PATH...\"\n\n# Get list of all files, filter for images, and process in parallel\ngsutil ls -r \"$GCS_PATH\" 2&gt;/dev/null | \\\n    grep -v '/$' | \\\n    grep -v ':$' | \\\n    grep -iE \"$IMAGE_PATTERN\" | \\\n    xargs -I {} -P \"$PARALLEL_JOBS\" bash -c 'process_file \"$@\"' _ {} \"$SERVICE_ACCOUNT\" \"$EXPIRATION\" | \\\n    tee \"$OUTPUT_FILE\"\n\necho \"\"\necho \"Done! Signed URLs written to $OUTPUT_FILE\"\necho \"Total images processed: $(wc -l &lt; \"$OUTPUT_FILE\")\"\n</code></pre>"},{"location":"workflows/batch_processing/integration/#notifications","title":"Notifications","text":"<p>Notifications are delivered to clients via HTTP POST requests sent to the specified webhook endpoint. Each notification  will include an Authorization header containing the Roboflow Publishable Key to authenticate the request.</p>"},{"location":"workflows/batch_processing/integration/#ingest-status-notifications","title":"<code>ingest-status</code> notifications","text":"<p>Currently, the ingest-status category includes a single notification type, with the potential for more to be added in  the future. This notification provides updates about the overall status of the data ingestion process.</p> <p>Each ingest-status notification will contain details about the progress or completion of the data ingestion,  allowing clients to track the status in real-time.</p> <pre><code>{\n    \"type\": \"roboflow-data-staging-notification-v1\",\n    \"event_id\": \"8c20f970-fe10-41e1-9ef2-e057c63c07ff\",\n    \"ingest_id\": \"8cd48813430f2be70b492db67e07cc86\",\n    \"batch_id\": \"test-batch-117\",\n    \"shard_id\": null,\n    \"notification\": {\n        \"type\": \"ingest-status-notification-v1\",\n        \"success\": false,\n        \"error_details\": {\n            \"type\": \"unsafe-url-detected\",\n            \"reason\": \"Untrusted domain found: https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\"\n        }\n    },\n    \"delivery_attempt\": 1\n}\n</code></pre>"},{"location":"workflows/batch_processing/integration/#files-status-notifications","title":"<code>files-status</code> notifications","text":"<p>In addition to the overall ingestion status, the system also allows clients to track the status of individual files  ingested into batches. This provides granular insights into the progress of each file during the ingestion process.</p> <p>Since multiple files can be ingested in a batch, notifications for each file are sent in bulk. However, the volume of  notifications may become large depending on the number of files involved in the batch.</p> <p>Despite this, the system optimizes the delivery of notifications, ensuring that you can stay informed about each file's  status without overwhelming your system.</p> <pre><code>{\n    \"type\": \"roboflow-data-staging-notification-v1\",\n    \"event_id\": \"8f42708b-aeb7-4b73-9d83-cf18518b6d81\",\n    \"ingest_id\": \"d5cb69aa-b2d1-4202-a1c1-0231f180bda9\",\n    \"batch_id\": \"prod-batch-1\",\n    \"shard_id\": \"0d40fa12-349e-439f-83f8-42b9b7987b33\",\n    \"notification\": {\n        \"type\": \"ingest-files-status-notification-v1\",\n        \"success\": true,\n        \"ingested_files\": [\n            \"000000494869.jpg\",\n            \"000000186042.jpg\"\n        ],\n        \"failed_files\": [\n            {\n                \"type\": \"file-size-limit-exceeded\",\n                \"file_name\": \"1d3717ec-6e11-4fd5-a91d-7e1eda235aa2-big_image.png\",\n                \"reason\": \"Max size of single image is 20971520B.\"\n            }\n        ],\n        \"content_truncated\": false\n    },\n    \"delivery_attempt\": 1\n}\n</code></pre>"},{"location":"workflows/batch_processing/integration/#job-kick-off_1","title":"Job kick off","text":"<p>By using the ingestion method outlined above, your system will be notified when the data is ready to be processed, allowing you to react to this event and trigger the batch processing job automatically.</p> <p>Batch jobs also support webhook notifications. Once the processing of the batch is complete, your system will be notified that the results are available. This notification enables you to pull the results and continue with any post-processing steps without manual intervention.</p> <p>By leveraging webhook notifications for both the ingestion and job processing stages, you can automate the entire pipeline, ensuring a smooth, hands-off experience.</p> <pre><code>inference rf-cloud batch-processing process-images-with-workflow --workflow-id &lt;workflow-id&gt; --batch-id &lt;batch-id&gt; --notifications-url &lt;webhook_url&gt;\n</code></pre>"},{"location":"workflows/batch_processing/integration/#format-of-notification","title":"Format of notification","text":"<pre><code>{\n  \"type\": \"roboflow-batch-job-notification-v1\",\n  \"event_id\": \"8f42708b-aeb7-4b73-9d83-cf18518b6d81\",\n  \"job_id\": \"&lt;your-batch-job-id&gt;\",\n  \"job_state\": \"success | fail\",\n  \"delivery_attempt\": 1\n}\n</code></pre>"},{"location":"workflows/batch_processing/integration/#data-export_1","title":"Data export","text":"<p>Once the batch job is completed, your system will be notified via the configured webhook. In response to this event, you can automatically trigger the pulling of results and initiate any further processing required on your end.</p> <p>This allows for a seamless, end-to-end automation, reducing manual intervention and ensuring that results are quickly integrated into your workflow once the job finishes.</p>"},{"location":"workflows/batch_processing/known_issues/","title":"Known issues with Roboflow Batch Processing","text":"<p>This page lists known issues, limitations, and workarounds for the Roboflow Batch Processing feature.  Our team actively monitors and maintains Batch Processing to ensure performance, reliability, and scalability. However,  as with any evolving system, certain edge cases or temporary bugs may affect functionality.</p> <p>If you encounter a problem not listed here, please report it through our  support channels, so we can investigate and improve your experience.</p>"},{"location":"workflows/batch_processing/known_issues/#job-timed-out","title":"Job timed out","text":""},{"location":"workflows/batch_processing/known_issues/#issue","title":"Issue","text":"<p>Batch jobs may terminate prematurely or behave unexpectedly if the Processing Timeout Hours value is  set too low relative to the job\u2019s size or complexity.</p>"},{"location":"workflows/batch_processing/known_issues/#details","title":"Details","text":"<p>The Processing Timeout Hours setting in the UI (or <code>--max-runtime-seconds</code> flag in CLI commands) defines the maximum  cumulative machine runtime across all parallel workers. This affects both image and video jobs triggered via:</p> <ul> <li> <p>Roboflow App UI</p> </li> <li> <p><code>inference rf-cloud batch-processing process-images-with-workflow</code> command</p> </li> <li> <p><code>inference rf-cloud batch-processing process-videos-with-workflow</code> command</p> </li> </ul>"},{"location":"workflows/batch_processing/known_issues/#important-notes","title":"Important notes","text":"<ul> <li> <p>Total compute time: The timeout represents total runtime across all machines. For example, if the limit is  2 hours and the job spawns 2 machines, they can each run for a maximum of 1 hour before termination  (2 machines \u00d7 1 hour = 2 hours total).</p> </li> <li> <p>Divided per chunk: Jobs are broken into processing chunks\u2014such as image shards or individual video  files\u2014to enable parallelism (similar to a map-reduce pattern). The specified timeout is divided evenly across these  chunks. If you set a short timeout and have many chunks, each may be given too little time to complete. </p> </li> <li> <p>Impact of machine type: Running complex Workflows (e.g., those with multiple deep learning models) on CPU  increases processing time. Use GPU where appropriate to avoid hitting the timeout.</p> </li> </ul>"},{"location":"workflows/batch_processing/known_issues/#recommendations","title":"Recommendations","text":"<ul> <li> <p>For large datasets or multi-stage Workflows, start with a generous timeout (e.g., 4\u20136 hours).</p> </li> <li> <p>Monitor actual job runtimes to inform future timeout settings.</p> </li> <li> <p>Consider reducing the number of chunks or using video frame sub-sampling for faster processing.</p> </li> <li> <p>We are actively working on smarter timeout defaults, better runtime estimates, and improved chunk scheduling to  make this more predictable.</p> </li> </ul>"},{"location":"workflows/batch_processing/known_issues/#workflow-with-sahi-runs-too-long","title":"Workflow with SAHI runs too long","text":""},{"location":"workflows/batch_processing/known_issues/#issue_1","title":"Issue","text":"<p>Some jobs \u2014 particularly those using SAHI for image or video processing\u2014may take significantly longer than expected  or hit timeout limits, especially when paired with large input resolutions and instance segmentation.</p>"},{"location":"workflows/batch_processing/known_issues/#root-causes-and-recommendations","title":"Root Causes and Recommendations","text":""},{"location":"workflows/batch_processing/known_issues/#excessive-number-of-slices","title":"Excessive number of slices","text":"<p>SAHI splits large images or video frames into smaller slices to run detection more accurately.  However, with default settings and high-resolution inputs, this can result in dozens or even hundreds of inferences  per image/frame.</p> <p>Recommendation: Check the configuration of the Image Slicer block. You can reduce the number of slices or downscale  input images before slicing using a Resize Image block earlier in the Workflow.</p>"},{"location":"workflows/batch_processing/known_issues/#try-larger-model-input-size-instead-of-sahi","title":"Try larger model input size instead of SAHI","text":"<p>In many cases, training a model that supports larger input dimensions can eliminate the need for SAHI entirely.  While such a model may be larger and require GPU usage, it can be more efficient than hundreds of smaller  inferences per image.</p> <p>Recommendation: This approach requires a solid dataset and should be tested on a small sample before fully  committing. It may not be practical if you're relying only on pre-trained models.</p>"},{"location":"workflows/batch_processing/known_issues/#instance-segmentation-bottleneck","title":"Instance Segmentation bottleneck","text":"<p>When SAHI is used with instance segmentation, the Detections Stitch block (especially with  Non-Maximum Suppression enabled) can become a major bottleneck.</p> <p>Recommendation: In extreme cases, stitching results for a single frame can take tens of seconds.  It is recommended for now to verify if SAHI is feasible for your instance segmentation use-case before fully committing  to the solution</p>"},{"location":"workflows/batch_processing/known_issues/#video-jobs-with-sahi","title":"Video jobs with SAHI","text":"<p>Process fewer frames using FPS sub-sampling:</p> <ul> <li> <p>It\u2019s often unnecessary to run inference on every frame.</p> </li> <li> <p>In the UI, use the Video FPS sub-sampling dropdown to skip frames.</p> </li> <li> <p>In the CLI, set the --max-video-fps flag with <code>inference rf-cloud batch-processing process-videos-with-workflow</code> command</p> </li> </ul> <p>This can significantly reduce job time and cost, especially for long or high-FPS videos.</p>"},{"location":"workflows/batch_processing/known_issues/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":""},{"location":"workflows/batch_processing/known_issues/#issue_2","title":"Issue","text":"<p>Jobs may fail due to Out of Memory (OOM) errors when the Workflow consumes more RAM or VRAM than the allocated machine  can provide. These errors most commonly occur due to excessive memory pressure on RAM, though GPU VRAM issues are  also possible when using large models.</p>"},{"location":"workflows/batch_processing/known_issues/#common-causes","title":"Common Causes","text":"<ul> <li> <p>SAHI + Instance Segmentation: This combination is known to be extremely memory-intensive. SAHI multiplies  inference calls per input, and instance segmentation generates large outputs (e.g., masks and scores). This often  leads to unstable behavior or crashes, especially with Detections Stitch and NMS.</p> </li> <li> <p>Too Many Workers Per Machine: The system allows you to parallelize jobs by assigning multiple workers per machine.  This can optimize cost and speed\u2014if your Workflow is lightweight enough. However, bulky Workflows  (e.g., those with multiple large models or complex post-processing) will exceed available memory when run with too  many workers on a single node.</p> </li> </ul>"},{"location":"workflows/batch_processing/known_issues/#recommendations_1","title":"Recommendations","text":"<p>Use fewer workers per machine (e.g., 1 or 2) when your Workflow includes:</p> <ul> <li> <p>Large models</p> </li> <li> <p>SAHI (especially with instance segmentation)</p> </li> <li> <p>High-resolution input images</p> </li> </ul> <p>If encountering OOM errors:</p> <ul> <li> <p>Lower the Workers Per Machine value under Advanced Options.</p> </li> <li> <p>Switch from CPU to GPU if your model or inference block requires higher memory throughput.</p> </li> <li> <p>Test your Workflow on a small dataset before running large batches.</p> </li> <li> <p>Reduce input resolution or simplify your Workflow by removing unneeded blocks.</p> </li> </ul>"},{"location":"workflows/blocks/","title":"Index","text":"Workflow Blocks Models   Object Detection Model Predict the location of objects with bounding boxes.   Instance Segmentation Model Predict the shape, size, and location of objects.   Single-Label Classification Model Apply a single tag to an image.   Multi-Label Classification Model Apply multiple tags to an image.   Keypoint Detection Model Predict skeletons on objects.   Anthropic Claude Run Anthropic Claude model with vision capabilities.   Google Gemini Run Google's Gemini model with vision capabilities.   OpenAI Run OpenAI's GPT models with vision capabilities.   Depth Estimation Run Depth Estimation on an image.   Florence-2 Model Run Florence-2 on an image   Qwen2.5-VL Run Qwen2.5-VL on an image.   SmolVLM2 Run SmolVLM2 on an image.   Moondream2 Run Moondream2 on an image.   YOLO-World Model Run a zero-shot object detection model.   CogVLM DEPRECATED! Run a self-hosted vision language model.   Seg Preview Seg Preview   Segment Anything 2 Model Convert bounding boxes to polygons, or run SAM2 on an entire image to generate a mask.   CLIP Embedding Model Generate an embedding of an image or string.   Perception Encoder Embedding Model Generate an embedding of an image or string.   Clip Comparison Compare CLIP image and text embeddings.   OCR Model Extract text from an image using DocTR optical character recognition.   EasyOCR Extract text from an image using EasyOCR optical character recognition.   Barcode Detection Detect and read barcodes in an image.   QR Code Detection Detect and read QR codes in an image.   Gaze Detection Detect faces and estimate gaze direction   Stability AI Inpainting Use segmentation masks to inpaint objects within an image.   Stability AI Outpainting Use object detection bounding box to crop the image and to outpaint within given directions.   Google Vision OCR Detect text in images using Google Vision API   LMM Run a large multimodal model such as ChatGPT-4v.   LMM For Classification Run a large multimodal model such as ChatGPT-4v for classification.   Stability AI Image Generation generate new images from text, or create variations of existing images.   Llama 3.2 Vision Run Llama model with Vision capabilities Visualizations   Bounding Box Visualization Draw a box around detected objects in an image.   Dot Visualization Draw dots on an image at specific coordinates based on provided detections.   Polygon Visualization Draw a polygon around detected objects in an image.   Label Visualization Draw labels on an image at specific coordinates based on provided detections.   Classification Label Visualization Visualize both single-label and multi-label classification predictions with customizable display options.   Background Color Visualization Apply a mask to cover all areas outside the detected regions in an image.   Blur Visualization Blur detected objects in an image.   Circle Visualization Draw a circle around detected objects in an image.   Icon Visualization Draw icons on an image either at specific static coordinates or dynamically based on detections.   Color Visualization Paint a solid color on detected objects in an image.   Corner Visualization Draw the corners of detected objects in an image.   Crop Visualization Draw scaled up crops of detections on the scene.   Ellipse Visualization Draw ellipses that highlight detected objects in an image.   Halo Visualization Paint a halo around detected objects in an image.   Mask Visualization Apply a mask over detected objects in an image.   Pixelate Visualization Pixelate detected objects in an image.   Triangle Visualization Draw triangle markers on an image at specific coordinates based on provided detections.   Line Counter Visualization Apply a mask over a line zone in an image.   Polygon Zone Visualization Apply a mask over a polygon zone in an image.   Model Comparison Visualization Visualize the difference between two models' detections.   Trace Visualization Draw traces based on detections tracking results.   Reference Path Visualization Draw a reference path in the image.   Keypoint Visualization Draw keypoints on detected objects in an image.   Grid Visualization Shows an array of images in a grid. Logic and branching   Continue If Conditionally stop execution of a branch.   Detections Filter Conditionally filter out model predictions.   Overlap Filter Filter objects overlapping some other class   Rate Limiter Limits the rate at which a branch of the Workflow will run.   Delta Filter Allow the execution of workflow to proceed if the input value has changed.   Detections Consensus Combine predictions from multiple detections models to make a decision about object presence. Data storage   Roboflow Dataset Upload Save images and predictions to your Roboflow Dataset.   Webhook Sink Send a request to a remote API with Workflow results.   CSV Formatter Create CSV files with specified columns.   Local File Sink Save data to a local file.   Data Aggregator Aggregate workflow data to produce time-based statistics. Notifications   Email Notification Send notification via e-mail.   Slack Notification Send notification via Slack.   Twilio SMS Notification Send notification via Twilio SMS service. Transformations   Dynamic Crop Crop an image using bounding boxes from a detection model.   Absolute Static Crop Crop an image using fixed pixel coordinates.   Camera Calibration Remove camera lens distortions from an image using a calibration table.   QR Code Generator Generate a QR code image from text input.   Relative Static Crop Crop an image proportional (%) to its dimensions.   Detection Offset Apply a padding around the width and height of detections.   Detections Transformation Apply transformations on detected bounding boxes.   Bounding Rectangle Find the minimal bounding box surrounding the detected polygon.   Detections Merge Merge multiple detections into a single bounding box.   Detections Combine Combines two sets of predictions into a single prediction.   Stitch Images Stitch two images by common parts. Classical computer vision   Image Preprocessing Resize, flip, or rotate an image.   Template Matching Locate instances of a given template within a specified image.   Dominant Color Get the dominant color of an image in RGB format.   Pixel Color Count Count the number of pixels that match a specific color within a given tolerance.   SIFT Comparison Compare SIFT descriptors from multiple images.   Image Contours Find and count the contours on an image.   SIFT Apply SIFT to an image.   Contrast Equalization Apply contrast equalization to an image.   Image Blur Apply a blur to an image.   Morphological Transformation Apply morphological transformation to an image.   Image Threshold Apply a threshold to an image.   Image Convert Grayscale Convert an RGB image to grayscale.   Camera Focus Calculate a score to indicate how well-focused a camera is.   Size Measurement Measure the dimensions of objects in relation to a reference object.   Distance Measurement Calculate the distance between two bounding boxes on a 2D plane. Video   Byte Tracker Track and update object positions across video frames using ByteTrack.   Time in Zone Track object time in zone.   PTZ Tracking (ONVIF) Control an ONVIF compatible PTZ camera to follow an object   Line Counter Count detections passing a line.   Velocity Calculate the velocity and speed of tracked objects with smoothing and unit conversion.   Path Deviation Calculate Fr\u00e9chet distance of object from the reference path.   Detections Stabilizer Apply a smoothing algorithm to reduce noise and flickering across video frames.   Buffer Returns an array of the last `length` values passed to it.   Identify Outliers Identify outlier embeddings compared to prior data.   Identify Changes Identify changes compared to prior data via embeddings. Advanced   Property Definition Define a variable from model predictions, such as the class names, confidences, or number of detections.   Expression Create a specific output based on defined input variables and configured rules.   Perspective Correction Adjust detection coordinates from a polygon-defined plane to a straight rectangular plane with specified width and height.   Stitch OCR Detections Combines OCR detection results into a coherent text string by organizing detections spatially.   Dynamic Zone Simplify polygons so they are geometrically convex and contain only the requested amount of vertices.   Cosine Similarity Calculate the cosine similarity between two embeddings.   Detections Classes Replacement Replace classes of detections with classes predicted by a chained classification model.   JSON Parser Parse raw string into JSON.   VLM as Classifier Parse a raw string into a classification prediction.   VLM as Detector Parses raw string into object-detection prediction.   Dimension Collapse Collapse dimensionality by aggregating nested data into a single list.   First Non Empty Or Default Take the first non-empty data element or the configured default value.   Roboflow Custom Metadata Add custom metadata to the Roboflow Model Monitoring dashboard.   Model Monitoring Inference Aggregator Periodically report an aggregated sample of inference results to Roboflow Model Monitoring.   Image Slicer Tile the input image into a list of smaller images to perform small object detection.   Detections Stitch Merges detections made against multiple pieces of input image into single detection.   Cache Get Fetches a previously stored value from a cache entry.   Cache Set Stores a value in a cache entry for later retrieval.   Environment Secrets Store Fetch secrets from environmental variables."},{"location":"workflows/blocks/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Models<ul> <li>Object Detection Model</li> <li>Instance Segmentation Model</li> <li>Single-Label Classification Model</li> <li>Multi-Label Classification Model</li> <li>Keypoint Detection Model</li> <li>Anthropic Claude</li> <li>Google Gemini</li> <li>OpenAI</li> <li>Depth Estimation</li> <li>Florence-2 Model</li> <li>Qwen2.5-VL</li> <li>SmolVLM2</li> <li>Moondream2</li> <li>YOLO-World Model</li> <li>CogVLM</li> <li>Seg Preview</li> <li>Segment Anything 2 Model</li> <li>CLIP Embedding Model</li> <li>Perception Encoder Embedding Model</li> <li>Clip Comparison</li> <li>OCR Model</li> <li>EasyOCR</li> <li>Barcode Detection</li> <li>QR Code Detection</li> <li>Gaze Detection</li> <li>Stability AI Inpainting</li> <li>Stability AI Outpainting</li> <li>Google Vision OCR</li> <li>LMM</li> <li>LMM For Classification</li> <li>Stability AI Image Generation</li> <li>Llama 3.2 Vision</li> </ul> </li> <li>Visualizations<ul> <li>Bounding Box Visualization</li> <li>Dot Visualization</li> <li>Polygon Visualization</li> <li>Label Visualization</li> <li>Classification Label Visualization</li> <li>Background Color Visualization</li> <li>Blur Visualization</li> <li>Circle Visualization</li> <li>Icon Visualization</li> <li>Color Visualization</li> <li>Corner Visualization</li> <li>Crop Visualization</li> <li>Ellipse Visualization</li> <li>Halo Visualization</li> <li>Mask Visualization</li> <li>Pixelate Visualization</li> <li>Triangle Visualization</li> <li>Line Counter Visualization</li> <li>Polygon Zone Visualization</li> <li>Model Comparison Visualization</li> <li>Trace Visualization</li> <li>Reference Path Visualization</li> <li>Keypoint Visualization</li> <li>Grid Visualization</li> </ul> </li> <li>Logic and Branching<ul> <li>Continue If</li> <li>Detections Filter</li> <li>Overlap Filter</li> <li>Rate Limiter</li> <li>Delta Filter</li> <li>Detections Consensus</li> </ul> </li> <li>Data Storage<ul> <li>Roboflow Dataset Upload</li> <li>Webhook Sink</li> <li>CSV Formatter</li> <li>Local File Sink</li> <li>Data Aggregator</li> </ul> </li> <li>Notifications<ul> <li>Email Notification</li> <li>Slack Notification</li> <li>Twilio SMS Notification</li> </ul> </li> <li>Transformations<ul> <li>Dynamic Crop</li> <li>Absolute Static Crop</li> <li>Camera Calibration</li> <li>QR Code Generator</li> <li>Relative Static Crop</li> <li>Detection Offset</li> <li>Detections Transformation</li> <li>Bounding Rectangle</li> <li>Detections Merge</li> <li>Detections Combine</li> <li>Stitch Images</li> </ul> </li> <li>Classical Computer Vision<ul> <li>Image Preprocessing</li> <li>Template Matching</li> <li>Dominant Color</li> <li>Pixel Color Count</li> <li>SIFT Comparison</li> <li>Image Contours</li> <li>SIFT</li> <li>Contrast Equalization</li> <li>Image Blur</li> <li>Morphological Transformation</li> <li>Image Threshold</li> <li>Image Convert Grayscale</li> <li>Camera Focus</li> <li>Size Measurement</li> <li>Distance Measurement</li> </ul> </li> <li>Video<ul> <li>Byte Tracker</li> <li>Time in Zone</li> <li>PTZ Tracking (ONVIF).md)</li> <li>Line Counter</li> <li>Velocity</li> <li>Path Deviation</li> <li>Detections Stabilizer</li> <li>Buffer</li> <li>Identify Outliers</li> <li>Identify Changes</li> </ul> </li> <li>Advanced<ul> <li>Property Definition</li> <li>Expression</li> <li>Perspective Correction</li> <li>Stitch OCR Detections</li> <li>Dynamic Zone</li> <li>Cosine Similarity</li> <li>Detections Classes Replacement</li> <li>JSON Parser</li> <li>VLM as Classifier</li> <li>VLM as Detector</li> <li>Dimension Collapse</li> <li>First Non Empty Or Default</li> <li>Roboflow Custom Metadata</li> <li>Model Monitoring Inference Aggregator</li> <li>Image Slicer</li> <li>Detections Stitch</li> <li>Cache Get</li> <li>Cache Set</li> <li>Environment Secrets Store</li> </ul> </li> </ul>"},{"location":"workflows/blocks/absolute_static_crop/","title":"Absolute Static Crop","text":"Class: <code>AbsoluteStaticCropBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.absolute_static_crop.v1.AbsoluteStaticCropBlockV1</p> <p>Crop a Region of Interest (RoI) from an image, using absolute coordinates.</p> <p>This is useful when placed after an ObjectDetection block as part of a multi-stage  workflow. For example, you could use an ObjectDetection block to detect objects, then  the AbsoluteStaticCrop block to crop objects, then an OCR block to run character  recognition on each of the individual cropped regions.</p>"},{"location":"workflows/blocks/absolute_static_crop/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/absolute_static_crop@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/absolute_static_crop/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>x_center</code> <code>int</code> Center X of static crop (absolute coordinate). \u2705 <code>y_center</code> <code>int</code> Center Y of static crop (absolute coordinate). \u2705 <code>width</code> <code>int</code> Width of static crop (absolute value). \u2705 <code>height</code> <code>int</code> Height of static crop (absolute value). \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/absolute_static_crop/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Absolute Static Crop</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>x_center</code> (<code>integer</code>): Center X of static crop (absolute coordinate).</li> <li><code>y_center</code> (<code>integer</code>): Center Y of static crop (absolute coordinate).</li> <li><code>width</code> (<code>integer</code>): Width of static crop (absolute value).</li> <li><code>height</code> (<code>integer</code>): Height of static crop (absolute value).</li> </ul> </li> <li> <p>output</p> <ul> <li><code>crops</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Absolute Static Crop</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/absolute_static_crop@v1\",\n    \"images\": \"$inputs.image\",\n    \"x_center\": 40,\n    \"y_center\": 40,\n    \"width\": 40,\n    \"height\": 40\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/anthropic_claude/","title":"Anthropic Claude","text":"Class: <code>AnthropicClaudeBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.anthropic_claude.v1.AnthropicClaudeBlockV1</p> <p>Ask a question to Anthropic Claude model with vision capabilities.</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>You need to provide your Anthropic API key to use the Claude model. </p>"},{"location":"workflows/blocks/anthropic_claude/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/anthropic_claude@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/anthropic_claude/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Claude model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your Anthropic API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_image_size</code> <code>int</code> Maximum size of the image - if input has larger side, it will be downscaled, keeping aspect ratio. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit Anthropic API limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/anthropic_claude/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Anthropic Claude</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Claude model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your Anthropic API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> <li><code>max_image_size</code> (<code>integer</code>): Maximum size of the image - if input has larger side, it will be downscaled, keeping aspect ratio.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Anthropic Claude</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/anthropic_claude@v1\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"claude-sonnet-4\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_image_size\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/background_color_visualization/","title":"Background Color Visualization","text":"Class: <code>BackgroundColorVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.background_color.v1.BackgroundColorVisualizationBlockV1</p> <p>The <code>BackgroundColorVisualization</code> block draws all areas outside of detected regions in an image with a specified color.</p>"},{"location":"workflows/blocks/background_color_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/background_color_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/background_color_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color</code> <code>str</code> Color of the background.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/background_color_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Background Color Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color</code> (<code>string</code>): Color of the background..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Background Color Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/background_color_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color\": \"WHITE\",\n    \"opacity\": 0.5\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/barcode_detection/","title":"Barcode Detection","text":"Class: <code>BarcodeDetectorBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.third_party.barcode_detection.v1.BarcodeDetectorBlockV1</p> <p>Detect the location of barcodes in an image.</p> <p>This block is useful for manufacturing and consumer packaged goods projects where you  need to detect a barcode region in an image. You can then apply Crop block to isolate  each barcode then apply further processing (i.e. OCR of the characters on a barcode).</p>"},{"location":"workflows/blocks/barcode_detection/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/barcode_detector@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/barcode_detection/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/barcode_detection/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Barcode Detection</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>bar_code_detection</code>): Prediction with barcode detection.</li> </ul> </li> </ul> Example JSON definition of step <code>Barcode Detection</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/barcode_detector@v1\",\n    \"images\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/blur_visualization/","title":"Blur Visualization","text":"Class: <code>BlurVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.blur.v1.BlurVisualizationBlockV1</p> <p>The <code>BlurVisualization</code> block blurs detected objects in an image using Supervision's <code>sv.BlurAnnotator</code>.</p>"},{"location":"workflows/blocks/blur_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/blur_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/blur_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>kernel_size</code> <code>int</code> Size of the average pooling kernel used for blurring.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/blur_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Blur Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the average pooling kernel used for blurring..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Blur Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/blur_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"kernel_size\": 15\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/bounding_box_visualization/","title":"Bounding Box Visualization","text":"Class: <code>BoundingBoxVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.bounding_box.v1.BoundingBoxVisualizationBlockV1</p> <p>The <code>BoundingBoxVisualization</code> block draws a box around detected objects in an image using Supervision's <code>sv.RoundBoxAnnotator</code>.</p>"},{"location":"workflows/blocks/bounding_box_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/bounding_box_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/bounding_box_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>thickness</code> <code>int</code> Set the thickness of the bounding box edges.. \u2705 <code>roundness</code> <code>float</code> Define the roundness of the bounding box corners.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/bounding_box_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Bounding Box Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>thickness</code> (<code>integer</code>): Set the thickness of the bounding box edges..</li> <li><code>roundness</code> (<code>float_zero_to_one</code>): Define the roundness of the bounding box corners..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Bounding Box Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2,\n    \"roundness\": 0.0\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/bounding_rectangle/","title":"Bounding Rectangle","text":"Class: <code>BoundingRectBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.bounding_rect.v1.BoundingRectBlockV1</p> <p>The <code>BoundingRect</code> is a transformer block designed to simplify polygon to the minimum boundig rectangle. This block is best suited when Zone needs to be created based on shape of detected object (i.e. basketball field, road segment, zebra crossing etc.) Input detections should be filtered beforehand and contain only desired classes of interest. Resulsts are stored in sv.Detections.data</p>"},{"location":"workflows/blocks/bounding_rectangle/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/bounding_rect@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/bounding_rectangle/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/bounding_rectangle/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Bounding Rectangle</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): .</li> </ul> </li> <li> <p>output</p> <ul> <li><code>detections_with_rect</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Bounding Rectangle</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/bounding_rect@v1\",\n    \"predictions\": \"$segmentation.predictions\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/buffer/","title":"Buffer","text":"Class: <code>BufferBlockV1</code> <p>Source: inference.core.workflows.core_steps.fusion.buffer.v1.BufferBlockV1</p> <p>Returns an array of the last <code>length</code> values passed to it. The newest elements are added to the beginning of the array.</p> <p>Useful for keeping a sliding window of images or detections for later processing, visualization, or comparison.</p>"},{"location":"workflows/blocks/buffer/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/buffer@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/buffer/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>length</code> <code>int</code> The number of elements to keep in the buffer. Older elements will be removed.. \u274c <code>pad</code> <code>bool</code> If True, the end of the buffer will be padded with <code>None</code> values so its size is always exactly <code>length</code>.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/buffer/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Buffer</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (Union[<code>image</code>, <code>*</code>, <code>list_of_values</code>]): Reference to step outputs at depth level n to be concatenated and moved into level n-1..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Buffer</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/buffer@v1\",\n    \"data\": \"$steps.visualization\",\n    \"length\": 5,\n    \"pad\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/byte_tracker/","title":"Byte Tracker","text":""},{"location":"workflows/blocks/byte_tracker/#v3","title":"v3","text":"Class: <code>ByteTrackerBlockV3</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.transformations.byte_tracker.v3.ByteTrackerBlockV3</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>ByteTrackerBlock</code> integrates ByteTrack, an advanced object tracking algorithm,  to manage object tracking across sequential video frames within workflows.</p> <p>This block accepts detections and their corresponding video frames as input,  initializing trackers for each detection based on configurable parameters like track  activation threshold, lost track buffer, minimum matching threshold, and frame rate.  These parameters allow fine-tuning of the tracking process to suit specific accuracy  and performance needs.</p> <p>New outputs introduced in <code>v3</code></p> <p>The block has not changed compared to <code>v2</code> apart from the fact that there are two  new outputs added:</p> <ul> <li> <p><code>new_instances</code>: delivers sv.Detections objects with bounding boxes that have  tracker IDs which were first seen - specific tracked instance will only be listed in that output once - when new tracker ID is generated </p> </li> <li> <p><code>already_seen_instances</code>: delivers sv.Detections objects with bounding boxes that have  tracker IDs which were already seen - specific tracked instance will only be listed in that output each time the tracker associates the bounding box with already seen tracker ID </p> </li> </ul>"},{"location":"workflows/blocks/byte_tracker/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/byte_tracker@v3</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/byte_tracker/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>track_activation_threshold</code> <code>float</code> Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.. \u2705 <code>lost_track_buffer</code> <code>int</code> Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.. \u2705 <code>minimum_matching_threshold</code> <code>float</code> Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.. \u2705 <code>minimum_consecutive_frames</code> <code>int</code> Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.. \u2705 <code>instances_cache_size</code> <code>int</code> Size of the instances cache to decide if specific tracked instance is new or already seen. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/byte_tracker/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Byte Tracker</code> in version <code>v3</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Objects to be tracked..</li> <li><code>track_activation_threshold</code> (<code>float_zero_to_one</code>): Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability..</li> <li><code>lost_track_buffer</code> (<code>integer</code>): Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps..</li> <li><code>minimum_matching_threshold</code> (<code>float_zero_to_one</code>): Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift..</li> <li><code>minimum_consecutive_frames</code> (<code>integer</code>): Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>new_instances</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>already_seen_instances</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Byte Tracker</code> in version <code>v3</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/byte_tracker@v3\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"track_activation_threshold\": 0.25,\n    \"lost_track_buffer\": 30,\n    \"minimum_matching_threshold\": 0.8,\n    \"minimum_consecutive_frames\": 1,\n    \"instances_cache_size\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/byte_tracker/#v2","title":"v2","text":"Class: <code>ByteTrackerBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.transformations.byte_tracker.v2.ByteTrackerBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>ByteTrackerBlock</code> integrates ByteTrack, an advanced object tracking algorithm,  to manage object tracking across sequential video frames within workflows.</p> <p>This block accepts detections and their corresponding video frames as input,  initializing trackers for each detection based on configurable parameters like track  activation threshold, lost track buffer, minimum matching threshold, and frame rate.  These parameters allow fine-tuning of the tracking process to suit specific accuracy  and performance needs.</p>"},{"location":"workflows/blocks/byte_tracker/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/byte_tracker@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/byte_tracker/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>track_activation_threshold</code> <code>float</code> Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.. \u2705 <code>lost_track_buffer</code> <code>int</code> Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.. \u2705 <code>minimum_matching_threshold</code> <code>float</code> Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.. \u2705 <code>minimum_consecutive_frames</code> <code>int</code> Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/byte_tracker/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Byte Tracker</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Objects to be tracked..</li> <li><code>track_activation_threshold</code> (<code>float_zero_to_one</code>): Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability..</li> <li><code>lost_track_buffer</code> (<code>integer</code>): Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps..</li> <li><code>minimum_matching_threshold</code> (<code>float_zero_to_one</code>): Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift..</li> <li><code>minimum_consecutive_frames</code> (<code>integer</code>): Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Byte Tracker</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/byte_tracker@v2\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"track_activation_threshold\": 0.25,\n    \"lost_track_buffer\": 30,\n    \"minimum_matching_threshold\": 0.8,\n    \"minimum_consecutive_frames\": 1\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/byte_tracker/#v1","title":"v1","text":"Class: <code>ByteTrackerBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.transformations.byte_tracker.v1.ByteTrackerBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>ByteTrackerBlock</code> integrates ByteTrack, an advanced object tracking algorithm,  to manage object tracking across sequential video frames within workflows.</p> <p>This block accepts detections and their corresponding video frames as input,  initializing trackers for each detection based on configurable parameters like track  activation threshold, lost track buffer, minimum matching threshold, and frame rate.  These parameters allow fine-tuning of the tracking process to suit specific accuracy  and performance needs.</p>"},{"location":"workflows/blocks/byte_tracker/#type-identifier_2","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/byte_tracker@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/byte_tracker/#properties_2","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>track_activation_threshold</code> <code>float</code> Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.. \u2705 <code>lost_track_buffer</code> <code>int</code> Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.. \u2705 <code>minimum_matching_threshold</code> <code>float</code> Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.. \u2705 <code>minimum_consecutive_frames</code> <code>int</code> Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/byte_tracker/#input-and-output-bindings_2","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Byte Tracker</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Objects to be tracked..</li> <li><code>track_activation_threshold</code> (<code>float_zero_to_one</code>): Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability..</li> <li><code>lost_track_buffer</code> (<code>integer</code>): Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps..</li> <li><code>minimum_matching_threshold</code> (<code>float_zero_to_one</code>): Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift..</li> <li><code>minimum_consecutive_frames</code> (<code>integer</code>): Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Byte Tracker</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/byte_tracker@v1\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"track_activation_threshold\": 0.25,\n    \"lost_track_buffer\": 30,\n    \"minimum_matching_threshold\": 0.8,\n    \"minimum_consecutive_frames\": 1\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/cache_get/","title":"Cache Get","text":"Class: <code>CacheGetBlockV1</code> <p>Source: inference.core.workflows.core_steps.cache.cache_get.v1.CacheGetBlockV1</p> <p>Fetches a previously stored value from a cache entry.</p> <p>Use the <code>Cache Set</code> block to store values in the cache.</p>"},{"location":"workflows/blocks/cache_get/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/cache_get@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/cache_get/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>key</code> <code>str</code> The key of the cache entry to fetch.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/cache_get/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Cache Get</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image data to use as a reference for the cache namespace..</li> <li><code>key</code> (<code>string</code>): The key of the cache entry to fetch..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Cache Get</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/cache_get@v1\",\n    \"image\": \"$inputs.image\",\n    \"key\": \"my_cache_key\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/cache_set/","title":"Cache Set","text":"Class: <code>CacheSetBlockV1</code> <p>Source: inference.core.workflows.core_steps.cache.cache_set.v1.CacheSetBlockV1</p> <p>Stores a value in a cache entry for later retrieval.</p> <p>Use the <code>Cache Get</code> block to fetch values from the cache.</p>"},{"location":"workflows/blocks/cache_set/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/cache_set@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/cache_set/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>key</code> <code>str</code> The key of the cache entry to set.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/cache_set/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Cache Set</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image data to use as a reference for the cache namespace..</li> <li><code>key</code> (<code>string</code>): The key of the cache entry to set..</li> <li><code>value</code> (Union[<code>*</code>, <code>list_of_values</code>]): The value to store in the cache..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Cache Set</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/cache_set@v1\",\n    \"image\": \"$inputs.image\",\n    \"key\": \"my_cache_key\",\n    \"value\": \"any_value\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/camera_calibration/","title":"Camera Calibration","text":"Class: <code>CameraCalibrationBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.camera_calibration.v1.CameraCalibrationBlockV1</p> <p>This block uses the OpenCV <code>calibrateCamera</code> function to remove lens distortions from an image. Please refer to OpenCV documentation where camera calibration methodology is described: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html</p> <p>This block requires following parameters in order to perform the calibration: Lens focal length along the x-axis and y-axis (fx, fy) Lens optical centers along the x-axis and y-axis (cx, cy) Radial distortion coefficients (k1, k2, k3) Tangential distortion coefficients (p1, p2)</p> <p>Based on above parameters, camera matrix will be built as follows: [[fx 0  cx][ 0 fy cy] [ 0  0  1 ]]</p> <p>Distortions coefficient will be passed as 5-tuple (k1, k2, p1, p2, k3)</p>"},{"location":"workflows/blocks/camera_calibration/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/camera-calibration@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/camera_calibration/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>fx</code> <code>float</code> Focal length along the x-axis. \u2705 <code>fy</code> <code>float</code> Focal length along the y-axis. \u2705 <code>cx</code> <code>float</code> Optical center along the x-axis. \u2705 <code>cy</code> <code>float</code> Optical center along the y-axis. \u2705 <code>k1</code> <code>float</code> Radial distortion coefficient k1. \u2705 <code>k2</code> <code>float</code> Radial distortion coefficient k2. \u2705 <code>k3</code> <code>float</code> Radial distortion coefficient k3. \u2705 <code>p1</code> <code>float</code> Distortion coefficient p1. \u2705 <code>p2</code> <code>float</code> Distortion coefficient p2. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/camera_calibration/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Camera Calibration</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): Image to remove distortions from.</li> <li><code>fx</code> (<code>float</code>): Focal length along the x-axis.</li> <li><code>fy</code> (<code>float</code>): Focal length along the y-axis.</li> <li><code>cx</code> (<code>float</code>): Optical center along the x-axis.</li> <li><code>cy</code> (<code>float</code>): Optical center along the y-axis.</li> <li><code>k1</code> (<code>float</code>): Radial distortion coefficient k1.</li> <li><code>k2</code> (<code>float</code>): Radial distortion coefficient k2.</li> <li><code>k3</code> (<code>float</code>): Radial distortion coefficient k3.</li> <li><code>p1</code> (<code>float</code>): Distortion coefficient p1.</li> <li><code>p2</code> (<code>float</code>): Distortion coefficient p2.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>calibrated_image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Camera Calibration</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/camera-calibration@v1\",\n    \"image\": \"$inputs.image\",\n    \"fx\": 0.123,\n    \"fy\": 0.123,\n    \"cx\": 0.123,\n    \"cy\": 0.123,\n    \"k1\": 0.123,\n    \"k2\": 0.123,\n    \"k3\": 0.123,\n    \"p1\": 0.123,\n    \"p2\": 0.123\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/camera_focus/","title":"Camera Focus","text":"Class: <code>CameraFocusBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.camera_focus.v1.CameraFocusBlockV1</p> <p>This block calculate the Brenner function score which is a measure of the texture in the image.  An in-focus image has a high Brenner function score, and contains texture at a smaller scale than  an out-of-focus image. Conversely, an out-of-focus image has a low Brenner function score, and   does not contain small-scale texture.</p>"},{"location":"workflows/blocks/camera_focus/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/camera_focus@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/camera_focus/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/camera_focus/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Camera Focus</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>focus_measure</code> (<code>float</code>): Float value.</li> </ul> </li> </ul> Example JSON definition of step <code>Camera Focus</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/camera_focus@v1\",\n    \"image\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/circle_visualization/","title":"Circle Visualization","text":"Class: <code>CircleVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.circle.v1.CircleVisualizationBlockV1</p> <p>The <code>CircleVisualization</code> block draws a circle around detected objects in an image using Supervision's <code>sv.CircleAnnotator</code>.</p>"},{"location":"workflows/blocks/circle_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/circle_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/circle_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/circle_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Circle Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Circle Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/circle_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/classification_label_visualization/","title":"Classification Label Visualization","text":"Class: <code>ClassificationLabelVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.classification_label.v1.ClassificationLabelVisualizationBlockV1</p> <p>Visualizes classification predictions with customizable labels and positioning options.  Perfect for creating clear, informative displays of model predictions!</p>"},{"location":"workflows/blocks/classification_label_visualization/#how-it-works","title":"How It Works","text":"<p>This visualization processes classification predictions by:</p> <ol> <li> <p>\ud83c\udfaf Analyzing predictions based on task type (single-label or multi-label)</p> </li> <li> <p>\ud83d\udcca Organizing results by confidence score</p> </li> <li> <p>\ud83c\udfa8 Rendering labels with customizable positioning and styling</p> </li> </ol>"},{"location":"workflows/blocks/classification_label_visualization/#parameters","title":"Parameters","text":"<ul> <li> <p><code>task_type</code>: Specifies how to handle predictions. Available options:</p> <ul> <li> <p>\"single-label\": Shows only the highest confidence prediction</p> </li> <li> <p>\"multi-label\": Displays all predictions above threshold</p> </li> </ul> </li> <li> <p><code>text_position</code>: Controls label placement with 9 options:</p> <ul> <li>Top: LEFT, CENTER, RIGHT</li> <li>Center: LEFT, CENTER, RIGHT</li> <li>Bottom: LEFT, CENTER, RIGHT</li> </ul> </li> <li> <p><code>text</code>: Determines what information to display:</p> <ul> <li>\"Class\": Only show class names</li> <li>\"Confidence\": Only show confidence scores</li> <li>\"Class and Confidence\": Show both</li> </ul> </li> <li> <p><code>text_padding</code>: Controls spacing between labels and from image edges</p> </li> </ul>"},{"location":"workflows/blocks/classification_label_visualization/#why-use-this-visualization","title":"Why Use This Visualization?","text":"<p>This is especially useful for:</p> <ul> <li> <p>\ud83c\udff7\ufe0f Creating clear, professional-looking prediction displays</p> </li> <li> <p>\ud83d\udcf1 Supporting different UI layouts with flexible positioning</p> </li> <li> <p>\ud83c\udfa8 Customizing appearance for different use cases</p> </li> <li> <p>\ud83d\udcca Showing prediction confidence in an intuitive way</p> </li> </ul>"},{"location":"workflows/blocks/classification_label_visualization/#example-usage","title":"Example Usage","text":"<p>Use this visualization after any classification model to display predictions in a clean,  organized format. Perfect for both single predictions and multiple class probabilities.</p>"},{"location":"workflows/blocks/classification_label_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/classification_label_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/classification_label_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>text</code> <code>str</code> The data to display in the text labels.. \u2705 <code>text_position</code> <code>str</code> The anchor position for placing the label.. \u2705 <code>text_color</code> <code>str</code> Color of the text.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text characters.. \u2705 <code>text_padding</code> <code>int</code> Padding around the text in pixels.. \u2705 <code>border_radius</code> <code>int</code> Radius of the label in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/classification_label_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Classification Label Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (<code>classification_prediction</code>): Classification predictions..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>text</code> (<code>string</code>): The data to display in the text labels..</li> <li><code>text_position</code> (<code>string</code>): The anchor position for placing the label..</li> <li><code>text_color</code> (<code>string</code>): Color of the text..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text characters..</li> <li><code>text_padding</code> (<code>integer</code>): Padding around the text in pixels..</li> <li><code>border_radius</code> (<code>integer</code>): Radius of the label in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Classification Label Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/classification_label_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.classification_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"text\": \"LABEL\",\n    \"text_position\": \"CENTER\",\n    \"text_color\": \"WHITE\",\n    \"text_scale\": 1.0,\n    \"text_thickness\": 1,\n    \"text_padding\": 10,\n    \"border_radius\": 0\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/clip_comparison/","title":"Clip Comparison","text":""},{"location":"workflows/blocks/clip_comparison/#v2","title":"v2","text":"Class: <code>ClipComparisonBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.clip_comparison.v2.ClipComparisonBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Use the OpenAI CLIP zero-shot classification model to classify images.</p> <p>This block accepts an image and a list of text prompts. The block then returns the  similarity of each text label to the provided image.</p> <p>This block is useful for classifying images without having to train a fine-tuned  classification model. For example, you could use CLIP to classify the type of vehicle  in an image, or if an image contains NSFW material.</p>"},{"location":"workflows/blocks/clip_comparison/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/clip_comparison@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/clip_comparison/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>classes</code> <code>List[str]</code> List of classes to calculate similarity against each input image. \u2705 <code>version</code> <code>str</code> Variant of CLIP model. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/clip_comparison/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Clip Comparison</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to calculate similarity against each input image.</li> <li><code>version</code> (<code>string</code>): Variant of CLIP model.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>similarities</code> (<code>list_of_values</code>): List of values of any type.</li> <li><code>max_similarity</code> (<code>float_zero_to_one</code>): <code>float</code> value in range <code>[0.0, 1.0]</code>.</li> <li><code>most_similar_class</code> (<code>string</code>): String value.</li> <li><code>min_similarity</code> (<code>float_zero_to_one</code>): <code>float</code> value in range <code>[0.0, 1.0]</code>.</li> <li><code>least_similar_class</code> (<code>string</code>): String value.</li> <li><code>classification_predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> </ul> </li> </ul> Example JSON definition of step <code>Clip Comparison</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/clip_comparison@v2\",\n    \"images\": \"$inputs.image\",\n    \"classes\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"version\": \"ViT-B-16\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/clip_comparison/#v1","title":"v1","text":"Class: <code>ClipComparisonBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.clip_comparison.v1.ClipComparisonBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Use the OpenAI CLIP zero-shot classification model to classify images.</p> <p>This block accepts an image and a list of text prompts. The block then returns the  similarity of each text label to the provided image.</p> <p>This block is useful for classifying images without having to train a fine-tuned  classification model. For example, you could use CLIP to classify the type of vehicle  in an image, or if an image contains NSFW material.</p>"},{"location":"workflows/blocks/clip_comparison/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/clip_comparison@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/clip_comparison/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>texts</code> <code>List[str]</code> List of texts to calculate similarity against each input image. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/clip_comparison/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Clip Comparison</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>texts</code> (<code>list_of_values</code>): List of texts to calculate similarity against each input image.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>similarity</code> (<code>list_of_values</code>): List of values of any type.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>Clip Comparison</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/clip_comparison@v1\",\n    \"images\": \"$inputs.image\",\n    \"texts\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/clip_embedding_model/","title":"CLIP Embedding Model","text":"Class: <code>ClipModelBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.clip.v1.ClipModelBlockV1</p> <p>Use a CLIP model to create semantic embeddings of text and images.</p> <p>This block accepts an image or string and returns an embedding. The embedding can be used to compare the similarity between different images or between images and text.</p>"},{"location":"workflows/blocks/clip_embedding_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/clip@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/clip_embedding_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>data</code> <code>str</code> The string or image to generate an embedding for.. \u2705 <code>version</code> <code>str</code> Variant of CLIP model. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/clip_embedding_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>CLIP Embedding Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (Union[<code>image</code>, <code>string</code>]): The string or image to generate an embedding for..</li> <li><code>version</code> (<code>string</code>): Variant of CLIP model.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>embedding</code> (<code>embedding</code>): A list of floating point numbers representing a vector embedding..</li> </ul> </li> </ul> Example JSON definition of step <code>CLIP Embedding Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/clip@v1\",\n    \"data\": \"$inputs.image\",\n    \"version\": \"ViT-B-16\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/cog_vlm/","title":"CogVLM","text":"Class: <code>CogVLMBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.cog_vlm.v1.CogVLMBlockV1</p> <p>CogVLM reached End Of Life</p> <p>Due to dependencies conflicts with newer models and security vulnerabilities discovered in <code>transformers</code> library patched in the versions of library incompatible with the model we announced End Of Life for CogVLM support in <code>inference</code>, effective since release <code>0.38.0</code>.</p> <p>We are leaving this block in ecosystem until release <code>0.42.0</code> for clients to get informed about change that  was introduced.</p> <p>Starting as of now, all Workflows using the block stop being functional (runtime error will be raised),  after inference release <code>0.42.0</code> - this block will be removed and Execution Engine will raise compilation  error seeing the block in Workflow definition. </p> <p>Ask a question to CogVLM, an open source vision-language model.</p> <p>This model requires a GPU and can only be run on self-hosted devices, and is not available on the Roboflow Hosted API.</p> <p>This model was previously part of the LMM block.</p>"},{"location":"workflows/blocks/cog_vlm/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/cog_vlm@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/cog_vlm/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Text prompt to the CogVLM model. \u2705 <code>json_output_format</code> <code>Dict[str, str]</code> Holds dictionary that maps name of requested output field into its description. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/cog_vlm/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>CogVLM</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the CogVLM model.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>structured_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>CogVLM</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/cog_vlm@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"json_output_format\": {\n        \"count\": \"number of cats in the picture\"\n    }\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/color_visualization/","title":"Color Visualization","text":"Class: <code>ColorVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.color.v1.ColorVisualizationBlockV1</p> <p>The <code>ColorVisualization</code> block paints a solid color on detected objects in an image using Supervision's <code>sv.ColorAnnotator</code>.</p>"},{"location":"workflows/blocks/color_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/color_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/color_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>opacity</code> <code>float</code> Transparency of the color overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/color_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Color Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the color overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Color Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/color_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"opacity\": 0.5\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/continue_if/","title":"Continue If","text":"Class: <code>ContinueIfBlockV1</code> <p>Source: inference.core.workflows.core_steps.flow_control.continue_if.v1.ContinueIfBlockV1</p> <p>Based on provided configuration, block decides if it should follow to pointed execution path</p>"},{"location":"workflows/blocks/continue_if/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/continue_if@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/continue_if/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>condition_statement</code> <code>StatementGroup</code> Define the conditional logic.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/continue_if/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Continue If</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>evaluation_parameters</code> (<code>*</code>): Data to be used in the conditional logic..</li> <li><code>next_steps</code> (step): Steps to execute if the condition evaluates to true..</li> </ul> </li> <li> <p>output</p> </li> </ul> Example JSON definition of step <code>Continue If</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/continue_if@v1\",\n    \"condition_statement\": {\n        \"statements\": [\n            {\n                \"comparator\": {\n                    \"type\": \"(Number) ==\"\n                },\n                \"left_operand\": {\n                    \"operand_name\": \"left\",\n                    \"type\": \"DynamicOperand\"\n                },\n                \"right_operand\": {\n                    \"type\": \"StaticOperand\",\n                    \"value\": 1\n                },\n                \"type\": \"BinaryStatement\"\n            }\n        ],\n        \"type\": \"StatementGroup\"\n    },\n    \"evaluation_parameters\": {\n        \"left\": \"$inputs.some\"\n    },\n    \"next_steps\": [\n        \"$steps.on_true\"\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/contrast_equalization/","title":"Contrast Equalization","text":"Class: <code>ContrastEqualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.contrast_equalization.v1.ContrastEqualizationBlockV1</p> <p>Apply contrast equalization to an image These are the same options provided for model preprocessing</p>"},{"location":"workflows/blocks/contrast_equalization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/contrast_equalization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/contrast_equalization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>equalization_type</code> <code>str</code> Type of contrast equalization to use.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/contrast_equalization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Contrast Equalization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>equalization_type</code> (<code>string</code>): Type of contrast equalization to use..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Contrast Equalization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/contrast_equalization@v1\",\n    \"image\": \"$inputs.image\",\n    \"equalization_type\": \"Equalization\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/corner_visualization/","title":"Corner Visualization","text":"Class: <code>CornerVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.corner.v1.CornerVisualizationBlockV1</p> <p>The <code>CornerVisualization</code> block draws the corners of detected objects in an image using Supervision's <code>sv.BoxCornerAnnotator</code>.</p>"},{"location":"workflows/blocks/corner_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/corner_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/corner_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <code>corner_length</code> <code>int</code> Length of the corner lines in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/corner_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Corner Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> <li><code>corner_length</code> (<code>integer</code>): Length of the corner lines in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Corner Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/corner_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 4,\n    \"corner_length\": 15\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/cosine_similarity/","title":"Cosine Similarity","text":"Class: <code>CosineSimilarityBlockV1</code> <p>Source: inference.core.workflows.core_steps.math.cosine_similarity.v1.CosineSimilarityBlockV1</p> <p>Calculate the cosine similarity between two embeddings.</p> <p>A cosine similarity of 1 means the two embeddings are identical, while a cosine similarity of 0 means the two embeddings are orthogonal. Greater values indicate greater similarity.</p>"},{"location":"workflows/blocks/cosine_similarity/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/cosine_similarity@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/cosine_similarity/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/cosine_similarity/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Cosine Similarity</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>embedding_1</code> (<code>embedding</code>): Embedding 1.</li> <li><code>embedding_2</code> (<code>embedding</code>): Embedding 2.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>similarity</code> (<code>float</code>): Float value.</li> </ul> </li> </ul> Example JSON definition of step <code>Cosine Similarity</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/cosine_similarity@v1\",\n    \"embedding_1\": \"$steps.clip_image.embedding\",\n    \"embedding_2\": \"$steps.clip_text.embedding\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/crop_visualization/","title":"Crop Visualization","text":"Class: <code>CropVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.crop.v1.CropVisualizationBlockV1</p> <p>The <code>CropVisualization</code> block draws scaled up crops of detections on the scene using Supervision's <code>sv.CropAnnotator</code>.</p>"},{"location":"workflows/blocks/crop_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/crop_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/crop_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the crop.. \u2705 <code>scale_factor</code> <code>float</code> The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.. \u2705 <code>border_thickness</code> <code>int</code> Thickness of the outline in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/crop_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Crop Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the crop..</li> <li><code>scale_factor</code> (<code>float</code>): The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection..</li> <li><code>border_thickness</code> (<code>integer</code>): Thickness of the outline in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Crop Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/crop_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"scale_factor\": 2.0,\n    \"border_thickness\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/csv_formatter/","title":"CSV Formatter","text":"Class: <code>CSVFormatterBlockV1</code> <p>Source: inference.core.workflows.core_steps.formatters.csv.v1.CSVFormatterBlockV1</p> <p>The CSV Formatter block prepares structured CSV content based on specified data configurations within  a workflow. It allows users to:</p> <ul> <li> <p>choose which data appears as columns</p> </li> <li> <p>apply operations to transform the data within the block</p> </li> <li> <p>aggregate whole batch of data into single CSV document (see Data Aggregation section)</p> </li> </ul> <p>The generated CSV content can be used as input for other blocks, such as File Sink or Email Notifications.</p>"},{"location":"workflows/blocks/csv_formatter/#defining-columns","title":"Defining columns","text":"<p>Use <code>columns_data</code> property to specify name of the columns and data sources. Defining UQL operations in  <code>columns_operations</code> you can perform specific operation on each column.</p> <p>Timestamp column</p> <p>The block automatically adds <code>timestamp</code> column and this column name is reserved and cannot be used.</p> <p>The value of timestamp would be in the following format: <code>2024-10-18T14:09:57.622297+00:00</code>, values  are scaled to UTC time zone.</p> <p>For example, the following definition <pre><code>columns_data = {\n    \"predictions\": \"$steps.model.predictions\",\n    \"reference\": \"$inputs.reference_class_names\",\n}\ncolumns_operations = {\n    \"predictions\": [\n        {\"type\": \"DetectionsPropertyExtract\", \"property_name\": \"class_name\"}\n    ],\n}\n</code></pre></p> <p>Will generate CSV content: <pre><code>timestamp,predictions,reference\n\"2024-10-16T11:15:15.336322+00:00\",\"['a', 'b', 'c']\",\"['a', 'b']\"\n</code></pre></p> <p>When applied on object detection predictions from a single image, assuming that <code>$inputs.reference_class_names</code> holds a list of reference classes.</p>"},{"location":"workflows/blocks/csv_formatter/#data-aggregation","title":"Data Aggregation","text":"<p>The block may take input from different blocks, hence its behavior may differ depending on context:</p> <ul> <li> <p>data <code>batch_size=1</code>: whenever single input is provided - block will provide the output as in the example above -  CSV header will be placed in the first row, the second row will hold the data</p> </li> <li> <p>data <code>batch_size&gt;1</code>: each datapoint will create one row in CSV document, but only the last batch element will be fed with the aggregated output, leaving other batch elements' outputs empty</p> </li> </ul>"},{"location":"workflows/blocks/csv_formatter/#when-should-i-expect-batch_size1","title":"When should I expect <code>batch_size=1</code>?","text":"<p>You may expect <code>batch_size=1</code> in the following scenarios:</p> <ul> <li> <p>CSV Formatter was connected to the output of block that only operates on one image and produces one prediction</p> </li> <li> <p>CSV Formatter was connected to the output of block that aggregates data for whole batch and produces single  non-empty output (which is exactly the characteristics of CSV Formatter itself)</p> </li> </ul>"},{"location":"workflows/blocks/csv_formatter/#when-should-i-expect-batch_size1_1","title":"When should I expect <code>batch_size&gt;1</code>?","text":"<p>You may expect <code>batch_size=1</code> in the following scenarios:</p> <ul> <li>CSV Formatter was connected to the output of block that produces single prediction for single image, but batch of images were fed - then CSV Formatter will aggregate the CSV content and output it in the position of the last batch element:</li> </ul> <pre><code>--- input_batch[0] ----&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ----&gt;  &lt;Empty&gt;\n--- input_batch[1] ----&gt; \u2502                       \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502      CSV Formatter    \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502                       \u2502 ----&gt;  &lt;Empty&gt;           \n--- input_batch[n] ----&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ----&gt;  {\"csv_content\": \"...\"}\n</code></pre> <p>Format of CSV document for <code>batch_size&gt;1</code></p> <p>If the example presented above is applied for larger input batch sizes - the output document structure  would be as follows:</p> <pre><code>timestamp,predictions,reference\n\"2024-10-16T11:15:15.336322+00:00\",\"['a', 'b', 'c']\",\"['a', 'b']\"\n\"2024-10-16T11:15:15.436322+00:00\",\"['b', 'c']\",\"['a', 'b']\"\n\"2024-10-16T11:15:15.536322+00:00\",\"['a', 'c']\",\"['a', 'b']\"\n</code></pre>"},{"location":"workflows/blocks/csv_formatter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/csv_formatter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/csv_formatter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>columns_data</code> <code>Dict[str, Union[bool, float, int, str]]</code> References data to be used to construct each and every column. \u2705 <code>columns_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each column. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/csv_formatter/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>CSV Formatter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>columns_data</code> (<code>*</code>): References data to be used to construct each and every column.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>csv_content</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>CSV Formatter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/csv_formatter@v1\",\n    \"columns_data\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"columns_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    }\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/data_aggregator/","title":"Data Aggregator","text":"Class: <code>DataAggregatorBlockV1</code> <p>Source: inference.core.workflows.core_steps.analytics.data_aggregator.v1.DataAggregatorBlockV1</p> <p>The Data Aggregator block collects and processes data from Workflows to generate time-based statistical  summaries. It allows users to define custom aggregation strategies over specified intervals, making it suitable  for creating analytics on data streams.</p> <p>The block enables:</p> <ul> <li> <p>feeding it with data from other Workflow blocks and applying in-place operations (for instance to extract  desired values out of model predictions)</p> </li> <li> <p>using multiple aggregation modes, including <code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, <code>count</code> and others</p> </li> <li> <p>specifying aggregation interval flexibly</p> </li> </ul>"},{"location":"workflows/blocks/data_aggregator/#feeding-data-aggregator","title":"Feeding Data Aggregator","text":"<p>You can specify the data to aggregate by referencing input sources using the <code>data</code> field. Optionally, for each specified <code>data</code> input you can apply chain of UQL operations with <code>data_operations</code> property.</p> <p>For example, the following configuration:</p> <pre><code>data = {\n    \"predictions_model_a\": \"$steps.model_a.predictions\",\n    \"predictions_model_b\": \"$steps.model_b.predictions\",\n}\ndata_operations = { \n    \"predictions_model_a\": [\n        {\"type\": \"DetectionsPropertyExtract\", \"property_name\": \"class_name\"}\n    ],\n    \"predictions_model_b\": [{\"type\": \"SequenceLength\"}]\n}\n</code></pre> <p>on each step run will at first take <code>predictions_model_a</code> to extract list of detected classes and calculate the number of predicted bounding boxes for <code>predictions_model_b</code>.</p>"},{"location":"workflows/blocks/data_aggregator/#specifying-data-aggregations","title":"Specifying data aggregations","text":"<p>For each input data referenced by <code>data</code> property you can specify list of aggregation operations, that include:</p> <ul> <li> <p><code>sum</code>: Taking the sum of values (requires data to be numeric)</p> </li> <li> <p><code>avg</code>: Taking the average of values (requires data to be numeric)</p> </li> <li> <p><code>max</code>: Taking the max of values (requires data to be numeric)</p> </li> <li> <p><code>min</code>: Taking the min of values (requires data to be numeric)</p> </li> <li> <p><code>count</code>: Counting the values - if provided value is list - operation will add length of the list into  aggregated state</p> </li> <li> <p><code>distinct</code>: deduplication of encountered values - providing list of unique values in the output. If  aggregation data is list - operation will add each element of the list into aggregated state.</p> </li> <li> <p><code>count_distinct</code>: counting occurrences of distinct values - providing number of different values that were  encountered. If aggregation data is list - operation will add each element of the list into aggregated state.</p> </li> <li> <p><code>count_distinct</code>: counting distinct values - providing number of different values that were  encountered. If aggregation data is list - operation will add each element of the list into aggregated state.</p> </li> <li> <p><code>values_counts</code>: counting occurrences of each distinct value - providing dictionary mapping each unique value  encountered into the number of observations. If aggregation data is list - operation will add each element of the list  into aggregated state.</p> </li> <li> <p><code>values_difference</code>: calculates the difference between max and min observed value (requires data to be numeric)</p> </li> </ul> <p>If we take the <code>data</code> and <code>data_operations</code> from the example above and specify <code>aggregation_mode</code> in the following way:</p> <pre><code>aggregation_mode = {\n    \"predictions_model_a\": [\"distinct\", \"count_distinct\"],\n    \"predictions_model_b\": [\"avg\"],\n}\n</code></pre> <p>Our aggregation report will contain the following values:</p> <pre><code>{\n    \"predictions_model_a_distinct\": [\"car\", \"person\", \"dog\"],\n    \"predictions_model_a_count_distinct\": {\"car\": 378, \"person\": 128, \"dog\": 37},\n    \"predictions_model_b_avg\": 7.35,\n}\n</code></pre> <p>where:</p> <ul> <li> <p><code>predictions_model_a_distinct</code> provides distinct classes predicted by model A in aggregation window</p> </li> <li> <p><code>predictions_model_a_count_distinct</code> provides number of classes instances predicted by model A in aggregation  window</p> </li> <li> <p><code>predictions_model_b_avg</code> provides average number of bounding boxes predicted by model B in aggregation window</p> </li> </ul>"},{"location":"workflows/blocks/data_aggregator/#interval-nature-of-the-block","title":"Interval nature of the block","text":"<p>Block behaviour is dictated by internal 'clock'</p> <p>Behaviour of this block differs from other, more classical blocks which output the data for each input. Data Aggregator block maintains its internal state that dictates when the data will be produced,  flushing internal aggregation state of the block. </p> <p>You can expect that most of the times, once fed with data, the block will produce empty outputs, effectively terminating downstream processing:</p> <pre><code>--- input_batch[0] ----&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ----&gt;  &lt;Empty&gt;\n--- input_batch[1] ----&gt; \u2502                       \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502     Data Aggregator   \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502                       \u2502 ----&gt;  &lt;Empty&gt;           \n--- input_batch[n] ----&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ----&gt;  &lt;Empty&gt;\n</code></pre> <p>But once for a while, the block will yield aggregated data and flush its internal state:</p> <pre><code>--- input_batch[0] ----&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ----&gt;  &lt;Empty&gt;\n--- input_batch[1] ----&gt; \u2502                       \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502     Data Aggregator   \u2502 ----&gt;  {&lt;aggregated_report&gt;}\n        ...              \u2502                       \u2502 ----&gt;  &lt;Empty&gt; # first datapoint added to new state          \n--- input_batch[n] ----&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ----&gt;  &lt;Empty&gt;\n</code></pre> <p>Setting the aggregation interval is possible with <code>interval</code> and <code>interval_unit</code> property. <code>interval</code> specifies the length of aggregation window and <code>interval_unit</code> bounds the <code>interval</code> value  into units. You can specify the interval based on:</p> <ul> <li> <p>time elapse: using <code>[\"seconds\", \"minutes\", \"hours\"]</code> as <code>interval_unit</code> will make the  Data Aggregator to yield the aggregated report based on time that elapsed since last report  was released - this setting is relevant for processing of video streams.</p> </li> <li> <p>number of runs: using <code>runs</code> as <code>interval_unit</code> - this setting is relevant for  processing of video files, as in this context wall-clock time elapse is not the proper way of getting meaningful reports.</p> </li> </ul>"},{"location":"workflows/blocks/data_aggregator/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/data_aggregator@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/data_aggregator/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>data_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. element of the data. \u274c <code>aggregation_mode</code> <code>Dict[str, List[str]]</code> Lists of aggregation operations to apply on each input data. \u274c <code>interval_unit</code> <code>str</code> Unit to measure <code>interval</code>. \u274c <code>interval</code> <code>int</code> Length of aggregation interval. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/data_aggregator/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Data Aggregator</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): References data to be used to construct each and every column.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Data Aggregator</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/data_aggregator@v1\",\n    \"data\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"data_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"aggregation_mode\": {\n        \"predictions\": [\n            \"distinct\",\n            \"count_distinct\"\n        ]\n    },\n    \"interval_unit\": \"seconds\",\n    \"interval\": 10\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/delta_filter/","title":"Delta Filter","text":"Class: <code>DeltaFilterBlockV1</code> <p>Source: inference.core.workflows.core_steps.flow_control.delta_filter.v1.DeltaFilterBlockV1</p> <p>The Delta Filter is a flow control block that triggers workflow steps only when an input value changes. It avoids redundant processing and optimizes system efficiency.</p> <pre><code>+----------------+      (value changes)       +----------------+\n| Previous Value |  -----------------------&gt;  |   Next Steps    |\n+----------------+                           +----------------+\n</code></pre> <p>Key Features:</p> <p>Change Detection: Tracks input values and only proceeds when a change is detected. Dynamic Value Support: Handles various input types (e.g., numbers, strings). Context-Aware Caching: Tracks changes on a per-video basis using video_identifier.</p> <p>Usage Instructions: Input Configuration: Set \"Input Value\" to reference the value to monitor (e.g., counter). Next Steps Setup: Define steps to execute on value change.</p> <p>Example Use Case:</p> <p>A video analysis workflow counts people in the zone. When the count changes, Delta Filter triggers downstream steps (e.g., setting variable in OPC), minimizing redundant processing.</p>"},{"location":"workflows/blocks/delta_filter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/delta_filter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/delta_filter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/delta_filter/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Delta Filter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>value</code> (<code>*</code>): Flow will be allowed to continue only if this value changes between frames..</li> <li><code>next_steps</code> (step): Steps to execute when the value changes..</li> </ul> </li> <li> <p>output</p> </li> </ul> Example JSON definition of step <code>Delta Filter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/delta_filter@v1\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"value\": \"$steps.line_counter.count_in\",\n    \"next_steps\": \"$steps.write_to_csv\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/depth_estimation/","title":"Depth Estimation","text":"Class: <code>DepthEstimationBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.depth_estimation.v1.DepthEstimationBlockV1</p> <pre><code>        \ud83c\udfaf This workflow block performs depth estimation on images using Apple's DepthPro model. It analyzes the spatial relationships\n        and depth information in images to create a depth map where:\n\n        \ud83d\udcca Each pixel's value represents its relative distance from the camera\n        \ud83d\udd0d Lower values (darker colors) indicate closer objects\n        \ud83d\udd2d Higher values (lighter colors) indicate further objects\n\n        The model outputs:\n        1. \ud83d\uddfa\ufe0f A depth map showing the relative distances of objects in the scene\n        2. \ud83d\udcd0 The camera's field of view (in degrees)\n        3. \ud83d\udd2c The camera's focal length\n\n        This is particularly useful for:\n        - \ud83c\udfd7\ufe0f Understanding 3D structure from 2D images\n        - \ud83c\udfa8 Creating depth-aware visualizations\n        - \ud83d\udccf Analyzing spatial relationships in scenes\n        - \ud83d\udd76\ufe0f Applications in augmented reality and 3D reconstruction\n\n        \u26a1 The model runs efficiently on Apple Silicon (M1-M4) using Metal Performance Shaders (MPS) for accelerated inference.\n</code></pre>"},{"location":"workflows/blocks/depth_estimation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/depth_estimation@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/depth_estimation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_version</code> <code>str</code> The Depth Estimation model to be used for inference.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/depth_estimation/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Depth Estimation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>normalized_depth</code> (<code>numpy_array</code>): Numpy array.</li> </ul> </li> </ul> Example JSON definition of step <code>Depth Estimation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/depth_estimation@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_version\": \"depth-anything-v2/small\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detection_offset/","title":"Detection Offset","text":"Class: <code>DetectionOffsetBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.detection_offset.v1.DetectionOffsetBlockV1</p> <p>Apply a fixed offset to the width and height of a detection.</p> <p>You can use this block to add padding around the result of a detection. This is useful  to ensure that you can analyze bounding boxes that may be within the region of an  object instead of being around an object.</p>"},{"location":"workflows/blocks/detection_offset/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detection_offset@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detection_offset/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>offset_width</code> <code>int</code> Offset for box width.. \u2705 <code>offset_height</code> <code>int</code> Offset for box height.. \u2705 <code>units</code> <code>str</code> Units for offset dimensions.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detection_offset/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detection Offset</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to offset dimensions for..</li> <li><code>offset_width</code> (<code>integer</code>): Offset for box width..</li> <li><code>offset_height</code> (<code>integer</code>): Offset for box height..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detection Offset</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detection_offset@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"offset_width\": 10,\n    \"offset_height\": 10,\n    \"units\": \"Pixels\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_classes_replacement/","title":"Detections Classes Replacement","text":"Class: <code>DetectionsClassesReplacementBlockV1</code> <p>Source: inference.core.workflows.core_steps.fusion.detections_classes_replacement.v1.DetectionsClassesReplacementBlockV1</p> <p>Combine results of detection model with classification results performed separately for  each and every bounding box. </p> <p>Bounding boxes without top class predicted by classification model are discarded,  for multi-label classification results, most confident label is taken as bounding box class.  </p>"},{"location":"workflows/blocks/detections_classes_replacement/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_classes_replacement@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_classes_replacement/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>fallback_class_name</code> <code>str</code> The class name to be used as a fallback if no class is predicted for a bounding box. \u2705 <code>fallback_class_id</code> <code>int</code> The class id to be used as a fallback if no class is predicted for a bounding box;if not specified or negative, the class id will be set to 9223372036854775807. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_classes_replacement/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Classes Replacement</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>object_detection_predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): The output of a detection model describing the bounding boxes that will have classes replaced..</li> <li><code>classification_predictions</code> (<code>classification_prediction</code>): The output of classification model for crops taken based on RoIs pointed as the other parameter.</li> <li><code>fallback_class_name</code> (<code>string</code>): The class name to be used as a fallback if no class is predicted for a bounding box.</li> <li><code>fallback_class_id</code> (<code>integer</code>): The class id to be used as a fallback if no class is predicted for a bounding box;if not specified or negative, the class id will be set to 9223372036854775807.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Classes Replacement</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n    \"object_detection_predictions\": \"$steps.my_object_detection_model.predictions\",\n    \"classification_predictions\": \"$steps.my_classification_model.predictions\",\n    \"fallback_class_name\": \"unknown\",\n    \"fallback_class_id\": 77\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_combine/","title":"Detections Combine","text":"Class: <code>DetectionsCombineBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.detections_combine.v1.DetectionsCombineBlockV1</p> <p>This block combines two sets of predictions into a single set of predictions.</p>"},{"location":"workflows/blocks/detections_combine/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_combine@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_combine/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_combine/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Combine</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>prediction_one</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): First set of predictions..</li> <li><code>prediction_two</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Second set of predictions..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Combine</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_combine@v1\",\n    \"prediction_one\": \"$steps.my_object_detection_model.predictions\",\n    \"prediction_two\": \"$steps.my_object_detection_model.predictions\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_consensus/","title":"Detections Consensus","text":"Class: <code>DetectionsConsensusBlockV1</code> <p>Source: inference.core.workflows.core_steps.fusion.detections_consensus.v1.DetectionsConsensusBlockV1</p> <p>Combine detections from multiple detection-based models based on a majority vote  strategy.</p> <p>This block is useful if you have multiple specialized models that you want to consult  to determine whether a certain object is present in an image.</p> <p>See the table below to explore the values you can use to configure the consensus block.</p>"},{"location":"workflows/blocks/detections_consensus/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_consensus@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_consensus/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>required_votes</code> <code>int</code> Required number of votes for single detection from different models to accept detection as output detection. \u2705 <code>class_aware</code> <code>bool</code> Flag to decide if merging detections is class-aware or only bounding boxes aware. \u2705 <code>iou_threshold</code> <code>float</code> IoU threshold to consider detections from different models as matching (increasing votes for region). \u2705 <code>confidence</code> <code>float</code> Confidence threshold for merged detections. \u2705 <code>classes_to_consider</code> <code>List[str]</code> Optional list of classes to consider in consensus procedure.. \u2705 <code>required_objects</code> <code>Optional[Dict[str, int], int]</code> If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to <code>InferenceParameter</code>, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.. \u2705 <code>presence_confidence_aggregation</code> <code>AggregationMode</code> Mode dictating aggregation of confidence scores and classes both in case of object presence deduction procedure.. \u274c <code>detections_merge_confidence_aggregation</code> <code>AggregationMode</code> Mode dictating aggregation of confidence scores and classes both in case of boxes consensus procedure. One of <code>average</code>, <code>max</code>, <code>min</code>. Default: <code>average</code>. While using for merging overlapping boxes, against classes - <code>average</code> equals to majority vote, <code>max</code> - for the class of detection with max confidence, <code>min</code> - for the class of detection with min confidence.. \u274c <code>detections_merge_coordinates_aggregation</code> <code>AggregationMode</code> Mode dictating aggregation of bounding boxes. One of <code>average</code>, <code>max</code>, <code>min</code>. Default: <code>average</code>. <code>average</code> means taking mean from all boxes coordinates, <code>min</code> - taking smallest box, <code>max</code> - taking largest box. This mode is not used for masks aggregation.. \u274c <code>detections_merge_mask_aggregation</code> <code>MaskAggregationMode</code> Mode dictating aggregation of masks. One of <code>union</code>, <code>intersection</code>, <code>max</code>, <code>min</code>. Default: <code>union</code>. <code>union</code> means taking union of all masks, <code>intersection</code> - taking intersection of all masks, <code>max</code> - taking largest mask, <code>min</code> - taking smallest mask.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_consensus/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Consensus</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions_batches</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Reference to detection-like model predictions made against single image to agree on model consensus.</li> <li><code>required_votes</code> (<code>integer</code>): Required number of votes for single detection from different models to accept detection as output detection.</li> <li><code>class_aware</code> (<code>boolean</code>): Flag to decide if merging detections is class-aware or only bounding boxes aware.</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): IoU threshold to consider detections from different models as matching (increasing votes for region).</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for merged detections.</li> <li><code>classes_to_consider</code> (<code>list_of_values</code>): Optional list of classes to consider in consensus procedure..</li> <li><code>required_objects</code> (Union[<code>dictionary</code>, <code>integer</code>]): If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to <code>InferenceParameter</code>, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> <li><code>object_present</code> (Union[<code>boolean</code>, <code>dictionary</code>]): Boolean flag if <code>boolean</code> or Dictionary if <code>dictionary</code>.</li> <li><code>presence_confidence</code> (Union[<code>float_zero_to_one</code>, <code>dictionary</code>]): <code>float</code> value in range <code>[0.0, 1.0]</code> if <code>float_zero_to_one</code> or Dictionary if <code>dictionary</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Consensus</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_consensus@v1\",\n    \"predictions_batches\": [\n        \"$steps.a.predictions\",\n        \"$steps.b.predictions\"\n    ],\n    \"required_votes\": 2,\n    \"class_aware\": true,\n    \"iou_threshold\": 0.3,\n    \"confidence\": 0.1,\n    \"classes_to_consider\": [\n        \"a\",\n        \"b\"\n    ],\n    \"required_objects\": 3,\n    \"presence_confidence_aggregation\": \"max\",\n    \"detections_merge_confidence_aggregation\": \"min\",\n    \"detections_merge_coordinates_aggregation\": \"min\",\n    \"detections_merge_mask_aggregation\": \"union\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_filter/","title":"Detections Filter","text":"Class: <code>DetectionsFilterBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.detections_filter.v1.DetectionsFilterBlockV1</p> <p>Conditionally filter out model predictions.</p>"},{"location":"workflows/blocks/detections_filter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_filter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_filter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>operations</code> <code>List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]</code> Definition of filtering logic.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_filter/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Filter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to filter..</li> <li><code>operations_parameters</code> (<code>*</code>): References to additional parameters that may be provided in runtime to parametrise operations.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Filter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_filter@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"operations\": [\n        {\n            \"filter_operation\": {\n                \"statements\": [\n                    {\n                        \"comparator\": {\n                            \"type\": \"in (Sequence)\"\n                        },\n                        \"left_operand\": {\n                            \"operations\": [\n                                {\n                                    \"property_name\": \"class_name\",\n                                    \"type\": \"ExtractDetectionProperty\"\n                                }\n                            ],\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"right_operand\": {\n                            \"operand_name\": \"classes\",\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"type\": \"BinaryStatement\"\n                    }\n                ],\n                \"type\": \"StatementGroup\"\n            },\n            \"type\": \"DetectionsFilter\"\n        }\n    ],\n    \"operations_parameters\": {\n        \"classes\": \"$inputs.classes\"\n    }\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_merge/","title":"Detections Merge","text":"Class: <code>DetectionsMergeBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.detections_merge.v1.DetectionsMergeBlockV1</p> <p>The <code>DetectionsMerge</code> block combines multiple detections into a single bounding box that encompasses all input detections. This is useful when you want to: - Merge overlapping or nearby detections of the same object - Create a single region that contains multiple detected objects - Simplify multiple detections into one larger detection</p> <p>The resulting detection will have: - A bounding box that contains all input detections - The classname of the merged detection is set to \"merged_detection\" by default, but can be customized via the <code>class_name</code> parameter - The confidence is set to the lowest confidence among all detections</p>"},{"location":"workflows/blocks/detections_merge/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_merge@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_merge/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>class_name</code> <code>str</code> The class name to assign to the merged detection.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_merge/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Merge</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Object detection predictions to merge into a single bounding box..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Merge</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_merge@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"class_name\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_stabilizer/","title":"Detections Stabilizer","text":"Class: <code>StabilizeTrackedDetectionsBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.stabilize_detections.v1.StabilizeTrackedDetectionsBlockV1</p> <p>This block stores last known position for each bounding box If box disappears then this block will bring it back so short gaps are filled with last known box position The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames) WARNING: this block will produce many short-lived bounding boxes for unstable trackers!</p>"},{"location":"workflows/blocks/detections_stabilizer/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stabilize_detections@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_stabilizer/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>smoothing_window_size</code> <code>int</code> Predicted movement of detection will be smoothed based on historical measurements of velocity, this parameter controls number of historical measurements taken under account when calculating smoothed velocity. Detections will be removed from generating smoothed predictions if they had been missing for longer than this number of frames.. \u2705 <code>bbox_smoothing_coefficient</code> <code>float</code> Bounding box smoothing coefficient applied when given tracker_id is present on current frame. This parameter must be initialized with value between 0 and 1. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_stabilizer/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Stabilizer</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Tracked detections.</li> <li><code>smoothing_window_size</code> (<code>integer</code>): Predicted movement of detection will be smoothed based on historical measurements of velocity, this parameter controls number of historical measurements taken under account when calculating smoothed velocity. Detections will be removed from generating smoothed predictions if they had been missing for longer than this number of frames..</li> <li><code>bbox_smoothing_coefficient</code> (<code>float_zero_to_one</code>): Bounding box smoothing coefficient applied when given tracker_id is present on current frame. This parameter must be initialized with value between 0 and 1.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Stabilizer</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stabilize_detections@v1\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"smoothing_window_size\": 5,\n    \"bbox_smoothing_coefficient\": 0.2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_stitch/","title":"Detections Stitch","text":"Class: <code>DetectionsStitchBlockV1</code> <p>Source: inference.core.workflows.core_steps.fusion.detections_stitch.v1.DetectionsStitchBlockV1</p> <p>This block merges detections that were inferred for multiple sub-parts of the same input image into single detection. </p> <p>Block may be helpful in the following scenarios: * to apply Slicing Adaptive Inference (SAHI) technique,  as a final step of procedure, which involves Image Slicer block and model block at previous stages. * to merge together detections performed by precise, high-resolution model applied as secondary model after coarse detection is performed in the first stage and Dynamic Crop is applied later. </p>"},{"location":"workflows/blocks/detections_stitch/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_stitch@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_stitch/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>overlap_filtering_strategy</code> <code>str</code> Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards lower-confidence detections, NMM combines them.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes. If intersection over union (IoU) is above this ratio, discard or merge the lower confidence box.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_stitch/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Stitch</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>reference_image</code> (<code>image</code>): Original image that was cropped to produce the predictions..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to be merged into the original image..</li> <li><code>overlap_filtering_strategy</code> (<code>string</code>): Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards lower-confidence detections, NMM combines them..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes. If intersection over union (IoU) is above this ratio, discard or merge the lower confidence box..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Stitch</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_stitch@v1\",\n    \"reference_image\": \"$inputs.image\",\n    \"predictions\": \"$steps.my_object_detection_model.predictions\",\n    \"overlap_filtering_strategy\": \"nms\",\n    \"iou_threshold\": 0.4\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/detections_transformation/","title":"Detections Transformation","text":"Class: <code>DetectionsTransformationBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.detections_transformation.v1.DetectionsTransformationBlockV1</p> <p>Block changes detected Bounding Boxes in a way specified in configuration.</p> <p>It supports such operations as changing the size of Bounding Boxes. </p>"},{"location":"workflows/blocks/detections_transformation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_transformation@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_transformation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>operations</code> <code>List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]</code> Transformations to be applied on the predictions.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_transformation/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Transformation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to transform..</li> <li><code>operations_parameters</code> (<code>*</code>): References to additional parameters that may be provided in runtime to parameterize operations.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Transformation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_transformation@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"operations\": [\n        {\n            \"filter_operation\": {\n                \"statements\": [\n                    {\n                        \"comparator\": {\n                            \"type\": \"in (Sequence)\"\n                        },\n                        \"left_operand\": {\n                            \"operations\": [\n                                {\n                                    \"property_name\": \"class_name\",\n                                    \"type\": \"ExtractDetectionProperty\"\n                                }\n                            ],\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"right_operand\": {\n                            \"operand_name\": \"classes\",\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"type\": \"BinaryStatement\"\n                    }\n                ],\n                \"type\": \"StatementGroup\"\n            },\n            \"type\": \"DetectionsFilter\"\n        }\n    ],\n    \"operations_parameters\": {\n        \"classes\": \"$inputs.classes\"\n    }\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/dimension_collapse/","title":"Dimension Collapse","text":"Class: <code>DimensionCollapseBlockV1</code> <p>Source: inference.core.workflows.core_steps.fusion.dimension_collapse.v1.DimensionCollapseBlockV1</p> <p>Takes multiple step outputs at data depth level n, concatenate them into list and reduce dimensionality to level n-1.</p> <p>Useful in scenarios like: * aggregation of classification results for dynamically cropped images * aggregation of OCR results for dynamically cropped images</p>"},{"location":"workflows/blocks/dimension_collapse/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dimension_collapse@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dimension_collapse/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dimension_collapse/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dimension Collapse</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): Reference to step outputs at depth level n to be concatenated and moved into level n-1..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Dimension Collapse</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dimension_collapse@v1\",\n    \"data\": \"$steps.ocr_step.results\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/distance_measurement/","title":"Distance Measurement","text":"Class: <code>DistanceMeasurementBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.distance_measurement.v1.DistanceMeasurementBlockV1</p> <p>Calculate the distance between two bounding boxes on a 2D plane, leveraging a perpendicular camera view and either a reference object or a pixel-to-unit scaling ratio for precise measurements.</p>"},{"location":"workflows/blocks/distance_measurement/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/distance_measurement@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/distance_measurement/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>object_1_class_name</code> <code>str</code> The class name of the first object.. \u274c <code>object_2_class_name</code> <code>str</code> The class name of the second object.. \u274c <code>reference_axis</code> <code>str</code> The axis along which the distance will be measured.. \u274c <code>calibration_method</code> <code>str</code> Select how to calibrate the measurement of distance between objects.. \u274c <code>reference_object_class_name</code> <code>str</code> The class name of the reference object.. \u2705 <code>reference_width</code> <code>float</code> Width of the reference object in centimeters. \u2705 <code>reference_height</code> <code>float</code> Height of the reference object in centimeters. \u2705 <code>pixel_ratio</code> <code>float</code> The pixel-to-centimeter ratio of the input image, i.e. 1 centimeter = 100 pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/distance_measurement/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Distance Measurement</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): The output of a detection model describing the bounding boxes that will be used to measure the objects..</li> <li><code>reference_object_class_name</code> (<code>string</code>): The class name of the reference object..</li> <li><code>reference_width</code> (<code>float</code>): Width of the reference object in centimeters.</li> <li><code>reference_height</code> (<code>float</code>): Height of the reference object in centimeters.</li> <li><code>pixel_ratio</code> (<code>float</code>): The pixel-to-centimeter ratio of the input image, i.e. 1 centimeter = 100 pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>distance_cm</code> (<code>integer</code>): Integer value.</li> <li><code>distance_pixel</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Distance Measurement</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/distance_measurement@v1\",\n    \"predictions\": \"$steps.model.predictions\",\n    \"object_1_class_name\": \"car\",\n    \"object_2_class_name\": \"person\",\n    \"reference_axis\": \"vertical\",\n    \"calibration_method\": \"&lt;block_does_not_provide_example&gt;\",\n    \"reference_object_class_name\": \"reference-object\",\n    \"reference_width\": 2.5,\n    \"reference_height\": 2.5,\n    \"pixel_ratio\": 100\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/dominant_color/","title":"Dominant Color","text":"Class: <code>DominantColorBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.dominant_color.v1.DominantColorBlockV1</p> <p>Extract the dominant color from an input image using K-means clustering.</p> <p>This block identifies the most prevalent color in an image. Processing time is dependant on color complexity and image size. Most images should complete in under half a second.</p> <p>The output is a list of RGB values representing the dominant color, making it easy  to use in further processing or visualization tasks.</p> <p>Note: The block operates on the assumption that the input image is in RGB format. </p>"},{"location":"workflows/blocks/dominant_color/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dominant_color@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dominant_color/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>color_clusters</code> <code>int</code> Number of dominant colors to identify. Higher values increase precision but may slow processing.. \u2705 <code>max_iterations</code> <code>int</code> Max number of iterations to perform. Higher values increase precision but may slow processing.. \u2705 <code>target_size</code> <code>int</code> Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dominant_color/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dominant Color</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>color_clusters</code> (<code>integer</code>): Number of dominant colors to identify. Higher values increase precision but may slow processing..</li> <li><code>max_iterations</code> (<code>integer</code>): Max number of iterations to perform. Higher values increase precision but may slow processing..</li> <li><code>target_size</code> (<code>integer</code>): Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>rgb_color</code> (<code>rgb_color</code>): RGB color.</li> </ul> </li> </ul> Example JSON definition of step <code>Dominant Color</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dominant_color@v1\",\n    \"image\": \"$inputs.image\",\n    \"color_clusters\": 4,\n    \"max_iterations\": 100,\n    \"target_size\": 100\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/dot_visualization/","title":"Dot Visualization","text":"Class: <code>DotVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.dot.v1.DotVisualizationBlockV1</p> <p>The <code>DotVisualization</code> block draws dots on an image at specific coordinates based on provided detections using Supervision's <code>sv.DotAnnotator</code>.</p>"},{"location":"workflows/blocks/dot_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dot_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dot_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the dot.. \u2705 <code>radius</code> <code>int</code> Radius of the dot in pixels.. \u2705 <code>outline_thickness</code> <code>int</code> Thickness of the outline of the dot in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dot_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dot Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the dot..</li> <li><code>radius</code> (<code>integer</code>): Radius of the dot in pixels..</li> <li><code>outline_thickness</code> (<code>integer</code>): Thickness of the outline of the dot in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Dot Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dot_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"radius\": 4,\n    \"outline_thickness\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/dynamic_crop/","title":"Dynamic Crop","text":"Class: <code>DynamicCropBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.dynamic_crop.v1.DynamicCropBlockV1</p> <p>Create dynamic crops from an image based on detections from detections-based model.</p> <p>This is useful when placed after an ObjectDetection block as part of a multi-stage  workflow. For example, you could use an ObjectDetection block to detect objects, then  the DynamicCropBlock block to crop objects, then an OCR block to run character recognition on  each of the individual cropped regions.</p> <p>In addition, for instance segmentation predictions (which provide segmentation mask for each  bounding box) it is possible to remove background in the crops, outside of detected instances. To enable that functionality, set <code>mask_opacity</code> to positive value and optionally tune  <code>background_color</code>.</p>"},{"location":"workflows/blocks/dynamic_crop/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dynamic_crop@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dynamic_crop/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>mask_opacity</code> <code>float</code> For instance segmentation, mask_opacity can be used to control background removal. Opacity 1.0 removes the background, while 0.0 leaves the crop unchanged.. \u2705 <code>background_color</code> <code>Union[Tuple[int, int, int], str]</code> For background removal based on segmentation mask, new background color can be selected. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dynamic_crop/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dynamic Crop</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The input image for this step..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Detection model output containing bounding boxes for cropping..</li> <li><code>mask_opacity</code> (<code>float_zero_to_one</code>): For instance segmentation, mask_opacity can be used to control background removal. Opacity 1.0 removes the background, while 0.0 leaves the crop unchanged..</li> <li><code>background_color</code> (Union[<code>string</code>, <code>rgb_color</code>]): For background removal based on segmentation mask, new background color can be selected. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67))..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>crops</code> (<code>image</code>): Image in workflows.</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Dynamic Crop</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dynamic_crop@v1\",\n    \"images\": \"$inputs.image\",\n    \"predictions\": \"$steps.my_object_detection_model.predictions\",\n    \"mask_opacity\": \"&lt;block_does_not_provide_example&gt;\",\n    \"background_color\": \"#431112\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/dynamic_zone/","title":"Dynamic Zone","text":"Class: <code>DynamicZonesBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.dynamic_zones.v1.DynamicZonesBlockV1</p> <p>The <code>DynamicZoneBlock</code> is a transformer block designed to simplify polygon so it's geometrically convex and then reduce number of vertices to requested amount. This block is best suited when Zone needs to be created based on shape of detected object (i.e. basketball field, road segment, zebra crossing etc.) Input detections should be filtered and contain only desired classes of interest.</p>"},{"location":"workflows/blocks/dynamic_zone/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dynamic_zone@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dynamic_zone/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>required_number_of_vertices</code> <code>int</code> Keep simplifying polygon until number of vertices matches this number. \u2705 <code>scale_ratio</code> <code>float</code> Expand resulting polygon along imaginary line from centroid to edge by this ratio. \u2705 <code>apply_least_squares</code> <code>bool</code> Apply least squares algorithm to fit resulting polygon edges to base contour. \u2705 <code>midpoint_fraction</code> <code>float</code> Fraction of vertices to keep in the middle of each edge before fitting least squares line. This parameter is useful when vertices of convex polygon are not aligned with edge that would be otherwise fitted to points closer to the center of each edge.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dynamic_zone/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dynamic Zone</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): .</li> <li><code>required_number_of_vertices</code> (<code>integer</code>): Keep simplifying polygon until number of vertices matches this number.</li> <li><code>scale_ratio</code> (<code>float</code>): Expand resulting polygon along imaginary line from centroid to edge by this ratio.</li> <li><code>apply_least_squares</code> (<code>boolean</code>): Apply least squares algorithm to fit resulting polygon edges to base contour.</li> <li><code>midpoint_fraction</code> (<code>float_zero_to_one</code>): Fraction of vertices to keep in the middle of each edge before fitting least squares line. This parameter is useful when vertices of convex polygon are not aligned with edge that would be otherwise fitted to points closer to the center of each edge..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>zones</code> (<code>list_of_values</code>): List of values of any type.</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> <li><code>simplification_converged</code> (<code>boolean</code>): Boolean flag.</li> </ul> </li> </ul> Example JSON definition of step <code>Dynamic Zone</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dynamic_zone@v1\",\n    \"predictions\": \"$segmentation.predictions\",\n    \"required_number_of_vertices\": 4,\n    \"scale_ratio\": 1.05,\n    \"apply_least_squares\": true,\n    \"midpoint_fraction\": 0.9\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/easy_ocr/","title":"EasyOCR","text":"Class: <code>EasyOCRBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.easy_ocr.v1.EasyOCRBlockV1</p> <p>Retrieve the characters in an image using EasyOCR Optical Character Recognition (OCR).</p> <p>This block returns the text within an image.</p> <p>You may want to use this block in combination with a detections-based block (i.e. ObjectDetectionBlock). An object detection model could isolate specific regions from an image (i.e. a shipping container ID in a logistics use case) for further processing. You can then use a DynamicCropBlock to crop the region of interest before running OCR.</p> <p>Using a detections model then cropping detections allows you to isolate your analysis on particular regions of an image.</p> <p>Note that EasyOCR has limitations running within containers on Apple Silicon.</p>"},{"location":"workflows/blocks/easy_ocr/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/easy_ocr@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/easy_ocr/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>language</code> <code>str</code> Language model to use for OCR. \u274c <code>quantize</code> <code>bool</code> Quantized models are smaller and faster, but may be less accurate and won't work correctly on all hardware.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/easy_ocr/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>EasyOCR</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>result</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>EasyOCR</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/easy_ocr@v1\",\n    \"images\": \"$inputs.image\",\n    \"language\": \"&lt;block_does_not_provide_example&gt;\",\n    \"quantize\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/ellipse_visualization/","title":"Ellipse Visualization","text":"Class: <code>EllipseVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.ellipse.v1.EllipseVisualizationBlockV1</p> <p>The <code>EllipseVisualization</code> block draws ellipses that highlight detected objects in an image using Supervision's <code>sv.EllipseAnnotator</code>.</p>"},{"location":"workflows/blocks/ellipse_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/ellipse_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/ellipse_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <code>start_angle</code> <code>int</code> Starting angle of the ellipse in degrees.. \u2705 <code>end_angle</code> <code>int</code> Ending angle of the ellipse in degrees.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/ellipse_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Ellipse Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> <li><code>start_angle</code> (<code>integer</code>): Starting angle of the ellipse in degrees..</li> <li><code>end_angle</code> (<code>integer</code>): Ending angle of the ellipse in degrees..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Ellipse Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/ellipse_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2,\n    \"start_angle\": -45,\n    \"end_angle\": 235\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/email_notification/","title":"Email Notification","text":"Class: <code>EmailNotificationBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.email_notification.v1.EmailNotificationBlockV1</p> <p>The Email Notification block allows users to send email notifications as part of a workflow.  It requires SMTP server setup to send the notification </p>"},{"location":"workflows/blocks/email_notification/#customizable-email-content","title":"Customizable Email Content","text":"<ul> <li> <p>Subject: Set the subject field to define the subject line of the email.</p> </li> <li> <p>Message: Use the message field to write the body content of the email. Message can be parametrised with data generated during workflow run. See Dynamic Parameters section.</p> </li> <li> <p>Recipients (To, CC, BCC): Define who will receive the email using <code>receiver_email</code>,  <code>cc_receiver_email</code>, and <code>bcc_receiver_email</code> properties. You can input a single email or a list.</p> </li> </ul>"},{"location":"workflows/blocks/email_notification/#dynamic-parameters","title":"Dynamic Parameters","text":"<p>Content of the message can be parametrised with Workflow execution outcomes. Take a look at the example message using dynamic parameters:</p> <pre><code>message = \"This is example notification. Predicted classes: {{ $parameters.predicted_classes }}\"\n</code></pre> <p>Message parameters are delivered by Workflows Execution Engine by setting proper data selectors in <code>message_parameters</code> field, for example:</p> <pre><code>message_parameters = {\n    \"predicted_classes\": \"$steps.model.predictions\"\n}\n</code></pre> <p>Selecting data is not the only option - data may be processed in the block. In the example below we wish to extract names of predicted classes. We can apply transformation for each parameter by setting <code>message_parameters_operations</code>:</p> <pre><code>message_parameters_operations = {\n    \"predictions\": [\n        {\"type\": \"DetectionsPropertyExtract\", \"property_name\": \"class_name\"}\n    ]\n}\n</code></pre> <p>As a result, in the e-mail that will be sent, you can expect:</p> <pre><code>This is example notification. Predicted classes: [\"class_a\", \"class_b\"].\n</code></pre>"},{"location":"workflows/blocks/email_notification/#configuring-smtp-server","title":"Configuring SMTP server","text":"<p>Those are the parameters configuring SMTP server: </p> <ul> <li> <p><code>smtp_server</code> - hostname of the SMTP server to use</p> </li> <li> <p><code>sender_email</code> - e-mail account to be used as sender</p> </li> <li> <p><code>sender_email_password</code> - password for sender e-mail account</p> </li> <li> <p><code>smtp_port</code> - port of SMTP service - defaults to <code>465</code></p> </li> </ul> <p>Block enforces SSL over SMTP.</p> <p>Typical scenario for using custom SMTP server involves sending e-mail through Google SMTP server. Take a look at Google tutorial to configure the  block properly. </p> <p>GMAIL password will not work if 2-step verification is turned on</p> <p>GMAIL users choosing custom SMTP server as e-mail service provider must configure  application password to avoid problems with 2-step verification protected account. Beware that application password must be kept protected - we recommend sending the password in Workflow  input and providing it each time by the caller, avoiding storing it in Workflow  definition.</p>"},{"location":"workflows/blocks/email_notification/#cooldown","title":"Cooldown","text":"<p>The block accepts <code>cooldown_seconds</code> (which defaults to <code>5</code> seconds) to prevent unintended bursts of  notifications. Please adjust it according to your needs, setting <code>0</code> indicate no cooldown. </p> <p>During cooldown period, consecutive runs of the step will cause <code>throttling_status</code> output to be set <code>True</code> and no notification will be sent.</p> <p>Cooldown limitations</p> <p>Current implementation of cooldown is limited to video processing - using this block in context of a  Workflow that is run behind HTTP service (Roboflow Hosted API, Dedicated Deployment or self-hosted  <code>inference</code> server) will have no effect for processing HTTP requests.  </p>"},{"location":"workflows/blocks/email_notification/#attachments","title":"Attachments","text":"<p>You may specify attachment files to be send with your e-mail. Attachments can only be generated  in runtime by dedicated blocks (for instance CSV Formatter)</p> <p>To include attachments, simply provide the attachment name and refer to other block outputs:</p> <pre><code>attachments = {\n    \"report.pdf\": \"$steps.report_generator.output\"\n}\n</code></pre>"},{"location":"workflows/blocks/email_notification/#async-execution","title":"Async execution","text":"<p>Configure the <code>fire_and_forget</code> property. Set it to True if you want the email to be sent in the background, allowing the  Workflow to proceed without waiting on e-mail to be sent. In this case you will not be able to rely on  <code>error_status</code> output which will always be set to <code>False</code>, so we recommend setting the <code>fire_and_forget=False</code> for debugging purposes.</p>"},{"location":"workflows/blocks/email_notification/#disabling-notifications-based-on-runtime-parameter","title":"Disabling notifications based on runtime parameter","text":"<p>Sometimes it would be convenient to manually disable the e-mail notifier block. This is possible  setting <code>disable_sink</code> flag to hold reference to Workflow input. with such setup, caller would be able to disable the sink when needed sending agreed input parameter.</p>"},{"location":"workflows/blocks/email_notification/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/email_notification@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/email_notification/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>subject</code> <code>str</code> Subject of the message.. \u274c <code>sender_email</code> <code>str</code> E-mail to be used to send the message.. \u2705 <code>receiver_email</code> <code>Union[List[str], str]</code> Destination e-mail address.. \u2705 <code>message</code> <code>str</code> Content of the message to be send.. \u274c <code>message_parameters</code> <code>Dict[str, Union[bool, float, int, str]]</code> Data to be used inside the message.. \u2705 <code>message_parameters_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> Preprocessing operations to be performed on message parameters.. \u274c <code>cc_receiver_email</code> <code>Optional[List[str], str]</code> Destination e-mail address.. \u2705 <code>bcc_receiver_email</code> <code>Optional[List[str], str]</code> Destination e-mail address.. \u2705 <code>smtp_server</code> <code>str</code> Custom SMTP server to be used.. \u2705 <code>sender_email_password</code> <code>str</code> Sender e-mail password be used when authenticating to SMTP server.. \u2705 <code>smtp_port</code> <code>int</code> SMTP server port.. \u274c <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <code>disable_sink</code> <code>bool</code> Boolean flag to disable block execution.. \u2705 <code>cooldown_seconds</code> <code>int</code> Number of seconds until a follow-up notification can be sent. . \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/email_notification/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Email Notification</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>sender_email</code> (<code>string</code>): E-mail to be used to send the message..</li> <li><code>receiver_email</code> (Union[<code>string</code>, <code>list_of_values</code>]): Destination e-mail address..</li> <li><code>message_parameters</code> (<code>*</code>): Data to be used inside the message..</li> <li><code>cc_receiver_email</code> (Union[<code>string</code>, <code>list_of_values</code>]): Destination e-mail address..</li> <li><code>bcc_receiver_email</code> (Union[<code>string</code>, <code>list_of_values</code>]): Destination e-mail address..</li> <li><code>attachments</code> (Union[<code>string</code>, <code>bytes</code>]): Attachments.</li> <li><code>smtp_server</code> (<code>string</code>): Custom SMTP server to be used..</li> <li><code>sender_email_password</code> (Union[<code>string</code>, <code>secret</code>]): Sender e-mail password be used when authenticating to SMTP server..</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> <li><code>disable_sink</code> (<code>boolean</code>): Boolean flag to disable block execution..</li> <li><code>cooldown_seconds</code> (<code>integer</code>): Number of seconds until a follow-up notification can be sent. .</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>throttling_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Email Notification</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/email_notification@v1\",\n    \"subject\": \"Workflow alert\",\n    \"sender_email\": \"sender@gmail.com\",\n    \"receiver_email\": \"receiver@gmail.com\",\n    \"message\": \"During last 5 minutes detected {{ $parameters.num_instances }} instances\",\n    \"message_parameters\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"message_parameters_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"cc_receiver_email\": \"cc-receiver@gmail.com\",\n    \"bcc_receiver_email\": \"bcc-receiver@gmail.com\",\n    \"attachments\": {\n        \"report.cvs\": \"$steps.csv_formatter.csv_content\"\n    },\n    \"smtp_server\": \"$inputs.smtp_server\",\n    \"sender_email_password\": \"$inputs.email_password\",\n    \"smtp_port\": 465,\n    \"fire_and_forget\": \"$inputs.fire_and_forget\",\n    \"disable_sink\": false,\n    \"cooldown_seconds\": \"$inputs.cooldown_seconds\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/environment_secrets_store/","title":"Environment Secrets Store","text":"Class: <code>EnvironmentSecretsStoreBlockV1</code> <p>Source: inference.core.workflows.core_steps.secrets_providers.environment_secrets_store.v1.EnvironmentSecretsStoreBlockV1</p> <p>The Environment Secrets Store block is a secure and flexible solution for fetching secrets stored as  environmental variables. It is designed to enable Workflows to access sensitive information,  such as API keys or service credentials, without embedding them directly into the Workflow definitions. </p> <p>This block simplifies the integration of external services while prioritizing security and adaptability. You can use secrets fetched from environment (which can be set by system administrator to be available in self-hosted <code>inference</code> server) to pass as inputs to other steps.</p> <p>Credentials security</p> <p>It is strongly advised to use secrets providers (available when running self-hosted <code>inference</code> server) or workflows parameters to pass credentials. Do not hardcode secrets in Workflows definitions.</p> <p>Blocks limitations</p> <p>This block can only run on self-hosted <code>inference</code> server, we Roboflow does not allow exporting env variables from Hosted Platform due to security concerns. </p>"},{"location":"workflows/blocks/environment_secrets_store/#block-configuration","title":"\ud83d\udee0\ufe0f Block configuration","text":"<p>Block has configuration parameter <code>variables_storing_secrets</code> that must be filled with list of environmental variables which will be exposed as block outputs. Thanks to that, you can use them as inputs for other blocks. Please note that names of outputs will be lowercased. For example, the following settings: <pre><code>variables_storing_secrets=[\"MY_SECRET_A\", \"MY_SECRET_B\"]\n</code></pre> will generate the following outputs:</p> <ul> <li> <p><code>my_secret_a</code></p> </li> <li> <p><code>my_secret_b</code></p> </li> </ul>"},{"location":"workflows/blocks/environment_secrets_store/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/environment_secrets_store@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/environment_secrets_store/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>variables_storing_secrets</code> <code>List[str]</code> List with names of environment variables to fetch. Each will create separate block output.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/environment_secrets_store/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Environment Secrets Store</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> </li> <li> <p>output</p> <ul> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Environment Secrets Store</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/environment_secrets_store@v1\",\n    \"variables_storing_secrets\": [\n        \"MY_API_KEY\",\n        \"OTHER_API_KEY\"\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/expression/","title":"Expression","text":"Class: <code>ExpressionBlockV1</code> <p>Source: inference.core.workflows.core_steps.formatters.expression.v1.ExpressionBlockV1</p> <p>Creates specific output based on defined input variables and configured rules - which is useful while creating business logic in workflows.</p> <p>Based on configuration, block takes input data, optionally performs operation on data,  save it as variables and evaluate switch-case like statements to get the final result.</p>"},{"location":"workflows/blocks/expression/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/expression@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/expression/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>data_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data before switch-case instruction. \u274c <code>switch</code> <code>CasesDefinition</code> Define the logic to be executed.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/expression/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Expression</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): References data to be used to construct results.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Expression</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/expression@v1\",\n    \"data\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"data_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"switch\": {\n        \"cases\": [\n            {\n                \"condition\": {\n                    \"statements\": [\n                        {\n                            \"comparator\": {\n                                \"type\": \"==\"\n                            },\n                            \"left_operand\": {\n                                \"operand_name\": \"class_name\",\n                                \"type\": \"DynamicOperand\"\n                            },\n                            \"right_operand\": {\n                                \"operand_name\": \"reference\",\n                                \"type\": \"DynamicOperand\"\n                            },\n                            \"type\": \"BinaryStatement\"\n                        }\n                    ],\n                    \"type\": \"StatementGroup\"\n                },\n                \"result\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"PASS\"\n                },\n                \"type\": \"CaseDefinition\"\n            }\n        ],\n        \"default\": {\n            \"type\": \"StaticCaseResult\",\n            \"value\": \"FAIL\"\n        },\n        \"type\": \"CasesDefinition\"\n    }\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/first_non_empty_or_default/","title":"First Non Empty Or Default","text":"Class: <code>FirstNonEmptyOrDefaultBlockV1</code> <p>Source: inference.core.workflows.core_steps.formatters.first_non_empty_or_default.v1.FirstNonEmptyOrDefaultBlockV1</p> <p>Takes input data which may not be present due to filtering or conditional execution and fills with default value to make it compliant with further processing.</p>"},{"location":"workflows/blocks/first_non_empty_or_default/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/first_non_empty_or_default@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/first_non_empty_or_default/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>default</code> <code>Any</code> Default value that will be placed whenever there is no data found. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/first_non_empty_or_default/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>First Non Empty Or Default</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): Reference data to replace empty values.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>First Non Empty Or Default</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/first_non_empty_or_default@v1\",\n    \"data\": \"$steps.my_step.predictions\",\n    \"default\": \"empty\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/florence2_model/","title":"Florence-2 Model","text":""},{"location":"workflows/blocks/florence2_model/#v2","title":"v2","text":"Class: <code>Florence2BlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.florence2.v2.Florence2BlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Dedicated inference server required (GPU recommended) - you may want to use dedicated deployment</p> <p>This Workflow block introduces Florence 2, a Visual Language Model (VLM) capable of performing a  wide range of tasks, including:</p> <ul> <li> <p>Object Detection</p> </li> <li> <p>Instance Segmentation</p> </li> <li> <p>Image Captioning</p> </li> <li> <p>Optical Character Recognition (OCR)</p> </li> <li> <p>and more...</p> </li> </ul> <p>Below is a comprehensive list of tasks supported by the model, along with descriptions on  how to utilize their outputs within the Workflows ecosystem:</p> <p>Task Descriptions:</p> <ul> <li> <p>Custom Prompt (<code>custom</code>) - Use free-form prompt to generate a response. Useful with finetuned models.</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Text Detection &amp; Recognition (OCR) (<code>ocr-with-text-detection</code>) - Model detects text regions in the image, and then performs OCR on each detected region</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Captioning (long) (<code>more-detailed-caption</code>) - Model provides a very long description of the image</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Object Detection (<code>open-vocabulary-object-detection</code>) - Model detects and returns the bounding boxes for the provided classes</p> </li> <li> <p>Detection &amp; Captioning (<code>object-detection-and-caption</code>) - Model detects prominent objects and captions them</p> </li> <li> <p>Prompted Object Detection (<code>phrase-grounded-object-detection</code>) - Based on the textual prompt, model detects objects matching the descriptions</p> </li> <li> <p>Prompted Instance Segmentation (<code>phrase-grounded-instance-segmentation</code>) - Based on the textual prompt, model segments objects matching the descriptions</p> </li> <li> <p>Segment Bounding Box (<code>detection-grounded-instance-segmentation</code>) - Model segments the object in the provided bounding box into a polygon</p> </li> <li> <p>Classification of Bounding Box (<code>detection-grounded-classification</code>) - Model classifies the object inside the provided bounding box</p> </li> <li> <p>Captioning of Bounding Box (<code>detection-grounded-caption</code>) - Model captions the object in the provided bounding box</p> </li> <li> <p>Text Recognition (OCR) for Bounding Box (<code>detection-grounded-ocr</code>) - Model performs OCR on the text inside the provided bounding box</p> </li> <li> <p>Regions of Interest proposal (<code>region-proposal</code>) - Model proposes Regions of Interest (Bounding Boxes) in the image</p> </li> </ul>"},{"location":"workflows/blocks/florence2_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/florence_2@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/florence2_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Florence-2 model. \u2705 <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>grounding_detection</code> <code>Optional[List[float], List[int]]</code> Detection to ground Florence-2 model. May be statically provided bounding box <code>[left_top_x, left_top_y, right_bottom_x, right_bottom_y]</code> or result of object-detection model. If the latter is true, one box will be selected based on <code>grounding_selection_mode</code>.. \u2705 <code>grounding_selection_mode</code> <code>str</code> . \u274c <code>model_id</code> <code>str</code> Model to be used. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/florence2_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Florence-2 Model</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Florence-2 model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>grounding_detection</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>, <code>list_of_values</code>]): Detection to ground Florence-2 model. May be statically provided bounding box <code>[left_top_x, left_top_y, right_bottom_x, right_bottom_y]</code> or result of object-detection model. If the latter is true, one box will be selected based on <code>grounding_selection_mode</code>..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Model to be used.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>raw_output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>parsed_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Florence-2 Model</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/florence_2@v2\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"grounding_detection\": \"$steps.detection.predictions\",\n    \"grounding_selection_mode\": \"first\",\n    \"model_id\": \"florence-2-base\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/florence2_model/#v1","title":"v1","text":"Class: <code>Florence2BlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.florence2.v1.Florence2BlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Dedicated inference server required (GPU recommended) - you may want to use dedicated deployment</p> <p>This Workflow block introduces Florence 2, a Visual Language Model (VLM) capable of performing a  wide range of tasks, including:</p> <ul> <li> <p>Object Detection</p> </li> <li> <p>Instance Segmentation</p> </li> <li> <p>Image Captioning</p> </li> <li> <p>Optical Character Recognition (OCR)</p> </li> <li> <p>and more...</p> </li> </ul> <p>Below is a comprehensive list of tasks supported by the model, along with descriptions on  how to utilize their outputs within the Workflows ecosystem:</p> <p>Task Descriptions:</p> <ul> <li> <p>Custom Prompt (<code>custom</code>) - Use free-form prompt to generate a response. Useful with finetuned models.</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Text Detection &amp; Recognition (OCR) (<code>ocr-with-text-detection</code>) - Model detects text regions in the image, and then performs OCR on each detected region</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Captioning (long) (<code>more-detailed-caption</code>) - Model provides a very long description of the image</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Object Detection (<code>open-vocabulary-object-detection</code>) - Model detects and returns the bounding boxes for the provided classes</p> </li> <li> <p>Detection &amp; Captioning (<code>object-detection-and-caption</code>) - Model detects prominent objects and captions them</p> </li> <li> <p>Prompted Object Detection (<code>phrase-grounded-object-detection</code>) - Based on the textual prompt, model detects objects matching the descriptions</p> </li> <li> <p>Prompted Instance Segmentation (<code>phrase-grounded-instance-segmentation</code>) - Based on the textual prompt, model segments objects matching the descriptions</p> </li> <li> <p>Segment Bounding Box (<code>detection-grounded-instance-segmentation</code>) - Model segments the object in the provided bounding box into a polygon</p> </li> <li> <p>Classification of Bounding Box (<code>detection-grounded-classification</code>) - Model classifies the object inside the provided bounding box</p> </li> <li> <p>Captioning of Bounding Box (<code>detection-grounded-caption</code>) - Model captions the object in the provided bounding box</p> </li> <li> <p>Text Recognition (OCR) for Bounding Box (<code>detection-grounded-ocr</code>) - Model performs OCR on the text inside the provided bounding box</p> </li> <li> <p>Regions of Interest proposal (<code>region-proposal</code>) - Model proposes Regions of Interest (Bounding Boxes) in the image</p> </li> </ul>"},{"location":"workflows/blocks/florence2_model/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/florence_2@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/florence2_model/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Florence-2 model. \u2705 <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>grounding_detection</code> <code>Optional[List[float], List[int]]</code> Detection to ground Florence-2 model. May be statically provided bounding box <code>[left_top_x, left_top_y, right_bottom_x, right_bottom_y]</code> or result of object-detection model. If the latter is true, one box will be selected based on <code>grounding_selection_mode</code>.. \u2705 <code>grounding_selection_mode</code> <code>str</code> . \u274c <code>model_version</code> <code>str</code> Model to be used. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/florence2_model/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Florence-2 Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Florence-2 model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>grounding_detection</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>, <code>list_of_values</code>]): Detection to ground Florence-2 model. May be statically provided bounding box <code>[left_top_x, left_top_y, right_bottom_x, right_bottom_y]</code> or result of object-detection model. If the latter is true, one box will be selected based on <code>grounding_selection_mode</code>..</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>raw_output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>parsed_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Florence-2 Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/florence_2@v1\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"grounding_detection\": \"$steps.detection.predictions\",\n    \"grounding_selection_mode\": \"first\",\n    \"model_version\": \"florence-2-base\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/gaze_detection/","title":"Gaze Detection","text":"Class: <code>GazeBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.gaze.v1.GazeBlockV1</p> <p>Run L2CS Gaze detection model on faces in images.</p> <p>This block can: 1. Detect faces in images and estimate their gaze direction 2. Estimate gaze direction on pre-cropped face images</p> <p>The gaze direction is represented by yaw and pitch angles in degrees.</p>"},{"location":"workflows/blocks/gaze_detection/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/gaze@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/gaze_detection/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>do_run_face_detection</code> <code>bool</code> Whether to run face detection. Set to False if input images are pre-cropped face images.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/gaze_detection/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Gaze Detection</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>do_run_face_detection</code> (<code>boolean</code>): Whether to run face detection. Set to False if input images are pre-cropped face images..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>face_predictions</code> (<code>keypoint_detection_prediction</code>): Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object.</li> <li><code>yaw_degrees</code> (<code>float</code>): Float value.</li> <li><code>pitch_degrees</code> (<code>float</code>): Float value.</li> </ul> </li> </ul> Example JSON definition of step <code>Gaze Detection</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/gaze@v1\",\n    \"images\": \"$inputs.image\",\n    \"do_run_face_detection\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/google_gemini/","title":"Google Gemini","text":"Class: <code>GoogleGeminiBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.google_gemini.v1.GoogleGeminiBlockV1</p> <p>Ask a question to Google's Gemini model with vision capabilities.</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>You need to provide your Google AI API key to use the Gemini model. </p> <p>WARNING!</p> <p>This block makes use of <code>/v1beta</code> API of Google Gemini model - the implementation may change  in the future, without guarantee of backward compatibility.</p>"},{"location":"workflows/blocks/google_gemini/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/google_gemini@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/google_gemini/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Gemini model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your Google AI API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit Google Gemini API limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/google_gemini/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Google Gemini</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Gemini model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your Google AI API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Google Gemini</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/google_gemini@v1\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"gemini-2.5-pro\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/google_vision_ocr/","title":"Google Vision OCR","text":"Class: <code>GoogleVisionOCRBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.google_vision_ocr.v1.GoogleVisionOCRBlockV1</p> <p>Detect text in images using Google Vision OCR.</p> <p>Supported types of text detection:</p> <ul> <li><code>text_detection</code>: optimized for areas of text within a larger image.</li> <li><code>ocr_text_detection</code>: optimized for dense text documents.</li> </ul> <p>You need to provide your Google Vision API key to use this block.</p>"},{"location":"workflows/blocks/google_vision_ocr/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/google_vision_ocr@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/google_vision_ocr/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>ocr_type</code> <code>str</code> Type of OCR to use. \u274c <code>language_hints</code> <code>List[str]</code> Optional list of language codes to pass to the OCR API. If not provided, the API will attempt to detect the language automatically.If provided, language codes must be supported by the OCR API, visit https://cloud.google.com/vision/docs/languages for list of supported language codes.. \u274c <code>api_key</code> <code>str</code> Your Google Vision API key. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/google_vision_ocr/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Google Vision OCR</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): Image to run OCR.</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your Google Vision API key.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>text</code> (<code>string</code>): String value.</li> <li><code>language</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Google Vision OCR</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/google_vision_ocr@v1\",\n    \"image\": \"$inputs.image\",\n    \"ocr_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"language_hints\": [\n        \"en\",\n        \"fr\"\n    ],\n    \"api_key\": \"xxx-xxx\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/grid_visualization/","title":"Grid Visualization","text":"Class: <code>GridVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.grid.v1.GridVisualizationBlockV1</p> <p>The <code>GridVisualization</code> block displays an array of images in a grid. It will automatically resize the images to in the specified width and height. The first image will be in the top left corner, and the rest will be added to the right of the previous image until the row is full.</p>"},{"location":"workflows/blocks/grid_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/grid_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/grid_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>width</code> <code>int</code> Width of the output image.. \u2705 <code>height</code> <code>int</code> Height of the output image.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/grid_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Grid Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>list_of_values</code>): Images to visualize.</li> <li><code>width</code> (<code>integer</code>): Width of the output image..</li> <li><code>height</code> (<code>integer</code>): Height of the output image..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Grid Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/grid_visualization@v1\",\n    \"images\": \"$steps.buffer.output\",\n    \"width\": 2560,\n    \"height\": 1440\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/halo_visualization/","title":"Halo Visualization","text":"Class: <code>HaloVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.halo.v1.HaloVisualizationBlockV1</p> <p>The <code>HaloVisualization</code> block uses a detected polygon from an instance segmentation to draw a halo using <code>sv.HaloAnnotator</code>.</p>"},{"location":"workflows/blocks/halo_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/halo_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/halo_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>opacity</code> <code>float</code> Transparency of the halo overlay.. \u2705 <code>kernel_size</code> <code>int</code> Size of the average pooling kernel used for creating the halo.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/halo_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Halo Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the halo overlay..</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the average pooling kernel used for creating the halo..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Halo Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/halo_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.instance_segmentation_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"opacity\": 0.8,\n    \"kernel_size\": 40\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/icon_visualization/","title":"Icon Visualization","text":"Class: <code>IconVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.icon.v1.IconVisualizationBlockV1</p> <p>The <code>IconVisualization</code> block draws icons on an image using Supervision's <code>sv.IconAnnotator</code>. It supports two modes: 1. Static Mode: Position an icon at a fixed location (e.g., for watermarks) 2. Dynamic Mode: Position icons based on detection coordinates</p>"},{"location":"workflows/blocks/icon_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/icon_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/icon_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>mode</code> <code>str</code> Mode for placing icons: 'static' for fixed position (watermark), 'dynamic' for detection-based. \u2705 <code>icon_width</code> <code>int</code> Width of the icon in pixels. \u2705 <code>icon_height</code> <code>int</code> Height of the icon in pixels. \u2705 <code>position</code> <code>str</code> Position relative to detection for dynamic mode. \u2705 <code>x_position</code> <code>int</code> X coordinate for static mode. Positive values from left edge, negative from right edge. \u2705 <code>y_position</code> <code>int</code> Y coordinate for static mode. Positive values from top edge, negative from bottom edge. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/icon_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Icon Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>icon</code> (<code>image</code>): The icon image to place on the input image (PNG with transparency recommended).</li> <li><code>mode</code> (<code>string</code>): Mode for placing icons: 'static' for fixed position (watermark), 'dynamic' for detection-based.</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to place icons on (required for dynamic mode).</li> <li><code>icon_width</code> (<code>integer</code>): Width of the icon in pixels.</li> <li><code>icon_height</code> (<code>integer</code>): Height of the icon in pixels.</li> <li><code>position</code> (<code>string</code>): Position relative to detection for dynamic mode.</li> <li><code>x_position</code> (<code>integer</code>): X coordinate for static mode. Positive values from left edge, negative from right edge.</li> <li><code>y_position</code> (<code>integer</code>): Y coordinate for static mode. Positive values from top edge, negative from bottom edge.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Icon Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/icon_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"icon\": \"$inputs.icon\",\n    \"mode\": \"static\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"icon_width\": 64,\n    \"icon_height\": 64,\n    \"position\": \"TOP_CENTER\",\n    \"x_position\": 10,\n    \"y_position\": 10\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/identify_changes/","title":"Identify Changes","text":"Class: <code>IdentifyChangesBlockV1</code> <p>Source: inference.core.workflows.core_steps.sampling.identify_changes.v1.IdentifyChangesBlockV1</p> <p>Identify changes compared to prior data via embeddings.</p> <p>This block accepts an embedding and compares it to a prior average and standard deviation for the rate of change. When things change faster or slower than they have in the past, the block will flag the data as an outlier.</p>"},{"location":"workflows/blocks/identify_changes/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/identify_changes@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/identify_changes/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>strategy</code> <code>str</code> The change identification algorithm to use.. \u274c <code>threshold_percentile</code> <code>float</code> The desired sensitivity. A higher value will result in more data points being classified as outliers.. \u2705 <code>warmup</code> <code>int</code> The number of data points to use for the initial average calculation. No outliers are identified during this period.. \u2705 <code>smoothing_factor</code> <code>float</code> The smoothing factor for the EMA algorithm. The default of 0.25 means the most recent data point will carry 25% weight in the average. Higher values will make the average more responsive to recent data points.. \u2705 <code>window_size</code> <code>int</code> The number of data points to consider in the sliding window algorithm.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/identify_changes/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Identify Changes</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>embedding</code> (<code>embedding</code>): Embedding of the current data..</li> <li><code>threshold_percentile</code> (<code>float_zero_to_one</code>): The desired sensitivity. A higher value will result in more data points being classified as outliers..</li> <li><code>warmup</code> (<code>integer</code>): The number of data points to use for the initial average calculation. No outliers are identified during this period..</li> <li><code>smoothing_factor</code> (<code>float_zero_to_one</code>): The smoothing factor for the EMA algorithm. The default of 0.25 means the most recent data point will carry 25% weight in the average. Higher values will make the average more responsive to recent data points..</li> <li><code>window_size</code> (<code>integer</code>): The number of data points to consider in the sliding window algorithm..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>is_outlier</code> (<code>boolean</code>): Boolean flag.</li> <li><code>percentile</code> (<code>float_zero_to_one</code>): <code>float</code> value in range <code>[0.0, 1.0]</code>.</li> <li><code>z_score</code> (<code>float</code>): Float value.</li> <li><code>average</code> (<code>embedding</code>): A list of floating point numbers representing a vector embedding..</li> <li><code>std</code> (<code>embedding</code>): A list of floating point numbers representing a vector embedding..</li> <li><code>warming_up</code> (<code>boolean</code>): Boolean flag.</li> </ul> </li> </ul> Example JSON definition of step <code>Identify Changes</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/identify_changes@v1\",\n    \"strategy\": \"Simple Moving Average (SMA)\",\n    \"embedding\": \"$steps.clip.embedding\",\n    \"threshold_percentile\": \"$inputs.sample_rate\",\n    \"warmup\": 100,\n    \"smoothing_factor\": 0.1,\n    \"window_size\": 5\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/identify_outliers/","title":"Identify Outliers","text":"Class: <code>IdentifyOutliersBlockV1</code> <p>Source: inference.core.workflows.core_steps.sampling.identify_outliers.v1.IdentifyOutliersBlockV1</p> <p>Identify outlier embeddings compared to prior data.</p> <p>This block accepts an embedding and compares it to a sample of prior data. If the embedding is an outlier, the block will return a boolean flag and the percentile of the embedding.</p>"},{"location":"workflows/blocks/identify_outliers/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/identify_outliers@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/identify_outliers/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>threshold_percentile</code> <code>float</code> The desired sensitivity. A higher value will result in more data points being classified as outliers.. \u2705 <code>warmup</code> <code>int</code> The number of data points to use for the initial average calculation. No outliers are identified during this period.. \u2705 <code>window_size</code> <code>int</code> The number of previous data points to consider in the sliding window algorithm.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/identify_outliers/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Identify Outliers</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>embedding</code> (<code>embedding</code>): Embedding of the current data..</li> <li><code>threshold_percentile</code> (<code>float_zero_to_one</code>): The desired sensitivity. A higher value will result in more data points being classified as outliers..</li> <li><code>warmup</code> (<code>integer</code>): The number of data points to use for the initial average calculation. No outliers are identified during this period..</li> <li><code>window_size</code> (<code>integer</code>): The number of previous data points to consider in the sliding window algorithm..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>is_outlier</code> (<code>boolean</code>): Boolean flag.</li> <li><code>percentile</code> (<code>float_zero_to_one</code>): <code>float</code> value in range <code>[0.0, 1.0]</code>.</li> <li><code>warming_up</code> (<code>boolean</code>): Boolean flag.</li> </ul> </li> </ul> Example JSON definition of step <code>Identify Outliers</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/identify_outliers@v1\",\n    \"embedding\": \"$steps.clip.embedding\",\n    \"threshold_percentile\": \"$inputs.sample_rate\",\n    \"warmup\": 100,\n    \"window_size\": 5\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_blur/","title":"Image Blur","text":"Class: <code>ImageBlurBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.image_blur.v1.ImageBlurBlockV1</p> <p>Apply a blur to an image.  The blur type and kernel size can be specified.</p>"},{"location":"workflows/blocks/image_blur/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_blur@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_blur/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>blur_type</code> <code>str</code> Type of Blur to perform on image.. \u2705 <code>kernel_size</code> <code>int</code> Size of the average pooling kernel used for blurring.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_blur/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Blur</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>blur_type</code> (<code>string</code>): Type of Blur to perform on image..</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the average pooling kernel used for blurring..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Blur</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_blur@v1\",\n    \"image\": \"$inputs.image\",\n    \"blur_type\": \"average\",\n    \"kernel_size\": 5\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_contours/","title":"Image Contours","text":"Class: <code>ImageContoursDetectionBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.contours.v1.ImageContoursDetectionBlockV1</p> <p>Finds the contours in an image. It returns the contours and number of contours. The input image should be thresholded before using this block.</p>"},{"location":"workflows/blocks/image_contours/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/contours_detection@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_contours/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>line_thickness</code> <code>int</code> Line thickness for drawing contours.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_contours/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Contours</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>line_thickness</code> (<code>integer</code>): Line thickness for drawing contours..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>contours</code> (<code>contours</code>): List of numpy arrays where each array represents contour points.</li> <li><code>hierarchy</code> (<code>numpy_array</code>): Numpy array.</li> <li><code>number_contours</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Contours</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/contours_detection@v1\",\n    \"image\": \"$inputs.image\",\n    \"line_thickness\": 3\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_convert_grayscale/","title":"Image Convert Grayscale","text":"Class: <code>ConvertGrayscaleBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.convert_grayscale.v1.ConvertGrayscaleBlockV1</p> <p>Block to convert an RGB image to grayscale. The output image will have only one channel.</p>"},{"location":"workflows/blocks/image_convert_grayscale/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/convert_grayscale@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_convert_grayscale/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_convert_grayscale/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Convert Grayscale</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Convert Grayscale</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/convert_grayscale@v1\",\n    \"image\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_preprocessing/","title":"Image Preprocessing","text":"Class: <code>ImagePreprocessingBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.image_preprocessing.v1.ImagePreprocessingBlockV1</p> <p>Apply a resize, flip, or rotation step to an image. </p> <p>Width and height are required for resizing. Degrees are required for rotating. Flip type is required for flipping.</p>"},{"location":"workflows/blocks/image_preprocessing/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_preprocessing@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_preprocessing/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Preprocessing task to be applied to the image.. \u274c <code>width</code> <code>int</code> Width of the image to be resized to.. \u2705 <code>height</code> <code>int</code> Height of the image to be resized to.. \u2705 <code>rotation_degrees</code> <code>int</code> Positive value to rotate clockwise, negative value to rotate counterclockwise. \u2705 <code>flip_type</code> <code>str</code> Type of flip to be applied to the image.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_preprocessing/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Preprocessing</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>width</code> (<code>integer</code>): Width of the image to be resized to..</li> <li><code>height</code> (<code>integer</code>): Height of the image to be resized to..</li> <li><code>rotation_degrees</code> (<code>integer</code>): Positive value to rotate clockwise, negative value to rotate counterclockwise.</li> <li><code>flip_type</code> (<code>string</code>): Type of flip to be applied to the image..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Preprocessing</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_preprocessing@v1\",\n    \"image\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"width\": 640,\n    \"height\": 640,\n    \"rotation_degrees\": 90,\n    \"flip_type\": \"vertical\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_slicer/","title":"Image Slicer","text":""},{"location":"workflows/blocks/image_slicer/#v2","title":"v2","text":"Class: <code>ImageSlicerBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.transformations.image_slicer.v2.ImageSlicerBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>This block enables Slicing Adaptive Inference (SAHI) technique in  Workflows providing implementation for first step of procedure - making slices out of input image.</p> <p>To use the block effectively, it must be paired with detection model (object-detection or  instance segmentation) running against output images from this block. At the end -  Detections Stitch block must be applied on top of predictions to merge them as if  the prediction was made against input image, not its slices.</p> <p>We recommend adjusting the size of slices to match the model's input size and the scale of objects in the dataset  the model was trained on. Models generally perform best on data that is similar to what they encountered during  training. The default size of slices is 640, but this might not be optimal if the model's input size is 320, as each  slice would be downsized by a factor of two during inference. Similarly, if the model's input size is 1280, each slice  will be artificially up-scaled. The best setup should be determined experimentally based on the specific data and model  you are using.</p> <p>To learn more about SAHI please visit Roboflow blog which describes the technique in details, yet not in context of Roboflow workflows.</p>"},{"location":"workflows/blocks/image_slicer/#changes-compared-to-v1","title":"Changes compared to v1","text":"<ul> <li> <p>All crops generated by slicer will be of equal size</p> </li> <li> <p>No duplicated crops will be created </p> </li> </ul>"},{"location":"workflows/blocks/image_slicer/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_slicer@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_slicer/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>slice_width</code> <code>int</code> Width of each slice, in pixels. \u2705 <code>slice_height</code> <code>int</code> Height of each slice, in pixels. \u2705 <code>overlap_ratio_width</code> <code>float</code> Overlap ratio between consecutive slices in the width dimension. \u2705 <code>overlap_ratio_height</code> <code>float</code> Overlap ratio between consecutive slices in the height dimension. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_slicer/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Slicer</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>slice_width</code> (<code>integer</code>): Width of each slice, in pixels.</li> <li><code>slice_height</code> (<code>integer</code>): Height of each slice, in pixels.</li> <li><code>overlap_ratio_width</code> (<code>float_zero_to_one</code>): Overlap ratio between consecutive slices in the width dimension.</li> <li><code>overlap_ratio_height</code> (<code>float_zero_to_one</code>): Overlap ratio between consecutive slices in the height dimension.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>slices</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Slicer</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_slicer@v2\",\n    \"image\": \"$inputs.image\",\n    \"slice_width\": 320,\n    \"slice_height\": 320,\n    \"overlap_ratio_width\": 0.2,\n    \"overlap_ratio_height\": 0.2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_slicer/#v1","title":"v1","text":"Class: <code>ImageSlicerBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.transformations.image_slicer.v1.ImageSlicerBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>This block enables Slicing Adaptive Inference (SAHI) technique in  Workflows providing implementation for first step of procedure - making slices out of input image.</p> <p>To use the block effectively, it must be paired with detection model (object-detection or  instance segmentation) running against output images from this block. At the end -  Detections Stitch block must be applied on top of predictions to merge them as if  the prediction was made against input image, not its slices.</p> <p>We recommend adjusting the size of slices to match the model's input size and the scale of objects in the dataset  the model was trained on. Models generally perform best on data that is similar to what they encountered during  training. The default size of slices is 640, but this might not be optimal if the model's input size is 320, as each  slice would be downsized by a factor of two during inference. Similarly, if the model's input size is 1280, each slice  will be artificially up-scaled. The best setup should be determined experimentally based on the specific data and model  you are using.</p> <p>To learn more about SAHI please visit Roboflow blog which describes the technique in details, yet not in context of Roboflow workflows.</p>"},{"location":"workflows/blocks/image_slicer/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_slicer@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_slicer/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>slice_width</code> <code>int</code> Width of each slice, in pixels. \u2705 <code>slice_height</code> <code>int</code> Height of each slice, in pixels. \u2705 <code>overlap_ratio_width</code> <code>float</code> Overlap ratio between consecutive slices in the width dimension. \u2705 <code>overlap_ratio_height</code> <code>float</code> Overlap ratio between consecutive slices in the height dimension. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_slicer/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Slicer</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>slice_width</code> (<code>integer</code>): Width of each slice, in pixels.</li> <li><code>slice_height</code> (<code>integer</code>): Height of each slice, in pixels.</li> <li><code>overlap_ratio_width</code> (<code>float_zero_to_one</code>): Overlap ratio between consecutive slices in the width dimension.</li> <li><code>overlap_ratio_height</code> (<code>float_zero_to_one</code>): Overlap ratio between consecutive slices in the height dimension.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>slices</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Slicer</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_slicer@v1\",\n    \"image\": \"$inputs.image\",\n    \"slice_width\": 320,\n    \"slice_height\": 320,\n    \"overlap_ratio_width\": 0.2,\n    \"overlap_ratio_height\": 0.2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/image_threshold/","title":"Image Threshold","text":"Class: <code>ImageThresholdBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.threshold.v1.ImageThresholdBlockV1</p> <p>Apply a threshold to an image. The image must be in grayscale.</p>"},{"location":"workflows/blocks/image_threshold/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/threshold@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_threshold/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>threshold_type</code> <code>str</code> Type of Edge Detection to perform.. \u2705 <code>thresh_value</code> <code>int</code> Threshold value.. \u2705 <code>max_value</code> <code>int</code> Maximum value for thresholding. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_threshold/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Threshold</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>threshold_type</code> (<code>string</code>): Type of Edge Detection to perform..</li> <li><code>thresh_value</code> (<code>integer</code>): Threshold value..</li> <li><code>max_value</code> (<code>integer</code>): Maximum value for thresholding.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Threshold</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/threshold@v1\",\n    \"image\": \"$inputs.image\",\n    \"threshold_type\": \"binary\",\n    \"thresh_value\": 127,\n    \"max_value\": 255\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/instance_segmentation_model/","title":"Instance Segmentation Model","text":""},{"location":"workflows/blocks/instance_segmentation_model/#v2","title":"v2","text":"Class: <code>RoboflowInstanceSegmentationModelBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.instance_segmentation.v2.RoboflowInstanceSegmentationModelBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on an instance segmentation model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_instance_segmentation_model@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>class_filter</code> <code>List[str]</code> List of accepted classes. Classes must exist in the model's training set.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more.. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return.. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Boolean flag to specify if NMS is to be used in class-agnostic mode.. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>mask_decode_mode</code> <code>str</code> Parameter of mask decoding in prediction post-processing.. \u2705 <code>tradeoff_factor</code> <code>float</code> Post-processing parameter to dictate tradeoff between fast and accurate.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Instance Segmentation Model</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of accepted classes. Classes must exist in the model's training set..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more..</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return..</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Boolean flag to specify if NMS is to be used in class-agnostic mode..</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>mask_decode_mode</code> (<code>string</code>): Parameter of mask decoding in prediction post-processing..</li> <li><code>tradeoff_factor</code> (<code>float_zero_to_one</code>): Post-processing parameter to dictate tradeoff between fast and accurate..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model id.</li> </ul> </li> </ul> Example JSON definition of step <code>Instance Segmentation Model</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_instance_segmentation_model@v2\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"class_agnostic_nms\": true,\n    \"max_candidates\": 3000,\n    \"mask_decode_mode\": \"accurate\",\n    \"tradeoff_factor\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/instance_segmentation_model/#v1","title":"v1","text":"Class: <code>RoboflowInstanceSegmentationModelBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.instance_segmentation.v1.RoboflowInstanceSegmentationModelBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on an instance segmentation model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_instance_segmentation_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>class_filter</code> <code>List[str]</code> List of accepted classes. Classes must exist in the model's training set.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more.. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return.. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Boolean flag to specify if NMS is to be used in class-agnostic mode.. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>mask_decode_mode</code> <code>str</code> Parameter of mask decoding in prediction post-processing.. \u2705 <code>tradeoff_factor</code> <code>float</code> Post-processing parameter to dictate tradeoff between fast and accurate.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Instance Segmentation Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of accepted classes. Classes must exist in the model's training set..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more..</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return..</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Boolean flag to specify if NMS is to be used in class-agnostic mode..</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>mask_decode_mode</code> (<code>string</code>): Parameter of mask decoding in prediction post-processing..</li> <li><code>tradeoff_factor</code> (<code>float_zero_to_one</code>): Post-processing parameter to dictate tradeoff between fast and accurate..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Instance Segmentation Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_instance_segmentation_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"class_agnostic_nms\": true,\n    \"max_candidates\": 3000,\n    \"mask_decode_mode\": \"accurate\",\n    \"tradeoff_factor\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/json_parser/","title":"JSON Parser","text":"Class: <code>JSONParserBlockV1</code> <p>Source: inference.core.workflows.core_steps.formatters.json_parser.v1.JSONParserBlockV1</p> <p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to JSON, and its keys are exposed as block outputs.</p> <p>Accepted formats: - valid JSON strings - JSON documents wrapped with Markdown tags (very common for GPT responses) <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever at least one of <code>expected_fields</code> cannot be retrieved from input</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed and returned, while <code>error_status</code> will remain <code>False</code></p> </li> </ul>"},{"location":"workflows/blocks/json_parser/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/json_parser@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/json_parser/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>raw_json</code> <code>str</code> The string with raw JSON to parse.. \u2705 <code>expected_fields</code> <code>List[str]</code> List of expected JSON fields. <code>error_status</code> field name is reserved and cannot be used.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/json_parser/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>JSON Parser</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>raw_json</code> (<code>language_model_output</code>): The string with raw JSON to parse..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>JSON Parser</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/json_parser@v1\",\n    \"raw_json\": [\n        \"$steps.lmm.output\"\n    ],\n    \"expected_fields\": [\n        \"field_a\",\n        \"field_b\"\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/keypoint_detection_model/","title":"Keypoint Detection Model","text":""},{"location":"workflows/blocks/keypoint_detection_model/#v2","title":"v2","text":"Class: <code>RoboflowKeypointDetectionModelBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.keypoint_detection.v2.RoboflowKeypointDetectionModelBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a keypoint detection model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_keypoint_detection_model@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>keypoint_confidence</code> <code>float</code> Confidence threshold to predict a keypoint as visible.. \u2705 <code>class_filter</code> <code>List[str]</code> List of accepted classes. Classes must exist in the model's training set.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more.. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return.. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Boolean flag to specify if NMS is to be used in class-agnostic mode.. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Keypoint Detection Model</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>keypoint_confidence</code> (<code>float_zero_to_one</code>): Confidence threshold to predict a keypoint as visible..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of accepted classes. Classes must exist in the model's training set..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more..</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return..</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Boolean flag to specify if NMS is to be used in class-agnostic mode..</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> <li><code>predictions</code> (<code>keypoint_detection_prediction</code>): Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model id.</li> </ul> </li> </ul> Example JSON definition of step <code>Keypoint Detection Model</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_keypoint_detection_model@v2\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"keypoint_confidence\": 0.3,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"class_agnostic_nms\": true,\n    \"max_candidates\": 3000,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/keypoint_detection_model/#v1","title":"v1","text":"Class: <code>RoboflowKeypointDetectionModelBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.keypoint_detection.v1.RoboflowKeypointDetectionModelBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a keypoint detection model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_keypoint_detection_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>keypoint_confidence</code> <code>float</code> Confidence threshold to predict a keypoint as visible.. \u2705 <code>class_filter</code> <code>List[str]</code> List of accepted classes. Classes must exist in the model's training set.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more.. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return.. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Boolean flag to specify if NMS is to be used in class-agnostic mode.. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Keypoint Detection Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>keypoint_confidence</code> (<code>float_zero_to_one</code>): Confidence threshold to predict a keypoint as visible..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of accepted classes. Classes must exist in the model's training set..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more..</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return..</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Boolean flag to specify if NMS is to be used in class-agnostic mode..</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>keypoint_detection_prediction</code>): Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Keypoint Detection Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_keypoint_detection_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"keypoint_confidence\": 0.3,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"class_agnostic_nms\": true,\n    \"max_candidates\": 3000,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/keypoint_visualization/","title":"Keypoint Visualization","text":"Class: <code>KeypointVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.keypoint.v1.KeypointVisualizationBlockV1</p> <p>The <code>KeypointVisualization</code> block uses a detections from an keypoint detection model to draw keypoints on objects using <code>sv.VertexAnnotator</code>.</p>"},{"location":"workflows/blocks/keypoint_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/keypoint_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/keypoint_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>annotator_type</code> <code>str</code> Type of annotator to be used for keypoint visualization.. \u274c <code>color</code> <code>str</code> Color of the keypoint.. \u2705 <code>text_color</code> <code>str</code> Text color of the keypoint.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text characters.. \u2705 <code>text_padding</code> <code>int</code> Padding around the text in pixels.. \u2705 <code>thickness</code> <code>int</code> Thickness of the outline in pixels.. \u2705 <code>radius</code> <code>int</code> Radius of the keypoint in pixels.. \u2705 <code>edges</code> <code>List[Any]</code> Mapping of keypoints to edges. List of pairs of indices.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/keypoint_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Keypoint Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (<code>keypoint_detection_prediction</code>): Predictions.</li> <li><code>color</code> (<code>string</code>): Color of the keypoint..</li> <li><code>text_color</code> (<code>string</code>): Text color of the keypoint..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text characters..</li> <li><code>text_padding</code> (<code>integer</code>): Padding around the text in pixels..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the outline in pixels..</li> <li><code>radius</code> (<code>integer</code>): Radius of the keypoint in pixels..</li> <li><code>edges</code> (<code>list_of_values</code>): Mapping of keypoints to edges. List of pairs of indices..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Keypoint Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/keypoint_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.keypoint_detection_model.predictions\",\n    \"annotator_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"color\": \"#A351FB\",\n    \"text_color\": \"black\",\n    \"text_scale\": 0.5,\n    \"text_thickness\": 1,\n    \"text_padding\": 10,\n    \"thickness\": 2,\n    \"radius\": 10,\n    \"edges\": \"$inputs.edges\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/label_visualization/","title":"Label Visualization","text":"Class: <code>LabelVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.label.v1.LabelVisualizationBlockV1</p> <p>The <code>LabelVisualization</code> block draws labels on an image at specific coordinates based on provided detections using Supervision's <code>sv.LabelAnnotator</code>.</p>"},{"location":"workflows/blocks/label_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/label_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/label_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>text</code> <code>str</code> The data to display in the text labels.. \u2705 <code>text_position</code> <code>str</code> The anchor position for placing the label.. \u2705 <code>text_color</code> <code>str</code> Color of the text.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text characters.. \u2705 <code>text_padding</code> <code>int</code> Padding around the text in pixels.. \u2705 <code>border_radius</code> <code>int</code> Radius of the label in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/label_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Label Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>text</code> (<code>string</code>): The data to display in the text labels..</li> <li><code>text_position</code> (<code>string</code>): The anchor position for placing the label..</li> <li><code>text_color</code> (<code>string</code>): Color of the text..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text characters..</li> <li><code>text_padding</code> (<code>integer</code>): Padding around the text in pixels..</li> <li><code>border_radius</code> (<code>integer</code>): Radius of the label in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Label Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/label_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"text\": \"LABEL\",\n    \"text_position\": \"CENTER\",\n    \"text_color\": \"WHITE\",\n    \"text_scale\": 1.0,\n    \"text_thickness\": 1,\n    \"text_padding\": 10,\n    \"border_radius\": 0\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/line_counter/","title":"Line Counter","text":""},{"location":"workflows/blocks/line_counter/#v2","title":"v2","text":"Class: <code>LineCounterBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.line_counter.v2.LineCounterBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>LineCounter</code> is an analytics block designed to count objects passing the line. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/line_counter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/line_counter@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/line_counter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>line_segment</code> <code>List[Any]</code> Line consisting of exactly two points. For line [[0, 100], [100, 100]], objects entering from the bottom will count as IN.. \u2705 <code>triggering_anchor</code> <code>str</code> The point on the detection that must cross the line to be counted.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/line_counter/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Line Counter</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to count line crossings for..</li> <li><code>line_segment</code> (<code>list_of_values</code>): Line consisting of exactly two points. For line [[0, 100], [100, 100]], objects entering from the bottom will count as IN..</li> <li><code>triggering_anchor</code> (<code>string</code>): The point on the detection that must cross the line to be counted..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>count_in</code> (<code>integer</code>): Integer value.</li> <li><code>count_out</code> (<code>integer</code>): Integer value.</li> <li><code>detections_in</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> <li><code>detections_out</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Line Counter</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/line_counter@v2\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"line_segment\": [\n        [\n            0,\n            50\n        ],\n        [\n            500,\n            50\n        ]\n    ],\n    \"triggering_anchor\": \"CENTER\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/line_counter/#v1","title":"v1","text":"Class: <code>LineCounterBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.line_counter.v1.LineCounterBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>LineCounter</code> is an analytics block designed to count objects passing the line. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/line_counter/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/line_counter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/line_counter/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>line_segment</code> <code>List[Any]</code> Line consisting of exactly two points. For line [[0, 100], [100, 100]], objects entering from the bottom will count as IN.. \u2705 <code>triggering_anchor</code> <code>str</code> The point on the detection that must cross the line to be counted.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/line_counter/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Line Counter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Predictions.</li> <li><code>line_segment</code> (<code>list_of_values</code>): Line consisting of exactly two points. For line [[0, 100], [100, 100]], objects entering from the bottom will count as IN..</li> <li><code>triggering_anchor</code> (<code>string</code>): The point on the detection that must cross the line to be counted..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>count_in</code> (<code>integer</code>): Integer value.</li> <li><code>count_out</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Line Counter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/line_counter@v1\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"line_segment\": [\n        [\n            0,\n            50\n        ],\n        [\n            500,\n            50\n        ]\n    ],\n    \"triggering_anchor\": \"CENTER\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/line_counter_visualization/","title":"Line Counter Visualization","text":"Class: <code>LineCounterZoneVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.line_zone.v1.LineCounterZoneVisualizationBlockV1</p> <p>The <code>LineCounterZoneVisualization</code> block draws line in an image with a specified color and opacity. Please note: line zone will be drawn on top of image passed to this block, this block should be placed before other visualization blocks in the workflow.</p>"},{"location":"workflows/blocks/line_counter_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/line_counter_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/line_counter_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>zone</code> <code>List[Any]</code> Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points.. \u2705 <code>color</code> <code>str</code> Color of the zone.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text in pixels.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>count_in</code> <code>int</code> Reference to the number of objects that crossed into the line zone.. \u2705 <code>count_out</code> <code>int</code> Reference to the number of objects that crossed out of the line zone.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/line_counter_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Line Counter Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>zone</code> (<code>list_of_values</code>): Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points..</li> <li><code>color</code> (<code>string</code>): Color of the zone..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text in pixels..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>count_in</code> (<code>integer</code>): Reference to the number of objects that crossed into the line zone..</li> <li><code>count_out</code> (<code>integer</code>): Reference to the number of objects that crossed out of the line zone..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Line Counter Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/line_counter_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"zone\": [\n        [\n            0,\n            50\n        ],\n        [\n            500,\n            50\n        ]\n    ],\n    \"color\": \"WHITE\",\n    \"thickness\": 2,\n    \"text_thickness\": 1,\n    \"text_scale\": 1.0,\n    \"count_in\": \"$steps.line_counter.count_in\",\n    \"count_out\": \"$steps.line_counter.count_out\",\n    \"opacity\": 0.3\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/llama3.2_vision/","title":"Llama 3.2 Vision","text":"Class: <code>LlamaVisionBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.llama_vision.v1.LlamaVisionBlockV1</p> <p>Ask a question to Llama 3.2 Vision model with vision capabilities.</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>Issues with structured prompting</p> <p>Model tends to be quite unpredictable when structured output (in our case JSON document) is expected. That problems may impact tasks like <code>structured-answering</code>, <code>classification</code> or <code>multi-label-classification</code>.</p> <p>The cause seems to be quite sensitive \"filters\" of inappropriate content embedded in model.</p>"},{"location":"workflows/blocks/llama3.2_vision/#api-providers-and-model-variants","title":"\ud83d\udee0\ufe0f API providers and model variants","text":"<p>Llama Vision 3.2 model is exposed via OpenRouter API and we require  passing OpenRouter API Key to run.</p> <p>There are different versions of the model supported:</p> <ul> <li> <p>smaller version (<code>11B</code>) is faster and cheaper, yet you can expect better quality of results using <code>90B</code> version</p> </li> <li> <p><code>Regular</code> version is paid (and usually faster) API, whereas <code>Free</code> is free for use for OpenRouter clients  (state at 01.01.2025)</p> </li> </ul> <p>As for now, OpenRouter is the only provider for Llama 3.2 Vision model, but we will keep you posted if the state of the matter changes.</p> <p>API Usage Charges</p> <p>OpenRouter is external third party providing access to the model and incurring charges on the usage. Please check out pricing before use:</p> <ul> <li> <p>Llama 3.2 Vision 11B (Regular)</p> </li> <li> <p>Llama 3.2 Vision 90B (Regular)</p> </li> </ul>"},{"location":"workflows/blocks/llama3.2_vision/#further-reading-and-acceptable-use-policy","title":"\ud83d\udca1 Further reading and Acceptable Use Policy","text":"<p>Model license</p> <p>Check out model license before use. </p> <p>Click here for the original model card.</p> <p>Usage of this model is subject to Meta's Acceptable Use Policy.</p>"},{"location":"workflows/blocks/llama3.2_vision/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/llama_3_2_vision@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/llama3.2_vision/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Llama model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your Llama Vision API key (dependent on provider, ex: OpenRouter API key). \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/llama3.2_vision/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Llama 3.2 Vision</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Llama model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (<code>string</code>): Your Llama Vision API key (dependent on provider, ex: OpenRouter API key).</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Llama 3.2 Vision</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"11B (Free) - OpenRouter\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/lmm/","title":"LMM","text":"Class: <code>LMMBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.lmm.v1.LMMBlockV1</p> <p>Ask a question to a Large Multimodal Model (LMM) with an image and text.</p> <p>You can specify arbitrary text prompts to an LMMBlock.</p> <p>The LLMBlock supports two LMMs:</p> <ul> <li>OpenAI's GPT-4 with Vision;</li> </ul> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. </p> <p>If you want to classify an image into one or more categories, we recommend using the  dedicated LMMForClassificationBlock.</p>"},{"location":"workflows/blocks/lmm/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/lmm@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/lmm/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Holds unconstrained text prompt to LMM mode. \u2705 <code>lmm_type</code> <code>str</code> Type of LMM to be used. \u2705 <code>lmm_config</code> <code>LMMConfig</code> Configuration of LMM. \u274c <code>remote_api_key</code> <code>str</code> Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code>.. \u2705 <code>json_output</code> <code>Dict[str, str]</code> Holds dictionary that maps name of requested output field into its description. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/lmm/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>LMM</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Holds unconstrained text prompt to LMM mode.</li> <li><code>lmm_type</code> (<code>string</code>): Type of LMM to be used.</li> <li><code>remote_api_key</code> (Union[<code>string</code>, <code>secret</code>]): Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code>..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>structured_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>LMM</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/lmm@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"lmm_type\": \"gpt_4v\",\n    \"lmm_config\": {\n        \"gpt_image_detail\": \"low\",\n        \"gpt_model_version\": \"gpt-4o\",\n        \"max_tokens\": 200\n    },\n    \"remote_api_key\": \"xxx-xxx\",\n    \"json_output\": {\n        \"count\": \"number of cats in the picture\"\n    }\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/lmm_for_classification/","title":"LMM For Classification","text":"Class: <code>LMMForClassificationBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.lmm_classifier.v1.LMMForClassificationBlockV1</p> <p>Classify an image into one or more categories using a Large Multimodal Model (LMM).</p> <p>You can specify arbitrary classes to an LMMBlock.</p> <p>The LLMBlock supports two LMMs:</p> <ul> <li>OpenAI's GPT-4 with Vision.</li> </ul> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. </p>"},{"location":"workflows/blocks/lmm_for_classification/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/lmm_for_classification@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/lmm_for_classification/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>lmm_type</code> <code>str</code> Type of LMM to be used. \u2705 <code>classes</code> <code>List[str]</code> List of classes that LMM shall classify against. \u2705 <code>lmm_config</code> <code>LMMConfig</code> Configuration of LMM. \u274c <code>remote_api_key</code> <code>str</code> Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code>.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/lmm_for_classification/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>LMM For Classification</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>lmm_type</code> (<code>string</code>): Type of LMM to be used.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes that LMM shall classify against.</li> <li><code>remote_api_key</code> (Union[<code>string</code>, <code>secret</code>]): Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code>..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>top</code> (<code>top_class</code>): String value representing top class predicted by classification model.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>LMM For Classification</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/lmm_for_classification@v1\",\n    \"images\": \"$inputs.image\",\n    \"lmm_type\": \"gpt_4v\",\n    \"classes\": [\n        \"a\",\n        \"b\"\n    ],\n    \"lmm_config\": {\n        \"gpt_image_detail\": \"low\",\n        \"gpt_model_version\": \"gpt-4o\",\n        \"max_tokens\": 200\n    },\n    \"remote_api_key\": \"xxx-xxx\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/local_file_sink/","title":"Local File Sink","text":"Class: <code>LocalFileSinkBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.local_file.v1.LocalFileSinkBlockV1</p> <p>The Local File Sink block saves workflow data as files on a local file system. It allows users to configure how  the data is stored, either:</p> <ul> <li> <p>aggregating multiple entries into a single file</p> </li> <li> <p>or saving each entry as a separate file. </p> </li> </ul> <p>This block is useful for logging, data export, or preparing files for subsequent processing.</p>"},{"location":"workflows/blocks/local_file_sink/#file-content-file-type-and-output-mode","title":"File Content, File Type and Output Mode","text":"<p><code>content</code> is expected to be the output from another block producing string values of specific types denoted by <code>file_type</code>.</p> <p><code>output_mode</code> set into <code>append_log</code> will make the block appending single file with consecutive entries passed to <code>content</code> input up to <code>max_entries_per_file</code>. In this mode it is important that </p> <p><code>file_type</code> in <code>append_log</code> mode</p> <p>Contrary to <code>separate_files</code> output mode, <code>append_log</code> mode may introduce subtle changes into the structure of the <code>content</code> to properly append it into existing file, hence setting proper <code>file_type</code> is crucial:</p> <ul> <li> <p><code>file_type=json</code>: in <code>append_log</code> mode, the block will create <code>*.jsonl</code> file in  JSON Lines format - for that to be possible, each JSON document will be parsed and dumped to ensure that it will fit into single line.</p> </li> <li> <p><code>file_type=csv</code>: in <code>append_log</code> mode, the block will deduct the first line from the  content (making it required for CSV content to always be shipped with header row) of  consecutive updates into the content of already created file.</p> </li> </ul> <p>Security considerations</p> <p>The block has an ability to write to the file system. If you find this unintended in your system,  you can disable the block setting environmental variable <code>ALLOW_WORKFLOW_BLOCKS_ACCESSING_LOCAL_STORAGE=False</code> in the environment which host Workflows Execution Engine.</p> <p>If you want to restrict the directory which may be used to write data - set  environmental variable <code>WORKFLOW_BLOCKS_WRITE_DIRECTORY</code> to the absolute path of directory which you allow to be used.</p>"},{"location":"workflows/blocks/local_file_sink/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/local_file_sink@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/local_file_sink/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>file_type</code> <code>str</code> Type of the file. \u274c <code>output_mode</code> <code>str</code> Decides how to organise the content of the file. \u274c <code>target_directory</code> <code>str</code> Target directory. \u2705 <code>file_name_prefix</code> <code>str</code> File name prefix. \u2705 <code>max_entries_per_file</code> <code>int</code> Defines how many datapoints can be appended to a single file. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/local_file_sink/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Local File Sink</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>content</code> (<code>string</code>): Content of the file to save.</li> <li><code>target_directory</code> (<code>string</code>): Target directory.</li> <li><code>file_name_prefix</code> (<code>string</code>): File name prefix.</li> <li><code>max_entries_per_file</code> (<code>string</code>): Defines how many datapoints can be appended to a single file.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Local File Sink</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/local_file_sink@v1\",\n    \"content\": \"$steps.csv_formatter.csv_content\",\n    \"file_type\": \"csv\",\n    \"output_mode\": \"append_log\",\n    \"target_directory\": \"some/location\",\n    \"file_name_prefix\": \"my_file\",\n    \"max_entries_per_file\": 1024\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/mask_visualization/","title":"Mask Visualization","text":"Class: <code>MaskVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.mask.v1.MaskVisualizationBlockV1</p> <p>The <code>MaskVisualization</code> block uses a detected polygon from an instance segmentation to draw a mask using <code>sv.MaskAnnotator</code>.</p>"},{"location":"workflows/blocks/mask_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/mask_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/mask_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/mask_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Mask Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Mask Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/mask_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.instance_segmentation_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"opacity\": 0.5\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/model_comparison_visualization/","title":"Model Comparison Visualization","text":"Class: <code>ModelComparisonVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.model_comparison.v1.ModelComparisonVisualizationBlockV1</p> <p>The <code>ModelComparisonVisualization</code> block draws all areas predicted by neither model with a specified color, lets overlapping areas of the predictions shine through, and colors areas predicted by only one model with a distinct color.</p>"},{"location":"workflows/blocks/model_comparison_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/model_comparison_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/model_comparison_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_a</code> <code>str</code> Color of the areas Model A predicted that Model B did not... \u2705 <code>color_b</code> <code>str</code> Color of the areas Model B predicted that Model A did not.. \u2705 <code>background_color</code> <code>str</code> Color of the areas neither model predicted.. \u2705 <code>opacity</code> <code>float</code> Transparency of the overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/model_comparison_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Model Comparison Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions_a</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Predictions.</li> <li><code>color_a</code> (<code>string</code>): Color of the areas Model A predicted that Model B did not...</li> <li><code>predictions_b</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Predictions.</li> <li><code>color_b</code> (<code>string</code>): Color of the areas Model B predicted that Model A did not..</li> <li><code>background_color</code> (<code>string</code>): Color of the areas neither model predicted..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Model Comparison Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/model_comparison_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions_a\": \"$steps.object_detection_model.predictions\",\n    \"color_a\": \"GREEN\",\n    \"predictions_b\": \"$steps.object_detection_model.predictions\",\n    \"color_b\": \"RED\",\n    \"background_color\": \"BLACK\",\n    \"opacity\": 0.7\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/","title":"Model Monitoring Inference Aggregator","text":"Class: <code>ModelMonitoringInferenceAggregatorBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.roboflow.model_monitoring_inference_aggregator.v1.ModelMonitoringInferenceAggregatorBlockV1</p> <p>This block \ud83d\udcca transforms inference data reporting to a whole new level by  periodically aggregating and sending a curated sample of predictions to  Roboflow Model Monitoring.</p>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p>Effortless Aggregation: Collects and organizes predictions in-memory, ensuring only the most relevant  and confident predictions are reported.</p> </li> <li> <p>Customizable Reporting Intervals: Choose how frequently (in seconds) data should be sent\u2014ensuring  optimal balance between granularity and resource efficiency.</p> </li> <li> <p>Debug-Friendly Mode: Fine-tune operations by enabling or disabling asynchronous background execution.</p> </li> </ul>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/#why-use-this-block","title":"\ud83d\udd0d Why Use This Block?","text":"<p>This block is a game-changer for projects relying on video processing in Workflows.  With its aggregation process, it identifies the most confident predictions across classes and sends  them at regular intervals in small messages to Roboflow backend - ensuring that video processing  performance is impacted to the least extent.</p> <p>Perfect for:</p> <ul> <li> <p>Monitoring production line performance in real-time \ud83c\udfed.</p> </li> <li> <p>Debugging and validating your model\u2019s performance over time \u23f1\ufe0f.</p> </li> <li> <p>Providing actionable insights from inference workflows with minimal overhead \ud83d\udd27.</p> </li> </ul>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/#limitations","title":"\ud83d\udea8 Limitations","text":"<ul> <li> <p>The block is should not be relied on when running Workflow in <code>inference</code> server or via HTTP request to Roboflow  hosted platform, as the internal state is not persisted in a memory that would be accessible for all requests to the server, causing aggregation to only have a scope of single request. We will solve that problem in future  releases if proven to be serious limitation for clients.</p> </li> <li> <p>This block do not have ability to separate aggregations for multiple videos processed by <code>InferencePipeline</code> -  effectively aggregating data for all video feeds connected to single process running <code>InferencePipeline</code>. </p> </li> </ul>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/model_monitoring_inference_aggregator@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>frequency</code> <code>int</code> Frequency of reporting (in seconds). For example, if 5 is provided, the block will report an aggregated sample of predictions every 5 seconds.. \u2705 <code>unique_aggregator_key</code> <code>str</code> Unique key used internally to track the session of inference results reporting. Must be unique for each step in your Workflow.. \u274c <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/model_monitoring_inference_aggregator/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Model Monitoring Inference Aggregator</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>classification_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to report to Roboflow Model Monitoring..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Model ID to report to Roboflow Model Monitoring..</li> <li><code>frequency</code> (<code>string</code>): Frequency of reporting (in seconds). For example, if 5 is provided, the block will report an aggregated sample of predictions every 5 seconds..</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Model Monitoring Inference Aggregator</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/model_monitoring_inference_aggregator@v1\",\n    \"predictions\": \"$steps.my_step.predictions\",\n    \"model_id\": \"my_project/3\",\n    \"frequency\": \"3\",\n    \"unique_aggregator_key\": \"session-1v73kdhfse\",\n    \"fire_and_forget\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/moondream2/","title":"Moondream2","text":"Class: <code>Moondream2BlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.moondream2.v1.Moondream2BlockV1</p> <p>This workflow block runs Moondream2, a multimodal vision-language model. You can use this block to run zero-shot object detection.</p>"},{"location":"workflows/blocks/moondream2/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/moondream2@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/moondream2/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Optional text prompt to provide additional context to Moondream2.. \u2705 <code>model_version</code> <code>str</code> The Moondream2 model to be used for inference.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/moondream2/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Moondream2</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Optional text prompt to provide additional context to Moondream2..</li> <li><code>model_version</code> (<code>roboflow_model_id</code>): The Moondream2 model to be used for inference..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Moondream2</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/moondream2@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"model_version\": \"moondream2/moondream2_2b_jul24\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/morphological_transformation/","title":"Morphological Transformation","text":"Class: <code>MorphologicalTransformationBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.morphological_transformation.v1.MorphologicalTransformationBlockV1</p> <p>Apply morphological transformation to an image</p> <p>See https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html</p> <p>Note: This block currently only supports single-channel (grayscale) images. Color images will be converted to grayscale before processing.</p>"},{"location":"workflows/blocks/morphological_transformation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/morphological_transformation@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/morphological_transformation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>kernel_size</code> <code>int</code> Size of the kernel to use for morphological transformation.. \u2705 <code>operation</code> <code>str</code> Type of morphological operation to use.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/morphological_transformation/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Morphological Transformation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the kernel to use for morphological transformation..</li> <li><code>operation</code> (<code>string</code>): Type of morphological operation to use..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Morphological Transformation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/morphological_transformation@v1\",\n    \"image\": \"$inputs.image\",\n    \"kernel_size\": \"5\",\n    \"operation\": \"Closing\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/multi_label_classification_model/","title":"Multi-Label Classification Model","text":""},{"location":"workflows/blocks/multi_label_classification_model/#v2","title":"v2","text":"Class: <code>RoboflowMultiLabelClassificationModelBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.multi_label_classification.v2.RoboflowMultiLabelClassificationModelBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a multi-label classification model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_multi_label_classification_model@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Multi-Label Classification Model</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model id.</li> </ul> </li> </ul> Example JSON definition of step <code>Multi-Label Classification Model</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_multi_label_classification_model@v2\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/multi_label_classification_model/#v1","title":"v1","text":"Class: <code>RoboflowMultiLabelClassificationModelBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.multi_label_classification.v1.RoboflowMultiLabelClassificationModelBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a multi-label classification model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_multi_label_classification_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Multi-Label Classification Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Multi-Label Classification Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_multi_label_classification_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/object_detection_model/","title":"Object Detection Model","text":""},{"location":"workflows/blocks/object_detection_model/#v2","title":"v2","text":"Class: <code>RoboflowObjectDetectionModelBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.object_detection.v2.RoboflowObjectDetectionModelBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a object-detection model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/object_detection_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_object_detection_model@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/object_detection_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>class_filter</code> <code>List[str]</code> List of accepted classes. Classes must exist in the model's training set.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more.. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return.. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Boolean flag to specify if NMS is to be used in class-agnostic mode.. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/object_detection_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Object Detection Model</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of accepted classes. Classes must exist in the model's training set..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more..</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return..</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Boolean flag to specify if NMS is to be used in class-agnostic mode..</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model id.</li> </ul> </li> </ul> Example JSON definition of step <code>Object Detection Model</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"class_agnostic_nms\": true,\n    \"max_candidates\": 3000,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/object_detection_model/#v1","title":"v1","text":"Class: <code>RoboflowObjectDetectionModelBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.object_detection.v1.RoboflowObjectDetectionModelBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a object-detection model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/object_detection_model/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_object_detection_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/object_detection_model/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>class_filter</code> <code>List[str]</code> List of accepted classes. Classes must exist in the model's training set.. \u2705 <code>iou_threshold</code> <code>float</code> Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more.. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return.. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Boolean flag to specify if NMS is to be used in class-agnostic mode.. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/object_detection_model/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Object Detection Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of accepted classes. Classes must exist in the model's training set..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Minimum overlap threshold between boxes to combine them into a single detection, used in NMS. Learn more..</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return..</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Boolean flag to specify if NMS is to be used in class-agnostic mode..</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Object Detection Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"class_agnostic_nms\": true,\n    \"max_candidates\": 3000,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/ocr_model/","title":"OCR Model","text":"Class: <code>OCRModelBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.ocr.v1.OCRModelBlockV1</p> <p>Retrieve the characters in an image using DocTR Optical Character Recognition (OCR).</p> <p>This block returns the text within an image.</p> <p>You may want to use this block in combination with a detections-based block (i.e. ObjectDetectionBlock). An object detection model could isolate specific regions from an image (i.e. a shipping container ID in a logistics use case) for further processing. You can then use a DynamicCropBlock to crop the region of interest before running OCR.</p> <p>Using a detections model then cropping detections allows you to isolate your analysis on particular regions of an image.</p>"},{"location":"workflows/blocks/ocr_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/ocr_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/ocr_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/ocr_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>OCR Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>result</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>OCR Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/ocr_model@v1\",\n    \"images\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/open_ai/","title":"OpenAI","text":""},{"location":"workflows/blocks/open_ai/#v3","title":"v3","text":"Class: <code>OpenAIBlockV3</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.openai.v3.OpenAIBlockV3</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Ask a question to OpenAI's GPT models with vision capabilities (including GPT-5 and GPT-4o).</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>Provide your OpenAI API key or set the value to <code>rf_key:account</code> (or <code>rf_key:user:&lt;id&gt;</code>) to proxy requests through Roboflow's API.</p>"},{"location":"workflows/blocks/open_ai/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/open_ai@v3</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/open_ai/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the OpenAI model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your OpenAI API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>image_detail</code> <code>str</code> Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit OpenAI limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/open_ai/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>OpenAI</code> in version <code>v3</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the OpenAI model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (Union[<code>string</code>, <code>ROBOFLOW_MANAGED_KEY</code>, <code>secret</code>]): Your OpenAI API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>image_detail</code> (<code>string</code>): Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity..</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>OpenAI</code> in version <code>v3</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/open_ai@v3\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"gpt-5\",\n    \"image_detail\": \"auto\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/open_ai/#v2","title":"v2","text":"Class: <code>OpenAIBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.openai.v2.OpenAIBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Ask a question to OpenAI's GPT models with vision capabilities (including GPT-4o and GPT-5).</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. </p>"},{"location":"workflows/blocks/open_ai/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/open_ai@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/open_ai/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the OpenAI model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your OpenAI API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>image_detail</code> <code>str</code> Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit OpenAI limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/open_ai/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>OpenAI</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the OpenAI model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your OpenAI API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>image_detail</code> (<code>string</code>): Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity..</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>OpenAI</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/open_ai@v2\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"gpt-4o\",\n    \"image_detail\": \"auto\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/open_ai/#v1","title":"v1","text":"Class: <code>OpenAIBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.foundation.openai.v1.OpenAIBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Ask a question to OpenAI's GPT-4 with Vision model.</p> <p>You can specify arbitrary text prompts to the OpenAIBlock.</p> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. </p> <p>This model was previously part of the LMM block.</p>"},{"location":"workflows/blocks/open_ai/#type-identifier_2","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/open_ai@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/open_ai/#properties_2","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Text prompt to the OpenAI model. \u2705 <code>openai_api_key</code> <code>str</code> Your OpenAI API key. \u2705 <code>openai_model</code> <code>str</code> Model to be used. \u2705 <code>json_output_format</code> <code>Dict[str, str]</code> Holds dictionary that maps name of requested output field into its description. \u274c <code>image_detail</code> <code>str</code> Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/open_ai/#input-and-output-bindings_2","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>OpenAI</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the OpenAI model.</li> <li><code>openai_api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your OpenAI API key.</li> <li><code>openai_model</code> (<code>string</code>): Model to be used.</li> <li><code>image_detail</code> (<code>string</code>): Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>structured_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>OpenAI</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/open_ai@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"openai_api_key\": \"xxx-xxx\",\n    \"openai_model\": \"gpt-4o\",\n    \"json_output_format\": {\n        \"count\": \"number of cats in the picture\"\n    },\n    \"image_detail\": \"auto\",\n    \"max_tokens\": 450\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/overlap_filter/","title":"Overlap Filter","text":"Class: <code>OverlapBlockV1</code> <p>Source: inference.core.workflows.core_steps.analytics.overlap.v1.OverlapBlockV1</p> <p>The <code>OverlapFilter</code> is an analytics block that filters out objects overlapping instances of some other class</p> <p>For instance, for filtering people on bicycles, \"bicycle\" could be used as the overlap class.</p> <p>Examples applications: people in a car, items on a pallet</p> <p>The filter will remove the overlap class from the results, and only return the objects that overlap it. So in the case above, bicycle will also be removed from the results.</p>"},{"location":"workflows/blocks/overlap_filter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/overlap@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/overlap_filter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>overlap_type</code> <code>str</code> Select center for centerpoint overlap, any for any overlap. \u274c <code>overlap_class_name</code> <code>str</code> Overlap Class Name. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/overlap_filter/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Overlap Filter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Object predictions.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>overlaps</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Overlap Filter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/overlap@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"overlap_type\": \"Center Overlap\",\n    \"overlap_class_name\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/path_deviation/","title":"Path Deviation","text":""},{"location":"workflows/blocks/path_deviation/#v2","title":"v2","text":"Class: <code>PathDeviationAnalyticsBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.path_deviation.v2.PathDeviationAnalyticsBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>PathDeviationAnalyticsBlock</code> is an analytics block designed to measure the Frechet distance of tracked objects from a user-defined reference path. The block requires detections to be tracked (i.e. each object must have a unique tracker_id assigned, which persists between frames).</p>"},{"location":"workflows/blocks/path_deviation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/path_deviation_analytics@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/path_deviation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>triggering_anchor</code> <code>str</code> Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS. \u2705 <code>reference_path</code> <code>List[Any]</code> Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/path_deviation/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Path Deviation</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Predictions.</li> <li><code>triggering_anchor</code> (<code>string</code>): Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS.</li> <li><code>reference_path</code> (<code>list_of_values</code>): Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> </ul> </li> <li> <p>output</p> <ul> <li><code>path_deviation_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Path Deviation</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/path_deviation_analytics@v2\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"triggering_anchor\": \"CENTER\",\n    \"reference_path\": \"$inputs.expected_path\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/path_deviation/#v1","title":"v1","text":"Class: <code>PathDeviationAnalyticsBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.path_deviation.v1.PathDeviationAnalyticsBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>PathDeviationAnalyticsBlock</code> is an analytics block designed to measure the Frechet distance of tracked objects from a user-defined reference path. The block requires detections to be tracked (i.e. each object must have a unique tracker_id assigned, which persists between frames).</p>"},{"location":"workflows/blocks/path_deviation/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/path_deviation_analytics@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/path_deviation/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>triggering_anchor</code> <code>str</code> Point on the detection that will be used to calculate the Frechet distance.. \u2705 <code>reference_path</code> <code>List[Any]</code> Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/path_deviation/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Path Deviation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Predictions.</li> <li><code>triggering_anchor</code> (<code>string</code>): Point on the detection that will be used to calculate the Frechet distance..</li> <li><code>reference_path</code> (<code>list_of_values</code>): Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> </ul> </li> <li> <p>output</p> <ul> <li><code>path_deviation_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Path Deviation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/path_deviation_analytics@v1\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"triggering_anchor\": \"CENTER\",\n    \"reference_path\": \"$inputs.expected_path\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/perception_encoder_embedding_model/","title":"Perception Encoder Embedding Model","text":"Class: <code>PerceptionEncoderModelBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.perception_encoder.v1.PerceptionEncoderModelBlockV1</p> <p>Use the Meta Perception Encoder model to create semantic embeddings of text and images.</p> <p>This block accepts an image or string and returns an embedding. The embedding can be used to compare similarity between different images or between images and text.</p>"},{"location":"workflows/blocks/perception_encoder_embedding_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/perception_encoder@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/perception_encoder_embedding_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>data</code> <code>str</code> The string or image to generate an embedding for.. \u2705 <code>version</code> <code>str</code> Variant of Perception Encoder model. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/perception_encoder_embedding_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Perception Encoder Embedding Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (Union[<code>image</code>, <code>string</code>]): The string or image to generate an embedding for..</li> <li><code>version</code> (<code>string</code>): Variant of Perception Encoder model.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>embedding</code> (<code>embedding</code>): A list of floating point numbers representing a vector embedding..</li> </ul> </li> </ul> Example JSON definition of step <code>Perception Encoder Embedding Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/perception_encoder@v1\",\n    \"data\": \"$inputs.image\",\n    \"version\": \"PE-Core-B16-224\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/perspective_correction/","title":"Perspective Correction","text":"Class: <code>PerspectiveCorrectionBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.perspective_correction.v1.PerspectiveCorrectionBlockV1</p> <p>The <code>PerspectiveCorrectionBlock</code> is a transformer block designed to correct coordinates of detections based on transformation defined by two polygons. This block is best suited when produced coordinates should be considered as if camera was placed directly above the scene and was not introducing distortions.</p>"},{"location":"workflows/blocks/perspective_correction/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/perspective_correction@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/perspective_correction/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>perspective_polygons</code> <code>List[Any]</code> Perspective polygons (for each batch at least one must be consisting of 4 vertices). \u2705 <code>transformed_rect_width</code> <code>int</code> Transformed rect width. \u2705 <code>transformed_rect_height</code> <code>int</code> Transformed rect height. \u2705 <code>extend_perspective_polygon_by_detections_anchor</code> <code>str</code> If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS and ALL to extend to contain whole bounding box. \u2705 <code>warp_image</code> <code>bool</code> If set to True, image will be warped into transformed rect. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/perspective_correction/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Perspective Correction</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Predictions.</li> <li><code>images</code> (<code>image</code>): The input image for this step..</li> <li><code>perspective_polygons</code> (<code>list_of_values</code>): Perspective polygons (for each batch at least one must be consisting of 4 vertices).</li> <li><code>transformed_rect_width</code> (<code>integer</code>): Transformed rect width.</li> <li><code>transformed_rect_height</code> (<code>integer</code>): Transformed rect height.</li> <li><code>extend_perspective_polygon_by_detections_anchor</code> (<code>string</code>): If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS and ALL to extend to contain whole bounding box.</li> <li><code>warp_image</code> (<code>boolean</code>): If set to True, image will be warped into transformed rect.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>corrected_coordinates</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> <li><code>warped_image</code> (<code>image</code>): Image in workflows.</li> <li><code>extended_transformed_rect_width</code> (<code>integer</code>): Integer value.</li> <li><code>extended_transformed_rect_height</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Perspective Correction</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/perspective_correction@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"images\": \"$inputs.image\",\n    \"perspective_polygons\": \"$steps.perspective_wrap.zones\",\n    \"transformed_rect_width\": 1000,\n    \"transformed_rect_height\": 1000,\n    \"extend_perspective_polygon_by_detections_anchor\": \"CENTER\",\n    \"warp_image\": false\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/pixel_color_count/","title":"Pixel Color Count","text":"Class: <code>PixelationCountBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.pixel_color_count.v1.PixelationCountBlockV1</p> <p>Count the number of pixels that match a specific color within a given tolerance.</p>"},{"location":"workflows/blocks/pixel_color_count/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/pixel_color_count@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/pixel_color_count/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>target_color</code> <code>Union[Tuple[int, int, int], str]</code> Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).. \u2705 <code>tolerance</code> <code>int</code> Tolerance for color matching.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/pixel_color_count/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Pixel Color Count</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>target_color</code> (Union[<code>string</code>, <code>rgb_color</code>]): Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67))..</li> <li><code>tolerance</code> (<code>integer</code>): Tolerance for color matching..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>matching_pixels_count</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Pixel Color Count</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/pixel_color_count@v1\",\n    \"image\": \"$inputs.image\",\n    \"target_color\": \"#431112\",\n    \"tolerance\": 10\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/pixelate_visualization/","title":"Pixelate Visualization","text":"Class: <code>PixelateVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.pixelate.v1.PixelateVisualizationBlockV1</p> <p>The <code>PixelateVisualization</code> block pixelates detected objects in an image using Supervision's <code>sv.PixelateAnnotator</code>.</p>"},{"location":"workflows/blocks/pixelate_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/pixelate_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/pixelate_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>pixel_size</code> <code>int</code> Size of the pixelation.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/pixelate_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Pixelate Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>pixel_size</code> (<code>integer</code>): Size of the pixelation..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Pixelate Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/pixelate_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"pixel_size\": 20\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/polygon_visualization/","title":"Polygon Visualization","text":"Class: <code>PolygonVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.polygon.v1.PolygonVisualizationBlockV1</p> <p>The <code>PolygonVisualization</code> block uses a detections from an instance segmentation to draw polygons around objects using <code>sv.PolygonAnnotator</code>.</p>"},{"location":"workflows/blocks/polygon_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/polygon_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/polygon_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>thickness</code> <code>int</code> Thickness of the outline in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/polygon_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Polygon Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the outline in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Polygon Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/polygon_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.instance_segmentation_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/polygon_zone_visualization/","title":"Polygon Zone Visualization","text":"Class: <code>PolygonZoneVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.polygon_zone.v1.PolygonZoneVisualizationBlockV1</p> <p>The <code>PolygonZoneVisualization</code> block draws polygon zone in an image with a specified color and opacity. Please note: zones will be drawn on top of image passed to this block, this block should be placed before other visualization blocks in the workflow.</p>"},{"location":"workflows/blocks/polygon_zone_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/polygon_zone_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/polygon_zone_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>zone</code> <code>List[Any]</code> Polygon zones (one for each batch) in a format [[(x1, y1), (x2, y2), (x3, y3), ...], ...]; each zone must consist of more than 2 points. \u2705 <code>color</code> <code>str</code> Color of the zone.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/polygon_zone_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Polygon Zone Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>zone</code> (<code>list_of_values</code>): Polygon zones (one for each batch) in a format [[(x1, y1), (x2, y2), (x3, y3), ...], ...]; each zone must consist of more than 2 points.</li> <li><code>color</code> (<code>string</code>): Color of the zone..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Polygon Zone Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/polygon_zone_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"zone\": \"$inputs.zones\",\n    \"color\": \"WHITE\",\n    \"opacity\": 0.3\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/property_definition/","title":"Property Definition","text":"Class: <code>PropertyDefinitionBlockV1</code> <p>Source: inference.core.workflows.core_steps.formatters.property_definition.v1.PropertyDefinitionBlockV1</p> <p>Define a field using properties from previous workflow steps.</p> <p>Example use-cases: * extraction of all class names for object-detection predictions * extraction of class / confidence from classification result * extraction ocr text from OCR result</p>"},{"location":"workflows/blocks/property_definition/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/property_definition@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/property_definition/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>operations</code> <code>List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]</code> List of operations to perform on the data.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/property_definition/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Property Definition</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): Data to extract property from..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Property Definition</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/property_definition@v1\",\n    \"data\": \"$steps.my_step.predictions\",\n    \"operations\": [\n        {\n            \"property_name\": \"class_name\",\n            \"type\": \"DetectionsPropertyExtract\"\n        }\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/ptz_tracking%28onvif%29/","title":"PTZ Tracking (ONVIF)","text":"Class: <code>ONVIFSinkBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.onvif_movement.v1.ONVIFSinkBlockV1</p> <p>This ONVIF block allows a workflow to control an ONVIF capable PTZ camera to follow a detected object.</p> <p>The block returns three values: * predictions: a predictions object containing the single prediction the camera is currently following (can be empty) * seeking: indicates whether or not the camera is currently seeking an object (set asynchronously)</p> <p>There are two modes:</p> <p>*Follow: The object it follows is the maximum confidence prediction out of all predictions passed into it. To follow a specific object, use the appropriate filters on the predictiion object to specify the object you want to follow. Additionally if a tracker is used, the camera will follow the tracked object until it disappears. Additionally, zoom can be toggled to get the camera to zoom into a position.</p> <p>*Move to Preset: The camera can also move to a defined preset position. The camera must support the GotoPreset service.</p> <p>Note that the tracking block uses the ONVIF continuous movement service. Tracking is adjusted on each successive workflow execution. If workflow execution stops, and the camera is currently moving, the camera will continue moving until it reaches the limits and will no longer be following an object.</p> <p>Use of a camera with variable speed movement is highly recommended for this block. \"Simulate variable speed\" can sometimes be used in place of this, but might result in jerky movements and hunting. This setting sends the camera a 100% movement command followed by a stop for a period in order to simulate a percentage speed movement. This can work in some cases, but the success varies depending on the camera's responsiveness.</p> <p>PID tuning is generally necessary for this block to avoid having the camera overshoot and hunt. Having a significant lag between the camera movement and video (using a lazy buffer consumption strategy) can make tuning extremely difficult. Using an eager buffer consumption strategy is recommended. Increasing the dead zone can also help, but can affect zooming.</p>"},{"location":"workflows/blocks/ptz_tracking%28onvif%29/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/onvif_sink@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/ptz_tracking%28onvif%29/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>camera_ip</code> <code>str</code> Camera IP address or hostname. \u2705 <code>camera_port</code> <code>int</code> Camera ONVIF port. \u2705 <code>camera_username</code> <code>str</code> Camera username. \u2705 <code>camera_password</code> <code>str</code> Camera password. \u2705 <code>movement_type</code> <code>str</code> Follow object or go to default position preset on execution. \u274c <code>simulate_variable_speed</code> <code>bool</code> Simulate variable speed on a lower end camera by using frequent stop commands. \u2705 <code>zoom_if_able</code> <code>bool</code> Attempt to zoom into an object so it fills the image. \u2705 <code>follow_tracker</code> <code>bool</code> Lock to the tracking id of the highest confidence prediction until idle or reset. A tracker must be added to the workflow.. \u2705 <code>dead_zone</code> <code>int</code> Camera will stop once bounding box is within this many pixels of FoV center (or border for zoom). Increasing dead zone helps avoid pan/tilt hunting, but decreasing dead zone helps avoid hunting after zoom.. \u2705 <code>default_position_preset</code> <code>str</code> Preset name for default position. This must be a valid camera preset name.. \u2705 <code>move_to_position_after_idle_seconds</code> <code>int</code> Move to the default position after this many seconds of not seeking (0 to disable). \u2705 <code>camera_update_rate_limit</code> <code>int</code> Minimum number of milliseconds between ONVIF movement updates. \u2705 <code>flip_x_movement</code> <code>bool</code> Flip X movement if image is mirrored horizontally. \u2705 <code>flip_y_movement</code> <code>bool</code> Flip Y movement if image is mirrored vertically. \u2705 <code>minimum_camera_speed</code> <code>float</code> Minimum camera speed as percent (0-1). Some cameras won't honor speeds below a certain amount.. \u2705 <code>pid_kp</code> <code>float</code> PID Kp (proportional) constant. Decrease Kp to reduce hunting at the expense of speed.. \u2705 <code>pid_ki</code> <code>float</code> PID Ki (integral) constant. Use to reduce steady state error, but it can usually be zero.. \u2705 <code>pid_kd</code> <code>float</code> PID Kd (derivative) constant. Increase Kd with lag between video and movement, but excessive Kd can also cause hunting.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/ptz_tracking%28onvif%29/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>PTZ Tracking (ONVIF)</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Object predictions.</li> <li><code>camera_ip</code> (<code>string</code>): Camera IP address or hostname.</li> <li><code>camera_port</code> (<code>integer</code>): Camera ONVIF port.</li> <li><code>camera_username</code> (<code>string</code>): Camera username.</li> <li><code>camera_password</code> (<code>secret</code>): Camera password.</li> <li><code>simulate_variable_speed</code> (<code>boolean</code>): Simulate variable speed on a lower end camera by using frequent stop commands.</li> <li><code>zoom_if_able</code> (<code>boolean</code>): Attempt to zoom into an object so it fills the image.</li> <li><code>follow_tracker</code> (<code>boolean</code>): Lock to the tracking id of the highest confidence prediction until idle or reset. A tracker must be added to the workflow..</li> <li><code>dead_zone</code> (<code>integer</code>): Camera will stop once bounding box is within this many pixels of FoV center (or border for zoom). Increasing dead zone helps avoid pan/tilt hunting, but decreasing dead zone helps avoid hunting after zoom..</li> <li><code>default_position_preset</code> (<code>string</code>): Preset name for default position. This must be a valid camera preset name..</li> <li><code>move_to_position_after_idle_seconds</code> (<code>integer</code>): Move to the default position after this many seconds of not seeking (0 to disable).</li> <li><code>camera_update_rate_limit</code> (<code>integer</code>): Minimum number of milliseconds between ONVIF movement updates.</li> <li><code>flip_x_movement</code> (<code>boolean</code>): Flip X movement if image is mirrored horizontally.</li> <li><code>flip_y_movement</code> (<code>boolean</code>): Flip Y movement if image is mirrored vertically.</li> <li><code>minimum_camera_speed</code> (<code>float_zero_to_one</code>): Minimum camera speed as percent (0-1). Some cameras won't honor speeds below a certain amount..</li> <li><code>pid_kp</code> (<code>float</code>): PID Kp (proportional) constant. Decrease Kp to reduce hunting at the expense of speed..</li> <li><code>pid_ki</code> (<code>float</code>): PID Ki (integral) constant. Use to reduce steady state error, but it can usually be zero..</li> <li><code>pid_kd</code> (<code>float</code>): PID Kd (derivative) constant. Increase Kd with lag between video and movement, but excessive Kd can also cause hunting..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>seeking</code> (<code>boolean</code>): Boolean flag.</li> </ul> </li> </ul> Example JSON definition of step <code>PTZ Tracking (ONVIF)</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/onvif_sink@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"camera_ip\": \"&lt;block_does_not_provide_example&gt;\",\n    \"camera_port\": \"&lt;block_does_not_provide_example&gt;\",\n    \"camera_username\": \"&lt;block_does_not_provide_example&gt;\",\n    \"camera_password\": \"&lt;block_does_not_provide_example&gt;\",\n    \"movement_type\": \"Follow\",\n    \"simulate_variable_speed\": true,\n    \"zoom_if_able\": true,\n    \"follow_tracker\": true,\n    \"dead_zone\": 50,\n    \"default_position_preset\": \"\",\n    \"move_to_position_after_idle_seconds\": \"&lt;block_does_not_provide_example&gt;\",\n    \"camera_update_rate_limit\": \"&lt;block_does_not_provide_example&gt;\",\n    \"flip_x_movement\": true,\n    \"flip_y_movement\": true,\n    \"minimum_camera_speed\": \"&lt;block_does_not_provide_example&gt;\",\n    \"pid_kp\": \"&lt;block_does_not_provide_example&gt;\",\n    \"pid_ki\": \"&lt;block_does_not_provide_example&gt;\",\n    \"pid_kd\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/qr_code_detection/","title":"QR Code Detection","text":"Class: <code>QRCodeDetectorBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.third_party.qr_code_detection.v1.QRCodeDetectorBlockV1</p> <p>Detect the location of a QR code.</p> <p>This block is useful for manufacturing and consumer packaged goods projects where you  need to detect a QR code region in an image. You can then apply Crop block to isolate  each QR code then apply further processing (i.e. read a QR code with a custom block).</p>"},{"location":"workflows/blocks/qr_code_detection/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/qr_code_detector@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/qr_code_detection/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/qr_code_detection/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>QR Code Detection</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>qr_code_detection</code>): Prediction with QR code detection.</li> </ul> </li> </ul> Example JSON definition of step <code>QR Code Detection</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/qr_code_detector@v1\",\n    \"images\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/qr_code_generator/","title":"QR Code Generator","text":"Class: <code>QRCodeGeneratorBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.qr_code_generator.v1.QRCodeGeneratorBlockV1</p> <p>Generate a QR code image from a string input (typically a URL).</p> <p>This block creates a QR code PNG image from the provided text input. It supports  various customization options including size control, error correction levels,  and visual styling. The generated QR code can be used in workflows where you need  to create QR codes for URLs, text content, or other data that needs to be encoded.</p> <p>The output is a PNG image that can be passed to other workflow blocks such as  visualizers or image processing blocks.</p>"},{"location":"workflows/blocks/qr_code_generator/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/qr_code_generator@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/qr_code_generator/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>text</code> <code>str</code> Text or URL to encode in the QR code. \u2705 <code>error_correct</code> <code>str</code> Increased error correction comes at the expense of data capacity (text length). Use higher error correction if the QR code is likely to be transformed or obscured, but use a lower error correction level if the URL is long and the QR code is clearly visible.. \u274c <code>border</code> <code>int</code> Border thickness in modules (default: 4). \u2705 <code>fill_color</code> <code>str</code> QR code block color. Supports hex (#FF0000), rgb(255, 0, 0), standard names (BLACK, WHITE, RED, etc.), or CSS3 color names.. \u2705 <code>back_color</code> <code>str</code> QR code background color. Supports hex (#FFFFFF), rgb(255, 255, 255), standard names (BLACK, WHITE, RED, etc.), or CSS3 color names.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/qr_code_generator/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>QR Code Generator</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>text</code> (<code>string</code>): Text or URL to encode in the QR code.</li> <li><code>border</code> (<code>integer</code>): Border thickness in modules (default: 4).</li> <li><code>fill_color</code> (<code>string</code>): QR code block color. Supports hex (#FF0000), rgb(255, 0, 0), standard names (BLACK, WHITE, RED, etc.), or CSS3 color names..</li> <li><code>back_color</code> (<code>string</code>): QR code background color. Supports hex (#FFFFFF), rgb(255, 255, 255), standard names (BLACK, WHITE, RED, etc.), or CSS3 color names..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>qr_code</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>QR Code Generator</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/qr_code_generator@v1\",\n    \"text\": \"https://roboflow.com\",\n    \"error_correct\": \"Low (~7% word recovery / highest data capacity)\",\n    \"border\": 2,\n    \"fill_color\": \"BLACK\",\n    \"back_color\": \"WHITE\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/qwen2.5_vl/","title":"Qwen2.5-VL","text":"Class: <code>Qwen25VLBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.qwen.v1.Qwen25VLBlockV1</p> <p>This workflow block runs Qwen2.5-VL\u2014a vision language model that accepts an image and an optional text prompt\u2014and returns a text answer based on a conversation template.</p>"},{"location":"workflows/blocks/qwen2.5_vl/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/qwen25vl@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/qwen2.5_vl/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Optional text prompt to provide additional context to Qwen2.5-VL. Otherwise it will just be None. \u274c <code>model_version</code> <code>str</code> The Qwen2.5-VL model to be used for inference.. \u2705 <code>system_prompt</code> <code>str</code> Optional system prompt to provide additional context to Qwen2.5-VL.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/qwen2.5_vl/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Qwen2.5-VL</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_version</code> (<code>roboflow_model_id</code>): The Qwen2.5-VL model to be used for inference..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parsed_output</code> (<code>dictionary</code>): Dictionary.</li> </ul> </li> </ul> Example JSON definition of step <code>Qwen2.5-VL</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/qwen25vl@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"What is in this image?\",\n    \"model_version\": \"qwen25-vl-7b\",\n    \"system_prompt\": \"You are a helpful assistant.\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/rate_limiter/","title":"Rate Limiter","text":"Class: <code>RateLimiterBlockV1</code> <p>Source: inference.core.workflows.core_steps.flow_control.rate_limiter.v1.RateLimiterBlockV1</p> <p>The Rate Limiter block controls the execution frequency of a branch within a Workflow by enforcing a  cooldown period. It ensures that the connected steps do not run more frequently than a specified interval,  helping to manage resource usage and prevent over-execution.</p>"},{"location":"workflows/blocks/rate_limiter/#block-usage","title":"Block usage","text":"<p>Rate Limiter is useful when you have two blocks that are directly connected, as shown below:</p> <p>--- input_a --&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 --- input_b --&gt; \u2502   step_1  \u2502 --&gt;  output_a --&gt;  \u2502   step_2  \u2502 --- input_c --&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>If you want to throttle the Step 2 execution rate - you should apply rate limiter in between:</p> <ul> <li> <p>keep the existing blocks configuration as is (do not change connections)</p> </li> <li> <p>set <code>depends_on</code> reference of Rate Limiter into <code>output_a</code></p> </li> <li> <p>set <code>next_steps</code> reference to be a list referring to <code>[$steps.step_2]</code></p> </li> <li> <p>adjust <code>cooldown_seconds</code> to specify what is the number of seconds that must be awaited before next time when <code>step_2</code> is fired </p> </li> </ul> <p>Cooldown limitations</p> <p>Current implementation of cooldown is limited to video processing - using this block in context of a  Workflow that is run behind HTTP service (Roboflow Hosted API, Dedicated Deployment or self-hosted  <code>inference</code> server) will have no effect for processing HTTP requests.  </p>"},{"location":"workflows/blocks/rate_limiter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/rate_limiter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/rate_limiter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>cooldown_seconds</code> <code>float</code> The minimum number of seconds between allowed executions.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/rate_limiter/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Rate Limiter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>depends_on</code> (<code>*</code>): Step immediately preceding this block..</li> <li><code>next_steps</code> (step): Steps to execute if allowed by the rate limit..</li> <li><code>video_reference_image</code> (<code>image</code>): Reference to a video frame to use for timestamp generation (if running faster than realtime on recorded video)..</li> </ul> </li> <li> <p>output</p> </li> </ul> Example JSON definition of step <code>Rate Limiter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/rate_limiter@v1\",\n    \"cooldown_seconds\": 1.0,\n    \"depends_on\": \"$steps.model\",\n    \"next_steps\": [\n        \"$steps.upload\"\n    ],\n    \"video_reference_image\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/reference_path_visualization/","title":"Reference Path Visualization","text":"Class: <code>ReferencePathVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.reference_path.v1.ReferencePathVisualizationBlockV1</p> <p>The Reference Path Visualization block draws reference path in the image. To be used in combination with Path deviation block - to display the reference path.</p>"},{"location":"workflows/blocks/reference_path_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/reference_path_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/reference_path_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>reference_path</code> <code>List[Any]</code> Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <code>color</code> <code>str</code> Color of the zone.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/reference_path_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Reference Path Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>reference_path</code> (<code>list_of_values</code>): Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> <li><code>color</code> (<code>string</code>): Color of the zone..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Reference Path Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/reference_path_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"reference_path\": \"$inputs.expected_path\",\n    \"color\": \"WHITE\",\n    \"thickness\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/relative_static_crop/","title":"Relative Static Crop","text":"Class: <code>RelativeStaticCropBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.relative_static_crop.v1.RelativeStaticCropBlockV1</p> <p>Crop a Region of Interest (RoI) from an image, using relative coordinates.</p> <p>This is useful when placed after an ObjectDetection block as part of a multi-stage  workflow. For example, you could use an ObjectDetection block to detect objects, then  the RelativeStaticCrop block to crop objects, then an OCR block to run character  recognition on each of the individual cropped regions.</p>"},{"location":"workflows/blocks/relative_static_crop/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/relative_statoic_crop@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/relative_static_crop/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>x_center</code> <code>float</code> Center X of static crop (relative coordinate 0.0-1.0). \u2705 <code>y_center</code> <code>float</code> Center Y of static crop (relative coordinate 0.0-1.0). \u2705 <code>width</code> <code>float</code> Width of static crop (relative value 0.0-1.0). \u2705 <code>height</code> <code>float</code> Height of static crop (relative value 0.0-1.0). \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/relative_static_crop/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Relative Static Crop</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>x_center</code> (<code>float_zero_to_one</code>): Center X of static crop (relative coordinate 0.0-1.0).</li> <li><code>y_center</code> (<code>float_zero_to_one</code>): Center Y of static crop (relative coordinate 0.0-1.0).</li> <li><code>width</code> (<code>float_zero_to_one</code>): Width of static crop (relative value 0.0-1.0).</li> <li><code>height</code> (<code>float_zero_to_one</code>): Height of static crop (relative value 0.0-1.0).</li> </ul> </li> <li> <p>output</p> <ul> <li><code>crops</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Relative Static Crop</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/relative_statoic_crop@v1\",\n    \"images\": \"$inputs.image\",\n    \"x_center\": 0.3,\n    \"y_center\": 0.3,\n    \"width\": 0.3,\n    \"height\": 0.3\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/roboflow_custom_metadata/","title":"Roboflow Custom Metadata","text":"Class: <code>RoboflowCustomMetadataBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.roboflow.custom_metadata.v1.RoboflowCustomMetadataBlockV1</p> <p>Block allows users to add custom metadata to each inference result in Roboflow Model Monitoring dashboard.</p> <p>This is useful for adding information specific to your use case. For example, if you want to be able to filter inferences by a specific label such as location, you can attach those labels to each inference with this block.</p> <p>For more information on Model Monitoring at Roboflow, see https://docs.roboflow.com/deploy/model-monitoring.</p>"},{"location":"workflows/blocks/roboflow_custom_metadata/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_custom_metadata@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/roboflow_custom_metadata/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>field_value</code> <code>str</code> This is the name of the metadata field you are creating. \u2705 <code>field_name</code> <code>str</code> Name of the field to be updated.. \u274c <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/roboflow_custom_metadata/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Roboflow Custom Metadata</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>classification_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to attach custom metadata to..</li> <li><code>field_value</code> (<code>string</code>): This is the name of the metadata field you are creating.</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Roboflow Custom Metadata</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_custom_metadata@v1\",\n    \"predictions\": \"$steps.my_step.predictions\",\n    \"field_value\": \"toronto\",\n    \"field_name\": \"The name of the value of the field\",\n    \"fire_and_forget\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/roboflow_dataset_upload/","title":"Roboflow Dataset Upload","text":""},{"location":"workflows/blocks/roboflow_dataset_upload/#v2","title":"v2","text":"Class: <code>RoboflowDatasetUploadBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.sinks.roboflow.dataset_upload.v2.RoboflowDatasetUploadBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Block let users save their images and predictions into Roboflow Dataset. Persisting data from production environments helps iteratively building more robust models. </p> <p>Block provides configuration options to decide how data should be stored and what are the limits  to be applied. We advice using this block in combination with rate limiter blocks to effectively  collect data that the model struggle with.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_dataset_upload@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>target_project</code> <code>str</code> Roboflow project where data will be saved.. \u2705 <code>data_percentage</code> <code>float</code> Percent of data that will be saved (0.0 to 100.0).. \u2705 <code>minutely_usage_limit</code> <code>int</code> Maximum number of image uploads allowed per minute.. \u274c <code>hourly_usage_limit</code> <code>int</code> Maximum number of image uploads allowed per hour.. \u274c <code>daily_usage_limit</code> <code>int</code> Maximum number of image uploads allowed per day.. \u274c <code>usage_quota_name</code> <code>str</code> A unique identifier for tracking usage quotas (minutely, hourly, daily limits).. \u274c <code>max_image_size</code> <code>Tuple[int, int]</code> Maximum size of the image to be saved. Bigger images will be downsized preserving aspect ratio.. \u274c <code>compression_level</code> <code>int</code> Compression level for the registered image.. \u274c <code>registration_tags</code> <code>List[str]</code> Tags to be attached to the registered image.. \u2705 <code>persist_predictions</code> <code>bool</code> Boolean flag to specify if model predictions should be saved along with the image.. \u2705 <code>disable_sink</code> <code>bool</code> Boolean flag to disable block execution.. \u2705 <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <code>labeling_batch_prefix</code> <code>str</code> Target batch name for the registered image.. \u2705 <code>labeling_batches_recreation_frequency</code> <code>str</code> Frequency in which new labeling batches are created for uploaded images.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Roboflow Dataset Upload</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to upload..</li> <li><code>target_project</code> (<code>roboflow_project</code>): Roboflow project where data will be saved..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>classification_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to be uploaded..</li> <li><code>data_percentage</code> (<code>float</code>): Percent of data that will be saved (0.0 to 100.0)..</li> <li><code>registration_tags</code> (Union[<code>string</code>, <code>list_of_values</code>]): Tags to be attached to the registered image..</li> <li><code>persist_predictions</code> (<code>boolean</code>): Boolean flag to specify if model predictions should be saved along with the image..</li> <li><code>disable_sink</code> (<code>boolean</code>): Boolean flag to disable block execution..</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> <li><code>labeling_batch_prefix</code> (<code>string</code>): Target batch name for the registered image..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Roboflow Dataset Upload</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_dataset_upload@v2\",\n    \"images\": \"$inputs.image\",\n    \"target_project\": \"my_dataset\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"data_percentage\": true,\n    \"minutely_usage_limit\": 10,\n    \"hourly_usage_limit\": 10,\n    \"daily_usage_limit\": 10,\n    \"usage_quota_name\": \"quota-for-data-sampling-1\",\n    \"max_image_size\": [\n        1920,\n        1080\n    ],\n    \"compression_level\": 95,\n    \"registration_tags\": [\n        \"location-florida\",\n        \"factory-name\",\n        \"$inputs.dynamic_tag\"\n    ],\n    \"persist_predictions\": true,\n    \"disable_sink\": true,\n    \"fire_and_forget\": \"&lt;block_does_not_provide_example&gt;\",\n    \"labeling_batch_prefix\": \"my_labeling_batch_name\",\n    \"labeling_batches_recreation_frequency\": \"never\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/roboflow_dataset_upload/#v1","title":"v1","text":"Class: <code>RoboflowDatasetUploadBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.sinks.roboflow.dataset_upload.v1.RoboflowDatasetUploadBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Block let users save their images and predictions into Roboflow Dataset. Persisting data from production environments helps iteratively building more robust models. </p> <p>Block provides configuration options to decide how data should be stored and what are the limits  to be applied. We advice using this block in combination with rate limiter blocks to effectively  collect data that the model struggle with.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_dataset_upload@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>target_project</code> <code>str</code> Roboflow project where data will be saved.. \u2705 <code>minutely_usage_limit</code> <code>int</code> Maximum number of image uploads allowed per minute.. \u274c <code>hourly_usage_limit</code> <code>int</code> Maximum number of image uploads allowed per hour.. \u274c <code>daily_usage_limit</code> <code>int</code> Maximum number of image uploads allowed per day.. \u274c <code>usage_quota_name</code> <code>str</code> A unique identifier for tracking usage quotas (minutely, hourly, daily limits).. \u274c <code>max_image_size</code> <code>Tuple[int, int]</code> Maximum size of the image to be saved. Bigger images will be downsized preserving aspect ratio.. \u274c <code>compression_level</code> <code>int</code> Compression level for the registered image.. \u274c <code>registration_tags</code> <code>List[str]</code> Tags to be attached to the registered image.. \u2705 <code>persist_predictions</code> <code>bool</code> Boolean flag to specify if model predictions should be saved along with the image.. \u274c <code>disable_sink</code> <code>bool</code> Boolean flag to disable block execution.. \u2705 <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <code>labeling_batch_prefix</code> <code>str</code> Target batch name for the registered image.. \u2705 <code>labeling_batches_recreation_frequency</code> <code>str</code> Frequency in which new labeling batches are created for uploaded images.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Roboflow Dataset Upload</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): Image to upload..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>classification_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to be uploaded..</li> <li><code>target_project</code> (<code>roboflow_project</code>): Roboflow project where data will be saved..</li> <li><code>registration_tags</code> (Union[<code>string</code>, <code>list_of_values</code>]): Tags to be attached to the registered image..</li> <li><code>disable_sink</code> (<code>boolean</code>): Boolean flag to disable block execution..</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> <li><code>labeling_batch_prefix</code> (<code>string</code>): Target batch name for the registered image..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Roboflow Dataset Upload</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_dataset_upload@v1\",\n    \"image\": \"$inputs.image\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"target_project\": \"my_project\",\n    \"minutely_usage_limit\": 10,\n    \"hourly_usage_limit\": 10,\n    \"daily_usage_limit\": 10,\n    \"usage_quota_name\": \"quota-for-data-sampling-1\",\n    \"max_image_size\": [\n        512,\n        512\n    ],\n    \"compression_level\": 75,\n    \"registration_tags\": [\n        \"location-florida\",\n        \"factory-name\",\n        \"$inputs.dynamic_tag\"\n    ],\n    \"persist_predictions\": true,\n    \"disable_sink\": true,\n    \"fire_and_forget\": true,\n    \"labeling_batch_prefix\": \"my_labeling_batch_name\",\n    \"labeling_batches_recreation_frequency\": \"never\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/seg_preview/","title":"Seg Preview","text":"Class: <code>SegPreviewBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.seg_preview.v1.SegPreviewBlockV1</p> <p>Seg Preview</p>"},{"location":"workflows/blocks/seg_preview/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/seg-preview@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/seg_preview/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>class_names</code> <code>List[str]</code> List of classes to recognise. \u2705 <code>threshold</code> <code>float</code> Threshold for predicted mask scores. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/seg_preview/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Seg Preview</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>class_names</code> (<code>list_of_values</code>): List of classes to recognise.</li> <li><code>threshold</code> (<code>float</code>): Threshold for predicted mask scores.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Seg Preview</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/seg-preview@v1\",\n    \"images\": \"$inputs.image\",\n    \"class_names\": [\n        \"car\",\n        \"person\"\n    ],\n    \"threshold\": 0.3\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/segment_anything2_model/","title":"Segment Anything 2 Model","text":"Class: <code>SegmentAnything2BlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.segment_anything2.v1.SegmentAnything2BlockV1</p> <p>Run Segment Anything 2, a zero-shot instance segmentation model, on an image.</p> <p>** Dedicated inference server required (GPU recomended) **</p> <p>You can use pass in boxes/predictions from other models to Segment Anything 2 to use as prompts for the model. If you pass in box detections from another model, the class names of the boxes will be forwarded to the predicted masks.  If using the model unprompted, the model will assign integers as class names / ids.</p>"},{"location":"workflows/blocks/segment_anything2_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/segment_anything@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/segment_anything2_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>version</code> <code>str</code> Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus. \u2705 <code>threshold</code> <code>float</code> Threshold for predicted masks scores. \u2705 <code>multimask_output</code> <code>bool</code> Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/segment_anything2_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Segment Anything 2 Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>boxes</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Bounding boxes (from another model) to convert to polygons.</li> <li><code>version</code> (<code>string</code>): Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus.</li> <li><code>threshold</code> (<code>float</code>): Threshold for predicted masks scores.</li> <li><code>multimask_output</code> (<code>boolean</code>): Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Segment Anything 2 Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/segment_anything@v1\",\n    \"images\": \"$inputs.image\",\n    \"boxes\": \"$steps.object_detection_model.predictions\",\n    \"version\": \"hiera_large\",\n    \"threshold\": 0.3,\n    \"multimask_output\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/sift/","title":"SIFT","text":"Class: <code>SIFTBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.sift.v1.SIFTBlockV1</p> <p>The Scale-Invariant Feature Transform (SIFT) algorithm is a popular method in computer vision for detecting  and describing features (interesting parts) in images. SIFT is used to find key points in an image and  describe them in a way that allows for recognizing the same objects or features in different images,  even if the images are taken from different angles, distances, or lighting conditions.</p> <p>Read more: https://en.wikipedia.org/wiki/Scale-invariant_feature_transform</p>"},{"location":"workflows/blocks/sift/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/sift@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/sift/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/sift/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>SIFT</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>keypoints</code> (<code>image_keypoints</code>): Image keypoints detected by classical Computer Vision method.</li> <li><code>descriptors</code> (<code>numpy_array</code>): Numpy array.</li> </ul> </li> </ul> Example JSON definition of step <code>SIFT</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/sift@v1\",\n    \"image\": \"$inputs.image\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/sift_comparison/","title":"SIFT Comparison","text":""},{"location":"workflows/blocks/sift_comparison/#v2","title":"v2","text":"Class: <code>SIFTComparisonBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.classical_cv.sift_comparison.v2.SIFTComparisonBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Compare SIFT descriptors from multiple images using FLANN-based matcher.</p> <p>This block is useful for determining if multiple images match based on their SIFT descriptors.</p>"},{"location":"workflows/blocks/sift_comparison/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/sift_comparison@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/sift_comparison/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>good_matches_threshold</code> <code>int</code> Threshold for the number of good matches to consider the images as matching. \u2705 <code>ratio_threshold</code> <code>float</code> Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.. \u2705 <code>matcher</code> <code>str</code> Matcher to use for comparing the SIFT descriptors. \u2705 <code>visualize</code> <code>bool</code> Whether to visualize the keypoints and matches between the two images. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/sift_comparison/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>SIFT Comparison</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>input_1</code> (Union[<code>image</code>, <code>numpy_array</code>]): Reference to Image or SIFT descriptors from the first image to compare.</li> <li><code>input_2</code> (Union[<code>image</code>, <code>numpy_array</code>]): Reference to Image or SIFT descriptors from the second image to compare.</li> <li><code>good_matches_threshold</code> (<code>integer</code>): Threshold for the number of good matches to consider the images as matching.</li> <li><code>ratio_threshold</code> (<code>float_zero_to_one</code>): Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering..</li> <li><code>matcher</code> (<code>string</code>): Matcher to use for comparing the SIFT descriptors.</li> <li><code>visualize</code> (<code>boolean</code>): Whether to visualize the keypoints and matches between the two images.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>images_match</code> (<code>boolean</code>): Boolean flag.</li> <li><code>good_matches_count</code> (<code>integer</code>): Integer value.</li> <li><code>keypoints_1</code> (<code>image_keypoints</code>): Image keypoints detected by classical Computer Vision method.</li> <li><code>descriptors_1</code> (<code>numpy_array</code>): Numpy array.</li> <li><code>keypoints_2</code> (<code>image_keypoints</code>): Image keypoints detected by classical Computer Vision method.</li> <li><code>descriptors_2</code> (<code>numpy_array</code>): Numpy array.</li> <li><code>visualization_1</code> (<code>image</code>): Image in workflows.</li> <li><code>visualization_2</code> (<code>image</code>): Image in workflows.</li> <li><code>visualization_matches</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>SIFT Comparison</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/sift_comparison@v2\",\n    \"input_1\": \"$inputs.image1\",\n    \"input_2\": \"$inputs.image2\",\n    \"good_matches_threshold\": 50,\n    \"ratio_threshold\": 0.7,\n    \"matcher\": \"FlannBasedMatcher\",\n    \"visualize\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/sift_comparison/#v1","title":"v1","text":"Class: <code>SIFTComparisonBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.classical_cv.sift_comparison.v1.SIFTComparisonBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Compare SIFT descriptors from multiple images using FLANN-based matcher.</p> <p>This block is useful for determining if multiple images match based on their SIFT descriptors.</p>"},{"location":"workflows/blocks/sift_comparison/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/sift_comparison@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/sift_comparison/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>good_matches_threshold</code> <code>int</code> Threshold for the number of good matches to consider the images as matching. \u2705 <code>ratio_threshold</code> <code>float</code> Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/sift_comparison/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>SIFT Comparison</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>descriptor_1</code> (<code>numpy_array</code>): Reference to SIFT descriptors from the first image to compare.</li> <li><code>descriptor_2</code> (<code>numpy_array</code>): Reference to SIFT descriptors from the second image to compare.</li> <li><code>good_matches_threshold</code> (<code>integer</code>): Threshold for the number of good matches to consider the images as matching.</li> <li><code>ratio_threshold</code> (<code>integer</code>): Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>good_matches_count</code> (<code>integer</code>): Integer value.</li> <li><code>images_match</code> (<code>boolean</code>): Boolean flag.</li> </ul> </li> </ul> Example JSON definition of step <code>SIFT Comparison</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/sift_comparison@v1\",\n    \"descriptor_1\": \"$steps.sift.descriptors\",\n    \"descriptor_2\": \"$steps.sift.descriptors\",\n    \"good_matches_threshold\": 50,\n    \"ratio_threshold\": 0.7\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/single_label_classification_model/","title":"Single-Label Classification Model","text":""},{"location":"workflows/blocks/single_label_classification_model/#v2","title":"v2","text":"Class: <code>RoboflowClassificationModelBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.multi_class_classification.v2.RoboflowClassificationModelBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a multi-class classification model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/single_label_classification_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_classification_model@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/single_label_classification_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/single_label_classification_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Single-Label Classification Model</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model id.</li> </ul> </li> </ul> Example JSON definition of step <code>Single-Label Classification Model</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_classification_model@v2\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/single_label_classification_model/#v1","title":"v1","text":"Class: <code>RoboflowClassificationModelBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.models.roboflow.multi_class_classification.v1.RoboflowClassificationModelBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>Run inference on a multi-class classification model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/single_label_classification_model/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_classification_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/single_label_classification_model/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier.. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions.. \u2705 <code>disable_active_learning</code> <code>bool</code> Boolean flag to disable project-level active learning for this block.. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for active learning, if enabled.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/single_label_classification_model/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Single-Label Classification Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier..</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Boolean flag to disable project-level active learning for this block..</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for active learning, if enabled..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Single-Label Classification Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/size_measurement/","title":"Size Measurement","text":"Class: <code>SizeMeasurementBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.size_measurement.v1.SizeMeasurementBlockV1</p> <p>The Size Measurement Block calculates the dimensions of objects relative to a reference object. It uses one model to detect the reference object and another to detect the objects to measure. The block outputs the dimensions of the objects in terms of the reference object.</p> <ul> <li>Reference Object: This is the known object used as a baseline for measurements. Its dimensions are known and used to scale the measurements of other objects.</li> <li>Object to Measure: This is the object whose dimensions are being calculated. The block measures these dimensions relative to the reference object.</li> </ul>"},{"location":"workflows/blocks/size_measurement/#block-usage","title":"Block Usage","text":"<p>To use the Size Measurement Block, follow these steps:</p> <ol> <li>Select Models: Choose a model to detect the reference object and another model to detect the objects you want to measure.</li> <li>Configure Inputs: Provide the predictions from both models as inputs to the block.</li> <li>Set Reference Dimensions: Specify the known dimensions of the reference object in the format 'width,height' or as a tuple (width, height).</li> <li>Run the Block: Execute the block to calculate the dimensions of the detected objects relative to the reference object.</li> </ol>"},{"location":"workflows/blocks/size_measurement/#example","title":"Example","text":"<p>Imagine you have a scene with a calibration card and several packages. The calibration card has known dimensions of 5.0 inches by 3.0 inches. You want to measure the dimensions of packages in the scene.</p> <ul> <li>Reference Object: Calibration card with dimensions 5.0 inches (width) by 3.0 inches (height).</li> <li>Objects to Measure: Packages detected in the scene.</li> </ul> <p>The block will use the known dimensions of the calibration card to calculate the dimensions of each package. For example, if a package is detected with a width of 100 pixels and a height of 60 pixels, and the calibration card is detected with a width of 50 pixels and a height of 30 pixels, the block will calculate the package's dimensions as:</p> <ul> <li>Width: (100 pixels / 50 pixels) * 5.0 inches = 10.0 inches</li> <li>Height: (60 pixels / 30 pixels) * 3.0 inches = 6.0 inches</li> </ul> <p>This allows you to obtain the real-world dimensions of the packages based on the reference object's known size.</p> <p>Watch the video tutorial</p>"},{"location":"workflows/blocks/size_measurement/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/size_measurement@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/size_measurement/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>reference_predictions</code> <code>List[Any]</code> Reference object used to calculate the dimensions of the specified objects. If multiple objects are provided, the highest confidence prediction will be used.. \u2705 <code>reference_dimensions</code> <code>Union[List[float], Tuple[float, float], str]</code> Dimensions of the reference object in desired units, (e.g. inches). Will be used to convert the pixel dimensions of the other objects to real-world units.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/size_measurement/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Size Measurement</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>object_predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to measure the dimensions of..</li> <li><code>reference_predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>, <code>list_of_values</code>]): Reference object used to calculate the dimensions of the specified objects. If multiple objects are provided, the highest confidence prediction will be used..</li> <li><code>reference_dimensions</code> (Union[<code>string</code>, <code>list_of_values</code>]): Dimensions of the reference object in desired units, (e.g. inches). Will be used to convert the pixel dimensions of the other objects to real-world units..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>dimensions</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Size Measurement</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/size_measurement@v1\",\n    \"object_predictions\": \"$segmentation.object_predictions\",\n    \"reference_predictions\": \"$segmentation.reference_predictions\",\n    \"reference_dimensions\": [\n        4.5,\n        3.0\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/slack_notification/","title":"Slack Notification","text":"Class: <code>SlackNotificationBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.slack.notification.v1.SlackNotificationBlockV1</p> <p>The Slack Notification block \ud83d\udce9 enables sending notifications via Slack, with customizable messages, attachments,  and cooldown mechanisms.</p> <p>The block requires Slack setup -  this article may help you  configuring everything properly.</p>"},{"location":"workflows/blocks/slack_notification/#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p>\ud83d\udce2 Send Messages: Deliver notifications to specified Slack channels.</p> </li> <li> <p>\ud83d\udd17 Dynamic Content: Craft notifications based on outputs from other Workflow steps.</p> </li> <li> <p>\ud83d\udcce Attach Files: Share reports, predictions or visualizations.</p> </li> <li> <p>\ud83d\udd52 Cooldown Control: Prevent duplicate notifications within a set time frame.</p> </li> <li> <p>\u2699\ufe0f Flexible Execution: Execute in the background or block Workflow execution for debugging.</p> </li> </ul>"},{"location":"workflows/blocks/slack_notification/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/slack_notification@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/slack_notification/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>slack_token</code> <code>str</code> View the Roboflow Blog or Slack Documentation to learn how to generate a Slack API token.. \u2705 <code>channel</code> <code>str</code> Identifier of Slack channel.. \u2705 <code>message</code> <code>str</code> Content of the message to be sent.. \u274c <code>message_parameters</code> <code>Dict[str, Union[bool, float, int, str]]</code> Data to be used in the message content.. \u2705 <code>message_parameters_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> Preprocessing operations to be performed on message parameters.. \u274c <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <code>disable_sink</code> <code>bool</code> Boolean flag to disable block execution.. \u2705 <code>cooldown_seconds</code> <code>int</code> Number of seconds until a follow-up notification can be sent. Maximum value: 900 seconds (15 minutes). \u2705 <code>cooldown_session_key</code> <code>str</code> Unique key used internally to implement cooldown. Must be unique for each step in your Workflow.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/slack_notification/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Slack Notification</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>slack_token</code> (Union[<code>string</code>, <code>secret</code>]): View the Roboflow Blog or Slack Documentation to learn how to generate a Slack API token..</li> <li><code>channel</code> (<code>string</code>): Identifier of Slack channel..</li> <li><code>message_parameters</code> (<code>*</code>): Data to be used in the message content..</li> <li><code>attachments</code> (Union[<code>string</code>, <code>bytes</code>]): Attachments to be sent in the message, such as a csv file or jpg image..</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> <li><code>disable_sink</code> (<code>boolean</code>): Boolean flag to disable block execution..</li> <li><code>cooldown_seconds</code> (<code>integer</code>): Number of seconds until a follow-up notification can be sent. Maximum value: 900 seconds (15 minutes).</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>throttling_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Slack Notification</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/slack_notification@v1\",\n    \"slack_token\": \"$inputs.slack_token\",\n    \"channel\": \"$inputs.slack_channel_id\",\n    \"message\": \"During last 5 minutes detected {{ $parameters.num_instances }} instances\",\n    \"message_parameters\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"message_parameters_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"attachments\": {\n        \"report.csv\": \"$steps.csv_formatter.csv_content\"\n    },\n    \"fire_and_forget\": \"$inputs.fire_and_forget\",\n    \"disable_sink\": false,\n    \"cooldown_seconds\": \"$inputs.cooldown_seconds\",\n    \"cooldown_session_key\": \"session-1v73kdhfse\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/smol_vlm2/","title":"SmolVLM2","text":"Class: <code>SmolVLM2BlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.smolvlm.v1.SmolVLM2BlockV1</p> <p>This workflow block runs SmolVLM2, a multimodal vision-language model. You can ask questions about images -- including documents and photos -- and get answers in natural language.</p>"},{"location":"workflows/blocks/smol_vlm2/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/smolvlm2@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/smol_vlm2/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Optional text prompt to provide additional context to SmolVLM2. Otherwise it will just be None. \u274c <code>model_version</code> <code>str</code> The SmolVLM2 model to be used for inference.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/smol_vlm2/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>SmolVLM2</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>model_version</code> (<code>roboflow_model_id</code>): The SmolVLM2 model to be used for inference..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parsed_output</code> (<code>dictionary</code>): Dictionary.</li> </ul> </li> </ul> Example JSON definition of step <code>SmolVLM2</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/smolvlm2@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"What is in this image?\",\n    \"model_version\": \"smolvlm2/smolvlm-2.2b-instruct\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/stability_ai_image_generation/","title":"Stability AI Image Generation","text":"Class: <code>StabilityAIImageGenBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.stability_ai.image_gen.v1.StabilityAIImageGenBlockV1</p> <p>The block wraps Stability AI image generation API and let users generate new images from text, or create variations of existing images.</p>"},{"location":"workflows/blocks/stability_ai_image_generation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stability_ai_image_gen@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stability_ai_image_generation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>strength</code> <code>float</code> controls how much influence the image parameter has on the generated image. A value of 0 would yield an image that is identical to the input. A value of 1 would be as if you passed in no image at all.. \u2705 <code>prompt</code> <code>str</code> Prompt to generate new images from text (what you wish to see). \u2705 <code>negative_prompt</code> <code>str</code> Negative prompt to image generation model (what you do not wish to see). \u2705 <code>model</code> <code>str</code> choose one of {'core', 'ultra', 'sd3'}. Default 'core' . \u2705 <code>api_key</code> <code>str</code> Your Stability AI API key. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stability_ai_image_generation/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stability AI Image Generation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to use as the starting point for the generation..</li> <li><code>strength</code> (<code>float_zero_to_one</code>): controls how much influence the image parameter has on the generated image. A value of 0 would yield an image that is identical to the input. A value of 1 would be as if you passed in no image at all..</li> <li><code>prompt</code> (<code>string</code>): Prompt to generate new images from text (what you wish to see).</li> <li><code>negative_prompt</code> (<code>string</code>): Negative prompt to image generation model (what you do not wish to see).</li> <li><code>model</code> (<code>string</code>): choose one of {'core', 'ultra', 'sd3'}. Default 'core' .</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your Stability AI API key.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Stability AI Image Generation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stability_ai_image_gen@v1\",\n    \"image\": \"$inputs.image\",\n    \"strength\": 0.3,\n    \"prompt\": \"my prompt\",\n    \"negative_prompt\": \"my prompt\",\n    \"model\": \"my prompt\",\n    \"api_key\": \"xxx-xxx\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/stability_ai_inpainting/","title":"Stability AI Inpainting","text":"Class: <code>StabilityAIInpaintingBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.stability_ai.inpainting.v1.StabilityAIInpaintingBlockV1</p> <p>The block wraps  Stability AI inpainting API and  let users use instance segmentation results to change the content of images in a creative way.</p>"},{"location":"workflows/blocks/stability_ai_inpainting/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stability_ai_inpainting@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stability_ai_inpainting/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>prompt</code> <code>str</code> Prompt to inpainting model (what you wish to see).. \u2705 <code>negative_prompt</code> <code>str</code> Negative prompt to inpainting model (what you do not wish to see).. \u2705 <code>api_key</code> <code>str</code> Your Stability AI API key.. \u2705 <code>invert_segmentation_mask</code> <code>bool</code> Invert segmentation mask to inpaint background instead of foreground.. \u2705 <code>preset</code> <code>StabilityAIPresets</code> Optional preset to apply when outpainting the image (what you wish to see). If not provided, the image will be outpainted without any preset. Avaliable presets: 3d-model, analog-film, anime, cinematic, comic-book, digital-art, enhance, fantasy-art, isometric, line-art, low-poly, modeling-compound, neon-punk, origami, photographic, pixel-art, tile-texture. \u274c <code>seed</code> <code>int</code> A specific value that is used to guide the 'randomness' of the generation. If not provided, a random seed is used. Must be a number between 0 and 4294967294. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stability_ai_inpainting/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stability AI Inpainting</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to inpaint..</li> <li><code>segmentation_mask</code> (<code>instance_segmentation_prediction</code>): Model predictions from segmentation model..</li> <li><code>prompt</code> (<code>string</code>): Prompt to inpainting model (what you wish to see)..</li> <li><code>negative_prompt</code> (<code>string</code>): Negative prompt to inpainting model (what you do not wish to see)..</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your Stability AI API key..</li> <li><code>invert_segmentation_mask</code> (<code>boolean</code>): Invert segmentation mask to inpaint background instead of foreground..</li> <li><code>seed</code> (<code>integer</code>): A specific value that is used to guide the 'randomness' of the generation. If not provided, a random seed is used. Must be a number between 0 and 4294967294.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Stability AI Inpainting</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stability_ai_inpainting@v1\",\n    \"image\": \"$inputs.image\",\n    \"segmentation_mask\": \"$steps.model.predictions\",\n    \"prompt\": \"my prompt\",\n    \"negative_prompt\": \"my prompt\",\n    \"api_key\": \"xxx-xxx\",\n    \"invert_segmentation_mask\": \"&lt;block_does_not_provide_example&gt;\",\n    \"preset\": \"3d-model\",\n    \"seed\": 200\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/stability_ai_outpainting/","title":"Stability AI Outpainting","text":"Class: <code>StabilityAIOutpaintingBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.stability_ai.outpainting.v1.StabilityAIOutpaintingBlockV1</p> <p>The block wraps  Stability AI outpainting API and  let users use object detection results to change the content of images in a creative way.</p> <p>The block sends crop of the image to the API together with directions where to outpaint. As a result, the API returns the image with outpainted regions. At least one of <code>left</code>, <code>right</code>, <code>up</code>, <code>down</code> must be provided, otherwise original image is returned.</p>"},{"location":"workflows/blocks/stability_ai_outpainting/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stability_ai_outpainting@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stability_ai_outpainting/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>creativity</code> <code>float</code> Creativity parameter for outpainting. Higher values result in more creative outpainting.. \u2705 <code>left</code> <code>int</code> Number of pixels to outpaint on the left side of the image. Max value is 2000.. \u2705 <code>right</code> <code>int</code> Number of pixels to outpaint on the right side of the image. Max value is 2000.. \u2705 <code>up</code> <code>int</code> Number of pixels to outpaint on the top side of the image. Max value is 2000.. \u2705 <code>down</code> <code>int</code> Number of pixels to outpaint on the bottom side of the image. Max value is 2000.. \u2705 <code>prompt</code> <code>str</code> Optional prompt to apply when outpainting the image (what you wish to see). If not provided, the image will be outpainted without any prompt.. \u2705 <code>preset</code> <code>StabilityAIPresets</code> Optional preset to apply when outpainting the image (what you wish to see). If not provided, the image will be outpainted without any preset. Avaliable presets: 3d-model, analog-film, anime, cinematic, comic-book, digital-art, enhance, fantasy-art, isometric, line-art, low-poly, modeling-compound, neon-punk, origami, photographic, pixel-art, tile-texture. \u274c <code>seed</code> <code>int</code> A specific value that is used to guide the 'randomness' of the generation. If not provided, a random seed is used. Must be a number between 0 and 4294967294. \u2705 <code>api_key</code> <code>str</code> Your Stability AI API key.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stability_ai_outpainting/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stability AI Outpainting</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to outpaint..</li> <li><code>creativity</code> (<code>float_zero_to_one</code>): Creativity parameter for outpainting. Higher values result in more creative outpainting..</li> <li><code>left</code> (<code>integer</code>): Number of pixels to outpaint on the left side of the image. Max value is 2000..</li> <li><code>right</code> (<code>integer</code>): Number of pixels to outpaint on the right side of the image. Max value is 2000..</li> <li><code>up</code> (<code>integer</code>): Number of pixels to outpaint on the top side of the image. Max value is 2000..</li> <li><code>down</code> (<code>integer</code>): Number of pixels to outpaint on the bottom side of the image. Max value is 2000..</li> <li><code>prompt</code> (<code>string</code>): Optional prompt to apply when outpainting the image (what you wish to see). If not provided, the image will be outpainted without any prompt..</li> <li><code>seed</code> (<code>integer</code>): A specific value that is used to guide the 'randomness' of the generation. If not provided, a random seed is used. Must be a number between 0 and 4294967294.</li> <li><code>api_key</code> (Union[<code>string</code>, <code>secret</code>]): Your Stability AI API key..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Stability AI Outpainting</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stability_ai_outpainting@v1\",\n    \"image\": \"$inputs.image\",\n    \"creativity\": 0.5,\n    \"left\": 200,\n    \"right\": 200,\n    \"up\": 200,\n    \"down\": 200,\n    \"prompt\": \"my prompt\",\n    \"preset\": \"3d-model\",\n    \"seed\": 200,\n    \"api_key\": \"xxx-xxx\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/stitch_images/","title":"Stitch Images","text":"Class: <code>StitchImagesBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.stitch_images.v1.StitchImagesBlockV1</p> <p>This block combines two related scenes both containing fair amount of details. Block is utilizing Scale Invariant Feature Transform (SIFT)</p>"},{"location":"workflows/blocks/stitch_images/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stitch_images@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stitch_images/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>max_allowed_reprojection_error</code> <code>float</code> Advanced parameter overwriting cv.findHomography ransacReprojThreshold parameter. Maximum allowed reprojection error to treat a point pair as an inlier. Increasing value of this parameter for low details photo may yield better results.. \u2705 <code>count_of_best_matches_per_query_descriptor</code> <code>int</code> Advanced parameter overwriting cv.BFMatcher.knnMatch <code>k</code> parameter. Count of best matches found per each query descriptor or less if a query descriptor has less than k possible matches in total.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stitch_images/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stitch Images</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image1</code> (<code>image</code>): First input image for this step..</li> <li><code>image2</code> (<code>image</code>): Second input image for this step..</li> <li><code>max_allowed_reprojection_error</code> (<code>float_zero_to_one</code>): Advanced parameter overwriting cv.findHomography ransacReprojThreshold parameter. Maximum allowed reprojection error to treat a point pair as an inlier. Increasing value of this parameter for low details photo may yield better results..</li> <li><code>count_of_best_matches_per_query_descriptor</code> (<code>integer</code>): Advanced parameter overwriting cv.BFMatcher.knnMatch <code>k</code> parameter. Count of best matches found per each query descriptor or less if a query descriptor has less than k possible matches in total..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>stitched_image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Stitch Images</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stitch_images@v1\",\n    \"image1\": \"$inputs.image1\",\n    \"image2\": \"$inputs.image2\",\n    \"max_allowed_reprojection_error\": 3,\n    \"count_of_best_matches_per_query_descriptor\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/stitch_ocr_detections/","title":"Stitch OCR Detections","text":"Class: <code>StitchOCRDetectionsBlockV1</code> <p>Source: inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.StitchOCRDetectionsBlockV1</p> <p>Combines OCR detection results into a coherent text string by organizing detections spatially. This transformation is perfect for turning individual OCR results into structured, readable text!</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#how-it-works","title":"How It Works","text":"<p>This transformation reconstructs the original text from OCR detection results by:</p> <ol> <li> <p>\ud83d\udcd0 Grouping text detections into rows based on their vertical (<code>y</code>) positions</p> </li> <li> <p>\ud83d\udccf Sorting detections within each row by horizontal (<code>x</code>) position</p> </li> <li> <p>\ud83d\udcdc Concatenating the text in reading order (left-to-right, top-to-bottom)</p> </li> </ol>"},{"location":"workflows/blocks/stitch_ocr_detections/#parameters","title":"Parameters","text":"<ul> <li> <p><code>tolerance</code>: Controls how close detections need to be vertically to be considered part of the same line of text. A higher tolerance will group detections that are further apart vertically.</p> </li> <li> <p><code>reading_direction</code>: Determines the order in which text is read. Available options:</p> <ul> <li> <p>\"left_to_right\": Standard left-to-right reading (e.g., English) \u27a1\ufe0f</p> </li> <li> <p>\"right_to_left\": Right-to-left reading (e.g., Arabic) \u2b05\ufe0f</p> </li> <li> <p>\"vertical_top_to_bottom\": Vertical reading from top to bottom \u2b07\ufe0f</p> </li> <li> <p>\"vertical_bottom_to_top\": Vertical reading from bottom to top \u2b06\ufe0f</p> </li> <li> <p>\"auto\": Automatically detects the reading direction based on the spatial arrangement of text elements.</p> </li> </ul> </li> </ul>"},{"location":"workflows/blocks/stitch_ocr_detections/#why-use-this-transformation","title":"Why Use This Transformation?","text":"<p>This is especially useful for:</p> <ul> <li> <p>\ud83d\udcd6 Converting individual character/word detections into a readable text block</p> </li> <li> <p>\ud83d\udcdd Reconstructing multi-line text from OCR results</p> </li> <li> <p>\ud83d\udd00 Maintaining proper reading order for detected text elements</p> </li> <li> <p>\ud83c\udf0f Supporting different writing systems and text orientations</p> </li> </ul>"},{"location":"workflows/blocks/stitch_ocr_detections/#example-usage","title":"Example Usage","text":"<p>Use this transformation after an OCR model that outputs individual words or characters, so you can reconstruct the original text layout in its intended format.</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stitch_ocr_detections@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>reading_direction</code> <code>str</code> The direction of the text in the image.. \u274c <code>tolerance</code> <code>int</code> The tolerance for grouping detections into the same line of text.. \u2705 <code>delimiter</code> <code>str</code> The delimiter to use for stitching text.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stitch OCR Detections</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): The output of an OCR detection model..</li> <li><code>tolerance</code> (<code>integer</code>): The tolerance for grouping detections into the same line of text..</li> <li><code>delimiter</code> (<code>string</code>): The delimiter to use for stitching text..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>ocr_text</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Stitch OCR Detections</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stitch_ocr_detections@v1\",\n    \"predictions\": \"$steps.my_ocr_detection_model.predictions\",\n    \"reading_direction\": \"right_to_left\",\n    \"tolerance\": 10,\n    \"delimiter\": \"\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/template_matching/","title":"Template Matching","text":"Class: <code>TemplateMatchingBlockV1</code> <p>Source: inference.core.workflows.core_steps.classical_cv.template_matching.v1.TemplateMatchingBlockV1</p> <p>Apply Template Matching to an image. Block is based on OpenCV library function called <code>cv2.matchTemplate(...)</code> that searches for a template image within a larger image. This is often used in computer vision tasks where  you need to find a specific object or pattern in a scene, like detecting logos, objects, or  specific regions in an image.</p> <p>Please take into account the following characteristics of block: * it tends to produce overlapping and duplicated predictions, hence we added NMS which can be disabled * block may find very large number of matches in some cases due to simplicity of methods being used -  in that cases NMS may be computationally intractable and should be disabled</p> <p>Output from the block is in a form of sv.Detections objects which can be nicely paired with other blocks accepting this kind of input (like visualization blocks).</p>"},{"location":"workflows/blocks/template_matching/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/template_matching@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/template_matching/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>matching_threshold</code> <code>float</code> The threshold value for template matching.. \u2705 <code>apply_nms</code> <code>bool</code> Flag to decide if NMS should be applied at the output detections.. \u2705 <code>nms_threshold</code> <code>float</code> The threshold value NMS procedure (if to be applied).. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/template_matching/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Template Matching</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>template</code> (<code>image</code>): The template image for this step..</li> <li><code>matching_threshold</code> (<code>float</code>): The threshold value for template matching..</li> <li><code>apply_nms</code> (<code>boolean</code>): Flag to decide if NMS should be applied at the output detections..</li> <li><code>nms_threshold</code> (<code>float_zero_to_one</code>): The threshold value NMS procedure (if to be applied)..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>number_of_matches</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Template Matching</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/template_matching@v1\",\n    \"image\": \"$inputs.image\",\n    \"template\": \"$inputs.template\",\n    \"matching_threshold\": 0.8,\n    \"apply_nms\": \"$inputs.apply_nms\",\n    \"nms_threshold\": \"$inputs.nms_threshold\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/timein_zone/","title":"Time in Zone","text":""},{"location":"workflows/blocks/timein_zone/#v3","title":"v3","text":"Class: <code>TimeInZoneBlockV3</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.time_in_zone.v3.TimeInZoneBlockV3</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>TimeInZoneBlock</code> is an analytics block designed to measure time spent by objects in a zone. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/timein_zone/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/time_in_zone@v3</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/timein_zone/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>zone</code> <code>List[Any]</code> Coordinates of the target zone.. \u2705 <code>triggering_anchor</code> <code>str</code> The point on the detection that must be inside the zone.. \u2705 <code>remove_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will be filtered out.. \u2705 <code>reset_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will have time reset.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/timein_zone/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Time in Zone</code> in version <code>v3</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to calculate the time spent in zone for..</li> <li><code>zone</code> (<code>list_of_values</code>): Coordinates of the target zone..</li> <li><code>triggering_anchor</code> (<code>string</code>): The point on the detection that must be inside the zone..</li> <li><code>remove_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will be filtered out..</li> <li><code>reset_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will have time reset..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>timed_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Time in Zone</code> in version <code>v3</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/time_in_zone@v3\",\n    \"image\": \"$inputs.image\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"zone\": [\n        [\n            100,\n            100\n        ],\n        [\n            100,\n            200\n        ],\n        [\n            300,\n            200\n        ],\n        [\n            300,\n            100\n        ]\n    ],\n    \"triggering_anchor\": \"CENTER\",\n    \"remove_out_of_zone_detections\": true,\n    \"reset_out_of_zone_detections\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/timein_zone/#v2","title":"v2","text":"Class: <code>TimeInZoneBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.time_in_zone.v2.TimeInZoneBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>TimeInZoneBlock</code> is an analytics block designed to measure time spent by objects in a zone. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/timein_zone/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/time_in_zone@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/timein_zone/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>zone</code> <code>List[Any]</code> Coordinates of the target zone.. \u2705 <code>triggering_anchor</code> <code>str</code> The point on the detection that must be inside the zone.. \u2705 <code>remove_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will be filtered out.. \u2705 <code>reset_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will have time reset.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/timein_zone/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Time in Zone</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to calculate the time spent in zone for..</li> <li><code>zone</code> (<code>list_of_values</code>): Coordinates of the target zone..</li> <li><code>triggering_anchor</code> (<code>string</code>): The point on the detection that must be inside the zone..</li> <li><code>remove_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will be filtered out..</li> <li><code>reset_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will have time reset..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>timed_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Time in Zone</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/time_in_zone@v2\",\n    \"image\": \"$inputs.image\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"zone\": [\n        [\n            100,\n            100\n        ],\n        [\n            100,\n            200\n        ],\n        [\n            300,\n            200\n        ],\n        [\n            300,\n            100\n        ]\n    ],\n    \"triggering_anchor\": \"CENTER\",\n    \"remove_out_of_zone_detections\": true,\n    \"reset_out_of_zone_detections\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/timein_zone/#v1","title":"v1","text":"Class: <code>TimeInZoneBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.analytics.time_in_zone.v1.TimeInZoneBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The <code>TimeInZoneBlock</code> is an analytics block designed to measure time spent by objects in a zone. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/timein_zone/#type-identifier_2","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/time_in_zone@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/timein_zone/#properties_2","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>zone</code> <code>List[Any]</code> Coordinates of the target zone.. \u2705 <code>triggering_anchor</code> <code>str</code> The point on the detection that must be inside the zone.. \u2705 <code>remove_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will be filtered out.. \u2705 <code>reset_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will have time reset.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/timein_zone/#input-and-output-bindings_2","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Time in Zone</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to calculate the time spent in zone for..</li> <li><code>zone</code> (<code>list_of_values</code>): Coordinates of the target zone..</li> <li><code>triggering_anchor</code> (<code>string</code>): The point on the detection that must be inside the zone..</li> <li><code>remove_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will be filtered out..</li> <li><code>reset_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will have time reset..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>timed_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Time in Zone</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/time_in_zone@v1\",\n    \"image\": \"$inputs.image\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"zone\": \"$inputs.zones\",\n    \"triggering_anchor\": \"CENTER\",\n    \"remove_out_of_zone_detections\": true,\n    \"reset_out_of_zone_detections\": true\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/trace_visualization/","title":"Trace Visualization","text":"Class: <code>TraceVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.trace.v1.TraceVisualizationBlockV1</p> <p>The <code>TraceVisualization</code> block draws tracker results on an image using Supervision's <code>sv.TraceAnnotator</code>.</p>"},{"location":"workflows/blocks/trace_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/trace_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/trace_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the label.. \u2705 <code>trace_length</code> <code>int</code> Maximum number of historical tracked objects positions to display.. \u2705 <code>thickness</code> <code>int</code> Thickness of the track visualization line.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/trace_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Trace Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the label..</li> <li><code>trace_length</code> (<code>integer</code>): Maximum number of historical tracked objects positions to display..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the track visualization line..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Trace Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/trace_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"trace_length\": 30,\n    \"thickness\": 1\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/triangle_visualization/","title":"Triangle Visualization","text":"Class: <code>TriangleVisualizationBlockV1</code> <p>Source: inference.core.workflows.core_steps.visualizations.triangle.v1.TriangleVisualizationBlockV1</p> <p>The <code>TriangleVisualization</code> block draws triangle markers on an image at specific coordinates based on provided detections using Supervision's <code>sv.TriangleAnnotator</code>.</p>"},{"location":"workflows/blocks/triangle_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/triangle_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/triangle_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>copy_image</code> <code>bool</code> Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations.. \u2705 <code>color_palette</code> <code>str</code> Select a color palette for the visualised elements.. \u2705 <code>palette_size</code> <code>int</code> Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes.. \u2705 <code>custom_colors</code> <code>List[str]</code> Define a list of custom colors for bounding boxes in HEX format.. \u2705 <code>color_axis</code> <code>str</code> Choose how bounding box colors are assigned.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the triangle.. \u2705 <code>base</code> <code>int</code> Base width of the triangle in pixels.. \u2705 <code>height</code> <code>int</code> Height of the triangle in pixels.. \u2705 <code>outline_thickness</code> <code>int</code> Thickness of the outline of the triangle in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/triangle_visualization/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Triangle Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image to visualize on..</li> <li><code>copy_image</code> (<code>boolean</code>): Enable this option to create a copy of the input image for visualization, preserving the original. Use this when stacking multiple visualizations..</li> <li><code>predictions</code> (Union[<code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to visualize..</li> <li><code>color_palette</code> (<code>string</code>): Select a color palette for the visualised elements..</li> <li><code>palette_size</code> (<code>integer</code>): Specify the number of colors in the palette. This applies when using custom or Matplotlib palettes..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): Define a list of custom colors for bounding boxes in HEX format..</li> <li><code>color_axis</code> (<code>string</code>): Choose how bounding box colors are assigned..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the triangle..</li> <li><code>base</code> (<code>integer</code>): Base width of the triangle in pixels..</li> <li><code>height</code> (<code>integer</code>): Height of the triangle in pixels..</li> <li><code>outline_thickness</code> (<code>integer</code>): Thickness of the outline of the triangle in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Triangle Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/triangle_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"base\": 10,\n    \"height\": 10,\n    \"outline_thickness\": 2\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/twilio_sms_notification/","title":"Twilio SMS Notification","text":"Class: <code>TwilioSMSNotificationBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.twilio.sms.v1.TwilioSMSNotificationBlockV1</p> <p>The \ud83d\udcf2 Twilio SMS Notification \u2709\ufe0f block enables sending text message notifications via the Twilio SMS service,  with flexible features such as dynamic content, message truncation, and cooldown management.</p> <p>The block requires Twilio setup -  this article may help you  configuring everything properly.</p>"},{"location":"workflows/blocks/twilio_sms_notification/#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p>\ud83d\udce2 Send SMS: Deliver SMS messages to designated recipients.</p> </li> <li> <p>\ud83d\udd17 Dynamic Content: Craft notifications based on outputs from other Workflow steps.</p> </li> <li> <p>\u2702\ufe0f Message Truncation: Automatically truncate messages exceeding the character limit.</p> </li> <li> <p>\ud83d\udd52 Cooldown Control: Prevent duplicate notifications within a set time frame.</p> </li> <li> <p>\u2699\ufe0f Flexible Execution: Execute in the background or block Workflow execution for debugging.</p> </li> </ul>"},{"location":"workflows/blocks/twilio_sms_notification/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/twilio_sms_notification@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/twilio_sms_notification/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>twilio_account_sid</code> <code>str</code> Twilio Account SID. Visit the Twilio Console to configure the SMS service and retrieve the value.. \u2705 <code>twilio_auth_token</code> <code>str</code> Twilio Auth Token. Visit the Twilio Console to configure the SMS service and retrieve the value.. \u2705 <code>sender_number</code> <code>str</code> Sender phone number. \u2705 <code>receiver_number</code> <code>str</code> Receiver phone number. \u2705 <code>message</code> <code>str</code> Content of the message to be sent.. \u274c <code>message_parameters</code> <code>Dict[str, Union[bool, float, int, str]]</code> Data to be used in the message content.. \u2705 <code>message_parameters_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> Preprocessing operations to be performed on message parameters.. \u274c <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or synchronously (False) for debugging and error handling.. \u2705 <code>disable_sink</code> <code>bool</code> Boolean flag to disable block execution.. \u2705 <code>cooldown_seconds</code> <code>int</code> Number of seconds until a follow-up notification can be sent. Maximum value: 900 seconds (15 minutes). \u2705 <code>cooldown_session_key</code> <code>str</code> Unique key used internally to implement cooldown. Must be unique for each step in your Workflow.. \u274c <code>length_limit</code> <code>int</code> Maximum number of characters in SMS notification (longer messages will be truncated).. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/twilio_sms_notification/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Twilio SMS Notification</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>twilio_account_sid</code> (Union[<code>string</code>, <code>secret</code>]): Twilio Account SID. Visit the Twilio Console to configure the SMS service and retrieve the value..</li> <li><code>twilio_auth_token</code> (Union[<code>string</code>, <code>secret</code>]): Twilio Auth Token. Visit the Twilio Console to configure the SMS service and retrieve the value..</li> <li><code>sender_number</code> (<code>string</code>): Sender phone number.</li> <li><code>receiver_number</code> (<code>string</code>): Receiver phone number.</li> <li><code>message_parameters</code> (<code>*</code>): Data to be used in the message content..</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or synchronously (False) for debugging and error handling..</li> <li><code>disable_sink</code> (<code>boolean</code>): Boolean flag to disable block execution..</li> <li><code>cooldown_seconds</code> (<code>integer</code>): Number of seconds until a follow-up notification can be sent. Maximum value: 900 seconds (15 minutes).</li> <li><code>length_limit</code> (<code>integer</code>): Maximum number of characters in SMS notification (longer messages will be truncated)..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>throttling_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Twilio SMS Notification</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/twilio_sms_notification@v1\",\n    \"twilio_account_sid\": \"$inputs.twilio_account_sid\",\n    \"twilio_auth_token\": \"$inputs.twilio_auth_token\",\n    \"sender_number\": \"+1234567890\",\n    \"receiver_number\": \"+1234567890\",\n    \"message\": \"During last 5 minutes detected {{ $parameters.num_instances }} instances\",\n    \"message_parameters\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"message_parameters_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"fire_and_forget\": \"$inputs.fire_and_forget\",\n    \"disable_sink\": false,\n    \"cooldown_seconds\": \"$inputs.cooldown_seconds\",\n    \"cooldown_session_key\": \"session-1v73kdhfse\",\n    \"length_limit\": \"$inputs.sms_length_limit\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/velocity/","title":"Velocity","text":"Class: <code>VelocityBlockV1</code> <p>Source: inference.core.workflows.core_steps.analytics.velocity.v1.VelocityBlockV1</p> <p>The <code>VelocityBlock</code> computes the velocity and speed of objects tracked across video frames. It includes options to smooth the velocity and speed measurements over time and to convert units from pixels per second to meters per second. It requires detections from Byte Track with unique <code>tracker_id</code> assigned to each object, which persists between frames. The velocities are calculated based on the displacement of object centers over time.</p> <p>Note: due to perspective and camera distortions calculated velocity will be different depending on object position in relation to the camera.</p>"},{"location":"workflows/blocks/velocity/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/velocity@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/velocity/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>smoothing_alpha</code> <code>float</code> Smoothing factor (alpha) for exponential moving average (0 &lt; alpha &lt;= 1). Lower alpha means more smoothing.. \u2705 <code>pixels_per_meter</code> <code>float</code> Conversion from pixels to meters. Velocity will be converted to meters per second using this value.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/velocity/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Velocity</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>instance_segmentation_prediction</code>, <code>object_detection_prediction</code>]): Model predictions to calculate the velocity for..</li> <li><code>smoothing_alpha</code> (<code>float</code>): Smoothing factor (alpha) for exponential moving average (0 &lt; alpha &lt;= 1). Lower alpha means more smoothing..</li> <li><code>pixels_per_meter</code> (<code>float</code>): Conversion from pixels to meters. Velocity will be converted to meters per second using this value..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>velocity_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Velocity</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/velocity@v1\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"smoothing_alpha\": 0.5,\n    \"pixels_per_meter\": 0.01\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/vl_mas_classifier/","title":"VLM as Classifier","text":""},{"location":"workflows/blocks/vl_mas_classifier/#v2","title":"v2","text":"Class: <code>VLMAsClassifierBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.formatters.vlm_as_classifier.v2.VLMAsClassifierBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to classification prediction and returned as block output.</p> <p>Accepted formats:</p> <ul> <li> <p>valid JSON strings</p> </li> <li> <p>JSON documents wrapped with Markdown tags (very common for GPT responses)</p> </li> </ul> <p>Example: <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever parsing cannot be completed</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed</p> </li> </ul>"},{"location":"workflows/blocks/vl_mas_classifier/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/vlm_as_classifier@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/vl_mas_classifier/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>classes</code> <code>List[str]</code> List of all classes used by the model, required to generate mapping between class name and class id.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/vl_mas_classifier/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>VLM as Classifier</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>vlm_output</code> (<code>language_model_output</code>): The string with raw classification prediction to parse..</li> <li><code>classes</code> (<code>list_of_values</code>): List of all classes used by the model, required to generate mapping between class name and class id..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> </ul> </li> </ul> Example JSON definition of step <code>VLM as Classifier</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n    \"image\": \"$inputs.image\",\n    \"vlm_output\": [\n        \"$steps.lmm.output\"\n    ],\n    \"classes\": [\n        \"$steps.lmm.classes\",\n        \"$inputs.classes\",\n        [\n            \"class_a\",\n            \"class_b\"\n        ]\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/vl_mas_classifier/#v1","title":"v1","text":"Class: <code>VLMAsClassifierBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.formatters.vlm_as_classifier.v1.VLMAsClassifierBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to classification prediction and returned as block output.</p> <p>Accepted formats:</p> <ul> <li> <p>valid JSON strings</p> </li> <li> <p>JSON documents wrapped with Markdown tags (very common for GPT responses)</p> </li> </ul> <p>Example: <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever parsing cannot be completed</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed</p> </li> </ul>"},{"location":"workflows/blocks/vl_mas_classifier/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/vlm_as_classifier@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/vl_mas_classifier/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>classes</code> <code>List[str]</code> List of all classes used by the model, required to generate mapping between class name and class id.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/vl_mas_classifier/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>VLM as Classifier</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>vlm_output</code> (<code>language_model_output</code>): The string with raw classification prediction to parse..</li> <li><code>classes</code> (<code>list_of_values</code>): List of all classes used by the model, required to generate mapping between class name and class id..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>VLM as Classifier</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n    \"image\": \"$inputs.image\",\n    \"vlm_output\": [\n        \"$steps.lmm.output\"\n    ],\n    \"classes\": [\n        \"$steps.lmm.classes\",\n        \"$inputs.classes\",\n        [\n            \"class_a\",\n            \"class_b\"\n        ]\n    ]\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/vl_mas_detector/","title":"VLM as Detector","text":""},{"location":"workflows/blocks/vl_mas_detector/#v2","title":"v2","text":"Class: <code>VLMAsDetectorBlockV2</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.formatters.vlm_as_detector.v2.VLMAsDetectorBlockV2</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to object-detection prediction and returned as block output.</p> <p>Accepted formats:</p> <ul> <li> <p>valid JSON strings</p> </li> <li> <p>JSON documents wrapped with Markdown tags</p> </li> </ul> <p>Example <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever parsing cannot be completed</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed</p> </li> </ul>"},{"location":"workflows/blocks/vl_mas_detector/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/vlm_as_detector@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/vl_mas_detector/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>classes</code> <code>List[str]</code> List of all classes used by the model, required to generate mapping between class name and class id.. \u2705 <code>model_type</code> <code>str</code> Type of the model that generated prediction. \u274c <code>task_type</code> <code>str</code> Task type to performed by model.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/vl_mas_detector/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>VLM as Detector</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>vlm_output</code> (<code>language_model_output</code>): The string with raw classification prediction to parse..</li> <li><code>classes</code> (<code>list_of_values</code>): List of all classes used by the model, required to generate mapping between class name and class id..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>inference_id</code> (<code>inference_id</code>): Inference identifier.</li> </ul> </li> </ul> Example JSON definition of step <code>VLM as Detector</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/vlm_as_detector@v2\",\n    \"image\": \"$inputs.image\",\n    \"vlm_output\": [\n        \"$steps.lmm.output\"\n    ],\n    \"classes\": [\n        \"$steps.lmm.classes\",\n        \"$inputs.classes\",\n        [\n            \"class_a\",\n            \"class_b\"\n        ]\n    ],\n    \"model_type\": [\n        \"google-gemini\",\n        \"anthropic-claude\",\n        \"florence-2\"\n    ],\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/vl_mas_detector/#v1","title":"v1","text":"Class: <code>VLMAsDetectorBlockV1</code> (there are multiple versions of this block) <p>Source: inference.core.workflows.core_steps.formatters.vlm_as_detector.v1.VLMAsDetectorBlockV1</p> <p>Warning: This block has multiple versions. Please refer to the specific version for details. You can learn more about how versions work here: Versioning</p> <p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to object-detection prediction and returned as block output.</p> <p>Accepted formats:</p> <ul> <li> <p>valid JSON strings</p> </li> <li> <p>JSON documents wrapped with Markdown tags</p> </li> </ul> <p>Example <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever parsing cannot be completed</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed</p> </li> </ul>"},{"location":"workflows/blocks/vl_mas_detector/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/vlm_as_detector@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/vl_mas_detector/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>classes</code> <code>List[str]</code> List of all classes used by the model, required to generate mapping between class name and class id.. \u2705 <code>model_type</code> <code>str</code> Type of the model that generated prediction. \u274c <code>task_type</code> <code>str</code> Task type to performed by model.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/vl_mas_detector/#input-and-output-bindings_1","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>VLM as Detector</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>vlm_output</code> (<code>language_model_output</code>): The string with raw classification prediction to parse..</li> <li><code>classes</code> (<code>list_of_values</code>): List of all classes used by the model, required to generate mapping between class name and class id..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>VLM as Detector</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/vlm_as_detector@v1\",\n    \"image\": \"$inputs.image\",\n    \"vlm_output\": [\n        \"$steps.lmm.output\"\n    ],\n    \"classes\": [\n        \"$steps.lmm.classes\",\n        \"$inputs.classes\",\n        [\n            \"class_a\",\n            \"class_b\"\n        ]\n    ],\n    \"model_type\": [\n        \"google-gemini\",\n        \"anthropic-claude\",\n        \"florence-2\"\n    ],\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/","title":"Webhook Sink","text":"Class: <code>WebhookSinkBlockV1</code> <p>Source: inference.core.workflows.core_steps.sinks.webhook.v1.WebhookSinkBlockV1</p> <p>The Webhook Sink block enables sending a data from Workflow into external APIs  by sending HTTP requests containing workflow results. It supports multiple HTTP methods  (GET, POST, PUT) and can be configured to send:</p> <ul> <li> <p>JSON payloads</p> </li> <li> <p>query parameters</p> </li> <li> <p>multipart-encoded files </p> </li> </ul> <p>This block is designed to provide flexibility for integrating workflows with remote systems  for data exchange, notifications, or other integrations.</p>"},{"location":"workflows/blocks/webhook_sink/#setting-query-parameters","title":"Setting Query Parameters","text":"<p>You can easily set query parameters for your request:</p> <pre><code>query_parameters = {\n    \"api_key\": \"$inputs.api_key\",\n}\n</code></pre> <p>will send request into the following URL: <code>https://your-host/some/resource?api_key=&lt;API_KEY_VALUE&gt;</code></p>"},{"location":"workflows/blocks/webhook_sink/#setting-headers","title":"Setting headers","text":"<p>Setting headers is as easy as setting query parameters:</p> <pre><code>headers = {\n    \"api_key\": \"$inputs.api_key\",\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/#sending-json-payloads","title":"Sending JSON payloads","text":"<p>You can set the body of your message to be JSON document that you construct specifying <code>json_payload</code>  and <code>json_payload_operations</code> properties. <code>json_payload</code> works similarly to <code>query_parameters</code> and  <code>headers</code>, but you can optionally apply UQL operations on top of JSON body elements.</p> <p>Let's assume that you want to send number of bounding boxes predicted by object detection model in body field named <code>detections_number</code>, then you need to specify configuration similar to the  following:</p> <pre><code>json_payload = {\n    \"detections_number\": \"$steps.model.predictions\",\n}\njson_payload_operations = {\n    \"detections_number\": [{\"type\": \"SequenceLength\"}]\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/#multipart-encoded-files-in-post-requests","title":"Multipart-Encoded Files in POST requests","text":"<p>Your endpoint may also accept multipart requests. You can form them in a way which is similar to  JSON payloads - setting <code>multi_part_encoded_files</code> and <code>multi_part_encoded_files_operations</code>.</p> <p>Let's assume you want to send the image in the request, then your configuration may be the following:</p> <pre><code>multi_part_encoded_files = {\n    \"image\": \"$inputs.image\",\n}\nmulti_part_encoded_files_operations = {\n    \"image\": [{\"type\": \"ConvertImageToJPEG\"}]\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/#cooldown","title":"Cooldown","text":"<p>The block accepts <code>cooldown_seconds</code> (which defaults to <code>5</code> seconds) to prevent unintended bursts of  notifications. Please adjust it according to your needs, setting <code>0</code> indicate no cooldown. </p> <p>During cooldown period, consecutive runs of the step will cause <code>throttling_status</code> output to be set <code>True</code> and no notification will be sent.</p> <p>Cooldown limitations</p> <p>Current implementation of cooldown is limited to video processing - using this block in context of a  Workflow that is run behind HTTP service (Roboflow Hosted API, Dedicated Deployment or self-hosted  <code>inference</code> server) will have no effect for processing HTTP requests.  </p>"},{"location":"workflows/blocks/webhook_sink/#async-execution","title":"Async execution","text":"<p>Configure the <code>fire_and_forget</code> property. Set it to True if you want the request to be sent in the background,  allowing the Workflow to proceed without waiting on data to be sent. In this case you will not be able to rely on  <code>error_status</code> output which will always be set to <code>False</code>, so we recommend setting the <code>fire_and_forget=False</code> for debugging purposes.</p>"},{"location":"workflows/blocks/webhook_sink/#disabling-notifications-based-on-runtime-parameter","title":"Disabling notifications based on runtime parameter","text":"<p>Sometimes it would be convenient to manually disable the Webhook sink block. This is possible  setting <code>disable_sink</code> flag to hold reference to Workflow input. with such setup, caller would be able to disable the sink when needed sending agreed input parameter.</p>"},{"location":"workflows/blocks/webhook_sink/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/webhook_sink@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/webhook_sink/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>url</code> <code>str</code> URL of the resource to make request. \u2705 <code>method</code> <code>str</code> HTTP method to be used. \u274c <code>query_parameters</code> <code>Dict[str, Union[List[Union[bool, float, int, str]], bool, float, int, str]]</code> Request query parameters. \u2705 <code>headers</code> <code>Dict[str, Union[bool, float, int, str]]</code> Request headers. \u2705 <code>json_payload</code> <code>Dict[str, Union[Dict[Any, Any], List[Any], bool, float, int, str]]</code> Fields to put into JSON payload. \u2705 <code>json_payload_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each value of <code>json_payload</code> parameter. \u274c <code>multi_part_encoded_files</code> <code>Dict[str, Union[Dict[Any, Any], List[Any], bool, float, int, str]]</code> Data to POST as Multipart-Encoded File. \u2705 <code>multi_part_encoded_files_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each value of <code>multi_part_encoded_files</code> parameter. \u274c <code>form_data</code> <code>Dict[str, Union[Dict[Any, Any], List[Any], bool, float, int, str]]</code> Fields to put into form-data. \u2705 <code>form_data_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractFrameMetadata, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, PickDetectionsByParentClass, RandomNumber, SequenceAggregate, SequenceApply, SequenceElementsCount, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, TimestampToISOFormat, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each value of <code>form_data</code> parameter. \u274c <code>request_timeout</code> <code>int</code> Number of seconds to wait for remote API response. \u2705 <code>fire_and_forget</code> <code>bool</code> Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling.. \u2705 <code>disable_sink</code> <code>bool</code> Boolean flag to disable block execution.. \u2705 <code>cooldown_seconds</code> <code>int</code> Number of seconds to wait until follow-up notification can be sent.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/webhook_sink/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>Webhook Sink</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>url</code> (<code>string</code>): URL of the resource to make request.</li> <li><code>query_parameters</code> (Union[<code>roboflow_project</code>, <code>roboflow_model_id</code>, <code>string</code>, <code>integer</code>, <code>roboflow_api_key</code>, <code>list_of_values</code>, <code>float</code>, <code>boolean</code>, <code>top_class</code>, <code>float_zero_to_one</code>]): Request query parameters.</li> <li><code>headers</code> (Union[<code>roboflow_project</code>, <code>roboflow_model_id</code>, <code>string</code>, <code>integer</code>, <code>roboflow_api_key</code>, <code>float</code>, <code>boolean</code>, <code>top_class</code>, <code>float_zero_to_one</code>]): Request headers.</li> <li><code>json_payload</code> (<code>*</code>): Fields to put into JSON payload.</li> <li><code>multi_part_encoded_files</code> (<code>*</code>): Data to POST as Multipart-Encoded File.</li> <li><code>form_data</code> (<code>*</code>): Fields to put into form-data.</li> <li><code>request_timeout</code> (<code>integer</code>): Number of seconds to wait for remote API response.</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag to run the block asynchronously (True) for faster workflows or  synchronously (False) for debugging and error handling..</li> <li><code>disable_sink</code> (<code>boolean</code>): Boolean flag to disable block execution..</li> <li><code>cooldown_seconds</code> (<code>integer</code>): Number of seconds to wait until follow-up notification can be sent..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>throttling_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Webhook Sink</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/webhook_sink@v1\",\n    \"url\": \"&lt;block_does_not_provide_example&gt;\",\n    \"method\": \"&lt;block_does_not_provide_example&gt;\",\n    \"query_parameters\": {\n        \"api_key\": \"$inputs.api_key\"\n    },\n    \"headers\": {\n        \"api_key\": \"$inputs.api_key\"\n    },\n    \"json_payload\": {\n        \"field\": \"$steps.model.predictions\"\n    },\n    \"json_payload_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"multi_part_encoded_files\": {\n        \"image\": \"$steps.visualization.image\"\n    },\n    \"multi_part_encoded_files_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"form_data\": {\n        \"field\": \"$inputs.field_value\"\n    },\n    \"form_data_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"request_timeout\": \"$inputs.request_timeout\",\n    \"fire_and_forget\": \"$inputs.fire_and_forget\",\n    \"disable_sink\": false,\n    \"cooldown_seconds\": \"$inputs.cooldown_seconds\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/blocks/yolo_world_model/","title":"YOLO-World Model","text":"Class: <code>YoloWorldModelBlockV1</code> <p>Source: inference.core.workflows.core_steps.models.foundation.yolo_world.v1.YoloWorldModelBlockV1</p> <p>Run YOLO-World, a zero-shot object detection model, on an image.</p> <p>YOLO-World accepts one or more text classes you want to identify in an image. The model  returns the location of objects that meet the specified class, if YOLO-World is able to  identify objects of that class.</p> <p>We recommend experimenting with YOLO-World to evaluate the model on your use case  before using this block in production. For example on how to effectively prompt  YOLO-World, refer to the Roboflow YOLO-World prompting  guide.</p>"},{"location":"workflows/blocks/yolo_world_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/yolo_world_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/yolo_world_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Enter a unique identifier for this step.. \u274c <code>class_names</code> <code>List[str]</code> One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects.. \u2705 <code>version</code> <code>str</code> Variant of YoloWorld model. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for detections. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/yolo_world_model/#input-and-output-bindings","title":"Input and Output Bindings","text":"<p>The available connections depend on its binding kinds. Check what binding kinds  <code>YOLO-World Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on..</li> <li><code>class_names</code> (<code>list_of_values</code>): One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects..</li> <li><code>version</code> (<code>string</code>): Variant of YoloWorld model.</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for detections.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>YOLO-World Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/yolo_world_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"class_names\": [\n        \"person\",\n        \"car\",\n        \"license plate\"\n    ],\n    \"version\": \"v2-s\",\n    \"confidence\": 0.005\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"workflows/gallery/","title":"Workflows gallery","text":"<p>The Workflows gallery offers example workflow definitions to help you understand what can be achieved with workflows.  Browse through the various categories to find inspiration and ideas for building your own workflows.</p> <ul> <li>Basic Workflows</li> <li>Workflows with multiple models</li> <li>Workflows enhanced by Roboflow Platform</li> <li>Workflows with classical Computer Vision methods</li> <li>Workflows with visualization blocks</li> <li>Workflows with Visual Language Models</li> <li>Data analytics in Workflows</li> <li>Workflows with dynamic Python Blocks</li> <li>Filtering resulting data based on value delta change</li> <li>Workflows with Depth Estimation</li> <li>Workflows for OCR</li> <li>Workflows with data transformations</li> <li>Workflows with flow control</li> <li>Workflows with foundation models</li> <li>Workflows with business logic</li> <li>Advanced inference techniques</li> <li>Integration with external apps</li> </ul>"},{"location":"workflows/gallery/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Basic Workflows</li> <li>Multiple Models</li> <li>Enhanced By Roboflow Platform</li> <li>Classical Computer Vision Methods</li> <li>Visualization Blocks</li> <li>Visual Language Models</li> <li>Data Analytics</li> <li>Dynamic Python Blocks</li> <li>Filtering Data</li> <li>Depth Estimation</li> <li>OCR</li> <li>Data Transformations</li> <li>Flow Control</li> <li>Foundation Models</li> <li>Business Logic</li> <li>Advanced Inference Techniques</li> <li>Integration With External Apps</li> </ul>"},{"location":"workflows/gallery/advanced_inference_techniques/","title":"Advanced inference techniques","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/advanced_inference_techniques/#sahi-in-workflows-object-detection","title":"SAHI in workflows - object detection","text":"<p>This example illustrates usage of SAHI  technique in workflows.</p> <p>Workflows implementation requires three blocks:</p> <ul> <li> <p>Image Slicer - which runs a sliding window over image and for each image prepares batch of crops </p> </li> <li> <p>detection model block (in our scenario Roboflow Object Detection model) - which is responsible  for making predictions on each crop</p> </li> <li> <p>Detections stitch - which combines partial predictions for each slice of the image into a single prediction</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"overlap_filtering_strategy\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/image_slicer@v1\",\n            \"name\": \"image_slicer\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"detection\",\n            \"image\": \"$steps.image_slicer.slices\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_stitch@v1\",\n            \"name\": \"stitch\",\n            \"reference_image\": \"$inputs.image\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"overlap_filtering_strategy\": \"$inputs.overlap_filtering_strategy\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bbox_visualiser\",\n            \"predictions\": \"$steps.stitch.predictions\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.stitch.predictions\",\n            \"coordinates_system\": \"own\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualisation\",\n            \"selector\": \"$steps.bbox_visualiser.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/advanced_inference_techniques/#sahi-in-workflows-instance-segmentation","title":"SAHI in workflows - instance segmentation","text":"<p>This example illustrates usage of SAHI  technique in workflows.</p> <p>Workflows implementation requires three blocks:</p> <ul> <li> <p>Image Slicer - which runs a sliding window over image and for each image prepares batch of crops </p> </li> <li> <p>detection model block (in our scenario Roboflow Instance Segmentation model) - which is responsible  for making predictions on each crop</p> </li> <li> <p>Detections stitch - which combines partial predictions for each slice of the image into a single prediction</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"overlap_filtering_strategy\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/image_slicer@v1\",\n            \"name\": \"image_slicer\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"detection\",\n            \"image\": \"$steps.image_slicer.slices\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_stitch@v1\",\n            \"name\": \"stitch\",\n            \"reference_image\": \"$inputs.image\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"overlap_filtering_strategy\": \"$inputs.overlap_filtering_strategy\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bbox_visualiser\",\n            \"predictions\": \"$steps.stitch.predictions\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.stitch.predictions\",\n            \"coordinates_system\": \"own\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualisation\",\n            \"selector\": \"$steps.bbox_visualiser.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/","title":"Basic Workflows","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-bounding-rect","title":"Workflow with bounding rect","text":"<p>This is the basic workflow that only contains a single object detection model and bounding rectangle extraction.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"InstanceSegmentationModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-seg-640\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_rect@v1\",\n            \"name\": \"bounding_rect\",\n            \"predictions\": \"$steps.detection.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.bounding_rect.detections_with_rect\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-embeddings","title":"Workflow with Embeddings","text":"<p>This Workflow shows how to use an embedding model to compare the similarity of two images with each other.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image_1\"\n        },\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image_2\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/clip@v1\",\n            \"name\": \"embedding_1\",\n            \"data\": \"$inputs.image_1\",\n            \"version\": \"RN50\"\n        },\n        {\n            \"type\": \"roboflow_core/clip@v1\",\n            \"name\": \"embedding_2\",\n            \"data\": \"$inputs.image_2\",\n            \"version\": \"RN50\"\n        },\n        {\n            \"type\": \"roboflow_core/cosine_similarity@v1\",\n            \"name\": \"cosine_similarity\",\n            \"embedding_1\": \"$steps.embedding_1.embedding\",\n            \"embedding_2\": \"$steps.embedding_2.embedding\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"similarity\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.cosine_similarity.similarity\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"image_embeddings\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.embedding_1.embedding\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-clip-comparison","title":"Workflow with CLIP Comparison","text":"<p>This is the basic workflow that only contains a single CLIP Comparison block. </p> <p>Please take a look at how batch-oriented WorkflowImage data is plugged to  detection step via input selector (<code>$inputs.image</code>) and how non-batch parameters  (reference set of texts that the each image in batch will be compared to) is dynamically specified - via <code>$inputs.reference</code> selector.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"reference\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ClipComparison\",\n            \"name\": \"comparison\",\n            \"images\": \"$inputs.image\",\n            \"texts\": \"$inputs.reference\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"similarity\",\n            \"selector\": \"$steps.comparison.similarity\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-detections-merge","title":"Workflow with detections merge","text":"<p>This workflow demonstrates how to merge multiple object detections into a single bounding box. This is useful when you want to: - Combine overlapping detections of the same object - Create a single region that contains multiple detected objects - Simplify multiple detections into one larger detection</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_merge@v1\",\n            \"name\": \"detections_merge\",\n            \"predictions\": \"$steps.detection.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.detections_merge.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-static-crop-and-object-detection-model","title":"Workflow with static crop and object detection model","text":"<p>This is the basic workflow that contains single transformation (static crop) followed by object detection model. This example may be inspiration for anyone who would like to run specific model only on specific part of the image. The Region of Interest does not necessarily have to be defined statically -  please note that coordinates of static crops are referred via input selectors,  which means that each time you run the workflow (for instance in each different physical location, where RoI for static crop is location-dependent) you may  provide different RoI coordinates.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.7\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"x_center\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"y_center\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"width\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"height\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"AbsoluteStaticCrop\",\n            \"name\": \"crop\",\n            \"image\": \"$inputs.image\",\n            \"x_center\": \"$inputs.x_center\",\n            \"y_center\": \"$inputs.y_center\",\n            \"width\": \"$inputs.width\",\n            \"height\": \"$inputs.height\"\n        },\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$steps.crop.crops\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"crop\",\n            \"selector\": \"$steps.crop.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.detection.*\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result_in_own_coordinates\",\n            \"selector\": \"$steps.detection.*\",\n            \"coordinates_system\": \"own\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-writing-data-to-opc-server","title":"Workflow writing data to OPC server","text":"<p>In this example data is written to OPC server.</p> <p>In order to write to OPC this block is making use of asyncua package.</p> <p>Writing to OPC enables workflows to expose insights extracted from camera to PLC controllers allowing factory automation engineers to take advantage of machine vision when building PLC logic.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_url\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_namespace\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_user_name\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_password\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_object_name\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_variable_name\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_value\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"opc_value_type\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_enterprise/opc_writer_sink@v1\",\n            \"name\": \"opc_writer\",\n            \"url\": \"$inputs.opc_url\",\n            \"namespace\": \"$inputs.opc_namespace\",\n            \"user_name\": \"$inputs.opc_user_name\",\n            \"password\": \"$inputs.opc_password\",\n            \"object_name\": \"$inputs.opc_object_name\",\n            \"variable_name\": \"$inputs.opc_variable_name\",\n            \"value\": \"$inputs.opc_value\",\n            \"value_type\": \"$inputs.opc_value_type\",\n            \"fire_and_forget\": false\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"opc_writer_results\",\n            \"selector\": \"$steps.opc_writer.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-single-object-detection-model","title":"Workflow with single object detection model","text":"<p>This is the basic workflow that only contains a single object detection model. </p> <p>Please take a look at how batch-oriented WorkflowImage data is plugged to  detection step via input selector (<code>$inputs.image</code>) and how non-batch parameters are dynamically specified - via <code>$inputs.model_id</code> and <code>$inputs.confidence</code> selectors.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.3\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.detection.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/data_analytics_in_workflows/","title":"Data analytics in Workflows","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/data_analytics_in_workflows/#workflow-producing-csv","title":"Workflow producing CSV","text":"<p>This example showcases how to export CSV file out of Workflow. Object detection results are  processed with CSV Formatter block to produce aggregated results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"additional_column_value\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/csv_formatter@v1\",\n            \"name\": \"csv_formatter\",\n            \"columns_data\": {\n                \"predicted_classes\": \"$steps.model.predictions\",\n                \"number_of_bounding_boxes\": \"$steps.model.predictions\",\n                \"additional_column\": \"$inputs.additional_column_value\"\n            },\n            \"columns_operations\": {\n                \"predicted_classes\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ],\n                \"number_of_bounding_boxes\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"csv\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.csv_formatter.csv_content\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/data_analytics_in_workflows/#aggregation-of-results-over-time","title":"Aggregation of results over time","text":"<p>This example shows how to aggregate and analyse predictions using Workflows.</p> <p>The key for data analytics in this example is Data Aggregator block which is fed with model  predictions and perform the following aggregations on each 6 consecutive predictions:</p> <ul> <li> <p>taking classes names from  bounding boxes, it outputs unique classes names, number of unique classes and number of bounding boxes for each class</p> </li> <li> <p>taking the number of detected bounding boxes in each prediction, it outputs minimum, maximum and total number  of bounding boxes per prediction in aggregated time window </p> </li> </ul> <p>Run on video to produce meaningful results</p> <p>This workflow will not work using the docs preview. You must run it on video file. Copy the template into your Roboflow app, start <code>inference</code> server and use video preview  to get the results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/data_aggregator@v1\",\n            \"name\": \"data_aggregation\",\n            \"data\": {\n                \"predicted_classes\": \"$steps.model.predictions\",\n                \"number_of_predictions\": \"$steps.model.predictions\"\n            },\n            \"data_operations\": {\n                \"predicted_classes\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ],\n                \"number_of_predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"aggregation_mode\": {\n                \"predicted_classes\": [\n                    \"distinct\",\n                    \"count_distinct\",\n                    \"values_counts\"\n                ],\n                \"number_of_predictions\": [\n                    \"min\",\n                    \"max\",\n                    \"sum\"\n                ]\n            },\n            \"interval\": 6,\n            \"interval_unit\": \"runs\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"aggregation_results\",\n            \"selector\": \"$steps.data_aggregation.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/data_analytics_in_workflows/#saving-workflow-results-into-file","title":"Saving Workflow results into file","text":"<p>This Workflow was created to achieve few ends:</p> <ul> <li> <p>getting predictions from object detection model and returning them to the caller</p> </li> <li> <p>persisting the predictions - each one in separate JSON file</p> </li> <li> <p>aggregating the predictions data - producing report on each 6th input image</p> </li> <li> <p>saving the results in CSV file, appending rows until file size is exceeded </p> </li> </ul> <p>Run on video to produce meaningful results</p> <p>This workflow will not work using the docs preview. You must run it on video file. Copy the template into your Roboflow app, start <code>inference</code> server and use video preview  to get the results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"target_directory\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/expression@v1\",\n            \"name\": \"json_formatter\",\n            \"data\": {\n                \"predictions\": \"$steps.model.predictions\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [],\n                \"default\": {\n                    \"type\": \"DynamicCaseResult\",\n                    \"parameter_name\": \"predictions\",\n                    \"operations\": [\n                        {\n                            \"type\": \"DetectionsToDictionary\"\n                        },\n                        {\n                            \"type\": \"ConvertDictionaryToJSON\"\n                        }\n                    ]\n                }\n            }\n        },\n        {\n            \"type\": \"roboflow_core/local_file_sink@v1\",\n            \"name\": \"predictions_sink\",\n            \"content\": \"$steps.json_formatter.output\",\n            \"file_type\": \"json\",\n            \"output_mode\": \"separate_files\",\n            \"target_directory\": \"$inputs.target_directory\",\n            \"file_name_prefix\": \"prediction\"\n        },\n        {\n            \"type\": \"roboflow_core/data_aggregator@v1\",\n            \"name\": \"data_aggregation\",\n            \"data\": {\n                \"predicted_classes\": \"$steps.model.predictions\",\n                \"number_of_predictions\": \"$steps.model.predictions\"\n            },\n            \"data_operations\": {\n                \"predicted_classes\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ],\n                \"number_of_predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"aggregation_mode\": {\n                \"predicted_classes\": [\n                    \"count_distinct\"\n                ],\n                \"number_of_predictions\": [\n                    \"min\",\n                    \"max\",\n                    \"sum\"\n                ]\n            },\n            \"interval\": 6,\n            \"interval_unit\": \"runs\"\n        },\n        {\n            \"type\": \"roboflow_core/csv_formatter@v1\",\n            \"name\": \"csv_formatter\",\n            \"columns_data\": {\n                \"number_of_distinct_classes\": \"$steps.data_aggregation.predicted_classes_count_distinct\",\n                \"min_number_of_bounding_boxes\": \"$steps.data_aggregation.number_of_predictions_min\",\n                \"max_number_of_bounding_boxes\": \"$steps.data_aggregation.number_of_predictions_max\",\n                \"total_number_of_bounding_boxes\": \"$steps.data_aggregation.number_of_predictions_sum\"\n            }\n        },\n        {\n            \"type\": \"roboflow_core/local_file_sink@v1\",\n            \"name\": \"reports_sink\",\n            \"content\": \"$steps.csv_formatter.csv_content\",\n            \"file_type\": \"csv\",\n            \"output_mode\": \"append_log\",\n            \"target_directory\": \"$inputs.target_directory\",\n            \"file_name_prefix\": \"aggregation_report\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.model.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/filtering_resulting_data_based_on_value_delta_change/","title":"Filtering resulting data based on value delta change","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/filtering_resulting_data_based_on_value_delta_change/#saving-workflow-results-into-file-but-only-if-value-changes-between-frames","title":"Saving Workflow results into file, but only if value changes between frames","text":"<p>This Workflow was created to achieve few ends:</p> <ul> <li> <p>getting predictions from object detection model</p> </li> <li> <p>filtering out predictions found outside of zone</p> </li> <li> <p>counting detections in zone</p> </li> <li> <p>if count of detection in zone changes save results to csv file</p> </li> </ul> <p>Run on video to produce meaningful results</p> <p>This workflow will not work using the docs preview. You must run it on video file. Copy the template into your Roboflow app, start <code>inference</code> server and use video preview  to get the results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"target_directory\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_filter@v1\",\n            \"name\": \"detections_filter\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsFilter\",\n                    \"filter_operation\": {\n                        \"type\": \"StatementGroup\",\n                        \"operator\": \"and\",\n                        \"statements\": [\n                            {\n                                \"type\": \"BinaryStatement\",\n                                \"left_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operand_name\": \"_\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractDetectionProperty\",\n                                            \"property_name\": \"center\"\n                                        }\n                                    ]\n                                },\n                                \"comparator\": {\n                                    \"type\": \"(Detection) in zone\"\n                                },\n                                \"right_operand\": {\n                                    \"type\": \"StaticOperand\",\n                                    \"value\": [\n                                        [\n                                            0,\n                                            0\n                                        ],\n                                        [\n                                            0,\n                                            1000\n                                        ],\n                                        [\n                                            1000,\n                                            1000\n                                        ],\n                                        [\n                                            1000,\n                                            0\n                                        ]\n                                    ]\n                                },\n                                \"negate\": false\n                            }\n                        ]\n                    }\n                }\n            ],\n            \"operations_parameters\": {}\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"data\": \"$steps.detections_filter.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"SequenceLength\"\n                }\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/delta_filter@v1\",\n            \"name\": \"delta_filter\",\n            \"value\": \"$steps.property_definition.output\",\n            \"image\": \"$inputs.image\",\n            \"next_steps\": [\n                \"$steps.csv_formatter\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/csv_formatter@v1\",\n            \"name\": \"csv_formatter\",\n            \"columns_data\": {\n                \"Class Name\": \"$steps.detections_filter.predictions\"\n            },\n            \"columns_operations\": {\n                \"Class Name\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ]\n            }\n        },\n        {\n            \"type\": \"roboflow_core/local_file_sink@v1\",\n            \"name\": \"reports_sink\",\n            \"content\": \"$steps.csv_formatter.csv_content\",\n            \"file_type\": \"csv\",\n            \"output_mode\": \"append_log\",\n            \"target_directory\": \"$inputs.target_directory\",\n            \"file_name_prefix\": \"csv_containing_changes\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"csv\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.csv_formatter.csv_content\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/integration_with_external_apps/","title":"Integration with external apps","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/integration_with_external_apps/#workflow-sending-notification-to-slack","title":"Workflow sending notification to Slack","text":"<p>This Workflow illustrates how to send notification to Slack.</p> Workflow definition <pre><code>{\n    \"version\": \"1.4.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"channel_id\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"slack_token\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/slack_notification@v1\",\n            \"name\": \"notification\",\n            \"slack_token\": \"$inputs.slack_token\",\n            \"message\": \"Detected {{ $parameters.predictions }} objects\",\n            \"channel\": \"$inputs.channel_id\",\n            \"message_parameters\": {\n                \"predictions\": \"$steps.detection.predictions\"\n            },\n            \"message_parameters_operations\": {\n                \"predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"fire_and_forget\": false,\n            \"cooldown_seconds\": 0,\n            \"cooldown_session_key\": \"some-unique-key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"status\",\n            \"selector\": \"$steps.notification.error_status\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/integration_with_external_apps/#workflow-sending-notification-with-attachments-to-slack","title":"Workflow sending notification with attachments to Slack","text":"<p>This Workflow illustrates how to send notification with attachments to Slack.</p> Workflow definition <pre><code>{\n    \"version\": \"1.4.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"channel_id\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"slack_token\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"image_serialization\",\n            \"data\": \"$inputs.image\",\n            \"operations\": [\n                {\n                    \"type\": \"ConvertImageToJPEG\"\n                }\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"predictions_serialization\",\n            \"data\": \"$steps.detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsToDictionary\"\n                },\n                {\n                    \"type\": \"ConvertDictionaryToJSON\"\n                }\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/slack_notification@v1\",\n            \"name\": \"notification\",\n            \"slack_token\": \"$inputs.slack_token\",\n            \"message\": \"Detected {{ $parameters.predictions }} objects\",\n            \"channel\": \"$inputs.channel_id\",\n            \"message_parameters\": {\n                \"predictions\": \"$steps.detection.predictions\"\n            },\n            \"message_parameters_operations\": {\n                \"predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"attachments\": {\n                \"image.jpg\": \"$steps.image_serialization.output\",\n                \"prediction.json\": \"$steps.predictions_serialization.output\"\n            },\n            \"fire_and_forget\": false,\n            \"cooldown_seconds\": 0,\n            \"cooldown_session_key\": \"some-unique-key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"status\",\n            \"selector\": \"$steps.notification.error_status\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/integration_with_external_apps/#workflow-sending-sms-notification-with-twilio","title":"Workflow sending SMS notification with Twilio","text":"<p>This Workflow illustrates how to send SMS notification with Twilio.</p> Workflow definition <pre><code>{\n    \"version\": \"1.4.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"account_sid\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"auth_token\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"sender_number\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"receiver_number\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/twilio_sms_notification@v1\",\n            \"name\": \"notification\",\n            \"twilio_account_sid\": \"$inputs.account_sid\",\n            \"twilio_auth_token\": \"$inputs.auth_token\",\n            \"message\": \"Detected {{ $parameters.predictions }} objects\",\n            \"sender_number\": \"$inputs.sender_number\",\n            \"receiver_number\": \"$inputs.receiver_number\",\n            \"message_parameters\": {\n                \"predictions\": \"$steps.detection.predictions\"\n            },\n            \"message_parameters_operations\": {\n                \"predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"fire_and_forget\": false,\n            \"cooldown_seconds\": 0,\n            \"cooldown_session_key\": \"some-unique-key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"status\",\n            \"selector\": \"$steps.notification.error_status\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_enhanced_by_roboflow_platform/","title":"Workflows enhanced by Roboflow Platform","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_enhanced_by_roboflow_platform/#data-collection-for-active-learning","title":"Data Collection for Active Learning","text":"<p>This example showcases how to stack models on top of each other - in this particular case, we detect objects using object detection models, requesting only \"dogs\" bounding boxes in the output of prediction. Additionally, we register cropped images in Roboflow dataset.</p> <p>Thanks to this setup, we are able to collect production data and continuously train better models over time.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"data_percentage\",\n            \"default_value\": 50.0\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"persist_predictions\",\n            \"default_value\": true\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"tag\",\n            \"default_value\": \"my_tag\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"disable_sink\",\n            \"default_value\": false\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"fire_and_forget\",\n            \"default_value\": true\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"labeling_batch_prefix\",\n            \"default_value\": \"some\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_classification_model@v2\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\",\n            \"confidence\": 0.09\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_dataset_upload@v2\",\n            \"name\": \"data_collection\",\n            \"images\": \"$steps.cropping.crops\",\n            \"predictions\": \"$steps.breds_classification.predictions\",\n            \"target_project\": \"my_project\",\n            \"usage_quota_name\": \"my_quota\",\n            \"data_percentage\": \"$inputs.data_percentage\",\n            \"persist_predictions\": \"$inputs.persist_predictions\",\n            \"minutely_usage_limit\": 10,\n            \"hourly_usage_limit\": 100,\n            \"daily_usage_limit\": 1000,\n            \"max_image_size\": [\n                100,\n                200\n            ],\n            \"compression_level\": 85,\n            \"registration_tags\": [\n                \"a\",\n                \"b\",\n                \"$inputs.tag\"\n            ],\n            \"disable_sink\": \"$inputs.disable_sink\",\n            \"fire_and_forget\": \"$inputs.fire_and_forget\",\n            \"labeling_batch_prefix\": \"$inputs.labeling_batch_prefix\",\n            \"labeling_batches_recreation_frequency\": \"never\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.breds_classification.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"registration_message\",\n            \"selector\": \"$steps.data_collection.message\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_for_ocr/","title":"Workflows for OCR","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_for_ocr/#workflow-with-doctr-model","title":"Workflow with DocTR model","text":"<p>This example showcases quite sophisticated workflows usage scenario that assume the following:</p> <ul> <li> <p>we have generic object detection model capable of recognising cars</p> </li> <li> <p>we have specialised object detection model trained to detect license plates in the images depicting single car only</p> </li> <li> <p>we have generic OCR model capable of recognising lines of texts from images</p> </li> </ul> <p>Our goal is to read license plates of every car we detect in the picture. We can achieve that goal with  workflow from this example. In the definition we can see that generic object detection model is applied first,  to make the job easier for the secondary (plates detection) model we enlarge bounding boxes, slightly  offsetting its dimensions with Detections Offset block - later we apply cropping to be able to run license plate detection for every detected car instance (increasing the depth of the batch). Once secondary model runs and we have bounding boxes for license plates - we crop previously cropped cars images to extract plates. Once this is done, plates crops are passed to OCR step which turns images of plates into text.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"car\"\n            ]\n        },\n        {\n            \"type\": \"DetectionOffset\",\n            \"name\": \"offset\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"image_metadata\": \"$steps.detection.image\",\n            \"prediction_type\": \"$steps.detection.prediction_type\",\n            \"offset_width\": 10,\n            \"offset_height\": 10\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cars_crops\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.offset.predictions\"\n        },\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"plates_detection\",\n            \"image\": \"$steps.cars_crops.crops\",\n            \"model_id\": \"vehicle-registration-plates-trudk/2\"\n        },\n        {\n            \"type\": \"DetectionOffset\",\n            \"name\": \"plates_offset\",\n            \"predictions\": \"$steps.plates_detection.predictions\",\n            \"image_metadata\": \"$steps.plates_detection.image\",\n            \"prediction_type\": \"$steps.plates_detection.prediction_type\",\n            \"offset_width\": 50,\n            \"offset_height\": 50\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"plates_crops\",\n            \"image\": \"$steps.cars_crops.crops\",\n            \"predictions\": \"$steps.plates_offset.predictions\"\n        },\n        {\n            \"type\": \"OCRModel\",\n            \"name\": \"ocr\",\n            \"image\": \"$steps.plates_crops.crops\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"cars_crops\",\n            \"selector\": \"$steps.cars_crops.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"plates_crops\",\n            \"selector\": \"$steps.plates_crops.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"plates_ocr\",\n            \"selector\": \"$steps.ocr.result\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_for_ocr/#google-vision-ocr","title":"Google Vision OCR","text":"<p>In this example, Google Vision OCR is used to extract text from input image. Additionally, example presents how to combine structured output of Google API with visualisation blocks.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_vision_ocr@v1\",\n            \"name\": \"google_vision_ocr\",\n            \"image\": \"$inputs.image\",\n            \"ocr_type\": \"text_detection\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bounding_box_visualization\",\n            \"predictions\": \"$steps.google_vision_ocr.predictions\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/label_visualization@v1\",\n            \"name\": \"label_visualization\",\n            \"predictions\": \"$steps.google_vision_ocr.predictions\",\n            \"image\": \"$steps.bounding_box_visualization.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"extracted_text\",\n            \"selector\": \"$steps.google_vision_ocr.text\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"text_detections\",\n            \"selector\": \"$steps.google_vision_ocr.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"text_visualised\",\n            \"selector\": \"$steps.label_visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_for_ocr/#workflow-with-model-detecting-individual-characters-and-text-stitching","title":"Workflow with model detecting individual characters and text stitching","text":"<p>This workflow extracts and organizes text from an image using OCR. It begins by analyzing the image with detection  model to detect individual characters or words and their positions. </p> <p>Then, it groups nearby text into lines based on a specified <code>tolerance</code> for spacing and arranges them in  reading order (<code>left-to-right</code>). </p> <p>The final output is a JSON field containing the structured text in readable, logical order, accurately reflecting  the layout of the original image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"ocr-oy9a7/1\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"tolerance\",\n            \"default_value\": 10\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"ocr_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"roboflow_core/stitch_ocr_detections@v1\",\n            \"name\": \"detections_stitch\",\n            \"predictions\": \"$steps.ocr_detection.predictions\",\n            \"reading_direction\": \"left_to_right\",\n            \"tolerance\": \"$inputs.tolerance\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"ocr_text\",\n            \"selector\": \"$steps.detections_stitch.ocr_text\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_business_logic/","title":"Workflows with business logic","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_business_logic/#workflow-with-extraction-of-classes-for-detections-1","title":"Workflow with extraction of classes for detections (1)","text":"<p>In practical use-cases you may find the need to inject pieces of business logic inside  your Workflow, such that it is easier to integrate with app created in Workflows ecosystem.</p> <p>Translation of model predictions into domain-specific language of your business is possible  with specialised blocks that let you parametrise such programming constructs  as switch-case statements.</p> <p>In this example, our goal is to:</p> <ul> <li> <p>tell how many objects are detected</p> </li> <li> <p>verify that the picture presents exactly two dogs</p> </li> </ul> <p>To achieve that goal, we run generic object detection model as first step, then we use special block called Property Definition that is capable of executing various operations to transform input data into desired output. We have two such blocks:</p> <ul> <li> <p><code>instances_counter</code> which takes object detection predictions and apply operation to extract sequence length -  effectively calculating number of instances of objects that were predicted</p> </li> <li> <p><code>property_extraction</code> which extracts class names from all detected bounding boxes</p> </li> </ul> <p><code>instances_counter</code> basically completes first goal of the workflow, but to satisfy the second one we need to  build evaluation logic that will tell \"PASS\" / \"FAIL\" based on comparison of extracted class names with  reference parameter (provided via Workflow input <code>$inputs.reference</code>). We can use Expression block to achieve  that goal - building custom case statements (checking if class names being list of classes  extracted from object detection prediction matches reference passed in the input).</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"reference\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"PropertyDefinition\",\n            \"name\": \"property_extraction\",\n            \"data\": \"$steps.general_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsPropertyExtract\",\n                    \"property_name\": \"class_name\"\n                }\n            ]\n        },\n        {\n            \"type\": \"PropertyDefinition\",\n            \"name\": \"instances_counter\",\n            \"data\": \"$steps.general_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"SequenceLength\"\n                }\n            ]\n        },\n        {\n            \"type\": \"Expression\",\n            \"name\": \"expression\",\n            \"data\": {\n                \"class_names\": \"$steps.property_extraction.output\",\n                \"reference\": \"$inputs.reference\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [\n                    {\n                        \"type\": \"CaseDefinition\",\n                        \"condition\": {\n                            \"type\": \"StatementGroup\",\n                            \"statements\": [\n                                {\n                                    \"type\": \"BinaryStatement\",\n                                    \"left_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"class_names\"\n                                    },\n                                    \"comparator\": {\n                                        \"type\": \"==\"\n                                    },\n                                    \"right_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"reference\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"result\": {\n                            \"type\": \"StaticCaseResult\",\n                            \"value\": \"PASS\"\n                        }\n                    }\n                ],\n                \"default\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"FAIL\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"detected_classes\",\n            \"selector\": \"$steps.property_extraction.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"number_of_detected_boxes\",\n            \"selector\": \"$steps.instances_counter.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"verdict\",\n            \"selector\": \"$steps.expression.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_business_logic/#workflow-with-extraction-of-classes-for-detections-2","title":"Workflow with extraction of classes for detections (2)","text":"<p>In practical use-cases you may find the need to inject pieces of business logic inside  your Workflow, such that it is easier to integrate with app created in Workflows ecosystem.</p> <p>Translation of model predictions into domain-specific language of your business is possible  with specialised blocks that let you parametrise such programming constructs  as switch-case statements.</p> <p>In this example, our goal is to:</p> <ul> <li> <p>run generic object detection model to find instances of dogs</p> </li> <li> <p>crop dogs detection</p> </li> <li> <p>run specialised dogs breed classifier to assign granular label for each dog</p> </li> <li> <p>compare predicted dogs breeds to verify if detected labels matches exactly reverence value passed in input.</p> </li> </ul> <p>This example is quite complex as it requires quite deep understanding of Workflows ecosystem. Let's start from the beginning - we run object detection model, crop its detections according to dogs class to perform  classification. This is quite typical for workflows (you may find such pattern in remaining examples). </p> <p>The complexity increases when we try to handle classification output. We need to have a list of classes for each input image, but for now we have complex objects with all classification predictions details provided by <code>breds_classification</code> step - what is more - we have batch of such predictions for each input image (as we created dogs crops based on object detection model predictions). To solve the  problem, at first we apply Property Definition step taking classifier predictions and turning them into strings representing predicted classes. We still have batch of class names at dimensionality level 2,  which needs to be brought into dimensionality level 1 to make a single comparison against reference  value for each input image. To achieve that effect we use Dimension Collapse block which does nothing else but grabs the batch of classes and turns it into list of classes at dimensionality level 1 - one  list for each input image.</p> <p>That would solve our problems, apart from one nuance that must be taken into account. First-stage model is not guaranteed to detect any dogs - and if that happens we do not execute cropping and further  processing for that image, leaving all outputs derived from downstream computations <code>None</code> which is suboptimal. To compensate for that, we may use First Non Empty Or Default block which will take  <code>outputs_concatenation</code> step output and replace missing values with empty list (as effectively this is  equivalent of not detecting any dog).</p> <p>Such prepared output of <code>empty_values_replacement</code> step may be now plugged into Expression block,  performing switch-case like logic to deduce if breeds of detected dogs match with reference value  passed to workflow execution.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"reference\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\",\n            \"confidence\": 0.09\n        },\n        {\n            \"type\": \"PropertyDefinition\",\n            \"name\": \"property_extraction\",\n            \"data\": \"$steps.breds_classification.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ]\n        },\n        {\n            \"type\": \"DimensionCollapse\",\n            \"name\": \"outputs_concatenation\",\n            \"data\": \"$steps.property_extraction.output\"\n        },\n        {\n            \"type\": \"FirstNonEmptyOrDefault\",\n            \"name\": \"empty_values_replacement\",\n            \"data\": [\n                \"$steps.outputs_concatenation.output\"\n            ],\n            \"default\": []\n        },\n        {\n            \"type\": \"Expression\",\n            \"name\": \"expression\",\n            \"data\": {\n                \"detected_classes\": \"$steps.empty_values_replacement.output\",\n                \"reference\": \"$inputs.reference\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [\n                    {\n                        \"type\": \"CaseDefinition\",\n                        \"condition\": {\n                            \"type\": \"StatementGroup\",\n                            \"statements\": [\n                                {\n                                    \"type\": \"BinaryStatement\",\n                                    \"left_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"detected_classes\"\n                                    },\n                                    \"comparator\": {\n                                        \"type\": \"==\"\n                                    },\n                                    \"right_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"reference\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"result\": {\n                            \"type\": \"StaticCaseResult\",\n                            \"value\": \"PASS\"\n                        }\n                    }\n                ],\n                \"default\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"FAIL\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"detected_classes\",\n            \"selector\": \"$steps.property_extraction.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"wrapped_classes\",\n            \"selector\": \"$steps.empty_values_replacement.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"verdict\",\n            \"selector\": \"$steps.expression.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/","title":"Workflows with classical Computer Vision methods","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-removing-camera-distortions","title":"Workflow removing camera distortions","text":"<p>In this example, we demonstrate how to remove distortions from the camera based on coefficients provided.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"images\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"fx\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"fy\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"cx\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"cy\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"k1\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"k2\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"k3\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"p1\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"p2\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/camera-calibration@v1\",\n            \"name\": \"camera_calibration\",\n            \"images\": \"$inputs.images\",\n            \"fx\": \"$inputs.fx\",\n            \"fy\": \"$inputs.fy\",\n            \"cx\": \"$inputs.cx\",\n            \"cy\": \"$inputs.cy\",\n            \"k1\": \"$inputs.k1\",\n            \"k2\": \"$inputs.k2\",\n            \"k3\": \"$inputs.k3\",\n            \"p1\": \"$inputs.p1\",\n            \"p2\": \"$inputs.p2\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"camera_calibration_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.camera_calibration.calibrated_image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-generating-camera-focus-measure","title":"Workflow generating camera focus measure","text":"<p>In this example, we demonstrate how to evaluate camera focus using a specific block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/camera_focus@v1\",\n            \"name\": \"camera_focus\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"camera_focus_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.camera_focus.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"camera_focus_measure\",\n            \"selector\": \"$steps.camera_focus.focus_measure\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-detecting-contours","title":"Workflow detecting contours","text":"<p>In this example we show how classical contour detection works in cooperation with blocks performing its pre-processing (conversion to gray and blur).</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/convert_grayscale@v1\",\n            \"name\": \"image_convert_grayscale\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/image_blur@v1\",\n            \"name\": \"image_blur\",\n            \"image\": \"$steps.image_convert_grayscale.image\"\n        },\n        {\n            \"type\": \"roboflow_core/threshold@v1\",\n            \"name\": \"image_threshold\",\n            \"image\": \"$steps.image_blur.image\",\n            \"thresh_value\": 200,\n            \"threshold_type\": \"binary_inv\"\n        },\n        {\n            \"type\": \"roboflow_core/contours_detection@v1\",\n            \"name\": \"image_contours\",\n            \"image\": \"$steps.image_threshold.image\",\n            \"raw_image\": \"$inputs.image\",\n            \"line_thickness\": 5\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"number_contours\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_contours.number_contours\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"contour_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_contours.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"contours\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_contours.contours\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"grayscale_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_convert_grayscale.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"blurred_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_blur.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"thresholded_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_threshold.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-calculating-pixels-with-dominant-color","title":"Workflow calculating pixels with dominant color","text":"<p>This example shows how Dominant Color block and Pixel Color Count block can be used together.</p> <p>First, dominant color gets detected and then number of pixels with that color is calculated.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/dominant_color@v1\",\n            \"name\": \"dominant_color\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/pixel_color_count@v1\",\n            \"name\": \"pixelation\",\n            \"image\": \"$inputs.image\",\n            \"target_color\": \"$steps.dominant_color.rgb_color\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"matching_pixels_count\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.pixelation.matching_pixels_count\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-calculating-dominant-color","title":"Workflow calculating dominant color","text":"<p>This example shows how Dominant Color block can be used against input image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/dominant_color@v1\",\n            \"name\": \"dominant_color\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"color\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.dominant_color.rgb_color\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-with-dynamic-zone-and-perspective-converter","title":"Workflow with dynamic zone and perspective converter","text":"<p>In this example dynamic zone with 4 vertices is calculated from detected segmentations. Perspective correction is applied to the input image as well as to detected segmentations based on this zone.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_instance_segmentation_model@v2\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-seg-640\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_filter@v1\",\n            \"name\": \"detections_filter\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsFilter\",\n                    \"filter_operation\": {\n                        \"type\": \"StatementGroup\",\n                        \"operator\": \"and\",\n                        \"statements\": [\n                            {\n                                \"type\": \"BinaryStatement\",\n                                \"negate\": false,\n                                \"left_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operand_name\": \"_\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractDetectionProperty\",\n                                            \"property_name\": \"class_name\"\n                                        }\n                                    ]\n                                },\n                                \"comparator\": {\n                                    \"type\": \"in (Sequence)\"\n                                },\n                                \"right_operand\": {\n                                    \"type\": \"StaticOperand\",\n                                    \"value\": [\n                                        \"banana\"\n                                    ]\n                                }\n                            }\n                        ]\n                    }\n                }\n            ],\n            \"operations_parameters\": {}\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_zone@v1\",\n            \"name\": \"dynamic_zone\",\n            \"predictions\": \"$steps.detections_filter.predictions\",\n            \"required_number_of_vertices\": 4\n        },\n        {\n            \"type\": \"roboflow_core/perspective_correction@v1\",\n            \"name\": \"perspective_correction\",\n            \"images\": \"$inputs.image\",\n            \"perspective_polygons\": \"$steps.dynamic_zone.zones\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"warp_image\": true,\n            \"extend_perspective_polygon_by_detections_anchor\": \"BOTTOM_CENTER\"\n        },\n        {\n            \"type\": \"roboflow_core/polygon_visualization@v1\",\n            \"name\": \"perspective_visualization\",\n            \"image\": \"$steps.perspective_correction.warped_image\",\n            \"predictions\": \"$steps.perspective_correction.corrected_coordinates\"\n        },\n        {\n            \"type\": \"roboflow_core/polygon_visualization@v1\",\n            \"name\": \"polygon_visualization\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.model.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"polygons_visualization\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.polygon_visualization.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"perspective_visualization\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.perspective_visualization.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"perspective_correction_outputs\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.perspective_correction.*\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"dynamic_zones\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.dynamic_zone.zones\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-resizing-the-input-image","title":"Workflow resizing the input image","text":"<p>This example shows how the Image Preprocessing block can be used to resize an input image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/image_preprocessing@v1\",\n            \"name\": \"resize_image\",\n            \"image\": \"$inputs.image\",\n            \"task_type\": \"resize\",\n            \"width\": 1000,\n            \"height\": 800\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"resized_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.resize_image.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#sift-in-workflows","title":"SIFT in Workflows","text":"<p>In this example we check how SIFT-based pattern matching works in cooperation with expression block.</p> <p>The Workflow first calculates SIFT features for input image and reference template,  then image features are compared to template features. At the end - switch-case  statement is built with Expression block to produce output. </p> <p>Important detail: If there is empty output from SIFT descriptors calculation for (which is a valid output if no feature gets recognised) the sift comparison won't  execute - hence First Non Empty Or Default block is used to provide default outcome  for <code>images_match</code> output of SIFT comparison block.</p> <p>Please note that a single image can be passed as template, and batch of images are passed as images to look for template. This workflow does also validate Execution Engine capabilities to broadcast batch-oriented inputs properly.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"template\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/sift@v1\",\n            \"name\": \"image_sift\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/sift@v1\",\n            \"name\": \"template_sift\",\n            \"image\": \"$inputs.template\"\n        },\n        {\n            \"type\": \"roboflow_core/sift_comparison@v1\",\n            \"name\": \"sift_comparison\",\n            \"descriptor_1\": \"$steps.image_sift.descriptors\",\n            \"descriptor_2\": \"$steps.template_sift.descriptors\",\n            \"good_matches_threshold\": 50\n        },\n        {\n            \"type\": \"roboflow_core/first_non_empty_or_default@v1\",\n            \"name\": \"empty_values_replacement\",\n            \"data\": [\n                \"$steps.sift_comparison.images_match\"\n            ],\n            \"default\": false\n        },\n        {\n            \"type\": \"roboflow_core/expression@v1\",\n            \"name\": \"is_match_expression\",\n            \"data\": {\n                \"images_match\": \"$steps.empty_values_replacement.output\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [\n                    {\n                        \"type\": \"CaseDefinition\",\n                        \"condition\": {\n                            \"type\": \"StatementGroup\",\n                            \"statements\": [\n                                {\n                                    \"type\": \"UnaryStatement\",\n                                    \"operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"images_match\"\n                                    },\n                                    \"operator\": {\n                                        \"type\": \"(Boolean) is True\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"result\": {\n                            \"type\": \"StaticCaseResult\",\n                            \"value\": \"MATCH\"\n                        }\n                    }\n                ],\n                \"default\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"NO MATCH\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.is_match_expression.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-stitching-images","title":"Workflow stitching images","text":"<p>In this example two images of the same scene are stitched together. Given enough shared details order of the images does not influence final result.</p> <p>Please note that images need to have enough common details for the algorithm to stitch them properly.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image1\"\n        },\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image2\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"count_of_best_matches_per_query_descriptor\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"max_allowed_reprojection_error\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/stitch_images@v1\",\n            \"name\": \"stitch_images\",\n            \"image1\": \"$inputs.image1\",\n            \"image2\": \"$inputs.image2\",\n            \"count_of_best_matches_per_query_descriptor\": \"$inputs.count_of_best_matches_per_query_descriptor\",\n            \"max_allowed_reprojection_error\": \"$inputs.max_allowed_reprojection_error\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"stitched_image\",\n            \"selector\": \"$steps.stitch_images.stitched_image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/","title":"Workflows with data transformations","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_data_transformations/#workflow-with-detections-class-remapping","title":"Workflow with detections class remapping","text":"<p>This workflow presents how to use Detections Transformation block that is going to  change the name of the following classes: <code>apple</code>, <code>banana</code> into <code>fruit</code>.</p> <p>In this example, we use non-strict mapping, causing new class <code>fruit</code> to be added to pool of classes - you can see that if <code>banana</code> or <code>apple</code> is detected, the class name changes to <code>fruit</code> and class id is 1024.</p> <p>You can test the execution submitting image like  this.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"model\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"class_rename\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsRename\",\n                    \"strict\": false,\n                    \"class_map\": {\n                        \"apple\": \"fruit\",\n                        \"banana\": \"fruit\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"original_predictions\",\n            \"selector\": \"$steps.model.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"renamed_predictions\",\n            \"selector\": \"$steps.class_rename.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/#workflow-with-detections-filtering","title":"Workflow with detections filtering","text":"<p>This example presents how to use Detections Transformation block to build workflow that is going to filter predictions based on:</p> <ul> <li> <p>predicted classes</p> </li> <li> <p>size of predicted bounding box relative to size of input image</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.3\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"filtering\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsFilter\",\n                    \"filter_operation\": {\n                        \"type\": \"StatementGroup\",\n                        \"operator\": \"and\",\n                        \"statements\": [\n                            {\n                                \"type\": \"BinaryStatement\",\n                                \"left_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractDetectionProperty\",\n                                            \"property_name\": \"class_name\"\n                                        }\n                                    ]\n                                },\n                                \"comparator\": {\n                                    \"type\": \"in (Sequence)\"\n                                },\n                                \"right_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operand_name\": \"classes\"\n                                }\n                            },\n                            {\n                                \"type\": \"BinaryStatement\",\n                                \"left_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractDetectionProperty\",\n                                            \"property_name\": \"size\"\n                                        }\n                                    ]\n                                },\n                                \"comparator\": {\n                                    \"type\": \"(Number) &gt;=\"\n                                },\n                                \"right_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operand_name\": \"image\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractImageProperty\",\n                                            \"property_name\": \"size\"\n                                        },\n                                        {\n                                            \"type\": \"Multiply\",\n                                            \"other\": 0.02\n                                        }\n                                    ]\n                                }\n                            }\n                        ]\n                    }\n                }\n            ],\n            \"operations_parameters\": {\n                \"image\": \"$inputs.image\",\n                \"classes\": \"$inputs.classes\"\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.filtering.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/#instance-segmentation-results-with-background-subtracted","title":"Instance Segmentation results with background subtracted","text":"<p>This example showcases how to extract all instances detected by instance segmentation model as separate crops without background.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-seg-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_instance_segmentation_model@v2\",\n            \"name\": \"segmentation\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.segmentation.predictions\",\n            \"mask_opacity\": 1.0\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"crops\",\n            \"selector\": \"$steps.cropping.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.segmentation.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/#workflow-with-detections-sorting","title":"Workflow with detections sorting","text":"<p>This workflow presents how to use Detections Transformation block that is going to  align predictions from object detection model such that results are sorted  ascending regarding confidence.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.75\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"sorting\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"SortDetections\",\n                    \"mode\": \"confidence\",\n                    \"ascending\": true\n                }\n            ],\n            \"operations_parameters\": {\n                \"image\": \"$inputs.image\",\n                \"classes\": \"$inputs.classes\"\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.sorting.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_depth_estimation/","title":"Workflows with Depth Estimation","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_depth_estimation/#depth-estimation","title":"Depth Estimation","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>Use Depth Estimation to estimate the depth of an image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/depth_estimation@v1\",\n            \"name\": \"depth_estimation\",\n            \"images\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.depth_estimation.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_dynamic_python_blocks/","title":"Workflows with dynamic Python Blocks","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_dynamic_python_blocks/#workflow-measuring-bounding-boxes-overlap","title":"Workflow measuring bounding boxes overlap","text":"<p>In real world use-cases you may not be able to find all pieces of functionalities required to complete  your workflow within existing blocks. </p> <p>In such cases you may create piece of python code and put it in workflow as a dynamic block. Specifically  here, we define two dynamic blocks:</p> <ul> <li> <p><code>OverlapMeasurement</code> which will accept object detection predictions and provide for boxes  of specific class matrix of overlap with all boxes of another class.</p> </li> <li> <p><code>MaximumOverlap</code> that will take overlap matrix produced by <code>OverlapMeasurement</code> and calculate maximum overlap.</p> </li> </ul> <p>Dynamic block may be used to create steps, exactly as if those blocks were standard Workflow blocks  existing in ecosystem. The workflow presented in the example predicts from object detection model and  calculates overlap matrix. Later, only if more than one object is detected, maximum overlap is calculated.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"dynamic_blocks_definitions\": [\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"OverlapMeasurement\",\n                \"inputs\": {\n                    \"predictions\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    },\n                    \"class_x\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    },\n                    \"class_y\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"overlap\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, predictions: sv.Detections, class_x: str, class_y: str) -&gt; BlockResult:\\n    bboxes_class_x = predictions[predictions.data[\\\"class_name\\\"] == class_x]\\n    bboxes_class_y = predictions[predictions.data[\\\"class_name\\\"] == class_y]\\n    overlap = []\\n    for bbox_x in bboxes_class_x:\\n        bbox_x_coords = bbox_x[0]\\n        bbox_overlaps = []\\n        for bbox_y in bboxes_class_y:\\n            if bbox_y[-1][\\\"detection_id\\\"] == bbox_x[-1][\\\"detection_id\\\"]:\\n                continue\\n            bbox_y_coords = bbox_y[0]\\n            x_min = max(bbox_x_coords[0], bbox_y_coords[0])\\n            y_min = max(bbox_x_coords[1], bbox_y_coords[1])\\n            x_max = min(bbox_x_coords[2], bbox_y_coords[2])\\n            y_max = min(bbox_x_coords[3], bbox_y_coords[3])\\n            # compute the area of intersection rectangle\\n            intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\\n            box_x_area = (bbox_x_coords[2] - bbox_x_coords[0] + 1) * (bbox_x_coords[3] - bbox_x_coords[1] + 1)\\n            local_overlap = intersection_area / (box_x_area + 1e-5)\\n            bbox_overlaps.append(local_overlap)\\n        overlap.append(bbox_overlaps)\\n    return  {\\\"overlap\\\": overlap}\\n\"\n            }\n        },\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"MaximumOverlap\",\n                \"inputs\": {\n                    \"overlaps\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"max_value\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, overlaps: List[List[float]]) -&gt; BlockResult:\\n    max_value = -1\\n    for overlap in overlaps:\\n        for overlap_value in overlap:\\n            if not max_value:\\n                max_value = overlap_value\\n            else:\\n                max_value = max(max_value, overlap_value)\\n    return {\\\"max_value\\\": max_value}\\n\"\n            }\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"model\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"OverlapMeasurement\",\n            \"name\": \"overlap_measurement\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"class_x\": \"dog\",\n            \"class_y\": \"dog\"\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"overlaps\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) &gt;=\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"overlaps\": \"$steps.overlap_measurement.overlap\"\n            },\n            \"next_steps\": [\n                \"$steps.maximum_overlap\"\n            ]\n        },\n        {\n            \"type\": \"MaximumOverlap\",\n            \"name\": \"maximum_overlap\",\n            \"overlaps\": \"$steps.overlap_measurement.overlap\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"overlaps\",\n            \"selector\": \"$steps.overlap_measurement.overlap\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"max_overlap\",\n            \"selector\": \"$steps.maximum_overlap.max_value\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_flow_control/","title":"Workflows with flow control","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_flow_control/#workflow-with-if-statement-applied-on-nested-batches","title":"Workflow with if statement applied on nested batches","text":"<p>In this test scenario we verify if we can successfully apply conditional branching when data dimensionality increases. We first make detections on input images and perform crop increasing dimensionality to 2. Then we make another detections on cropped images and check if inside crop we only see one instance of class dog (very naive way of making sure that bboxes contain only single objects). Only if that condition is true, we run classification model - to classify dog breed.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"first_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"enlarging_boxes\",\n            \"predictions\": \"$steps.first_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsOffset\",\n                    \"offset_x\": 50,\n                    \"offset_y\": 50\n                }\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"first_crop\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.enlarging_boxes.predictions\"\n        },\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"second_detection\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"prediction\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) ==\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"prediction\": \"$steps.second_detection.predictions\"\n            },\n            \"next_steps\": [\n                \"$steps.classification\"\n            ]\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"classification\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"dog_classification\",\n            \"selector\": \"$steps.classification.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_flow_control/#workflow-with-if-statement-applied-on-non-batch-oriented-input","title":"Workflow with if statement applied on non batch-oriented input","text":"<p>In this test scenario we show that we can use non-batch oriented conditioning (ContinueIf block).</p> <p>If statement is effectively applied on input parameter that would determine path of execution for all data passed in <code>image</code> input. When the value matches expectation - all dependent steps will be executed, otherwise only the independent ones.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"first_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"enlarging_boxes\",\n            \"predictions\": \"$steps.first_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsOffset\",\n                    \"offset_x\": 50,\n                    \"offset_y\": 50\n                }\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"first_crop\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.enlarging_boxes.predictions\"\n        },\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"second_detection\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"prediction\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) ==\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"prediction\": \"$steps.second_detection.predictions\"\n            },\n            \"next_steps\": [\n                \"$steps.classification\"\n            ]\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"classification\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"dog_classification\",\n            \"selector\": \"$steps.classification.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_foundation_models/","title":"Workflows with foundation models","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_foundation_models/#gaze-detection-workflow","title":"Gaze Detection Workflow","text":"<p>This workflow uses L2CS-Net to detect faces and estimate their gaze direction. The output includes: - Face detections with facial landmarks - Gaze angles (yaw and pitch) in degrees - Visualization of facial landmarks</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"do_run_face_detection\",\n            \"default_value\": true\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/gaze@v1\",\n            \"name\": \"gaze\",\n            \"images\": \"$inputs.image\",\n            \"do_run_face_detection\": \"$inputs.do_run_face_detection\"\n        },\n        {\n            \"type\": \"roboflow_core/keypoint_visualization@v1\",\n            \"name\": \"visualization\",\n            \"predictions\": \"$steps.gaze.face_predictions\",\n            \"image\": \"$inputs.image\",\n            \"annotator_type\": \"vertex\",\n            \"color\": \"#A351FB\",\n            \"text_color\": \"black\",\n            \"text_scale\": 0.5,\n            \"text_thickness\": 1,\n            \"text_padding\": 10,\n            \"thickness\": 2,\n            \"radius\": 10\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"face_predictions\",\n            \"selector\": \"$steps.gaze.face_predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"yaw_degrees\",\n            \"selector\": \"$steps.gaze.yaw_degrees\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"pitch_degrees\",\n            \"selector\": \"$steps.gaze.pitch_degrees\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualization\",\n            \"selector\": \"$steps.visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_foundation_models/#workflow-with-segment-anything-2-model","title":"Workflow with Segment Anything 2 model","text":"<p>Meta AI introduced very capable segmentation model called SAM 2 which has capabilities of producing segmentation masks for instances of objects. </p> <p>EXAMPLE REQUIRES DEDICATED DEPLOYMENT and will not run in preview!</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"mask_threshold\",\n            \"default_value\": 0.0\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"version\",\n            \"default_value\": \"hiera_tiny\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/segment_anything@v1\",\n            \"name\": \"segment_anything\",\n            \"images\": \"$inputs.image\",\n            \"threshold\": \"$inputs.mask_threshold\",\n            \"version\": \"$inputs.version\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.segment_anything.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/","title":"Workflows with multiple models","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_multiple_models/#workflow-detection-model-followed-by-classifier","title":"Workflow detection model followed by classifier","text":"<p>This example showcases how to stack models on top of each other - in this particular case, we detect objects using object detection models, requesting only \"dogs\" bounding boxes in the output of prediction. </p> <p>Based on the model predictions, we take each bounding box with dog and apply dynamic cropping to be able to run classification model for each and every instance of dog separately. Please note that for each inserted image we will have nested batch of crops (with size  dynamically determined in runtime, based on first model predictions) and for each crop we apply secondary model.</p> <p>Secondary model is supposed to make prediction from dogs breed classifier model  to assign detailed class for each dog instance.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_classification_model@v2\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\",\n            \"confidence\": 0.09\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.breds_classification.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/#workflow-with-classifier-providing-detailed-labels-for-detected-objects","title":"Workflow with classifier providing detailed labels for detected objects","text":"<p>This example illustrates how helpful Workflows could be when you have generic object detection model  (capable of detecting common classes - like dogs) and specific classifier (capable of providing granular  predictions for narrow high-level classes of objects - like dogs breed classifier). Having list of classifier predictions for each detected dog is not handy way of dealing with output -  as you kind of loose the information about location of specific dog. To avoid this problem, you may want to replace class labels of original bounding boxes (from the first model localising dogs) with classes predicted by classifier.</p> <p>In this example, we use Detections Classes Replacement block which is also interesting from the  perspective of difference of its inputs dimensionality levels. <code>object_detection_predictions</code> input has level 1 (there is one prediction with bboxes for each input image) and <code>classification_predictions</code> has level 2 (there are bunch of classification results for each input image). The block combines that two inputs and produces result at dimensionality level 1 - exactly the same as predictions from  object detection model.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\",\n            \"confidence\": 0.09\n        },\n        {\n            \"type\": \"DetectionsClassesReplacement\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.breds_classification.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"original_predictions\",\n            \"selector\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions_with_replaced_classes\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/#workflow-presenting-models-ensemble","title":"Workflow presenting models ensemble","text":"<p>This workflow presents how to combine predictions from multiple models running against the same  input image with the block called Detections Consensus. </p> <p>First, we run two object detections models steps and we combine their predictions. Fusion may be  performed in different scenarios based on Detections Consensus step configuration:</p> <ul> <li> <p>you may combine predictions from models detecting different objects and then require only single  model vote to add predicted bounding box to the output prediction</p> </li> <li> <p>you may combine predictions from models detecting the same objects and expect multiple positive  votes to accept bounding box to the output prediction - this way you may improve the quality of  predictions</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection_1\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": 0.3\n        },\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection_2\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": 0.83\n        },\n        {\n            \"type\": \"DetectionsConsensus\",\n            \"name\": \"consensus\",\n            \"predictions_batches\": [\n                \"$steps.detection_1.predictions\",\n                \"$steps.detection_2.predictions\"\n            ],\n            \"required_votes\": 2,\n            \"required_objects\": {\n                \"person\": 2\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.consensus.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/#comparison-of-detection-models-predictions","title":"Comparison of detection models predictions","text":"<p>This example showcases how to compare predictions from two different models using Workflows and  Model Comparison Visualization block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_1\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_2\",\n            \"default_value\": \"yolov8n-1280\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_1\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_2\"\n        },\n        {\n            \"type\": \"roboflow_core/model_comparison_visualization@v1\",\n            \"name\": \"model_comparison_visualization\",\n            \"image\": \"$inputs.image\",\n            \"predictions_a\": \"$steps.model_1.predictions\",\n            \"predictions_b\": \"$steps.model.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_1_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_2_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model_1.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualization\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model_comparison_visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/","title":"Workflows with Visual Language Models","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-anthropic-claude-with-arbitrary-prompt","title":"Prompting Anthropic Claude with arbitrary prompt","text":"<p>In this example, Anthropic Claude model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"Give me dominant color of the image\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-ocr-model","title":"Using Anthropic Claude as OCR model","text":"<p>In this example, Anthropic Claude model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-visual-question-answering-system","title":"Using Anthropic Claude as Visual Question Answering system","text":"<p>In this example, Anthropic Claude model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-image-captioning-system","title":"Using Anthropic Claude as Image Captioning system","text":"<p>In this example, Anthropic Claude model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\",\n            \"temperature\": 1.0\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-multi-class-classifier","title":"Using Anthropic Claude as multi-class classifier","text":"<p>In this example, Anthropic Claude model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v2</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"classes\": \"$steps.claude.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"claude_result\",\n            \"selector\": \"$steps.claude.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-multi-label-classifier","title":"Using Anthropic Claude as multi-label classifier","text":"<p>In this example, Anthropic Claude model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v2</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"classes\": \"$steps.claude.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-to-provide-structured-json","title":"Using Anthropic Claude to provide structured JSON","text":"<p>In this example, Anthropic Claude model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.claude.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-object-detection-model","title":"Using Anthropic Claude as object-detection model","text":"<p>In this example, Anthropic Claude model is expected to provide output, which can later be parsed by dedicated <code>roboflow_core/vlm_as_detector@v1</code> block which transforms string into <code>sv.Detections</code>,  which can later be used by other blocks processing object-detection predictions.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"object-detection\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_detector@v2\",\n            \"name\": \"parser\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"image\": \"$inputs.image\",\n            \"classes\": \"$steps.claude.classes\",\n            \"model_type\": \"anthropic-claude\",\n            \"task_type\": \"object-detection\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"claude_result\",\n            \"selector\": \"$steps.claude.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-secondary-classifier","title":"Using Anthropic Claude as secondary classifier","text":"<p>In this example, Anthropic Claude model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"classes\": \"$steps.claude.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-grounded-classification","title":"Florence 2 - grounded classification","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use object detection model to find regions of interest in the  input image, which are later classified by Florence 2 model. With Workflows it is possible  to pass <code>grounding_detection</code> as an input for all of the tasks named <code>detection-grounded-*</code>.</p> <p>Grounding detection can either be input parameter or output of detection model. If the  latter is true, one should choose <code>grounding_selection_mode</code> - as Florence do only support  a single bounding box as grounding - when multiple detections can be provided, block will select one based on parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-classification\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-grounded-segmentation","title":"Florence 2 - grounded segmentation","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use object detection model to find regions of interest in the  input image and run segmentation of selected region with Florence 2. With Workflows it is  possible to pass <code>grounding_detection</code> as an input for all of the tasks named  <code>detection-grounded-*</code>.</p> <p>Grounding detection can either be input parameter or output of detection model. If the  latter is true, one should choose <code>grounding_selection_mode</code> - as Florence do only support  a single bounding box as grounding - when multiple detections can be provided, block will select one based on parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-instance-segmentation\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-grounded-captioning","title":"Florence 2 - grounded captioning","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use object detection model to find regions of interest in the  input image and run captioning of selected region with Florence 2. With Workflows it is  possible to pass <code>grounding_detection</code> as an input for all of the tasks named  <code>detection-grounded-*</code>.</p> <p>Grounding detection can either be input parameter or output of detection model. If the  latter is true, one should choose <code>grounding_selection_mode</code> - as Florence do only support  a single bounding box as grounding - when multiple detections can be provided, block will select one based on parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-instance-segmentation\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-object-detection","title":"Florence 2 - object detection","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use Florence 2 as zero-shot object detection model, specifically  performing open-vocabulary detection. Input parameter <code>classes</code> can be used to provide list of objects that model should find. Beware that Florence 2 is prone to  seek for all of the classes provided in your list - so if you select class which is not visible in the image, you can expect either big bounding box covering whole image,  or multiple bounding boxes over one of detected instance, with auxiliary boxes providing not meaningful labels for all of the objects you specified in class list.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-instance-segmentation\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-googles-gemini-with-arbitrary-prompt","title":"Prompting Google's Gemini with arbitrary prompt","text":"<p>In this example, Google's Gemini model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"Give me dominant color of the image\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-ocr-model","title":"Using Google's Gemini as OCR model","text":"<p>In this example, Google's Gemini model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-visual-question-answering-system","title":"Using Google's Gemini as Visual Question Answering system","text":"<p>In this example, Google's Gemini model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-image-captioning-system","title":"Using Google's Gemini as Image Captioning system","text":"<p>In this example, Google's Gemini model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\",\n            \"temperature\": 1.0\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-multi-class-classifier","title":"Using Google's Gemini as multi-class classifier","text":"<p>In this example, Google's Gemini model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v2</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"classes\": \"$steps.gemini.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"gemini_result\",\n            \"selector\": \"$steps.gemini.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-multi-label-classifier","title":"Using Google's Gemini as multi-label classifier","text":"<p>In this example, Google's Gemini model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v2</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"classes\": \"$steps.gemini.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-to-provide-structured-json","title":"Using Google's Gemini to provide structured JSON","text":"<p>In this example, Google's Gemini model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.gemini.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-object-detection-model","title":"Using Google's Gemini as object-detection model","text":"<p>In this example, Google's Gemini model is expected to provide output, which can later be parsed by dedicated <code>roboflow_core/vlm_as_detector@v1</code> block which transforms string into <code>sv.Detections</code>,  which can later be used by other blocks processing object-detection predictions.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"object-detection\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_detector@v2\",\n            \"name\": \"parser\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"image\": \"$inputs.image\",\n            \"classes\": \"$steps.gemini.classes\",\n            \"model_type\": \"google-gemini\",\n            \"task_type\": \"object-detection\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"gemini_result\",\n            \"selector\": \"$steps.gemini.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-different-versions-of-googles-gemini-for-image-captioning","title":"Using different versions of Google's Gemini for Image Captioning","text":"<p>In this example, we test different Gemini model versions for image captioning.     This workflow allows specifying any supported Gemini model version as input parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_version\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"$inputs.model_version\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-secondary-classifier","title":"Using Google's Gemini as secondary classifier","text":"<p>In this example, Google's Gemini model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"classes\": \"$steps.gemini.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-llama-vision-32-with-arbitrary-prompt","title":"Prompting LLama Vision 3.2 with arbitrary prompt","text":"<p>In this example, LLama Vision 3.2 model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.llama.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-as-ocr-model","title":"Using LLama Vision 3.2 as OCR model","text":"<p>In this example, LLama Vision 3.2 model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.llama.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-as-visual-question-answering-system","title":"Using LLama Vision 3.2 as Visual Question Answering system","text":"<p>In this example, LLama Vision 3.2 model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.llama.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-as-image-captioning-system","title":"Using LLama Vision 3.2 as Image Captioning system","text":"<p>In this example, LLama Vision 3.2 model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.llama.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-as-multi-class-classifier","title":"Using LLama Vision 3.2 as multi-class classifier","text":"<p>In this example, LLama Vision 3.2 model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v2</code> block which turns LLama Vision 3.2 output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.llama.output\",\n            \"classes\": \"$steps.llama.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"llama_result\",\n            \"selector\": \"$steps.llama.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-as-multi-label-classifier","title":"Using LLama Vision 3.2 as multi-label classifier","text":"<p>In this example, LLama Vision 3.2 model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns LLama Vision 3.2 output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.llama.output\",\n            \"classes\": \"$steps.llama.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-to-provide-structured-json","title":"Using LLama Vision 3.2 to provide structured JSON","text":"<p>In this example, LLama Vision 3.2 model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.llama.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"llama_output\",\n            \"selector\": \"$steps.llama.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-llama-vision-32-as-secondary-classifier","title":"Using LLama Vision 3.2 as secondary classifier","text":"<p>In this example, LLama Vision 3.2 model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/llama_3_2_vision@v1\",\n            \"name\": \"llama\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"11B (Regular) - OpenRouter\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.llama.output\",\n            \"classes\": \"$steps.llama.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#moondream-2-object-detection","title":"Moondream 2 - object detection","text":"<p>Use Moondream2 to detect objects in an image.</p> <pre><code>You can pass in a prompt to the model to specify what you want to detect. The model will return a list of detection coordinates corresponding to the prompt.\n\nThis block only works with one class at a time. This is because Moondream2 does not allow zero shot detection on more than one class at once.\n</code></pre> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/moondream2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"prompt\": \"dog\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-gpt-with-arbitrary-prompt","title":"Prompting GPT with arbitrary prompt","text":"<p>In this example, GPT model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-ocr-model","title":"Using GPT as OCR model","text":"<p>In this example, GPT model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"gpt-4o-mini\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-visual-question-answering-system","title":"Using GPT as Visual Question Answering system","text":"<p>In this example, GPT model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-image-captioning-system","title":"Using GPT as Image Captioning system","text":"<p>In this example, GPT model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-multi-class-classifier","title":"Using GPT as multi-class classifier","text":"<p>In this example, GPT model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v2</code> block which turns GPT output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gpt.output\",\n            \"classes\": \"$steps.gpt.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"gpt_result\",\n            \"selector\": \"$steps.gpt.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-multi-label-classifier","title":"Using GPT as multi-label classifier","text":"<p>In this example, GPT model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns GPT output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gpt.output\",\n            \"classes\": \"$steps.gpt.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-to-provide-structured-json","title":"Using GPT to provide structured JSON","text":"<p>In this example, GPT model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.gpt.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-secondary-classifier","title":"Using GPT as secondary classifier","text":"<p>In this example, GPT model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v2\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.gpt.output\",\n            \"classes\": \"$steps.gpt.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#smolvlm2","title":"SmolVLM2","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>Use SmolVLM2 to ask questions about images, including documents and photos, and get answers in natural language.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/smolvlm2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"lmm\",\n            \"prompt\": \"What is in this image?\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-stability-ai-with-arbitrary-prompt","title":"Prompting Stability-AI with arbitrary prompt","text":"<p>In this example, Stability-AI image generation model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\",\n            \"default_value\": \"Raccoon in space suit\"\n        },\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/stability_ai_image_gen@v1\",\n            \"name\": \"stability_ai_image_generation\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\",\n            \"image\": \"$inputs.image\",\n            \"strength\": 0.3\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"stability_ai_image_generation\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.stability_ai_image_generation.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visualization_blocks/","title":"Workflows with visualization blocks","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_visualization_blocks/#workflow-with-multi-label-classification-label-visualization","title":"Workflow with multi-label classification label visualization","text":"<p>This workflow demonstrates how to visualize the predictions of a multi-label classification model.  It is compatable with single-label and multi-label classification tasks. It is also  compatible with supervision visualization fields like text position, color, scale, etc.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"deepfashion2-1000-items/1\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_multi_label_classification_model@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/classification_label_visualization@v1\",\n            \"name\": \"classification_label_visualization\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"text\": \"Class and Confidence\",\n            \"text_position\": \"CENTER\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"classification_label_visualization\",\n            \"selector\": \"$steps.classification_label_visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visualization_blocks/#workflow-with-single-label-classification-label-visualization","title":"Workflow with single-label classification label visualization","text":"<p>This workflow demonstrates how to visualize the predictions of a single-label classification model.  It is compatable with single-label and multi-label classification tasks. It is also  compatible with supervision visualization fields like text position, color, scale, etc.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"fruit-ee3k2/1\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/classification_label_visualization@v1\",\n            \"name\": \"classification_label_visualization\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"num_classifications\": \"$inputs.num_classifications\",\n            \"text\": \"Class and Confidence\",\n            \"color_axis\": \"INDEX\",\n            \"color_palette\": \"ROBOFLOW\",\n            \"text_scale\": 1,\n            \"text_color\": \"BLACK\",\n            \"text_padding\": 28\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"classification_label_visualization\",\n            \"selector\": \"$steps.classification_label_visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visualization_blocks/#predictions-from-different-models-visualised-together","title":"Predictions from different models visualised together","text":"<p>This workflow showcases how predictions from different models (even from nested  batches created from input images) may be visualised together.</p> <p>Our scenario covers:</p> <ul> <li> <p>Detecting cars using YOLOv8 model</p> </li> <li> <p>Dynamically cropping input images to run secondary model (license plates detector) for each  car instance</p> </li> <li> <p>Stitching together all predictions for licence plates into single prediction</p> </li> <li> <p>Fusing cars detections and license plates detections into single prediction</p> </li> <li> <p>Visualizing final predictions</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"car_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"car\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.car_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v2\",\n            \"name\": \"plates_detection\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"vehicle-registration-plates-trudk/2\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_stitch@v1\",\n            \"name\": \"stitch\",\n            \"reference_image\": \"$inputs.image\",\n            \"predictions\": \"$steps.plates_detection.predictions\",\n            \"overlap_filtering_strategy\": \"nms\"\n        },\n        {\n            \"type\": \"DetectionsConsensus\",\n            \"name\": \"consensus\",\n            \"predictions_batches\": [\n                \"$steps.car_detection.predictions\",\n                \"$steps.stitch.predictions\"\n            ],\n            \"required_votes\": 1\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bbox_visualiser\",\n            \"predictions\": \"$steps.consensus.predictions\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.consensus.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualisation\",\n            \"selector\": \"$steps.bbox_visualiser.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/kinds/","title":"Kinds","text":"<p>In Workflows, some values can\u2019t be set in advance and are only determined during execution.  This is similar to writing a function where you don\u2019t know the exact input values upfront \u2014 they\u2019re only  provided at runtime, either from user inputs or from other function outputs. </p> <p>To manage this, Workflows use selectors, which act like references, pointing to data without containing it directly. </p> <p>selectors</p> <p>Selectors might refer to a named input - for example input image - like <code>$inputs.image</code>  or predictions generated by a previous step - like <code>$steps.my_model.predictions</code></p> <p>In the Workflows ecosystem, users focus on data purpose (e.g., \u201cimage\u201d) without worrying about its exact format.  Meanwhile, developers building workflow blocks need precise data formats. Kinds serve both needs - they simplify data handling for users while ensuring developers work with the correct data structure.</p>"},{"location":"workflows/kinds/#what-are-the-kinds","title":"What are the Kinds?","text":"<p>Kinds is Workflows type system with each kind defining:</p> <ul> <li> <p>name - expressing semantic meaning of the underlying data - like <code>image</code> or <code>point</code>;</p> </li> <li> <p>Python data representation - the data type and format that blocks creators should expect when handling  the data within blocks;</p> </li> <li> <p>optional serialized data representation - defining what is the format of the kind that  external systems should use to integrate with Workflows ecosystem - when needed, custom kinds serializers and deserializers are provided to ensure seamless translation; </p> </li> </ul> <p>Using kinds streamlines compatibility: when a step outputs data of a certain kind and another step requires that  same kind, the workflow engine assumes they\u2019ll be compatible, reducing the need for compatibility checks and  providing compile-time verification of Workflows definitions.</p> <p>Note</p> <p>As for now, <code>kinds</code> are such simplistic that do not support types polymorphism - and developers are asked to use unions of kinds to solve that problem. As defining extensive unions of kinds may be  problematic, this problem will probably be addressed in Execution Engine <code>v2</code>.</p> <p>Warning</p> <p>In <code>inference</code> release <code>0.18.0</code> we decided to make drastic move to heal the ecosystem  from the problem with ambiguous kinds names (<code>Batch[X]</code> vs <code>X</code> - see more  here). </p> <p>The change is breaking only if there is remote Workflow plugin depending on imports from <code>inference.core.workflows.execution_engine.entities.types</code> module, which is not the case to the best of our knowledge. We removed problematic kinds as if they never existed in the ecosystem and fixed all blocks from <code>roboflow_core</code> plugin. If there is anyone impacted by the change - here is the  migration guide.</p> <p>This warning will be removed end of Q1 2025.</p> <p>Warning</p> <p>Support for proper serialization and deserialization of any arbitrary kind was  introduced in Execution Engine <code>v1.3.0</code> (released with inference <code>0.26.0</code>). Workflows plugins created prior that change may be updated - see refreshed  Blocks Bundling page.</p> <p>This warning will be removed end of Q1 2025.</p>"},{"location":"workflows/kinds/#kinds-declared-in-roboflow-plugins","title":"Kinds declared in Roboflow plugins","text":"<ul> <li><code>*</code>: Equivalent of any element</li> <li><code>ROBOFLOW_MANAGED_KEY</code>: Roboflow-managed key or credential</li> <li><code>bar_code_detection</code>: Prediction with barcode detection</li> <li><code>boolean</code>: Boolean flag</li> <li><code>bytes</code>: This kind represent bytes</li> <li><code>classification_prediction</code>: Predictions from classifier</li> <li><code>contours</code>: List of numpy arrays where each array represents contour points</li> <li><code>detection</code>: Single element of detections-based prediction (like <code>object_detection_prediction</code>)</li> <li><code>dictionary</code>: Dictionary</li> <li><code>embedding</code>: A list of floating point numbers representing a vector embedding.</li> <li><code>float_zero_to_one</code>: <code>float</code> value in range <code>[0.0, 1.0]</code></li> <li><code>float</code>: Float value</li> <li><code>image_keypoints</code>: Image keypoints detected by classical Computer Vision method</li> <li><code>image_metadata</code>: Dictionary with image metadata required by supervision</li> <li><code>image</code>: Image in workflows</li> <li><code>inference_id</code>: Inference identifier</li> <li><code>instance_segmentation_prediction</code>: Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object</li> <li><code>integer</code>: Integer value</li> <li><code>keypoint_detection_prediction</code>: Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object</li> <li><code>language_model_output</code>: LLM / VLM output</li> <li><code>list_of_values</code>: List of values of any type</li> <li><code>numpy_array</code>: Numpy array</li> <li><code>object_detection_prediction</code>: Prediction with detected bounding boxes in form of sv.Detections(...) object</li> <li><code>parent_id</code>: Identifier of parent for step output</li> <li><code>point</code>: Single point in 2D</li> <li><code>prediction_type</code>: String value with type of prediction</li> <li><code>qr_code_detection</code>: Prediction with QR code detection</li> <li><code>rgb_color</code>: RGB color</li> <li><code>roboflow_api_key</code>: Roboflow API key</li> <li><code>roboflow_model_id</code>: Roboflow model id</li> <li><code>roboflow_project</code>: Roboflow project name</li> <li><code>secret</code>: Secret value</li> <li><code>serialised_payloads</code>: Serialised element that is usually accepted by sink</li> <li><code>string</code>: String value</li> <li><code>timestamp</code>: Timestamp object</li> <li><code>top_class</code>: String value representing top class predicted by classification model</li> <li><code>video_metadata</code>: Video image metadata</li> <li><code>zone</code>: Definition of polygon zone</li> </ul>"},{"location":"workflows/kinds/SUMMARY/","title":"SUMMARY","text":"<ul> <li>*</li> <li>ROBOFLOW_MANAGED_KEY</li> <li>bar_code_detection</li> <li>boolean</li> <li>bytes</li> <li>classification_prediction</li> <li>contours</li> <li>detection</li> <li>dictionary</li> <li>embedding</li> <li>float_zero_to_one</li> <li>float</li> <li>image_keypoints</li> <li>image_metadata</li> <li>image</li> <li>inference_id</li> <li>instance_segmentation_prediction</li> <li>integer</li> <li>keypoint_detection_prediction</li> <li>language_model_output</li> <li>list_of_values</li> <li>numpy_array</li> <li>object_detection_prediction</li> <li>parent_id</li> <li>point</li> <li>prediction_type</li> <li>qr_code_detection</li> <li>rgb_color</li> <li>roboflow_api_key</li> <li>roboflow_model_id</li> <li>roboflow_project</li> <li>secret</li> <li>serialised_payloads</li> <li>string</li> <li>timestamp</li> <li>top_class</li> <li>video_metadata</li> <li>zone</li> </ul>"},{"location":"workflows/kinds/bar_code_detection/","title":"<code>bar_code_detection</code> Kind","text":"<p>Prediction with barcode detection</p>"},{"location":"workflows/kinds/bar_code_detection/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/bar_code_detection/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/bar_code_detection/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/bar_code_detection/#details","title":"Details","text":"<p>This kind represents batch of predictions regarding barcodes location and data their provide.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    1.0, 1.0, 1.0, 1.0]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={\n        'class_name': array(['barcode', 'barcode', 'barcode', 'barcode'], dtype='&lt;U13')\n        'detection_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'inference_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'prediction_type': array([\n            'barcode-detection', 'barcode-detection', \n            'barcode-detection', 'barcode-detection'\n        ], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'root_parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'scaling_relative_to_parent': array([1, 1, 1, 1]),\n        'scaling_relative_to_root_parent': array([1, 1, 1, 1]),\n        'data': np.array(['qr-code-1-data', 'qr-code-2-data', 'qr-code-3-data', 'qr-code-4-data'])\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> <li> <p><code>data</code> - extracted barcode</p> </li> </ul> <p>SERIALISATION: Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: ObjectDetectionInferenceResponse</p>"},{"location":"workflows/kinds/boolean/","title":"<code>boolean</code> Kind","text":"<p>Boolean flag</p>"},{"location":"workflows/kinds/boolean/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/boolean/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>bool</code></p>"},{"location":"workflows/kinds/boolean/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>bool</code></p>"},{"location":"workflows/kinds/boolean/#details","title":"Details","text":"<p>This kind represents boolean value - <code>True</code> or <code>False</code></p>"},{"location":"workflows/kinds/bytes/","title":"<code>bytes</code> Kind","text":"<p>This kind represent bytes</p>"},{"location":"workflows/kinds/bytes/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/bytes/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/bytes/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>bytes</code></p>"},{"location":"workflows/kinds/bytes/#details","title":"Details","text":"<p>This kind represent bytes. Default serializer turns bytes into base64-encoded string and this is the source of different data representation.</p>"},{"location":"workflows/kinds/classification_prediction/","title":"<code>classification_prediction</code> Kind","text":"<p>Predictions from classifier</p>"},{"location":"workflows/kinds/classification_prediction/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/classification_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/classification_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/classification_prediction/#details","title":"Details","text":"<p>This kind represent predictions from Classification Models.</p> <p>Examples: <pre><code># in case of multi-class classification\n{\n    \"image\": {\"height\": 128, \"width\": 256},\n    \"predictions\": [{\"class_name\": \"A\", \"class_id\": 0, \"confidence\": 0.3}],\n    \"top\": \"A\",\n    \"confidence\": 0.3,\n    \"parent_id\": \"some\",\n    \"prediction_type\": \"classification\",\n    \"inference_id\": \"some\",\n    \"root_parent_id\": \"some\",\n}\n\n# in case of multi-label classification\n{\n    \"image\": {\"height\": 128, \"width\": 256},\n    \"predictions\": {\n        \"a\": {\"confidence\": 0.3, \"class_id\": 0},\n        \"b\": {\"confidence\": 0.3, \"class_id\": 1},\n    }\n    \"predicted_classes\": [\"a\", \"b\"],\n    \"parent_id\": \"some\",\n    \"prediction_type\": \"classification\",\n    \"inference_id\": \"some\",\n    \"root_parent_id\": \"some\",\n}\n</code></pre></p>"},{"location":"workflows/kinds/contours/","title":"<code>contours</code> Kind","text":"<p>List of numpy arrays where each array represents contour points</p>"},{"location":"workflows/kinds/contours/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/contours/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[list]</code></p>"},{"location":"workflows/kinds/contours/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[np.ndarray]</code></p>"},{"location":"workflows/kinds/contours/#details","title":"Details","text":"<p>This kind represents a value of a list of numpy arrays where each array represents contour points.</p> <p>Example: <pre><code>[\n    np.array([[10, 10],\n              [20, 20],\n              [30, 30]], dtype=np.int32),\n    np.array([[50, 50],\n              [60, 60],\n              [70, 70]], dtype=np.int32)\n]\n</code></pre></p>"},{"location":"workflows/kinds/detection/","title":"<code>detection</code> Kind","text":"<p>Single element of detections-based prediction (like <code>object_detection_prediction</code>)</p>"},{"location":"workflows/kinds/detection/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/detection/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Tuple[list, Optional[list], Optional[float], Optional[float], Optional[int], dict]</code></p>"},{"location":"workflows/kinds/detection/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Tuple[np.ndarray, Optional[np.ndarray], Optional[float], Optional[float], Optional[int], dict]</code></p>"},{"location":"workflows/kinds/detection/#details","title":"Details","text":"<p>This kind represents single detection in prediction from a model that detects multiple elements (like object detection or instance segmentation model). It is represented as a tuple that is created from <code>sv.Detections(...)</code> object while iterating over its content. <code>workflows</code> utilises <code>data</code> property of <code>sv.Detections(...)</code> to keep additional metadata which will be available in the tuple. Some properties may not always be present. Take a look at documentation of  <code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code> kinds to discover which additional metadata are available.</p> <p>More technical details about  iterating over <code>sv.Detections(...)</code></p>"},{"location":"workflows/kinds/dictionary/","title":"<code>dictionary</code> Kind","text":"<p>Dictionary</p>"},{"location":"workflows/kinds/dictionary/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/dictionary/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/dictionary/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/dictionary/#details","title":"Details","text":"<p>Not available.</p>"},{"location":"workflows/kinds/embedding/","title":"<code>embedding</code> Kind","text":"<p>A list of floating point numbers representing a vector embedding.</p>"},{"location":"workflows/kinds/embedding/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/embedding/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[float]</code></p>"},{"location":"workflows/kinds/embedding/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[float]</code></p>"},{"location":"workflows/kinds/embedding/#details","title":"Details","text":"<p>This kind represents a vector embedding. It is a list of floating point numbers.</p> <p>Embeddings are used in various machine learning tasks like clustering, classification, and similarity search. They are used to represent data in a continuous, low-dimensional space.</p> <p>Typically, vectors that are close to each other in the embedding space are considered similar.</p>"},{"location":"workflows/kinds/float/","title":"<code>float</code> Kind","text":"<p>Float value</p>"},{"location":"workflows/kinds/float/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/float/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float/#details","title":"Details","text":"<p>Example: <pre><code>1.3\n2.7\n</code></pre></p>"},{"location":"workflows/kinds/float_zero_to_one/","title":"<code>float_zero_to_one</code> Kind","text":"<p><code>float</code> value in range <code>[0.0, 1.0]</code></p>"},{"location":"workflows/kinds/float_zero_to_one/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/float_zero_to_one/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float_zero_to_one/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float_zero_to_one/#details","title":"Details","text":"<p>This kind represents float value from 0.0 to 1.0. </p> <p>Examples: <pre><code>0.1\n0.4\n0.999\n</code></pre></p>"},{"location":"workflows/kinds/image/","title":"<code>image</code> Kind","text":"<p>Image in workflows</p>"},{"location":"workflows/kinds/image/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/image/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>WorkflowImageData</code></p>"},{"location":"workflows/kinds/image/#details","title":"Details","text":"<p>This is the representation of image in <code>workflows</code>. Underlying data type has different internal and external representation. As an input we support:</p> <p>Update added in Execution Engine <code>v1.2.0</code></p> <p><code>video_metadata</code> added as optional property - should be injected in context of video processing to  provide necessary context for blocks dedicated to video processing.</p> <ul> <li> <p><code>np.ndarray</code> image when Workflows Execution Engine is used directly in <code>inference</code> python package (array can be provided in a form of dictionary presented below, if <code>video_metadata</code> is intended to be injected)</p> </li> <li> <p>dictionary compatible with inference image utils:</p> </li> </ul> <pre><code>{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`,\n    \"video_metadata\": {  \n        # optional - can be added in context of video processing - introduced in \n        # Execution Engine `v1.2.0` - released in inference `v0.23.0`\n        \"video_identifier\": \"rtsp://some.com/stream1\",\n        \"comes_from_video_file\": False,\n        \"fps\": 23.99,\n        \"measured_fps\": 20.05,\n        \"frame_number\": 24,\n        \"frame_timestamp\": \"2024-08-21T11:13:44.313999\", \n    }  \n}\n</code></pre> <p>Whe using Workflows Execution Engine exposed behind <code>inference</code> server, two most common <code>type</code> values are <code>base64</code> and  <code>url</code>.</p> <p>Internally, <code>WorkflowImageData</code> is used. If you are a Workflow block developer, we advise checking out usage guide.</p>"},{"location":"workflows/kinds/image_keypoints/","title":"<code>image_keypoints</code> Kind","text":"<p>Image keypoints detected by classical Computer Vision method</p>"},{"location":"workflows/kinds/image_keypoints/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/image_keypoints/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_keypoints/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_keypoints/#details","title":"Details","text":"<p>The kind represents image keypoints that are detected by classical Computer Vision methods. Underlying representation is serialised OpenCV KeyPoint object.</p> <p>Examples: <pre><code>{\n    \"pt\": (2.429290294647217, 1197.7939453125),\n    \"size\": 1.9633429050445557,\n    \"angle\": 183.4322509765625,\n    \"response\": 0.03325376659631729,\n    \"octave\": 6423039,\n    \"class_id\": -1\n}\n</code></pre></p>"},{"location":"workflows/kinds/image_metadata/","title":"<code>image_metadata</code> Kind","text":"<p>Dictionary with image metadata required by supervision</p>"},{"location":"workflows/kinds/image_metadata/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/image_metadata/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_metadata/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_metadata/#details","title":"Details","text":"<p>This kind represent batch of prediction metadata providing information about the image that prediction was made against.</p> <p>Examples: <pre><code>[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n</code></pre></p>"},{"location":"workflows/kinds/inference_id/","title":"<code>inference_id</code> Kind","text":"<p>Inference identifier</p>"},{"location":"workflows/kinds/inference_id/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/inference_id/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/inference_id/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/inference_id/#details","title":"Details","text":"<p>This kind represents identifier of inference process, which is usually opaque string used as correlation identifier for external systems (like Roboflow Model Monitoring).</p> <p>Examples: <pre><code>b1851e3d-a145-4540-a39e-875f21f6cd84\n</code></pre></p>"},{"location":"workflows/kinds/instance_segmentation_prediction/","title":"<code>instance_segmentation_prediction</code> Kind","text":"<p>Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object</p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#details","title":"Details","text":"<p>This kind represents single instance segmentation prediction in form of  <code>sv.Detections(...)</code> object.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='&lt;U1'),\n        'detection_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640]]),\n        'inference_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'prediction_type': array(['instance-segmentation'], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0]]),\n        'root_parent_dimensions': array([[425, 640]]),\n        'parent_coordinates': array([[0, 0]]),\n        'parent_dimensions': array([[425, 640]]),\n        'scaling_relative_to_parent': array([1]),\n        'scaling_relative_to_root_parent': array([1]),\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> </ul> <p>SERIALISATION:</p> <p>Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: InstanceSegmentationInferenceResponse</p>"},{"location":"workflows/kinds/integer/","title":"<code>integer</code> Kind","text":"<p>Integer value</p>"},{"location":"workflows/kinds/integer/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/integer/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>int</code></p>"},{"location":"workflows/kinds/integer/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>int</code></p>"},{"location":"workflows/kinds/integer/#details","title":"Details","text":"<p>Examples: <pre><code>1\n2\n</code></pre></p>"},{"location":"workflows/kinds/keypoint_detection_prediction/","title":"<code>keypoint_detection_prediction</code> Kind","text":"<p>Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object</p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#details","title":"Details","text":"<p>This kind represents single keypoints prediction in form of  <code>sv.Detections(...)</code> object.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='&lt;U1'),\n        'detection_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640]]),\n        'inference_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'prediction_type': array(['instance-segmentation'], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0]]),\n        'root_parent_dimensions': array([[425, 640]]),\n        'parent_coordinates': array([[0, 0]]),\n        'parent_dimensions': array([[425, 640]]),\n        'scaling_relative_to_parent': array([1]),\n        'scaling_relative_to_root_parent': array([1]),\n        'keypoints_class_name': array(),  # variable length array of type object - one 1D array of str for each box\n        'keypoints_class_id': array(),  # variable length array of type object - one 1D array of int for each box\n        'keypoints_confidence': array(),  # variable length array of type object - one 1D array of float for each box\n        'keypoints_xy': array(),  # variable length array of type object - one 2D array for bbox with (x, y) coords\n    }\n)\n</code></pre></p> <p>Prior to sv.Keypoints(...) we introduced  keypoints detection based on <code>sv.Detections(...)</code> object. The decision was suboptimal so we would need to revert in the future, but for now this is the format of data for keypoints detection. </p> <p>The design of metadata is also suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> <li> <p><code>keypoints_class_name</code> array of variable size 1D arrays of string with key points class names</p> </li> <li> <p><code>keypoints_class_id</code> array of variable size 1D arrays of int with key points class ids</p> </li> <li> <p><code>keypoints_confidence</code> array of variable size 1D arrays of float with key points confidence</p> </li> <li> <p><code>keypoints_xy</code> array of variable size 2D arrays of coordinates of keypoints in <code>(x, y)</code> format</p> </li> </ul> <p>SERIALISATION:</p> <p>Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output, but loosing keypoints details - which can be recovered if output  JSON field is parsed. Entity details: KeypointsDetectionInferenceResponse</p>"},{"location":"workflows/kinds/language_model_output/","title":"<code>language_model_output</code> Kind","text":"<p>LLM / VLM output</p>"},{"location":"workflows/kinds/language_model_output/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/language_model_output/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/language_model_output/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/language_model_output/#details","title":"Details","text":"<p>This kind represents output generated by language model. It is Python string, which can be processed  by blocks transforming LLMs / VLMs output into structured form.</p> <p>Examples: <pre><code>{\"predicted_class\": \"car\", \"confidence\": 0.7}  # which is example JSON with classification prediction\n\"The is A.\"  # which is example unstructured generation for VQA task \n</code></pre></p>"},{"location":"workflows/kinds/list_of_values/","title":"<code>list_of_values</code> Kind","text":"<p>List of values of any type</p>"},{"location":"workflows/kinds/list_of_values/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/list_of_values/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[Any]</code></p>"},{"location":"workflows/kinds/list_of_values/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[Any]</code></p>"},{"location":"workflows/kinds/list_of_values/#details","title":"Details","text":"<p>This kind represents Python list of Any values.</p> <p>Examples: <pre><code>[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n</code></pre></p>"},{"location":"workflows/kinds/numpy_array/","title":"<code>numpy_array</code> Kind","text":"<p>Numpy array</p>"},{"location":"workflows/kinds/numpy_array/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/numpy_array/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>list</code></p>"},{"location":"workflows/kinds/numpy_array/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>np.ndarray</code></p>"},{"location":"workflows/kinds/numpy_array/#details","title":"Details","text":"<p>Any np.ndarray object</p>"},{"location":"workflows/kinds/object_detection_prediction/","title":"<code>object_detection_prediction</code> Kind","text":"<p>Prediction with detected bounding boxes in form of sv.Detections(...) object</p>"},{"location":"workflows/kinds/object_detection_prediction/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/object_detection_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/object_detection_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/object_detection_prediction/#details","title":"Details","text":"<p>This kind represents single object detection prediction in form of  <code>sv.Detections(...)</code> object.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={\n        'class_name': array(['car', 'truck', 'car', 'car'], dtype='&lt;U13')\n        'detection_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'inference_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'prediction_type': array([\n            'object-detection', 'object-detection', \n            'object-detection', 'object-detection'\n        ], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'root_parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'scaling_relative_to_parent': array([1, 1, 1, 1]),\n        'scaling_relative_to_root_parent': array([1, 1, 1, 1]),\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> </ul> <p>SERIALISATION:</p> <p>Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: ObjectDetectionInferenceResponse</p>"},{"location":"workflows/kinds/parent_id/","title":"<code>parent_id</code> Kind","text":"<p>Identifier of parent for step output</p>"},{"location":"workflows/kinds/parent_id/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/parent_id/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/parent_id/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/parent_id/#details","title":"Details","text":"<p>This kind represent batch of prediction metadata providing information about the context of prediction. For example - whenever there is a workflow with multiple models - such that first model detect objects  and then other models make their predictions based on crops from first model detections - <code>parent_id</code> helps to figure out which detection of the first model is associated to which downstream predictions.</p> <p>Examples: <pre><code>\"uuid-1\"\n\"uuid-2\"\n</code></pre></p>"},{"location":"workflows/kinds/point/","title":"<code>point</code> Kind","text":"<p>Single point in 2D</p>"},{"location":"workflows/kinds/point/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/point/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Tuple[int, int]</code></p>"},{"location":"workflows/kinds/point/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Tuple[int, int]</code></p>"},{"location":"workflows/kinds/point/#details","title":"Details","text":"<p>Not available.</p>"},{"location":"workflows/kinds/prediction_type/","title":"<code>prediction_type</code> Kind","text":"<p>String value with type of prediction</p>"},{"location":"workflows/kinds/prediction_type/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/prediction_type/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/prediction_type/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/prediction_type/#details","title":"Details","text":"<p>This kind represent batch of prediction metadata providing information about the type of prediction.</p> <p>Examples: <pre><code>\"object-detection\"\n\"instance-segmentation\"\n</code></pre></p>"},{"location":"workflows/kinds/qr_code_detection/","title":"<code>qr_code_detection</code> Kind","text":"<p>Prediction with QR code detection</p>"},{"location":"workflows/kinds/qr_code_detection/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/qr_code_detection/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/qr_code_detection/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/qr_code_detection/#details","title":"Details","text":"<p>This kind represents batch of predictions regarding QR codes location and data their provide.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    1.0, 1.0, 1.0, 1.0]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={\n        'class_name': array(['qr_code', 'qr_code', 'qr_code', 'qr_code'], dtype='&lt;U13')\n        'detection_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'inference_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'prediction_type': array([\n            'qrcode-detection', 'qrcode-detection', \n            'qrcode-detection', 'qrcode-detection'\n        ], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'root_parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'scaling_relative_to_parent': array([1, 1, 1, 1]),\n        'scaling_relative_to_root_parent': array([1, 1, 1, 1]),\n        'data': np.array(['qr-code-1-data', 'qr-code-2-data', 'qr-code-3-data', 'qr-code-4-data'])\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> <li> <p><code>data</code> - extracted QR code</p> </li> </ul> <p>SERIALISATION: Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: ObjectDetectionInferenceResponse</p>"},{"location":"workflows/kinds/rgb_color/","title":"<code>rgb_color</code> Kind","text":"<p>RGB color</p>"},{"location":"workflows/kinds/rgb_color/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/rgb_color/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Tuple[int, int, int]</code></p>"},{"location":"workflows/kinds/rgb_color/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Tuple[int, int, int]</code></p>"},{"location":"workflows/kinds/rgb_color/#details","title":"Details","text":"<p>This kind represents RGB color as a tuple (R, G, B).</p> <p>Examples: <pre><code>(128, 32, 64)\n(255, 255, 255)\n</code></pre></p>"},{"location":"workflows/kinds/roboflow_api_key/","title":"<code>roboflow_api_key</code> Kind","text":"<p>Roboflow API key</p>"},{"location":"workflows/kinds/roboflow_api_key/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_api_key/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_api_key/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_api_key/#details","title":"Details","text":"<p>This kind represents API key that grants access to Roboflow platform. To learn more about Roboflow API keys visit this page.</p>"},{"location":"workflows/kinds/roboflow_managed_key/","title":"<code>ROBOFLOW_MANAGED_KEY</code> Kind","text":"<p>Roboflow-managed key or credential</p>"},{"location":"workflows/kinds/roboflow_managed_key/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_managed_key/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_managed_key/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_managed_key/#details","title":"Details","text":"<p>This kind represents a key or credential managed by Roboflow or stored in your Roboflow account. It can be used for for third party APIs that Roboflow can proxy requests on behalf of the user.</p> <p>If set to <code>rf_key:account</code> third party api calls will use Roboflow owned API key and proxied requests will be charged corosponding credits.  You can set the value to <code>rf_key:user:&lt;key_id&gt;</code> where key_id references a third party key you have stored in your Roboflow settigns.</p>"},{"location":"workflows/kinds/roboflow_model_id/","title":"<code>roboflow_model_id</code> Kind","text":"<p>Roboflow model id</p>"},{"location":"workflows/kinds/roboflow_model_id/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_model_id/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_model_id/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_model_id/#details","title":"Details","text":"<p>This kind represents value specific for Roboflow platform. At the platform, models are identified with special strings in the format: <code>&lt;project_name&gt;/&lt;version&gt;</code>. You should expect this value to be provided once this kind is used. In some special cases, Roboflow  platform accepts model alias as model id which will not conform provided schema. List of aliases can be found here.  </p>"},{"location":"workflows/kinds/roboflow_project/","title":"<code>roboflow_project</code> Kind","text":"<p>Roboflow project name</p>"},{"location":"workflows/kinds/roboflow_project/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_project/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_project/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_project/#details","title":"Details","text":"<p>This kind represents value specific for Roboflow platform. At the platform, each project has  unique name and the value behind this kind represent this name. To learn more  on how to kick-off with Roboflow project - visit this page. </p>"},{"location":"workflows/kinds/secret/","title":"<code>secret</code> Kind","text":"<p>Secret value</p>"},{"location":"workflows/kinds/secret/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/secret/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/secret/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/secret/#details","title":"Details","text":"<p>This kind represents a secret - password or other credential that should remain confidential.</p>"},{"location":"workflows/kinds/serialised_payloads/","title":"<code>serialised_payloads</code> Kind","text":"<p>Serialised element that is usually accepted by sink</p>"},{"location":"workflows/kinds/serialised_payloads/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/serialised_payloads/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[Union[str, dict]]</code></p>"},{"location":"workflows/kinds/serialised_payloads/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[Union[str, bytes, dict]]</code></p>"},{"location":"workflows/kinds/serialised_payloads/#details","title":"Details","text":"<p>This value represents list of serialised values. Each serialised value is either string or bytes - if something else is provided - it will be attempted to be serialised  to JSON.</p> <p>Examples: <pre><code>[\"some\", b\"other\", {\"my\": \"dictionary\"}]\n</code></pre></p> <p>This kind is to be used in combination with sinks blocks and serializers blocks. Serializer should output value of this kind which shall then be accepted by sink.</p>"},{"location":"workflows/kinds/string/","title":"<code>string</code> Kind","text":"<p>String value</p>"},{"location":"workflows/kinds/string/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/string/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/string/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/string/#details","title":"Details","text":"<p>Examples: <pre><code>\"my string value\"\n</code></pre></p>"},{"location":"workflows/kinds/timestamp/","title":"<code>timestamp</code> Kind","text":"<p>Timestamp object</p>"},{"location":"workflows/kinds/timestamp/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/timestamp/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/timestamp/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>datetime</code></p>"},{"location":"workflows/kinds/timestamp/#details","title":"Details","text":"<p>Representation of timestamp in Workflows. </p> <p>Internally represented as <code>datetime.datetime</code> object (with time-zone info not required).  Can be serialized / deserialized to / from ISO-format timestamps.</p>"},{"location":"workflows/kinds/top_class/","title":"<code>top_class</code> Kind","text":"<p>String value representing top class predicted by classification model</p>"},{"location":"workflows/kinds/top_class/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/top_class/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/top_class/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/top_class/#details","title":"Details","text":"<p>The kind represent top classes predicted by classification model.</p> <p>Example: <pre><code>\"car\"\n</code></pre></p>"},{"location":"workflows/kinds/video_metadata/","title":"<code>video_metadata</code> Kind","text":"<p>Video image metadata</p>"},{"location":"workflows/kinds/video_metadata/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/video_metadata/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/video_metadata/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>VideoMetadata</code></p>"},{"location":"workflows/kinds/video_metadata/#details","title":"Details","text":"<p>Deprecated since Execution Engine <code>v1.2.0</code></p> <p><code>inference</code> maintainers decided to sunset <code>video_metadata</code> kind in favour of auxiliary metadata added to <code>image</code> kind. </p> <p>This is representation of metadata that describe images that come from videos. It is helpful in cases of stateful video processing, as the metadata may bring  pieces of information that are required by specific blocks.</p> <p>The kind has different internal end external representation. As input we support: <pre><code>{\n    \"video_identifier\": \"rtsp://some.com/stream1\",\n    \"comes_from_video_file\": False,\n    \"fps\": 23.99,\n    \"measured_fps\": 20.05,\n    \"frame_number\": 24,\n    \"frame_timestamp\": \"2024-08-21T11:13:44.313999\", \n}   \n</code></pre> Internally, <code>VideoMetadata</code> is used. If you are a Workflow block developer, we advise checking out usage guide.</p>"},{"location":"workflows/kinds/wildcard/","title":"<code>*</code> Kind","text":"<p>Equivalent of any element</p>"},{"location":"workflows/kinds/wildcard/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/wildcard/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Any</code></p>"},{"location":"workflows/kinds/wildcard/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Any</code></p>"},{"location":"workflows/kinds/wildcard/#details","title":"Details","text":"<p>This is a special kind that represents Any value - which is to be used by default if  kind annotation is not specified. It will not tell the compiler what is to be expected as a specific value in runtime, but at the same time, it makes it possible to run the  workflow when we do not know or do not care about types. </p> <p>Important note: Usage of this kind reduces execution engine capabilities to predict  problems with workflow and make those problems to be visible while running the workflow.</p>"},{"location":"workflows/kinds/zone/","title":"<code>zone</code> Kind","text":"<p>Definition of polygon zone</p>"},{"location":"workflows/kinds/zone/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/zone/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[Tuple[int, int]]</code></p>"},{"location":"workflows/kinds/zone/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[Tuple[int, int]]</code></p>"},{"location":"workflows/kinds/zone/#details","title":"Details","text":"<p>List of points defining polygon zone in format [(x, y)]</p>"},{"location":"workflows/video_processing/overview/","title":"Video Processing with Workflows","text":"<p>We've begun our journey into video processing using Workflows. Over time, we've expanded the number of  video-specific blocks (e.g., the ByteTracker block) and continue to dedicate efforts toward improving  their performance and robustness. The current state of this work is as follows:</p> <ul> <li> <p>We've introduced the <code>WorkflowVideoMetadata</code> input to store metadata related to video frames,  including declared FPS, measured FPS, timestamp, video source identifier, and file/stream flags. While this may not be the final approach  for handling video metadata, it allows us to build stateful video-processing blocks at this stage.  If your Workflow includes any blocks requiring input of kind <code>video_metadata</code>, you must define this input in  your Workflow. The metadata functions as a batch-oriented parameter, treated by the Execution Engine in the same way as <code>WorkflowImage</code>.</p> </li> <li> <p>The <code>InferencePipeline</code> supports  video processing with Workflows  by automatically injecting <code>WorkflowVideoMetadata</code> into the <code>video_metadata</code> field. This allows you to seamlessly run your Workflow using the <code>InferencePipeline</code> within the <code>inference</code> Python package.</p> </li> <li> <p>In the <code>0.21.0</code> release, we've initiated efforts to enable video processing management via the <code>inference</code> server API.  This means that eventually, no custom scripts will be required to process video using Workflows and <code>InferencePipeline</code>. You'll simply call an endpoint, specify the video source and the workflow, and the server will handle the rest\u2014allowing you to focus on consuming the results.</p> </li> </ul>"},{"location":"workflows/video_processing/overview/#video-management-api-comments-and-status-update","title":"Video management API - comments and status update","text":"<p>This is an experimental feature, and breaking changes may be introduced over time. There is a list of known issues. Please visit the page to raise new issues or comment on existing ones.</p>"},{"location":"workflows/video_processing/overview/#release-0210","title":"Release <code>0.21.0</code>","text":"<ul> <li> <p>Added basic endpoints to <code>list</code>, <code>start</code>, <code>pause</code>, <code>resume</code>, <code>terminate</code> and <code>consume</code> results of <code>InferencePipelines</code> running under control of <code>inference</code> server. Endpoints are enabled in <code>inference</code> server docker images for CPU, GPU and Jetson devices. Running inference server there would let you call  <code>http://127.0.0.1:9001/docs</code> to retrieve OpenAPI schemas for endpoints.</p> </li> <li> <p>Added HTTP client for new endpoints into <code>InferenceHTTPClient</code> from <code>inference_sdk</code>. Here you may find examples on how to use the client and API to start processing videos today:</p> </li> </ul> <p>Note</p> <p>Package version <code>inference~=0.21.0</code> is used in this example. Because the feature is experimental, the code may evolve over time.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://192.168.0.115:9001\",\n    api_key=\"&lt;ROBOFLOW-API-KEY&gt;\"\n)\n\n# to list active pipelines\nclient.list_inference_pipelines()\n\n# start processing - single stream\nclient.start_inference_pipeline_with_workflow(\n    video_reference=[\"rtsp://192.168.0.73:8554/live0.stream\"],\n    workspace_name=\"&lt;YOUR-WORKSPACE&gt;\",\n    workflow_id=\"&lt;YOUR-WORKFLOW-ID&gt;\",\n    results_buffer_size=5,  # results are consumed from in-memory buffer - optionally you can control its size\n)\n\n# start processing - one RTSP stream and one camera\n# USB camera cannot be passed easily to docker running on MacBook, but on Jetson devices it works :)\n\nclient.start_inference_pipeline_with_workflow(\n    video_reference=[\"rtsp://192.168.0.73:8554/live0.stream\", 0],\n    workspace_name=\"&lt;YOUR-WORKSPACE&gt;\",\n    workflow_id=\"&lt;YOUR-WORKFLOW-ID&gt;\",\n    batch_collection_timeout=0.05,  # for consumption of multiple video sources it is ADVISED to \n    # set batch collection timeout (defined as fraction of seconds - 0.05 = 50ms)\n)\n\n# start_inference_pipeline_with_workflow(...) will provide you pipeline_id which may be used to:\n\n# * get pipeline status\nclient.get_inference_pipeline_status(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * pause pipeline\nclient.pause_inference_pipeline(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * resume pipeline\nclient.resume_inference_pipeline(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * terminate pipeline\nclient.terminate_inference_pipeline(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * consume pipeline results\nclient.consume_inference_pipeline_result(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n    excluded_fields=[\"workflow_output_field_to_exclude\"]  # this is optional\n    # if you wanted to get rid of some outputs to save bandwidth - feel free to discard them\n)\n</code></pre> <p>The client presented above can be used to preview workflow outputs in a very naive way. Let's assume that the workflow you defined runs an object detection model and renders its output using Workflows visualization blocks that register the output image in the <code>preview</code> field. You can use the following script to poll and display processed video frames:</p> <pre><code>import cv2\nfrom inference_sdk import InferenceHTTPClient\nfrom inference.core.utils.image_utils import load_image\n\n\nclient = InferenceHTTPClient(\n    api_url=f\"http://127.0.0.1:9001\",\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n)\n\nwhile True:\n    result = client.consume_inference_pipeline_result(pipeline_id=\"&lt;PIPELINE-ID&gt;\")\n    if not result[\"outputs\"] or not result[\"outputs\"][0]:\n        # \"outputs\" key contains list of workflow results - why list? InferencePipeline can \n        # run on multiple video sources at the same time - each \"round\" it attempts to \n        # grab single frame from all sources and run through Workflows Execution Engine\n        # * when sources are inactive that may not be possible, hence empty list can be returned\n        # * when one of the source do not provide frame (for instance due to batch collection timeout)\n        # outputs list may contain `None` values!\n        continue\n    # let's assume single source\n    source_result = result[\"outputs\"][0]\n    image, _ = load_image(source_result[\"preview\"])  # \"preview\" is the name of workflow output with image\n    cv2.imshow(\"frame\", image)\n    cv2.waitKey(1)\n</code></pre>"}]}