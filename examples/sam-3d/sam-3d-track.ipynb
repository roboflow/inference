{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a980153e",
   "metadata": {},
   "source": [
    "# Monocular 3D object tracking with SAM-3D Objects\n",
    "\n",
    "This notebook requires a self-hosted inference server with a 32GB+ VRAM GPU. See the README for the recommended setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://localhost:9001\"\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "SEGMENTATION_MODEL_ID = \"rfdetr-seg-preview\"\n",
    "SAM3_3D_MODEL_ID = \"sam3-3d-objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervision.assets import download_assets, VideoAssets\n",
    "\n",
    "INPUT_VIDEO_PATH = download_assets(VideoAssets.MILK_BOTTLING_PLANT)\n",
    "# INPUT_VIDEO_PATH = download_assets(VideoAssets.VEHICLES)\n",
    "\n",
    "# FPS to sample the input video\n",
    "SAMPLE_FPS = 5\n",
    "# Limit the number of frames to process\n",
    "MAX_FRAMES = None\n",
    "\n",
    "OUTPUT_DIR = \"sam-3d-track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd81d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_sdk import InferenceHTTPClient\n",
    "\n",
    "client = InferenceHTTPClient(api_url=API_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "def detect_and_track(image: np.ndarray, tracker: sv.ByteTrack) -> sv.Detections:\n",
    "    result = client.infer(image, model_id=SEGMENTATION_MODEL_ID)\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    # remove low-confidence detections\n",
    "    detections = detections[detections.confidence > 0.5]\n",
    "\n",
    "    # update tracker and add tracking labels to detections\n",
    "    tracker.update_with_detections(detections)\n",
    "    # occasionally a -1 ID sneaks through\n",
    "    detections = detections[detections.tracker_id != -1]\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(image: np.ndarray, detections: sv.Detections) -> np.ndarray:\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    trace_annotator = sv.TraceAnnotator()\n",
    "\n",
    "    labels = [\n",
    "        f\"#{tracker_id} ({class_name})\"\n",
    "        for tracker_id, class_name in zip(detections.tracker_id, detections.data[\"class_name\"])\n",
    "    ]\n",
    "    annotated = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
    "    annotated = label_annotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
    "    annotated = trace_annotator.annotate(scene=annotated, detections=detections)\n",
    "\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_objects(image: np.ndarray, detections: sv.Detections) -> sv.Detections:\n",
    "    # flatten polygons to the expected [x1 y1 x2 y2 ... xN yN] format\n",
    "    mask_input = [\n",
    "        np.array(sv.mask_to_polygons(mask)[0]).flatten().tolist()\n",
    "        for mask in detections.mask\n",
    "    ]\n",
    "\n",
    "    sam3_3d_result = client.sam3_3d_infer(\n",
    "        inference_input=image,\n",
    "        mask_input=mask_input,\n",
    "        model_id=SAM3_3D_MODEL_ID,\n",
    "        # 'Fast' SAM-3D config\n",
    "        output_meshes=False,\n",
    "        output_scene=False,\n",
    "        with_mesh_postprocess=False,\n",
    "        with_texture_baking=False,\n",
    "        use_distillations=True,\n",
    "    )\n",
    "\n",
    "    detections.data[\"sam3_3d\"] = sam3_3d_result[\"objects\"]\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64decode\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from pytorch3d.io import IO\n",
    "from pytorch3d.transforms.rotation_conversions import quaternion_to_matrix\n",
    "\n",
    "import rerun as rr\n",
    "\n",
    "def log_to_rerun(annotated: np.ndarray, detections: sv.Detections, tracker: sv.ByteTrack | None, index: int):\n",
    "    rr.set_time(\"tick\", sequence=index)\n",
    "    rr.log(\"/camera/image\", rr.Image(annotated, color_model=\"bgr\"))\n",
    "\n",
    "    # Coordinate transforms used in make_scene_glb\n",
    "    z_to_y_up = torch.tensor([[1, 0, 0], [0, 0, -1], [0, 1, 0]], dtype=torch.float)\n",
    "    y_to_z_up = torch.tensor([[1, 0, 0], [0, 0, 1], [0, -1, 0]], dtype=torch.float)\n",
    "    R_view = torch.tensor([[-1, 0, 0], [0, 0, -1], [0, -1, 0]], dtype=torch.float)\n",
    "\n",
    "    # Clear removed tracks\n",
    "    removed_track_ids = [track.external_track_id for track in tracker.removed_tracks]\n",
    "    for tracker_id in removed_track_ids:\n",
    "        obj_id = f\"#{tracker_id}\"\n",
    "        print(f\"Clear objects/{obj_id}\")\n",
    "        rr.log(f\"objects/{obj_id}\", rr.Clear(recursive=True))\n",
    "\n",
    "    # Add or update active tracks\n",
    "    for i in range(len(detections)):\n",
    "        det = detections[i]\n",
    "        obj_id = f\"#{det.tracker_id}\"\n",
    "        if \"sam3_3d\" not in det.data:\n",
    "            print(f\"No 3D data available for {obj_id}\")\n",
    "            continue\n",
    "        obj_sam3_3d = det.data[\"sam3_3d\"][0]\n",
    "\n",
    "        obj_ply = IO().load_pointcloud(BytesIO(b64decode(obj_sam3_3d[\"gaussian_ply\"])))\n",
    "        obj_pts = obj_ply.points_list()[0]\n",
    "        obj_pts = obj_pts[::100, :]  # Keep 1% of points to speed up rendering\n",
    "        obj_box_size = (obj_pts.amax(dim=0) - obj_pts.amin(dim=0))\n",
    "        obj_rgb = sv.annotators.utils.resolve_color(sv.ColorPalette.DEFAULT, detections, i).as_rgb()\n",
    "\n",
    "        metadata = obj_sam3_3d[\"metadata\"]\n",
    "        t = torch.tensor(metadata[\"translation\"], dtype=torch.float)\n",
    "        R = quaternion_to_matrix(torch.tensor(metadata[\"rotation\"], dtype=torch.float))\n",
    "        s = torch.tensor(metadata[\"scale\"], dtype=torch.float)\n",
    "        # 1. Z-up â†’ Y-up coordinate conversion (row-vector convention throughout SAM3D)\n",
    "        # 2. PyTorch3D quaternion_to_matrix is column-vector (R @ v), but SAM3D uses it\n",
    "        #    row-vector (v @ R), so pass R.T to Rerun's column-vector mat3x3\n",
    "        # 3. R_view: global scene correction from make_scene_glb, applied in world space\n",
    "        t = t @ z_to_y_up @ R_view\n",
    "        R = R_view @ y_to_z_up @ R.T @ z_to_y_up\n",
    "\n",
    "        rr.log(\n",
    "            f\"objects/{obj_id}\",\n",
    "            rr.Boxes3D(sizes=obj_box_size, colors=obj_rgb, labels=obj_id),\n",
    "            rr.Transform3D(translation=t, mat3x3=R, scale=s),\n",
    "        )\n",
    "        rr.log(\n",
    "            f\"objects/{obj_id}/pts\",\n",
    "            rr.Points3D(positions=obj_pts, colors=obj_rgb),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize 3D viewer\n",
    "rr.init(\"sam-3d-track\")\n",
    "rr.save(os.path.join(OUTPUT_DIR, \"rerun_log.rrd\"))\n",
    "rr.log(\"/\", rr.ViewCoordinates.RIGHT_HAND_Y_UP, rr.TransformAxes3D(0.5), static=True)\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = sv.ByteTrack(frame_rate=SAMPLE_FPS, lost_track_buffer=SAMPLE_FPS)\n",
    "\n",
    "# Read and process the video\n",
    "video_info = sv.VideoInfo.from_video_path(INPUT_VIDEO_PATH)\n",
    "stride = int(video_info.fps / SAMPLE_FPS)\n",
    "frames = sv.get_video_frames_generator(INPUT_VIDEO_PATH, stride=stride)\n",
    "video_info.fps = SAMPLE_FPS\n",
    "\n",
    "with sv.VideoSink(os.path.join(OUTPUT_DIR, \"annotated.mp4\"), video_info) as sink:\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for index, frame in enumerate(frames):\n",
    "        if MAX_FRAMES and index > (MAX_FRAMES - 1):\n",
    "            break\n",
    "\n",
    "        frame_start = time.perf_counter()\n",
    "\n",
    "        detections = detect_and_track(frame, tracker)\n",
    "        annotated = annotate(frame, detections)\n",
    "        sink.write_frame(annotated)\n",
    "\n",
    "        detections = get_3d_objects(frame, detections)\n",
    "        log_to_rerun(annotated, detections, tracker, index)\n",
    "\n",
    "        elapsed = time.perf_counter() - frame_start\n",
    "        print(f\"Finished processing frame #{index} in {elapsed:.2f} sec\")\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Finished processing video in {elapsed:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the standalone viewer app\n",
    "# rerun [OUTPUT_DIR]/rerun_log.rrd\n",
    "rr.notebook_show()\n",
    "rr.log_file_from_path(os.path.join(OUTPUT_DIR, \"rerun_log.rrd\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
